title,abstract,link,,authors,date,,,,,,
Volumetric Environment Representation for Vision-Language Navigation,,,,Liu (None) | Wenguan Wang (Zhejiang University) | Yi Yang (Zhejiang University),,,,,,,
SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction,"Creating high-quality 3D models of clothed humans from single images for real-world applications is crucial. Despite recent advancements, accurately reconstructing humans in complex poses or with loose clothing from in-the-wild images, along with predicting textures for unseen areas, remains a significant challenge. A key limitation of previous methods is their insufficient prior guidance in transitioning from 2D to 3D and in texture prediction. In response, we introduce SIFU (Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction), a novel approach combining a Side-view Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU employs a cross-attention mechanism within the transformer, using SMPL-X normals as queries to effectively decouple side-view features in the process of mapping 2D features to 3D. This method not only improves the precision of the 3D models but also their robustness, especially when SMPL-X estimates are not perfect. Our texture refinement process leverages text-to-image diffusion-based prior to generate realistic and consistent textures for invisible views. Through extensive experiments, SIFU surpasses SOTA methods in both geometry and texture reconstruction, showcasing enhanced robustness in complex scenarios and achieving an unprecedented Chamfer and P2S measurement. Our approach extends to practical applications such as 3D printing and scene building, demonstrating its broad utility in real-world scenarios. Project page https://river-zhang.github.io/SIFU-projectpage/ .",http://arxiv.org/abs/2312.06704v2,,Zechuan Zhang (Zhejiang University) | Zongxin Yang (Zhejiang University) | Yi Yang (Zhejiang University),2023-12-10 11:45:45+00:00,,,,,,
Psychometry: An Omnifit Model for Image Reconstruction from Human Brain Activity,,,,Ruijie Quan (Zhejiang University) | Wenguan Wang (Zhejiang University) | Zhibo Tian (Lanzhou University) | Fan Ma (None) | Yi Yang (Zhejiang University),,,,,,,
Clustering for Protein Representation Learning,,,,Ruijie Quan (Zhejiang University) | Wenguan Wang (Zhejiang University) | Fan Ma (None) | Hehe Fan (None) | Yi Yang (Zhejiang University),,,,,,,
Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval,,,,Yucheng Suo (Zhejiang University) | Fan Ma (None) | Linchao Zhu (None) | Yi Yang (Zhejiang University),,,,,,,
S2VNet: Universal Multi-Class Medical Image Segmentation via Clustering-based Slice-to-Volume Propagation,,,,Yuhang Ding (University Of Technology Sydney) | Liulei Li (Zhejiang University) | Wenguan Wang (Zhejiang University) | Yi Yang (Zhejiang University),,,,,,,
LSKNet: Towards High-Performance and Efficient 3D Perception with Large Sparse Kernels,,,,Tuo Feng (University Of Technology Sydney) | Wenguan Wang (Zhejiang University) | Fan Ma (None) | Yi Yang (Zhejiang University),,,,,,,
Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields,"Generalizable NeRF can directly synthesize novel views across new scenes, eliminating the need for scene-specific retraining in vanilla NeRF. A critical enabling factor in these approaches is the extraction of a generalizable 3D representation by aggregating source-view features. In this paper, we propose an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF. Different from existing methods that consider cross-view and along-epipolar information independently, EVE-NeRF conducts the view-epipolar feature aggregation in an entangled manner by injecting the scene-invariant appearance continuity and geometry consistency priors to the aggregation process. Our approach effectively mitigates the potential lack of inherent geometric and appearance constraint resulting from one-dimensional interactions, thus further boosting the 3D representation generalizablity. EVE-NeRF attains state-of-the-art performance across various evaluation scenarios. Extensive experiments demonstate that, compared to prevailing single-dimensional aggregation, the entangled network excels in the accuracy of 3D scene geometry and appearance reconstruction. Our code is publicly available at https://github.com/tatakai1/EVENeRF.",http://arxiv.org/abs/2311.11845v2,,Zhiyuan Min (Zhejiang University) | Yawei Luo (Zhejiang University) | Wei Yang (Huazhong University Of Science And Technology) | Yuesong Wang (None) | Yi Yang (Zhejiang University),2023-11-20 15:35:00+00:00,,,,,,
MuViC: Multi-Instance Visual Controller For Multi-Instance Generation,,,,Dewei Zhou (None) | You Li (Zhejiang University) | Fan Ma (None) | Xiaoting Zhang (Huawei Technologies Ltd.) | Yi Yang (Zhejiang University),,,,,,,
CapHuman: Capture Your Moments in Parallel Universes,"We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, facial expressions, and illuminations in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ""encode then learn to align"" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.",http://arxiv.org/abs/2402.00627v2,,Chao Liang (Zhejiang University) | Fan Ma (None) | Linchao Zhu (None) | Yingying Deng (None) | Yi Yang (Zhejiang University),2024-02-01 14:41:59+00:00,,,,,,
Vista-LLaMA: Reliable Video Teller via Equal Distance to Visual Tokens,"Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as ""hallucination"", as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-LLaMA omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.io/Vista-LLaMA.",http://arxiv.org/abs/2312.08870v1,,Fan Ma (None) | Xiaojie Jin (ByteDance/TikTok) | Heng Wang (Bytedance) | Yuchen Xian (Zhejiang University) | Jiashi Feng (ByteDance) | Yi Yang (Zhejiang University),2023-12-12 09:47:59+00:00,,,,,,
Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers,"This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements.",http://arxiv.org/abs/2403.07214v1,,"Subhadeep Koley (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",2024-03-12 00:02:03+00:00,,,,,,
How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?,,,,"Subhadeep Koley (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",,,,,,,
You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval,"Two primary input modalities prevail in image retrieval: sketch and text. While text is widely used for inter-category retrieval tasks, sketches have been established as the sole preferred modality for fine-grained image retrieval due to their ability to capture intricate visual details. In this paper, we question the reliance on sketches alone for fine-grained image retrieval by simultaneously exploring the fine-grained representation capabilities of both sketch and text, orchestrating a duet between the two. The end result enables precise retrievals previously unattainable, allowing users to pose ever-finer queries and incorporate attributes like colour and contextual cues from text. For this purpose, we introduce a novel compositionality framework, effectively combining sketches and text using pre-trained CLIP models, while eliminating the need for extensive fine-grained textual descriptions. Last but not least, our system extends to novel applications in composite image retrieval, domain attribute transfer, and fine-grained generation, providing solutions for various real-world scenarios.",http://arxiv.org/abs/2403.07222v1,,"Subhadeep Koley (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",2024-03-12 00:27:18+00:00,,,,,,
It's All About Your Sketch: Democratising Sketch Control in Diffusion Models,"This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of ""what you sketch is what you get"". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.",http://arxiv.org/abs/2403.07234v1,,"Subhadeep Koley (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Deeptanshu Sekhri (University Of Surrey) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",2024-03-12 01:05:25+00:00,,,,,,
A Little Trust Goes a Long Way: What Sketch Explainability Really Means for Downstream Tasks,"In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks. The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks.",http://arxiv.org/abs/2403.09480v1,,"Hmrishav Bandyopadhyay (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Aneeshan Sain (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",2024-03-14 15:22:33+00:00,,,,,,
SketchINR: A First Look into Sketches as Implicit Neural Representations,"We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\times$ and $10\times$ data compression over raster and vector sketches, respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render $\sim$$100\times$ faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR's compact high-fidelity representation will support future work in modelling long and complex sketches.",http://arxiv.org/abs/2403.09344v1,,"Hmrishav Bandyopadhyay (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Pinaki Nath Chowdhury (University Of Surrey) | Aneeshan Sain (University Of Surrey) | Tao Xiang (University Of Surrey) | Timothy Hospedales (None) | Yi-Zhe Song (None)",2024-03-14 12:49:29+00:00,,,,,,
Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes,"In this paper, we democratise 3D content creation, enabling precise generation of 3D shapes from abstract sketches while overcoming limitations tied to drawing skills. We introduce a novel part-level modelling and alignment framework that facilitates abstraction modelling and cross-modal correspondence. Leveraging the same part-level decoder, our approach seamlessly extends to sketch modelling by establishing correspondence between CLIPasso edgemaps and projected 3D part regions, eliminating the need for a dataset pairing human sketches and 3D shapes. Additionally, our method introduces a seamless in-position editing process as a byproduct of cross-modal part-aligned modelling. Operating in a low-dimensional implicit space, our approach significantly reduces computational demands and processing time.",http://arxiv.org/abs/2312.04043v1,,"Hmrishav Bandyopadhyay (University Of Surrey) | Subhadeep Koley (University Of Surrey) | Ayan Das (University Of Surrey) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Tao Xiang (University Of Surrey) | Yi-Zhe Song (None)",2023-12-07 05:04:33+00:00,,,,,,
DemoCaricature: Democratising Caricature Generation with a Rough Sketch,"In this paper, we democratise caricature generation, empowering individuals to effortlessly craft personalised caricatures with just a photo and a conceptual sketch. Our objective is to strike a delicate balance between abstraction and identity, while preserving the creativity and subjectivity inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing alongside single-image personalisation, selectively applying nuanced edits to cross-attention layers for a seamless merge of identity and style. Additionally, we propose Random Mask Reconstruction to enhance robustness, directing the model to focus on distinctive identity and style features. Crucially, our aim is not to replace artists but to eliminate accessibility barriers, allowing enthusiasts to engage in the artistry.",http://arxiv.org/abs/2312.04364v1,,"Dar-Yen Chen (None) | Ayan Kumar Bhunia (University Of Surrey, United Kingdom) | Subhadeep Koley (University Of Surrey) | Aneeshan Sain (University Of Surrey) | Pinaki Nath Chowdhury (University Of Surrey) | Yi-Zhe Song (None)",2023-12-07 15:35:42+00:00,,,,,,
Wired Perspectives: Multi-View Wire Art Embraces Generative AI,"Creating multi-view wire art (MVWA), a static 3D sculpture with diverse interpretations from different viewpoints, is a complex task even for skilled artists. In response, we present DreamWire, an AI system enabling everyone to craft MVWA easily. Users express their vision through text prompts or scribbles, freeing them from intricate 3D wire organisation. Our approach synergises 3D B\'ezier curves, Prim's algorithm, and knowledge distillation from diffusion models or their variants (e.g., ControlNet). This blend enables the system to represent 3D wire art, ensuring spatial continuity and overcoming data scarcity. Extensive evaluation and analysis are conducted to shed insight on the inner workings of the proposed system, including the trade-off between connectivity and visual aesthetics.",http://arxiv.org/abs/2311.15421v1,,Zhiyu Qu (University Of Surrey) | LAN YANG (Beijing University Of Posts And Telecommunications) | Honggang Zhang (Beijing University Of Posts And Telecommunications) | Tao Xiang (University Of Surrey) | Kaiyue Pang (SketchX AI) | Yi-Zhe Song (None),2023-11-26 21:09:00+00:00,,,,,,
From SAM to CAMs: Exploring Segment Anything Model for Weakly Supervised Semantic Segmentation,,,,Hyeokjun Kweon (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Quantifying Task Priority for Multi-Task Optimization,,,,Wooseong Jeong (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Weakly Supervised Point Cloud Semantic Segmentation via Artificial Oracle,,,,Hyeokjun Kweon (KAIST) | Jihun Kim (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Frequency-aware Event-based Video Deblurring for Real-World Motion Blur,,,,Taewoo Kim (KAIST) | Hoonhee Cho (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning,,,,Jaewoo Jeong (KAIST) | Daehee Park (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions,,,,Yujeong Chae (KAIST) | Hyeonseong Kim (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
TTA-EVF: Test-Time Adaptation for Event-based Video Frame Interpolation via Reliable Pixel and Sample Estimation,,,,Hoonhee Cho (KAIST) | Taewoo Kim (KAIST) | Yuhwan Jeong (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
Class Tokens Infusion for Weakly Supervised Semantic Segmentation,,,,Sung-Hoon Yoon (KAIST) | Hoyong Kwon (KAIST) | Hyeonseong Kim (KAIST) | Kuk-Jin Yoon (KAIST),,,,,,,
T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory,"Trajectory prediction is a challenging problem that requires considering interactions among multiple actors and the surrounding environment. While data-driven approaches have been used to address this complex problem, they suffer from unreliable predictions under distribution shifts during test time. Accordingly, several online learning methods have been proposed using regression loss from the ground truth of observed data leveraging the auto-labeling nature of trajectory prediction task. We mainly tackle the following two issues. First, previous works underfit and overfit as they only optimize the last layer of the motion decoder. To this end, we employ the masked autoencoder (MAE) for representation learning to encourage complex interaction modeling in shifted test distribution for updating deeper layers. Second, utilizing the sequential nature of driving data, we propose an actor-specific token memory that enables the test-time learning of actor-wise motion characteristics. Our proposed method has been validated across various challenging cross-dataset distribution shift scenarios including nuScenes, Lyft, Waymo, and Interaction. Our method surpasses the performance of existing state-of-the-art online learning methods in terms of both prediction accuracy and computational efficiency. The code is available at https://github.com/daeheepark/T4P.",http://arxiv.org/abs/2403.10052v1,,Daehee Park (KAIST) | Jaeseok Jeong (KAIST) | Sung-Hoon Yoon (KAIST) | Jaewoo Jeong (KAIST) | Kuk-Jin Yoon (KAIST),2024-03-15 06:47:14+00:00,,,,,,
Intrinsic Image Diffusion for Single-view Material Estimation,"We present Intrinsic Image Diffusion, a generative model for appearance decomposition of indoor scenes. Given a single input view, we sample multiple possible material explanations represented as albedo, roughness, and metallic maps. Appearance decomposition poses a considerable challenge in computer vision due to the inherent ambiguity between lighting and material properties and the lack of real datasets. To address this issue, we advocate for a probabilistic formulation, where instead of attempting to directly predict the true material properties, we employ a conditional generative model to sample from the solution space. Furthermore, we show that utilizing the strong learned prior of recent diffusion models trained on large-scale real-world images can be adapted to material estimation and highly improves the generalization to real images. Our method produces significantly sharper, more consistent, and more detailed materials, outperforming state-of-the-art methods by $1.5dB$ on PSNR and by $45\%$ better FID score on albedo prediction. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.",http://arxiv.org/abs/2312.12274v1,,Peter Kocsis (None) | Vincent Sitzmann (Massachusetts Institute Of Technology) | Matthias Nie??ner (Technical University Of Munich),2023-12-19 15:56:19+00:00,,,,,,
DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars,"DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person, offering intuitive control over both pose and expression. We propose a diffusion-based neural renderer that leverages generic 2D priors to produce compelling images of faces. For coarse guidance of the expression and head pose, we render a neural parametric head model (NPHM) from the target viewpoint, which acts as a proxy geometry of the person. Additionally, to enhance the modeling of intricate facial expressions, we condition DiffusionAvatars directly on the expression codes obtained from NPHM via cross-attention. Finally, to synthesize consistent surface details across different viewpoints and expressions, we rig learnable spatial features to the head's surface via TriPlane lookup in NPHM's canonical space. We train DiffusionAvatars on RGB videos and corresponding tracked NPHM meshes of a person and test the obtained avatars in both self-reenactment and animation scenarios. Our experiments demonstrate that DiffusionAvatars generates temporally consistent and visually appealing videos for novel poses and expressions of a person, outperforming existing approaches.",http://arxiv.org/abs/2311.18635v1,,"Tobias Kirschstein (Department Of Informatics, Technische Universit??t M??nchen) | Simon Giebenhain (Technische Universit??t M??nchen) | Matthias Nie??ner (Technical University Of Munich)",2023-11-30 15:43:13+00:00,,,,,,
DPHMs: Diffusion Parametric Head Models for Depth-based Tracking,"We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences. While recent volumetric head models, such as NPHMs, can now excel in representing high-fidelity head geometries, tracking and reconstruction heads from real-world single-view depth sequences remains very challenging, as the fitting to partial and noisy observations is underconstrained. To tackle these challenges, we propose a latent diffusion-based prior to regularize volumetric head reconstruction and tracking. This prior-based regularizer effectively constrains the identity and expression codes to lie on the underlying latent manifold which represents plausible head shapes. To evaluate the effectiveness of the diffusion-based prior, we collect a dataset of monocular Kinect sequences consisting of various complex facial expression motions and rapid transitions. We compare our method to state-of-the-art tracking methods, and demonstrate improved head identity reconstruction as well as robust expression tracking.",http://arxiv.org/abs/2312.01068v1,,Jiapeng Tang (Technische Universit??t M??nchen) | Angela Dai (None) | Yinyu Nie (Huawei Technologies Ltd.) | Lev Markhasin (None) | Justus Thies (Max-Planck Institute For Intelligent Systems) | Matthias Nie??ner (Technical University Of Munich),2023-12-02 08:34:22+00:00,,,,,,
DiffuScene: Denoising Diffusion Model for Generative Indoor Scene Synthesis,"We present DiffuScene for indoor 3D scene synthesis based on a novel scene configuration denoising diffusion model. It generates 3D instance properties stored in an unordered object set and retrieves the most similar geometry for each object configuration, which is characterized as a concatenation of different attributes, including location, size, orientation, semantics, and geometry features. We introduce a diffusion network to synthesize a collection of 3D indoor objects by denoising a set of unordered object attributes. Unordered parametrization simplifies and eases the joint distribution approximation. The shape feature diffusion facilitates natural object placements, including symmetries. Our method enables many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.",http://arxiv.org/abs/2303.14207v2,,Jiapeng Tang (Technische Universit??t M??nchen) | Yinyu Nie (Huawei Technologies Ltd.) | Lev Markhasin (None) | Angela Dai (None) | Justus Thies (Max-Planck Institute For Intelligent Systems) | Matthias Nie??ner (Technical University Of Munich),2023-03-24 18:00:15+00:00,,,,,,
FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models,"We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation.",http://arxiv.org/abs/2312.08459v1,,Shivangi Aneja (Technical University Of Munich) | Justus Thies (Max-Planck Institute For Intelligent Systems) | Angela Dai (None) | Matthias Nie??ner (Technical University Of Munich),2023-12-13 19:01:07+00:00,,,,,,
SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors,"We propose SceneTex, a novel method for effectively generating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features without accurate geometric and style cues, SceneTex formulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are properly reflected. At its core, SceneTex proposes a multiresolution texture field to implicitly encode the mesh appearance. We optimize the target texture via a score-distillation-based objective function in respective RGB renderings. To further secure the style consistency across views, we introduce a cross-attention decoder to predict the RGB values by cross-attending to the pre-sampled reference locations in each instance. SceneTex enables various and accurate texture synthesis for 3D-FRONT scenes, demonstrating significant improvements in visual quality and prompt fidelity over the prior texture generation methods.",http://arxiv.org/abs/2311.17261v1,,Dave Chen (Technische Universit??t M??nchen) | Haoxuan Li (Technische Universit??t M??nchen) | Hsin-Ying Lee (Snap) | Sergey Tulyakov (Snap) | Matthias Nie??ner (Technical University Of Munich),2023-11-28 22:49:57+00:00,,,,,,
GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians,"We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.",http://arxiv.org/abs/2312.02069v1,,"Shenhan Qian (Technische Universit??t M??nchen) | Tobias Kirschstein (Department Of Informatics, Technische Universit??t M??nchen) | Liam Schoneveld (Woven By Toyota) | Davide Davoli (Toyota Motor Europe NV/SA Associated Partner By Contracted Services) | Simon Giebenhain (Technische Universit??t M??nchen) | Matthias Nie??ner (Technical University Of Munich)",2023-12-04 17:28:35+00:00,,,,,,
MonoNPHM: Dynamic Head Reconstruction from Monocular Videos,"We present Monocular Neural Parametric Head Models (MonoNPHM) for dynamic 3D head reconstructions from monocular RGB videos. To this end, we propose a latent appearance space that parameterizes a texture field on top of a neural parametric model. We constrain predicted color values to be correlated with the underlying geometry such that gradients from RGB effectively influence latent geometry codes during inverse rendering. To increase the representational capacity of our expression space, we augment our backward deformation field with hyper-dimensions, thus improving color and geometry representation in topologically challenging expressions. Using MonoNPHM as a learned prior, we approach the task of 3D head reconstruction using signed distance field based volumetric rendering. By numerically inverting our backward deformation field, we incorporated a landmark loss using facial anchor points that are closely tied to our canonical geometry representation. To evaluate the task of dynamic face reconstruction from monocular RGB videos we record 20 challenging Kinect sequences under casual conditions. MonoNPHM outperforms all baselines with a significant margin, and makes an important step towards easily accessible neural parametric face models through RGB tracking.",http://arxiv.org/abs/2312.06740v1,,"Simon Giebenhain (Technische Universit??t M??nchen) | Tobias Kirschstein (Department Of Informatics, Technische Universit??t M??nchen) | Markos Georgopoulos (Synthesia) | Martin R??nz (Synthesia) | Lourdes Agapito (University College London) | Matthias Nie??ner (Technical University Of Munich)",2023-12-11 17:55:05+00:00,,,,,,
ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models,"3D asset generation is getting massive amounts of attention, inspired by the recent success of text-guided 2D content creation. Existing text-to-3D methods use pretrained text-to-image diffusion models in an optimization problem or fine-tune them on synthetic data, which often results in non-photorealistic 3D objects without backgrounds. In this paper, we present a method that leverages pretrained text-to-image models as a prior, and learn to generate multi-view images in a single denoising process from real-world data. Concretely, we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the text-to-image model. Moreover, we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods, the results generated by our method are consistent, and have favorable visual quality (-30% FID, -37% KID).",http://arxiv.org/abs/2403.01807v1,,Lukas Hoellein (None) | Alja?? Bo??i?? (Facebook) | Norman M??ller (Meta) | David Novotny (Facebook) | Hung-Yu Tseng (Meta) | Christian Richardt (Meta Reality Labs) | Michael Zollhoefer (Meta) | Matthias Nie??ner (Technical University Of Munich),2024-03-04 07:57:05+00:00,,,,,,
MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers,"We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.",http://arxiv.org/abs/2311.15475v1,,Yawar Siddiqui (Technical University Munich) | Antonio Alliegro (Politecnico Di Torino) | Alexey Artemov (Technische Universit??t M??nchen) | Tatiana Tommasi (Politecnico Di Torino) | Daniele Sirigatti (Audi AG) | Vladislav Rosov (AUDI AG) | Angela Dai (None) | Matthias Nie??ner (Technical University Of Munich),2023-11-27 01:20:11+00:00,,,,,,
Ungeneralizable Examples,,,,Jingwen Ye (National University Of Singapore) | Xinchao Wang (National University Of Singapore),,,,,,,
Neural Lineage,,,,Runpeng Yu (None) | Xinchao Wang (National University Of Singapore),,,,,,,
Distilled Datamodel with Reverse Gradient Matching,,,,"Jingwen Ye (National University Of Singapore) | Ruonan Yu (National University Of Singaore, National University Of Singapore) | Songhua Liu (None) | Xinchao Wang (National University Of Singapore)",,,,,,,
Relation Rectification in Diffusion Model,,,,Yinwei Wu (National University Of Singapore) | Xingyi Yang (National University Of Singapore) | Xinchao Wang (National University Of Singapore),,,,,,,
Unsegment Anything by Simulating Deformation,,,,Jiahao Lu (National University Of Singapore) | Xingyi Yang (National University Of Singapore) | Xinchao Wang (National University Of Singapore),,,,,,,
DeepCache: Accelerating Diffusion Models for Free,"Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their remarkable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the perspective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$ for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, DeepCache effectively achieves comparable or even marginally improved results with DDIM or PLMS. The code is available at https://github.com/horseee/DeepCache",http://arxiv.org/abs/2312.00858v2,,Xinyin Ma (National University Of Singapore) | Gongfan Fang (None) | Xinchao Wang (National University Of Singapore),2023-12-01 17:01:06+00:00,,,,,,
MindBridge: A Cross-Subject Brain Decoding Framework,,,,Shizun Wang (National University Of Singapore) | Songhua Liu (None) | Zhenxiong Tan (National University Of Singapore) | Xinchao Wang (National University Of Singapore),,,,,,,
InceptionNeXt: When Inception Meets ConvNeXt,"Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an identity mapping. With this new Inception depthwise convolution, we build a series of networks, namely IncepitonNeXt, which not only enjoy high throughputs but also maintain competitive performance. For instance, InceptionNeXt-T achieves 1.6x higher training throughputs than ConvNeX-T, as well as attains 0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt can serve as an economical baseline for future architecture design to reduce carbon footprint. Code is available at https://github.com/sail-sg/inceptionnext.",http://arxiv.org/abs/2303.16900v1,,"Weihao Yu (None) | Pan Zhou (Sea Group) | Shuicheng Yan (National University Of Singapore, Department Of Electrical And Computer Engineering) | Xinchao Wang (National University Of Singapore)",2023-03-29 17:59:58+00:00,,,,,,
EventDance: Unsupervised Cross-modal Source-free Adaptation for Event-based Object Recognition,,,,Xu Zheng (HKUST) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
How the Hell does NeRF Matter for Real-Time RGB-D SLAM: A Novel Benchmark for Scene Representation and Geometric Rendering,,,,Tongyan Hua (HKUST(GZ)) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion,,,,Hao Ai (The Hong Kong University Of Science And Technology (Guangzhou Campus)) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
"Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation",,,,Xu Zheng (HKUST) | Pengyuan Zhou (Aarhus University) | ATHANASIOS (ICT) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All,,,,Yuanhuiyi Lyu (Hong Kong University Of Science And Technology) | Xu Zheng (HKUST) | Jiazhou Zhou (Hong Kong University Of Science And Technology) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More,,,,Jiazhou Zhou (Hong Kong University Of Science And Technology) | Xu Zheng (HKUST) | Yuanhuiyi Lyu (Hong Kong University Of Science And Technology) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation,,,,WEIMING ZHANG (None) | Yexin Liu (The Hong Kong University Of Science And Technology) | Xu Zheng (HKUST) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach,,,,Guoqiang Liang (Hong Kong University Of Science And Technology) | Kanghao Chen (Hong Kong University Of Science And Technology) | Hangyu Li (Hong Kong University Of Science And Technology) | Yunfan Lu (Hong Kong University Of Science And Technology(GuangZhou)) | Lin Wang (Hong Kong University Of Science And Technology),,,,,,,
SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering,,,,Tao Hu (Nanyang Technological University) | Fangzhou Hong (Nanyang Technological University) | Ziwei Liu (Nanyang Technological University),,,,,,,
GauHuman: Articulated Gaussian Splatting for Real-Time 3D Human Rendering,,,,Shoukang Hu (Nanyang Technological University) | Tao Hu (Nanyang Technological University) | Ziwei Liu (Nanyang Technological University),,,,,,,
Link-Context Learning for Multimodal LLMs,,,,"Yan Tai (Ningbo Institute Of Digital Twin, Eastern Institute Of Technology, Ningbo, China) | Weichen Fan (Sensetime Research) | Zhao Zhang (Sensetime Research) | Ziwei Liu (Nanyang Technological University)",,,,,,,
FreeU: Free Lunch in Diffusion U-Net,"In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a ""free lunch"" that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method-termed ""FreeU"" - that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net's skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/FreeU/.",http://arxiv.org/abs/2309.11497v2,,Chenyang Si (Nanyang Technological University Singapore) | Ziqi Huang (Nanyang Technological University) | Yuming Jiang (Nanyang Technological University) | Ziwei Liu (Nanyang Technological University),2023-09-20 17:56:18+00:00,,,,,,
CityDreamer: Compositional Generative Model of Unbounded 3D Cities,"In recent years, extensive research has focused on 3D natural scene generation, but the domain of 3D city generation has not received as much exploration. This is due to the greater challenges posed by 3D city generation, mainly because humans are more sensitive to structural distortions in urban environments. Additionally, generating 3D cities is more complex than 3D natural scenes since buildings, as objects of the same class, exhibit a wider range of appearances compared to the relatively consistent appearance of objects like trees in natural scenes. To address these challenges, we propose CityDreamer, a compositional generative model designed specifically for unbounded 3D cities, which separates the generation of building instances from other background objects, such as roads, green lands, and water areas, into distinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth, containing a vast amount of real-world city imagery to enhance the realism of the generated 3D cities both in their layouts and appearances. Through extensive experiments, CityDreamer has proven its superiority over state-of-the-art methods in generating a wide range of lifelike 3D cities.",http://arxiv.org/abs/2309.00610v1,,Haozhe Xie (Nanyang Technological University) | Zhaoxi Chen (Nanyang Technological University) | Fangzhou Hong (Nanyang Technological University) | Ziwei Liu (Nanyang Technological University),2023-09-01 17:57:02+00:00,,,,,,
VideoBooth: Diffusion-based Video Generation with Image Prompts,"Text-driven video generation witnesses rapid progress. However, merely using text prompts is not enough to depict the desired subject appearance that accurately aligns with users' intents, especially for customized content creation. In this paper, we study the task of video generation with image prompts, which provide more accurate and direct content control beyond the text prompts. Specifically, we propose a feed-forward framework VideoBooth, with two dedicated designs: 1) We propose to embed image prompts in a coarse-to-fine manner. Coarse visual embeddings from image encoder provide high-level encodings of image prompts, while fine visual embeddings from the proposed attention injection module provide multi-scale and detailed encoding of image prompts. These two complementary embeddings can faithfully capture the desired appearance. 2) In the attention injection module at fine level, multi-scale image prompts are fed into different cross-frame attention layers as additional keys and values. This extra spatial information refines the details in the first frame and then it is propagated to the remaining frames, which maintains temporal consistency. Extensive experiments demonstrate that VideoBooth achieves state-of-the-art performance in generating customized high-quality videos with subjects specified in image prompts. Notably, VideoBooth is a generalizable framework where a single model works for a wide range of image prompts with feed-forward pass.",http://arxiv.org/abs/2312.00777v1,,Yuming Jiang (Nanyang Technological University) | Tianxing Wu (Nanyang Technological University) | Shuai Yang (Nanyang Technological University) | Chenyang Si (Nanyang Technological University Singapore) | Dahua Lin (The Chinese University Of Hong Kong) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) | Ziwei Liu (Nanyang Technological University),2023-12-01 18:55:40+00:00,,,,,,
HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting,"Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian",http://arxiv.org/abs/2311.17061v2,,Xian Liu (The Chinese University Of Hong Kong) | Xiaohang Zhan (Tencent) | Jiaxiang Tang (Baidu) | Ying Shan (Tencent) | Gang Zeng (Peking University) | Dahua Lin (The Chinese University Of Hong Kong) | Xihui Liu (The University Of Hong Kong) | Ziwei Liu (Nanyang Technological University),2023-11-28 18:59:58+00:00,,,,,,
VBench: Comprehensive Benchmark Suite for Video Generative Models,"Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects ""video generation quality"" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has three appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. We will open-source VBench, including all prompts, evaluation methods, generated videos, and human preference annotations, and also include more video generation models in VBench to drive forward the field of video generation.",http://arxiv.org/abs/2311.17982v1,,Ziqi Huang (Nanyang Technological University) | Yinan He (Shanghai AI Laboratory) | Jiashuo Yu (Shanghai AI Laboratory) | Fan Zhang (None) | Chenyang Si (Nanyang Technological University Singapore) | Yuming Jiang (Nanyang Technological University) | Yuanhan Zhang (Nanyang Technological University) | Tianxing Wu (Nanyang Technological University) | Jin Qingyang (Nanyang Technological University) | Nattapol Chanpaisit (Nanyang Technological University) | Yaohui Wang (Shanghai AI Laboratory) | Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) | Limin Wang (Nanjing University) | Dahua Lin (The Chinese University Of Hong Kong) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Ziwei Liu (Nanyang Technological University),2023-11-29 18:39:01+00:00,,,,,,
Digital Life Project: Autonomous 3D Characters with Social Intelligence,"In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character's digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant bodily movements. Additionally, a motion captioning module further allows the virtual character to recognize and appropriately respond to human players' actions. Homepage: https://digital-life-project.com/",http://arxiv.org/abs/2312.04547v1,,Zhongang Cai (Nanyang Technological University) | Jianping Jiang (Peking University) | Zhongfei Qing (SenseTime Research) | Xinying Guo (Nanyang Technological University) | Mingyuan Zhang (Nanyang Technological University) | Zhengyu Lin (Sensetime) | Haiy Mei (None) | Chen Wei (SenseTime International PTE. LTD.) | Wang Ruisi (Nanyang Technological University) | Wanqi Yin (SenseTime Research ) | Liang Pan (Shanghai AI Lab) | Xiangyu Fan (Chinese University Of Hong Kong) | Han Du (Universit??t Des Saarlandes) | Peng Gao (SenseTime LTD.) | Zhitao Yang (SenseTime Co Ltd.) | Yang Gao (SenseTime) | Jiaqi Li (SenseTime) | Tianxiang Ren (Xiamen University) | YuKun Wei (Sensetime Research) | Xiaogang Wang (The Chinese University Of Hong Kong) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) | Lei Yang (The Chinese University Of Hong Kong) | Ziwei Liu (Nanyang Technological University),2023-12-07 18:58:59+00:00,,,,,,
Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis,"Vector-Quantized (VQ-based) generative models usually consist of two basic components, i.e., VQ tokenizers and generative transformers. Prior research focuses on improving the reconstruction fidelity of VQ tokenizers but rarely examines how the improvement in reconstruction affects the generation ability of generative transformers. In this paper, we surprisingly find that improving the reconstruction fidelity of VQ tokenizers does not necessarily improve the generation. Instead, learning to compress semantic features within VQ tokenizers significantly improves generative transformers' ability to capture textures and structures. We thus highlight two competing objectives of VQ tokenizers for image synthesis: semantic compression and details preservation. Different from previous work that only pursues better details preservation, we propose Semantic-Quantized GAN (SeQ-GAN) with two learning phases to balance the two objectives. In the first phase, we propose a semantic-enhanced perceptual loss for better semantic compression. In the second phase, we fix the encoder and codebook, but enhance and finetune the decoder to achieve better details preservation. The proposed SeQ-GAN greatly improves VQ-based generative models and surpasses the GAN and Diffusion Models on both unconditional and conditional image generation. Our SeQ-GAN (364M) achieves Frechet Inception Distance (FID) of 6.25 and Inception Score (IS) of 140.9 on 256x256 ImageNet generation, a remarkable improvement over VIT-VQGAN (714M), which obtains 11.2 FID and 97.2 IS.",http://arxiv.org/abs/2212.03185v2,,Yuchao Gu (None) | Xintao Wang (Tencent) | Yixiao Ge (Tencent) | Ying Shan (Tencent) | Mike Zheng Shou (National University Of Singapore),2022-12-06 17:58:38+00:00,,,,,,
Bootstrapping SparseFormers from Vision Foundation Models,"The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code will be publicly available at https://github.com/showlab/sparseformer",http://arxiv.org/abs/2312.01987v1,,"Ziteng Gao (National University Of Singapore) | Zhan Tong (Ant Group) | Kevin Qinghong Lin (National University Of Singaore, National University Of Singapore) | Joya Chen (National University Of Singapore) | Mike Zheng Shou (National University Of Singapore)",2023-12-04 16:04:41+00:00,,,,,,
L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream,,,,Jingtao Sun (National University Of Singapore) | Yaonan Wang (Hunan University) | Mingtao Feng (Xidian University) | Yulan Guo (SUN YAT-SEN UNIVERSITY) | Ajmal Mian (University Of Western Australia) | Mike Zheng Shou (National University Of Singapore),,,,,,,
MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,"This paper studies the human image animation task, which aims to generate a video of a certain reference identity following a particular motion sequence. Existing animation works typically employ the frame-warping technique to animate the reference image towards the target motion. Despite achieving reasonable results, these approaches face challenges in maintaining temporal consistency throughout the animation due to the lack of temporal modeling and poor preservation of reference identity. In this work, we introduce MagicAnimate, a diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity. To achieve this, we first develop a video diffusion model to encode temporal information. Second, to maintain the appearance coherence across frames, we introduce a novel appearance encoder to retain the intricate details of the reference image. Leveraging these two innovations, we further employ a simple video fusion technique to encourage smooth transitions for long video animation. Empirical results demonstrate the superiority of our method over baseline approaches on two benchmarks. Notably, our approach outperforms the strongest baseline by over 38% in terms of video fidelity on the challenging TikTok dancing dataset. Code and model will be made available.",http://arxiv.org/abs/2311.16498v1,,"Zhongcong Xu (National University Of Singaore, National University Of Singapore) | Jianfeng Zhang (NUS) | Jun Hao Liew (ByteDance) | Hanshu Yan (ByteDance) | Jia-Wei Liu (National University Of Singapore) | Chenxu Zhang (Bytedance) | Jiashi Feng (ByteDance) | Mike Zheng Shou (National University Of Singapore)",2023-11-27 18:32:31+00:00,,,,,,
Adding Universal Compatibility of Plugins for Upgraded Diffusion Model,"We introduce X-Adapter, a universal upgrader to enable the pretrained plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the upgraded text-to-image diffusion model (e.g., SDXL) without further retraining. We achieve this goal by training an additional network to control the frozen upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a frozen copy of the old model to preserve the connectors of different plugins. Additionally, X-Adapter adds trainable mapping layers that bridge the decoders from models of different versions for feature remapping. The remapped features will be used as guidance for the upgraded model. To enhance the guidance ability of X-Adapter, we employ a null-text training strategy for the upgraded model. After training, we also introduce a two-stage denoising strategy to align the initial latents of X-Adapter and the upgraded model. Thanks to our strategies, X-Adapter demonstrates universal compatibility with various plugins and also enables plugins of different versions to work together, thereby expanding the functionalities of diffusion community. To verify the effectiveness of the proposed method, we conduct extensive experiments and the results show that X-Adapter may facilitate wider application in the upgraded foundational diffusion model.",http://arxiv.org/abs/2312.02238v2,,Lingmin Ran (National University Of Singapore) | Xiaodong Cun (Tencent AI Lab) | Jia-Wei Liu (National University Of Singapore) | Rui Zhao (None) | Song Zijie (Fudan University) | Xintao Wang (Tencent) | Jussi Keppo (National University Of Singapore) | Mike Zheng Shou (National University Of Singapore),2023-12-04 09:19:38+00:00,,,,,,
DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing,"Despite recent progress in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Prior attempts to address this challenge by introducing video-2D representations encounter significant difficulties with large-scale motion- and view-change videos, especially in human-centric scenarios. To overcome this, we propose to introduce the dynamic Neural Radiance Fields (NeRF) as the innovative video representation, where the editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide consistent and controllable editing, we propose the image-based video-NeRF editing pipeline with a set of innovative designs, including multi-view multi-pose Score Distillation Sampling (SDS) from both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction losses, text-guided local parts super-resolution, and style transfer. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% for human preference. Code will be released at https://showlab.github.io/DynVideo-E/.",http://arxiv.org/abs/2310.10624v2,,Jia-Wei Liu (National University Of Singapore) | Yan-Pei Cao (Tencent ARC Lab) | Jay Zhangjie Wu (National University Of Singapore) | Weijia Mao (NUS) | Yuchao Gu (None) | Rui Zhao (None) | Jussi Keppo (National University Of Singapore) | Ying Shan (Tencent) | Mike Zheng Shou (National University Of Singapore),2023-10-16 17:48:10+00:00,,,,,,
ViT-Lens: Towards Omni-modal Representations,"Though the success of CLIP-based training recipes in vision-language models, their scalability to more modalities (e.g., 3D, audio, etc.) is limited to large-scale data, which is expensive or even inapplicable for rare modalities. In this paper, we present ViT-Lens that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning to a pre-defined space. Specifically, the modality-specific lens is tuned to project multimodal signals to the shared embedding space, which are then processed by a strong ViT that carries pre-trained image knowledge. The encoded multimodal representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. A well-trained lens with a ViT backbone has the potential to serve as one of these foundation models, supervising the learning of subsequent modalities. ViT-Lens provides a unified solution for representation learning of increasing modalities with two appealing benefits: (i) Exploiting the pretrained ViT across tasks and domains effectively with efficient data regime; (ii) Emergent downstream capabilities of novel modalities are demonstrated due to the modality alignment space. We evaluate ViT-Lens in the context of 3D as an initial verification. In zero-shot 3D classification, ViT-Lens achieves substantial improvements over previous state-of-the-art, showing 52.0% accuracy on Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore, we enable zero-shot 3D question-answering by simply integrating the trained 3D lens into the InstructBLIP model without any adaptation. We will release the results of ViT-Lens on more modalities in the near future.",http://arxiv.org/abs/2308.10185v1,,Stan Weixian Lei (National University Of Singapore) | Yixiao Ge (Tencent) | Kun Yi (Tencent ARC Lab) | Jianfeng Zhang (NUS) | Difei Gao (None) | Dylan Sun (University Of Southern California) | Yuying Ge (University Of Hong Kong) | Ying Shan (Tencent) | Mike Zheng Shou (National University Of Singapore),2023-08-20 07:26:51+00:00,,,,,,
LIVE: Online Large Video-Language Model for Streaming Video,,,,"Joya Chen (National University Of Singapore) | Zhaoyang Lv (None) | Shiwei Wu (University Of Science And Technology Of China) | Kevin Qinghong Lin (National University Of Singaore, National University Of Singapore) | Chenan Song (National University Of Singaore, National University Of Singapore) | Difei Gao (None) | Jia-Wei Liu (National University Of Singapore) | Ziteng Gao (National University Of Singapore) | Dongxing Mao (SUTD) | Mike Zheng Shou (National University Of Singapore)",,,,,,,
Tune-An-Ellipse: CLIP Has Potential to Find What You Want,,,,Jinheng Xie (None) | Songhe Deng (None) | Bing Li (King Abdullah University Of Science And Technology) | Haozhe Liu (King Abdullah University Of Science And Technology) | Yawen Huang (None) | Yefeng Zheng (None) | J??rgen Schmidhuber (King Abdullah University Of Science And Technology) | Bernard Ghanem (KAUST) | Linlin Shen (None) | Mike Zheng Shou (National University Of Singapore),,,,,,,
AssistGUI: Task-Oriented Desktop Graphical User Interface Automation,"Graphical User Interface (GUI) automation holds significant promise for assisting users with complex tasks, thereby boosting human productivity. Existing works leveraging Large Language Model (LLM) or LLM-based AI agents have shown capabilities in automating tasks on Android and Web platforms. However, these tasks are primarily aimed at simple device usage and entertainment operations. This paper presents a novel benchmark, AssistGUI, to evaluate whether models are capable of manipulating the mouse and keyboard on the Windows platform in response to user-requested tasks. We carefully collected a set of 100 tasks from nine widely-used software applications, such as, After Effects and MS Word, each accompanied by the necessary project files for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied Agent framework, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks. Our experimental results reveal that our GUI Parser and Reasoning mechanism outshine existing methods in performance. Nevertheless, the potential remains substantial, with the best model attaining only a 46% success rate on our benchmark. We conclude with a thorough analysis of the current methods' limitations, setting the stage for future breakthroughs in this domain.",http://arxiv.org/abs/2312.13108v2,,"Difei Gao (None) | Lei Ji (Research, Microsoft) | Zechen Bai (Show Lab, National University Of Singapore) | Mingyu Ouyang (National University Of Singaore) | Peiran Li (National University Of Singaore, National University Of Singapore) | Dongxing Mao (SUTD) | Qin WU (National University Of Singapore) | Weichen Zhang (National University Of Singapore) | Peiyi Wang (National University Of Singaore, National University Of Singapore) | Xiangwu Guo (South China University Of Technology) | Hengxu Wang (National University Of Singaore, National University Of Singapore) | Luowei Zhou (Google) | Mike Zheng Shou (National University Of Singapore)",2023-12-20 15:28:38+00:00,,,,,,
VMINer: Versatile Multi-view Inverse Rendering with Near- and Far-field Light Sources,,,,Fan Fei (Peking University) | Jiajun Tang (Peking University) | Ping Tan (Hong Kong University Of Science And Technology) | Boxin Shi (Peking University),,,,,,,
Language-guided Image Reflection Separation,,,,Haofeng Zhong (Peking University) | Yuchen Hong (Peking University) | Shuchen Weng (Peking University) | Jinxiu Liang (None) | Boxin Shi (Peking University),,,,,,,
Neural Underwater Scene Representation,,,,Yunkai Tang (Peking University) | Chengxuan Zhu (Peking University) | Renjie Wan (None) | Chao Xu (Peking University) | Boxin Shi (Peking University),,,,,,,
NB-GTR: Narrow-Band Guided Turbulence Removal,,,,Yifei Xia (Peking University) | Chu Zhou (Peking University) | Chengxuan Zhu (Peking University) | Minggui Teng (Peking University) | Chao Xu (Peking University) | Boxin Shi (Peking University),,,,,,,
EventPS: Real-Time Photometric Stereo Using an Event Camera,,,,Bohan Yu (None) | Jieji Ren (Shanghai Jiao Tong University) | Jin Han (None) | Feishi Wang (Peking University) | Jinxiu Liang (None) | Boxin Shi (Peking University),,,,,,,
Latency Correction for Event-guided Deblurring and Frame Interpolation,,,,Yixin Yang (Peking University) | Jinxiu Liang (None) | Bohan Yu (None) | Yan Chen (Tsinghua University) | Jimmy S. Ren (SenseTime Research) | Boxin Shi (Peking University),,,,,,,
Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction,"Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties, but it lacks key texture appearance for hand mesh reconstruction. In this paper, we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time, space, and information dimensions,EvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader, which allows our model to generalize effectively in challenging scenes, even when trained solely on standard scenes, thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both, and shows the potential of generalization to outdoor scenes and another type of event camera.",http://arxiv.org/abs/2403.07346v1,,Jianping Jiang (Peking University) | Xinyu Zhou (Peking University) | Bingxuan Wang (Peking University) | Xiaoming Deng (Institute Of Software Chinese Academy Of Sciences) | Chao Xu (Peking University) | Boxin Shi (Peking University),2024-03-12 06:04:50+00:00,,,,,,
EvDiG: Event-guided Direct and Global Components Separation,,,,Xinyu Zhou (Peking University) | Peiqi Duan (None) | Boyu Li (Peking University) | Chu Zhou (Peking University) | Chao Xu (Peking University) | Boxin Shi (Peking University),,,,,,,
HDR and HFR Video from Rolling-Mixed-Bit Spikings,,,,"Yakun Chang (None) | Yeliduosi Xiaokaiti (Peking University) | Yujia Liu (School Of Computer Science, Peking University, Beijing, China) | Bin Fan (None) | Zhaojun Huang (Peking University) | Tiejun Huang (Peking University) | Boxin Shi (Peking University)",,,,,,,
VidToMe: Video Token Merging for Zero-Shot Video Editing,"Diffusion models have made significant advances in generating high-quality images, but their application to video generation has remained challenging due to the complexity of temporal motion. Zero-shot video editing offers a solution by utilizing pre-trained image diffusion models to translate source videos into new ones. Nevertheless, existing methods struggle to maintain strict temporal consistency and efficient memory consumption. In this work, we propose a novel approach to enhance temporal consistency in generated videos by merging self-attention tokens across frames. By aligning and compressing temporally redundant tokens across frames, our method improves temporal coherence and reduces memory consumption in self-attention computations. The merging strategy matches and aligns tokens according to the temporal correspondence between frames, facilitating natural temporal consistency in generated video frames. To manage the complexity of video processing, we divide videos into chunks and develop intra-chunk local token merging and inter-chunk global token merging, ensuring both short-term video continuity and long-term content consistency. Our video editing approach seamlessly extends the advancements in image editing to video editing, rendering favorable results in temporal consistency over state-of-the-art methods.",http://arxiv.org/abs/2312.10656v2,,"Xirui Li (Shanghai Jiao Tong University) | Chao Ma (Shanghai Jiao Tong University) | Xiaokang Yang (Shanghai Jiao Tong University, China) | Ming-Hsuan Yang (University Of California At Merced)",2023-12-17 09:05:56+00:00,,,,,,
Exploiting Diffusion Prior for Generalizable Pixel-Level Semantic Prediction,"Contents generated by recent advanced Text-to-Image (T2I) diffusion models are sometimes too imaginative for existing off-the-shelf property semantic predictors to estimate due to the immitigable domain gap. We introduce DMP, a pipeline utilizing pre-trained T2I models as a prior for pixel-level semantic prediction tasks. To address the misalignment between deterministic prediction tasks and stochastic T2I models, we reformulate the diffusion process through a sequence of interpolations, establishing a deterministic mapping between input RGB images and output prediction distributions. To preserve generalizability, we use low-rank adaptation to fine-tune pre-trained models. Extensive experiments across five tasks, including 3D property estimation, semantic segmentation, and intrinsic image decomposition, showcase the efficacy of the proposed method. Despite limited-domain training data, the approach yields faithful estimations for arbitrary images, surpassing existing state-of-the-art algorithms.",http://arxiv.org/abs/2311.18832v1,,"Hsin-Ying Lee (University Of California, Merced) | Hung-Yu Tseng (Meta) | Hsin-Ying Lee (Snap) | Ming-Hsuan Yang (University Of California At Merced)",2023-11-30 18:59:44+00:00,,,,,,
Text-Driven Image Editing via Learnable Regions,"Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region-based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pretrained text-to-image model and introduces a bounding box generator to find the edit regions that are aligned with the textual prompts. We show that this simple approach enables flexible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences or long paragraphs. We conduct an extensive user study to compare our method against state-of-the-art methods. Experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that align with the language descriptions provided. Our project webpage: https://yuanze-lin.me/LearnableRegions_page.",http://arxiv.org/abs/2311.16432v1,,"Yuanze Lin (University Of Oxford) | Yi-Wen Chen (University Of California, Merced) | Yi-Hsuan Tsai (Google) | Lu Jiang (Carnegie Mellon University) | Ming-Hsuan Yang (University Of California At Merced)",2023-11-28 02:27:31+00:00,,,,,,
RTracker: Recoverable Tracking via PN Tree Structured Memory,,,,"Yuqing Huang (Harbin Institute Of Technology) | Xin Li (Peng Cheng Laboratory) | Zikun Zhou (Peng Cheng Laboratory) | Yaowei Wang (Pengcheng Laboratory) | Zhenyu He (Harbin Institute Of Technology, Shenzhen) | Ming-Hsuan Yang (University Of California At Merced)",,,,,,,
DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes,"We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes. For complex scenes with moving objects, we first sequentially and progressively model the static background of the entire scene with incremental static 3D Gaussians. We then leverage a composite dynamic Gaussian graph to handle multiple moving objects, individually reconstructing each object and restoring their accurate positions and occlusion relationships within the scene. We further use a LiDAR prior for Gaussian Splatting to reconstruct scenes with greater details and maintain panoramic consistency. DrivingGaussian outperforms existing methods in driving scene reconstruction and enables photorealistic surround-view synthesis with high-fidelity and multi-camera consistency. Our project page is at: https://github.com/VDIGPKU/DrivingGaussian.",http://arxiv.org/abs/2312.07920v2,,Xiaoyu Zhou (Peking University) | Zhiwei Lin (Peking University) | Xiaojun Shan (Peking Univerisity) | Yongtao Wang (Peking University) | Deqing Sun (Google) | Ming-Hsuan Yang (University Of California At Merced),2023-12-13 06:30:51+00:00,,,,,,
Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence,"While pre-trained large-scale vision models have shown significant promise for semantic correspondence, their features often struggle to grasp the geometry and orientation of instances. This paper identifies the importance of being geometry-aware for semantic correspondence and reveals a limitation of the features of current foundation models under simple post-processing. We show that incorporating this information can markedly enhance semantic correspondence performance with simple but effective solutions in both zero-shot and supervised settings. We also construct a new challenging benchmark for semantic correspondence built from an existing animal pose estimation dataset, for both pre-training validating models. Our method achieves a PCK@0.10 score of 64.2 (zero-shot) and 85.6 (supervised) on the challenging SPair-71k dataset, outperforming the state-of-the-art by 4.3p and 11.0p absolute gains, respectively. Our code and datasets will be publicly available.",http://arxiv.org/abs/2311.17034v1,,Junyi Zhang (Shanghai Jiao Tong University) | Charles Herrmann (Google) | Junhwa Hur (Google) | Eric Chen (University Of Illinois Urbana-Champaign) | Varun Jampani (Google Research) | Deqing Sun (Google) | Ming-Hsuan Yang (University Of California At Merced),2023-11-28 18:45:13+00:00,,,,,,
UniGS: Unified Representation for Image Generation and Segmentation,"This paper introduces a novel unified representation of diffusion models for image generation and segmentation. Specifically, we use a colormap to represent entity-level masks, addressing the challenge of varying entity numbers while aligning the representation closely with the image RGB domain. Two novel modules, including the location-aware color palette and progressive dichotomy module, are proposed to support our mask representation. On the one hand, a location-aware palette guarantees the colors' consistency to entities' locations. On the other hand, the progressive dichotomy module can efficiently decode the synthesized colormap to high-quality entity-level masks in a depth-first binary search without knowing the cluster numbers. To tackle the issue of lacking large-scale segmentation training data, we employ an inpainting pipeline and then improve the flexibility of diffusion models across various tasks, including inpainting, image synthesis, referring segmentation, and entity segmentation. Comprehensive experiments validate the efficiency of our approach, demonstrating comparable segmentation mask quality to state-of-the-art and adaptability to multiple tasks. The code will be released at \href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}.",http://arxiv.org/abs/2312.01985v1,,"Lu Qi (University Of California, Merced) | Lehan Yang (University Of Sydney) | Weidong Guo (Tencent) | Yu Xu (University Of Waterloo) | Bo Du (Wuhan University) | Varun Jampani (Google Research) | Ming-Hsuan Yang (University Of California At Merced)",2023-12-04 15:59:27+00:00,,,,,,
Motion-adaptive Separable Collaborative Filters for Blind Motion Deblurring,,,,"Chengxu Liu (Xi'an Jiao Tong University) | Xuan Wang (Megvii Technology) | Xiangyu Xu (Xi'an Jiao Tong University) | Ruhao Tian (Xi'an Jiao Tong University) | Shuai Li (Megvii Technology) | Xueming Qian (Xi'an Jiao Tong University, Tsinghua University) | Ming-Hsuan Yang (University Of California At Merced)",,,,,,,
No More Ambiguity in 360???  Room Layout via Bi-Layout Estimation ,,,,"Yu-Ju Tsai (University Of California, Merced) | Jin Cheng Jhang (National Tsing Hua University) | JINGJING ZHENG (None) | Wei Wang (Amazon) | Albert Chen (Amazon) | Min Sun (None) | Cheng-Hao Kuo (Amazon) | Ming-Hsuan Yang (University Of California At Merced)",,,,,,,
Programmable Motion Generation for Open-set Motion Control Tasks,,,,Hanchao Liu (Tsinghua University) | Xiaohang Zhan (Tencent) | Shaoli Huang (Tencent AI Lab) | Tai-Jiang Mu (Tsinghua University) | Ying Shan (Tencent),,,,,,,
PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding,"Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts. However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability. In this work, we introduce PhotoMaker, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information. Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration. This paves the way for more intriguing and practically valuable applications. Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data. Under the nourishment of the dataset constructed through the proposed pipeline, our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications. Our project page is available at https://photo-maker.github.io/",http://arxiv.org/abs/2312.04461v1,,"Zhen Li (Nankai University & Tencent) | Mingdeng Cao (The University Of Tokyo) | Xintao Wang (Tencent) | Zhongang Qi (Tencent PCG ARC Lab) | Ming-Ming Cheng (Nankai University, Tsinghua University) | Ying Shan (Tencent)",2023-12-07 17:32:29+00:00,,,,,,
YOLO-World: Real-Time Open-Vocabulary Object Detection,"The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.",http://arxiv.org/abs/2401.17270v3,,Tianheng Cheng (Huazhong University Of Science And Technology) | Lin Song (Tencent AI Lab) | Yixiao Ge (Tencent) | Wenyu Liu (Huazhong University Of Science And Technology) | Xinggang Wang (Huazhong University Of Science And Technology) | Ying Shan (Tencent),2024-01-30 18:59:38+00:00,,,,,,
"UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition","Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but there are two unresolved and critical issues that demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performance in image recognition. For example, our models achieve an ImageNet accuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%, demonstrating better performance and higher speed than a number of recently proposed powerful competitors. 2) We discover that large kernels are the key to unlocking the exceptional performance of ConvNets in domains where they were originally not proficient. With certain modality-related preprocessing approaches, the proposed model achieves state-of-the-art performance on time-series forecasting and audio recognition tasks even without modality-specific customization to the architecture. Code and all the models at https://github.com/AILab-CVC/UniRepLKNet.",http://arxiv.org/abs/2311.15599v1,,Xiaohan Ding (Tencent AI Lab) | Yiyuan Zhang (The Chinese University Of Hong Kong) | Yixiao Ge (Tencent) | Sijie Zhao (Tencent AI Lab) | Lin Song (Tencent AI Lab) | Xiangyu Yue (None) | Ying Shan (Tencent),2023-11-27 07:48:50+00:00,,,,,,
SEED-Bench: Benchmarking Multimodal Large Language Models,,,,Bohao Li (None) | Yuying Ge (University Of Hong Kong) | Yixiao Ge (Tencent) | Guangzhi Wang (National University Of Singapore) | Rui Wang (Fudan University) | Ruimao Zhang (The Chinese University Of Hong Kong (Shenzhen)) | Ying Shan (Tencent),,,,,,,
LoRA-Sparse: Low-Rank Approximation for Sparse Large Language Models,,,,Lin Song (Tencent AI Lab) | Yukang Chen (None) | Shuai Yang (Hong Kong University Of Science And Technology (Guangzhou)) | Xiaohan Ding (Tencent AI Lab) | Yixiao Ge (Tencent) | Ying-Cong Chen (The Hong Kong University Of Science And Technology) | Ying Shan (Tencent),,,,,,,
Overcoming Data Limitations for High-Quality Video Diffusion Models,"Text-to-video generation aims to produce a video based on a given prompt. Recently, several commercial video models have been able to generate plausible videos with minimal noise, excellent details, and high aesthetic scores. However, these models rely on large-scale, well-filtered, high-quality videos that are not accessible to the community. Many existing research works, which train models using the low-quality WebVid-10M dataset, struggle to generate high-quality videos because the models are optimized to fit WebVid-10M. In this work, we explore the training scheme of video models extended from Stable Diffusion and investigate the feasibility of leveraging low-quality videos and synthesized high-quality images to obtain a high-quality video model. We first analyze the connection between the spatial and temporal modules of video models and the distribution shift to low-quality videos. We observe that full training of all modules results in a stronger coupling between spatial and temporal modules than only training temporal modules. Based on this stronger coupling, we shift the distribution to higher quality without motion degradation by finetuning spatial modules with high-quality images, resulting in a generic high-quality video model. Evaluations are conducted to demonstrate the superiority of the proposed method, particularly in picture quality, motion, and concept composition.",http://arxiv.org/abs/2401.09047v1,,Haoxin Chen (Tencent AI Lab) | Yong Zhang (Tencent AI Lab) | Xiaodong Cun (Tencent AI Lab) | Menghan Xia (Tencent AI Lab) | Xintao Wang (Tencent) | CHAO WENG (Tencent AI Lab) | Ying Shan (Tencent),2024-01-17 08:30:32+00:00,,,,,,
EvalCrafter: Benchmarking and Evaluating Large Video Generation Models,"The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services are released for generating high-visual quality videos. However, these methods often use a few academic metrics, for example, FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a new framework and pipeline to exhaustively evaluate the performance of the generated videos. To achieve this, we first conduct a new prompt list for text-to-video generation by analyzing the real-world prompt list with the help of the large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmarks, in terms of visual qualities, content qualities, motion qualities, and text-caption alignment with around 18 objective metrics. To obtain the final leaderboard of the models, we also fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed opinion alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.",http://arxiv.org/abs/2310.11440v2,,"Yaofang Liu (City University Of Hong Kong) | Xiaodong Cun (Tencent AI Lab) | Xuebo Liu (Harbin Institute Of Technolgy, Shenzhen) | Xintao Wang (Tencent) | Yong Zhang (Tencent AI Lab) | Haoxin Chen (Tencent AI Lab) | Yang Liu (National University Of Defense Technology) | Tieyong Zeng (The Chinese University Of Hong Kong) | Raymond Chan (City University Of Hong Kong) | Ying Shan (Tencent)",2023-10-17 17:50:46+00:00,,,,,,
SmartEdit: Exploring Complex Instruction-based Image Editing with Large Language Models,"Current instruction-based editing methods, such as InstructPix2Pix, often fail to produce satisfactory results in complex scenarios due to their dependence on the simple CLIP text encoder in diffusion models. To rectify this, this paper introduces SmartEdit, a novel approach to instruction-based image editing that leverages Multimodal Large Language Models (MLLMs) to enhance their understanding and reasoning capabilities. However, direct integration of these elements still faces challenges in situations requiring complex reasoning. To mitigate this, we propose a Bidirectional Interaction Module that enables comprehensive bidirectional information interactions between the input image and the MLLM output. During training, we initially incorporate perception data to boost the perception and understanding capabilities of diffusion models. Subsequently, we demonstrate that a small amount of complex instruction editing data can effectively stimulate SmartEdit's editing capabilities for more complex instructions. We further construct a new evaluation dataset, Reason-Edit, specifically tailored for complex instruction-based image editing. Both quantitative and qualitative results on this evaluation dataset indicate that our SmartEdit surpasses previous methods, paving the way for the practical application of complex instruction-based image editing.",http://arxiv.org/abs/2312.06739v1,,"Yuzhou Huang (None) | Liangbin Xie (Macau) | Xintao Wang (Tencent) | Ziyang Yuan (Tsinghua University) | Xiaodong Cun (Tencent AI Lab) | Yixiao Ge (Tencent) | Jiantao Zhou (University Of Macau) | Chao Dong (SIAT) | Rui Huang (The Chinese University Of Hong Kong, Shenzhen) | Ruimao Zhang (The Chinese University Of Hong Kong (Shenzhen)) | Ying Shan (Tencent)",2023-12-11 17:54:11+00:00,,,,,,
LLaFS: When Large-Language Models Meet Few-Shot Segmentation,"This paper proposes LLaFS, the first attempt to leverage large language models (LLMs) in few-shot segmentation. In contrast to the conventional few-shot segmentation methods that only rely on the limited and biased information from the annotated support images, LLaFS leverages the vast prior knowledge gained by LLM as an effective supplement and directly uses the LLM to segment images in a few-shot manner. To enable the text-based LLM to handle image-related tasks, we carefully design an input instruction that allows the LLM to produce segmentation results represented as polygons, and propose a region-attribute table to simulate the human visual mechanism and provide multi-modal guidance. We also synthesize pseudo samples and use curriculum learning for pretraining to augment data and achieve better optimization. LLaFS achieves state-of-the-art results on multiple datasets, showing the potential of using LLMs for few-shot computer vision tasks. Code will be available at https://github.com/lanyunzhu99/LLaFS.",http://arxiv.org/abs/2311.16926v3,,Lanyun Zhu (Singapore University Of Technology And Design) | Tianrun Chen (Zhejiang University) | Deyi Ji (None) | Jieping Ye (Alibaba Group) | Jun Liu (None),2023-11-28 16:31:27+00:00,,,,,,
Addressing Background Context Bias in Few-Shot Segmentation through Iterative Modulation,,,,Lanyun Zhu (Singapore University Of Technology And Design) | Tianrun Chen (Zhejiang University) | Jianxiong Yin (NVIDIA) | Simon See (NVIDIA) | Jun Liu (None),,,,,,,
LLM-AR: When Large Language Model Meets Skeleton-Based Action Recognition,,,,Haoxuan Qu (Singapore University Of Technology And Design) | Yujun Cai (Meta) | Jun Liu (None),,,,,,,
Action Detection via an Image Diffusion Process,,,,Lin Geng Foo (Singapore University Of Technology And Design) | Tianjiao Li (Singapore University Of Technology And Design) | Hossein Rahmani (Lancaster University) | Jun Liu (None),,,,,,,
6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation,"Estimating the 6D object pose from a single RGB image often involves noise and indeterminacy due to challenges such as occlusions and cluttered backgrounds. Meanwhile, diffusion models have shown appealing performance in generating high-quality images from random noise with high indeterminacy through step-by-step denoising. Inspired by their denoising capability, we propose a novel diffusion-based framework (6D-Diff) to handle the noise and indeterminacy in object pose estimation for better performance. In our framework, to establish accurate 2D-3D correspondence, we formulate 2D keypoints detection as a reverse diffusion (denoising) process. To facilitate such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion process and condition the reverse process on the object features. Extensive experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our framework.",http://arxiv.org/abs/2401.00029v2,,Li Xu (Singapore University Of Technology And Design) | Haoxuan Qu (Singapore University Of Technology And Design) | Yujun Cai (Meta) | Jun Liu (None),2023-12-29 05:28:35+00:00,,,,,,
Representing Signs as Language: A New Method for Sign Language Translation from Videos,,,,Jia Gong (Singapore University Of Technology And Design) | Lin Geng Foo (Singapore University Of Technology And Design) | Yixuan He (Singapore University Of Technology And Design) | Hossein Rahmani (Lancaster University) | Jun Liu (None),,,,,,,
LTGC: Long-Tail Recognition via leveraging Generated Content,"Long-tail recognition is challenging because it requires the model to learn good representations from tail categories and address imbalances across all categories. In this paper, we propose a novel generative and fine-tuning framework, LTGC, to handle long-tail recognition via leveraging generated content. Firstly, inspired by the rich implicit knowledge in large-scale models (e.g., large language models, LLMs), LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content. We then propose several novel designs for LTGC to ensure the quality of the generated data and to efficiently fine-tune the model using both the generated and original data. The visualization demonstrates the effectiveness of the generation module in LTGC, which produces accurate and diverse tail data. Additionally, the experimental results demonstrate that our LTGC outperforms existing state-of-the-art methods on popular long-tailed benchmarks.",http://arxiv.org/abs/2403.05854v3,,Qihao Zhao (Beijing University Of Chemical Technology) | Yalun Dai (Nanyang Technological University) | Hao Li (Northwest Polytechnical University) | Wei Hu (Beijing Univeristy Of Chemical Technology) | Fan Zhang (Beijing University Of Chemical Technology) | Jun Liu (None),2024-03-09 09:52:15+00:00,,,,,,
FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Fully-Supervised Action Segmentation,,,,Zijia Lu (Northeastern University) | Ehsan Elhamifar (None),,,,,,,
Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos,,,,Yuhan Shen (Northeastern University) | Ehsan Elhamifar (None),,,,,,,
Learning to Predict Task Progress by Self-Supervised Video Alignment,,,,Gerard Donahue (Northeastern University) | Ehsan Elhamifar (None),,,,,,,
Semantic-Aware Multi-Label Adversarial Attacks,,,,Hassan Mahmood (Northeastern University) | Ehsan Elhamifar (None),,,,,,,
Error Detection in Egocentric Procedural Task Videos,,,,"Shih-Po Lee (Northeastern University) | Zijia Lu (Northeastern University) | Zekun Zhang (Stony Brook University) | Minh Hoai (State University Of New York, Stony Brook) | Ehsan Elhamifar (None)",,,,,,,
Rethinking Generalizable Face Anti-spoofing via Hierarchical Prototype-guided Distribution Refinement in Hyperbolic Space,,,,"Chengyang Hu (Shanghai Jiao Tong University) | Ke-Yue Zhang (Tencent) | Taiping Yao (Tencent Youtu Lab) | Shouhong Ding (Tencent Youtu Lab) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model,"In this paper, we address the challenge of image resolution variation for the Segment Anything Model (SAM). SAM, known for its zero-shot generalizability, exhibits a performance degradation when faced with datasets with varying image sizes. Previous approaches tend to resize the image to a fixed size or adopt structure modifications, hindering the preservation of SAM's rich prior knowledge. Besides, such task-specific tuning necessitates a complete retraining of the model, which is cost-expensive and unacceptable for deployment in the downstream tasks. In this paper, we reformulate this issue as a length extrapolation problem, where token sequence length varies while maintaining a consistent patch size for images of different sizes. To this end, we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's adaptability to varying image resolutions while eliminating the need for structure modifications. Firstly, we introduce a new scaling factor to ensure consistent magnitude in the attention layer's dot product values when the token sequence length changes. Secondly, we present a bias-mode attention mask that allows each token to prioritize neighboring information, mitigating the impact of untrained distant information. Our BA-SAM demonstrates efficacy in two scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets, including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to significantly mitigate performance degradation in the zero-shot setting and achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we propose a generalized model and benchmark, showcasing BA-SAM's generalizability across all four datasets simultaneously.",http://arxiv.org/abs/2401.02317v2,,"Song Yiran (None) | Qianyu Zhou (Shanghai Jiao Tong University) | Xiangtai Li (Nanyang Technological University) | Deng-Ping Fan (ETH Zurich) | Xuequan Lu (La Trobe University) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",2024-01-04 15:34:44+00:00,,,,,,
Test-Time Domain Generalization for Face Anti-Spoofing,,,,"Qianyu Zhou (Shanghai Jiao Tong University) | Ke-Yue Zhang (Tencent) | Taiping Yao (Tencent Youtu Lab) | Xuequan Lu (La Trobe University) | Shouhong Ding (Tencent Youtu Lab) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
Re-thinking Data Availablity Attacks Against Deep Neural Networks,"The unauthorized use of personal data for commercial purposes and the clandestine acquisition of private data for training machine learning models continue to raise concerns. In response to these issues, researchers have proposed availability attacks that aim to render data unexploitable. However, many current attack methods are rendered ineffective by adversarial training. In this paper, we re-examine the concept of unlearnable examples and discern that the existing robust error-minimizing noise presents an inaccurate optimization objective. Building on these observations, we introduce a novel optimization paradigm that yields improved protection results with reduced computational time requirements. We have conducted extensive experiments to substantiate the soundness of our approach. Moreover, our method establishes a robust foundation for future research in this area.",http://arxiv.org/abs/2305.10691v1,,"Bin Fang (Shanghai Jiao Tong University) | Bo Li (Vivo Mobile Communication Co.,Ltd.) | Shuang Wu (Tencent YouTu Lab) | Shouhong Ding (Tencent Youtu Lab) | Ran Yi (Shanghai Jiao Tong University) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",2023-05-18 04:03:51+00:00,,,,,,
Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text,,,,"Junshu Tang (None) | Yanhong Zeng (None) | Ke Fan (Shanghai Jiao Tong University) | Xuheng Wang (Tsinghua University) | Bo Dai (Shanghai AI Laboratory) | Kai Chen (Shanghai AI Laboratory) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection,,,,"Xiaofan Li (East China Normal University) | Zhizhong Zhang (East China Normal University) | Xin Tan (East China Normal University) | Yanyun Qu (Xiamen University) | Chengwei Chen (2nd Military Medical University) | Yuan Xie (East China Normal University) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection,,,,"Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University) | Wenbing Zhu (Fudan University) | Bin-Bin Gao (None) | Zhenye Gan (Tencent Youtu Lab) | Jiangning Zhang (Tencent Youtu Lab) | Zhihao Gu (Shanghai Jiao Tong University) | Bruce Qian (None) | Mingang Chen (Shanghai Development Center Of Computer Software Technology) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation,,,,"Chen Sichen (Shanghai Jiao Tong University) | Yingyi Zhang (Tencent Youtu Lab) | Siming Huang (Duke University) | Ran Yi (Shanghai Jiao Tong University) | Ke Fan (Shanghai Jiao Tong University) | Ruixin Zhang (Tencent Youtu Lab) | Peixian Chen (Xiamen University) | Jun Wang (None) | Shouhong Ding (Tencent Youtu Lab) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University)",,,,,,,
FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models,,,,Jinglin Xu (University Of Science And Technology Beijing) | Yijie Guo (Peking University) | Yuxin Peng (Peking University),,,,,,,
FineSports: A Multi-person Hierarchical Sports Video Dataset for Fine-grained Action Understanding,,,,Jinglin Xu (University Of Science And Technology Beijing) | Guohao Zhao (Peking University) | Sibo Yin (Peking University) | Wenhao Zhou (University Of Science And Technology Beijing) | Yuxin Peng (Peking University),,,,,,,
FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment,,,,Jinglin Xu (University Of Science And Technology Beijing) | Sibo Yin (Peking University) | Guohao Zhao (Peking University) | Zishuo Wang (None) | Yuxin Peng (Peking University),,,,,,,
SGC-Occ: Semantic-Geometry Consistent 3D Occupancy Prediction for Autonomous Driving,,,,Zhiwen Yang (Peking University) | Xiangteng He (Peking University) | Yuxin Peng (Peking University),,,,,,,
Learning Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification,,,,Zhenyu Cui (None) | Jiahaun Zhou (Peking University) | Xun Wang (ByteDance Inc) | Manyu Zhu (Bytedance) | Yuxin Peng (Peking University),,,,,,,
FlowIE???Efficient Image Enhancement via Rectified Flow,,,,"Yixuan Zhu (None) | Wenliang Zhao (Automation, Tsinghua University) | Ao Li (Tsinghua University) | Yansong Tang (Tsinghua University) | Jie Zhou (None) | Jiwen Lu (Tsinghua University)",,,,,,,
DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery,,,,"Yixuan Zhu (None) | Ao Li (Tsinghua University) | Yansong Tang (Tsinghua University) | Wenliang Zhao (Automation, Tsinghua University) | Jie Zhou (None) | Jiwen Lu (Tsinghua University)",,,,,,,
SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction,"3D occupancy prediction is an important task for the robustness of vision-centric autonomous driving, which aims to predict whether each point is occupied in the surrounding 3D space. Existing methods usually require 3D occupancy labels to produce meaningful results. However, it is very laborious to annotate the occupancy status of each voxel. In this paper, we propose SelfOcc to explore a self-supervised way to learn 3D occupancy using only video sequences. We first transform the images into the 3D space (e.g., bird's eye view) to obtain 3D representation of the scene. We directly impose constraints on the 3D representations by treating them as signed distance fields. We can then render 2D images of previous and future frames as self-supervision signals to learn the 3D representations. We propose an MVS-embedded strategy to directly optimize the SDF-induced weights with multiple depth proposals. Our SelfOcc outperforms the previous best method SceneRF by 58.7% using a single frame as input on SemanticKITTI and is the first self-supervised work that produces reasonable 3D occupancy for surround cameras on nuScenes. SelfOcc produces high-quality depth and achieves state-of-the-art results on novel depth synthesis, monocular depth estimation, and surround-view depth estimation on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code: https://github.com/huang-yh/SelfOcc.",http://arxiv.org/abs/2311.12754v2,,Yuanhui Huang (Tsinghua University) | Wenzhao Zheng (Tsinghua University) | Borui Zhang (Tsinghua University) | Jie Zhou (None) | Jiwen Lu (Tsinghua University),2023-11-21 17:59:14+00:00,,,,,,
MirageRoom: 3D Scene Segmentation with 2D Pre-trained Models by Mirage Projection,,,,Haowen Sun (Tsinghua University) | Yueqi Duan (None) | Juncheng Yan (None) | Yifan Liu (Tsinghua University) | Jiwen Lu (Tsinghua University),,,,,,,
Towards Accurate Post-training Quantization for Diffusion Models,"In this paper, we propose an accurate data-free post-training quantization framework of diffusion models (ADP-DM) for efficient image generation. Conventional data-free quantization methods learn shared quantization functions for tensor discretization regardless of the generation timesteps, while the activation distribution differs significantly across various timesteps. The calibration images are acquired in random timesteps which fail to provide sufficient information for generalizable quantization function learning. Both issues cause sizable quantization errors with obvious image generation performance degradation. On the contrary, we design group-wise quantization functions for activation discretization in different timesteps and sample the optimal timestep for informative calibration image generation, so that our quantized diffusion model can reduce the discretization errors with negligible computational overhead. Specifically, we partition the timesteps according to the importance weights of quantization functions in different groups, which are optimized by differentiable search algorithms. We also select the optimal timestep for calibration image generation by structural risk minimizing principle in order to enhance the generalization ability in the deployment of quantized diffusion model. Extensive experimental results show that our method outperforms the state-of-the-art post-training quantization of diffusion model by a sizable margin with similar computational cost.",http://arxiv.org/abs/2305.18723v3,,Changyuan Wang (Tsinghua University) | Ziwei Wang (Tsinghua University) | Xiuwei Xu (Tsinghua University) | Yansong Tang (Tsinghua University) | Jie Zhou (None) | Jiwen Lu (Tsinghua University),2023-05-30 04:00:35+00:00,,,,,,
Memory-based Adapters for Online 3D Scene Perception,"In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \href{https://xuxw98.github.io/Online3D/}{Project page}.",http://arxiv.org/abs/2403.06974v1,,"Xiuwei Xu (Tsinghua University) | Chong Xia (Tsinghua University) | Ziwei Wang (Tsinghua University) | Linqing Zhao (Tianjin University, Tsinghua University) | Yueqi Duan (None) | Jie Zhou (None) | Jiwen Lu (Tsinghua University)",2024-03-11 17:57:41+00:00,,,,,,
LowRankOcc: Tensor Decomposition and Low-Rank Recovery for Vision-based 3D Semantic Occupancy Prediction,,,,"Linqing Zhao (Tianjin University, Tsinghua University) | Xiuwei Xu (Tsinghua University) | Ziwei Wang (Tsinghua University) | Yunpeng Zhang (PhiGent Robotics) | Borui Zhang (Tsinghua University) | Wenzhao Zheng (Tsinghua University) | Dalong Du (PhiGent Robotics) | Jie Zhou (None) | Jiwen Lu (Tsinghua University)",,,,,,,
Unified Language-driven Zero-shot Domain Adaptation,,,,Senqiao Yang (Harbin Institute Of Technology) | Zhuotao Tian (The Chinese University Of Hong Kong) | Li Jiang (Max Planck Institute For Informatics) | Jiaya Jia (The Chinese University Of Hong Kong),,,,,,,
Video-P2P: Video Editing with Cross-attention Control,"This paper presents Video-P2P, a novel framework for real-world video editing with cross-attention control. While attention control has proven effective for image editing with pre-trained image generation models, there are currently no large-scale video generation models publicly available. Video-P2P addresses this limitation by adapting an image generation diffusion model to complete various video editing tasks. Specifically, we propose to first tune a Text-to-Set (T2S) model to complete an approximate inversion and then optimize a shared unconditional embedding to achieve accurate video inversion with a small memory cost. For attention control, we introduce a novel decoupled-guidance strategy, which uses different guidance strategies for the source and target prompts. The optimized unconditional embedding for the source prompt improves reconstruction ability, while an initialized unconditional embedding for the target prompt enhances editability. Incorporating the attention maps of these two branches enables detailed editing. These technical designs enable various text-driven editing applications, including word swap, prompt refinement, and attention re-weighting. Video-P2P works well on real-world videos for generating new characters while optimally preserving their original poses and scenes. It significantly outperforms previous approaches.",http://arxiv.org/abs/2303.04761v1,,Shaoteng Liu (The Chinese University Of Hong Kong) | Yuechen Zhang (None) | Wenbo Li (Huawei Technologies Ltd.) | Zhe Lin (Adobe Research) | Jiaya Jia (The Chinese University Of Hong Kong),2023-03-08 17:53:49+00:00,,,,,,
Prompt Highlighter: Interactive Control for Multi-Modal LLMs,"This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and 1552.5 in MME-perception. The code is available at: https://github.com/dvlab-research/Prompt-Highlighter/",http://arxiv.org/abs/2312.04302v1,,Yuechen Zhang (None) | Shengju Qian (The Chinese University Of Hong Kong) | Bohao Peng (The Chinese University Of Hong Kong) | Shu Liu (The Chinese University Of Hong Kong) | Jiaya Jia (The Chinese University Of Hong Kong),2023-12-07 13:53:29+00:00,,,,,,
SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training,,,,"WU Sitong (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Haoru Tan (HKU) | Zhuotao Tian (The Chinese University Of Hong Kong) | Yukang Chen (None) | Xiaojuan Qi (University Of Oxford) | Jiaya Jia (The Chinese University Of Hong Kong)",,,,,,,
GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding,"Self-supervised 3D representation learning aims to learn effective representations from large-scale unlabeled point clouds. Most existing approaches adopt point discrimination as the pretext task, which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs. However, this approach often results in semantically identical points having dissimilar representations, leading to a high number of false negatives and introducing a ""semantic conflict"" problem. To address this issue, we propose GroupContrast, a novel approach that combines segment grouping and semantic-aware contrastive learning. Segment grouping partitions points into semantically meaningful regions, which enhances semantic coherence and provides semantic guidance for the subsequent contrastive representation learning. Semantic-aware contrastive learning augments the semantic information extracted from segment grouping and helps to alleviate the issue of ""semantic conflict"". We conducted extensive experiments on multiple 3D scene understanding tasks. The results demonstrate that GroupContrast learns semantically meaningful representations and achieves promising transfer learning performance.",http://arxiv.org/abs/2403.09639v1,,"Chengyao Wang (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Li Jiang (Max Planck Institute For Informatics) | Xiaoyang Wu (The University Of Hong Kong) | Zhuotao Tian (The Chinese University Of Hong Kong) | Bohao Peng (The Chinese University Of Hong Kong) | Hengshuang Zhao (The University Of Hong Kong) | Jiaya Jia (The Chinese University Of Hong Kong)",2024-03-14 17:59:59+00:00,,,,,,
LISA: Reasoning Segmentation via Large Language Model,"Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at https://github.com/dvlab-research/LISA.",http://arxiv.org/abs/2308.00692v2,,"Xin Lai (None) | Zhuotao Tian (The Chinese University Of Hong Kong) | Yukang Chen (None) | Yanwei Li (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Yuhui Yuan (Microsoft Research Asia) | Shu Liu (The Chinese University Of Hong Kong) | Jiaya Jia (The Chinese University Of Hong Kong)",2023-08-01 17:50:17+00:00,,,,,,
Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation,,,,Bohao Peng (The Chinese University Of Hong Kong) | Xiaoyang Wu (The University Of Hong Kong) | Li Jiang (Max Planck Institute For Informatics) | Yukang Chen (None) | Hengshuang Zhao (The University Of Hong Kong) | Zhuotao Tian (The Chinese University Of Hong Kong) | Jiaya Jia (The Chinese University Of Hong Kong),,,,,,,
Structure-from-Motion from Pixel-wise Correspondences,,,,"Philipp Lindenberger (Department Of Computer Science, ETHZ - ETH Zurich) | Paul-Edouard Sarlin (ETH Zurich) | Marc Pollefeys (ETH Zurich / Microsoft)",,,,,,,
Efficient Solution of Point-Line Absolute Pose,,,,"Petr Hruby (Department Of Computer Science, ETHZ - ETH Zurich) | Timothy Duff (University Of Washington) | Marc Pollefeys (ETH Zurich / Microsoft)",,,,,,,
LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry,"Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.",http://arxiv.org/abs/2401.01887v1,,"Weirong Chen (Technical University Of Munich) | Le Chen (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Rui Wang (Microsoft) | Marc Pollefeys (ETH Zurich / Microsoft)",2024-01-03 18:57:27+00:00,,,,,,
F3 Loc: Fusion and Filtering for Floorplan Localization ,"In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.",http://arxiv.org/abs/2403.03370v1,,Changan Chen (None) | Rui Wang (Microsoft) | Christoph Vogel (Microsoft) | Marc Pollefeys (ETH Zurich / Microsoft),2024-03-05 23:32:26+00:00,,,,,,
GLACE: Global Local Accelerated Coordinate Encoding,,,,Fangjinhua Wang (None) | Xudong Jiang (ETHZ - ETH Zurich) | Silvano Galliani (Microsoft) | Christoph Vogel (Microsoft) | Marc Pollefeys (ETH Zurich / Microsoft),,,,,,,
3D Neural Edge Reconstruction,,,,Lei Li (ETH Zurich) | Songyou Peng (ETH Zurich & MPI T??bingen) | Zehao Yu (None) | Shaohui Liu (ETH Zurich) | R??mi Pautrat (Microsoft Mixed Reality & AI Lab) | Xiaochuan Yin (Utopilot) | Marc Pollefeys (ETH Zurich / Microsoft),,,,,,,
Enhancing Post-training Quantization Calibration through Contrastive Learning,,,,Yuzhang Shang (Illinois Institute Of Technology) | Gaowen Liu (None) | Ramana Kompella (Cisco) | Yan Yan (Illinois Institute Of Technology),,,,,,,
Efficient Multitask Dense Predictor via Binarization,,,,"Yuzhang Shang (Illinois Institute Of Technology) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology) | Gaowen Liu (None) | Ramana Kompella (Cisco) | Yan Yan (Illinois Institute Of Technology)",,,,,,,
Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer,,,,Junyi Wu (None) | Bin Duan (Illinois Tech) | Weitai Kang (None) | Hao Tang (ETH Zurich And CMU) | Yan Yan (Illinois Institute Of Technology),,,,,,,
On the Faithfulness of Vision Transformer Explanations,,,,Junyi Wu (None) | Weitai Kang (None) | Hao Tang (ETH Zurich And CMU) | Yuan Hong (University Of Connecticut) | Yan Yan (Illinois Institute Of Technology),,,,,,,
Versatile Navigation under Partial Observability via Value-Guided Diffusion Policy,,,,Gengyu Zhang (Illinois Institute Of Technology) | Hao Tang (ETH Zurich And CMU) | Yan Yan (Illinois Institute Of Technology),,,,,,,
Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling,"Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front \& back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches. Code: https://github.com/lizhe00/AnimatableGaussians",http://arxiv.org/abs/2311.16096v2,,Zhe Li (Tsinghua University) | Zerong Zheng (Tsinghua University) | Lizhen Wang (Tsinghua University) | Yebin Liu (Tsinghua University),2023-11-27 18:59:04+00:00,,,,,,
HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models,,,,Mengcheng Li (Tsinghua University) | Hongwen Zhang (Beijing Normal University) | Yuxiang Zhang (Tsinghua University) | Ruizhi Shao (Tsinghua University) | Tao Yu (Tsinghua University) | Yebin Liu (Tsinghua University),,,,,,,
Control4D: Efficient 4D Portrait Editing with Text,"We introduce Control4D, an innovative framework for editing dynamic 4D portraits using text instructions. Our method addresses the prevalent challenges in 4D editing, notably the inefficiencies of existing 4D representations and the inconsistent editing effect caused by diffusion-based editors. We first propose GaussianPlanes, a novel 4D representation that makes Gaussian Splatting more structured by applying plane-based decomposition in 3D space and time. This enhances both efficiency and robustness in 4D editing. Furthermore, we propose to leverage a 4D generator to learn a more continuous generation space from inconsistent edited images produced by the diffusion-based editor, which effectively improves the consistency and quality of 4D editing. Comprehensive evaluation demonstrates the superiority of Control4D, including significantly reduced training time, high-quality rendering, and spatial-temporal consistency in 4D portrait editing. The link to our project website is https://control4darxiv.github.io.",http://arxiv.org/abs/2305.20082v2,,Ruizhi Shao (Tsinghua University) | Jingxiang Sun (None) | Cheng Peng (Tsinghua University) | Zerong Zheng (Tsinghua University) | Boyao ZHOU (Tsinghua University) | Hongwen Zhang (Beijing Normal University) | Yebin Liu (Tsinghua University),2023-05-31 17:55:28+00:00,,,,,,
Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians,"Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.",http://arxiv.org/abs/2312.03029v1,,Yuelang Xu (Tsinghua University) | Benwang Chen (Tsinghua University) | Zhe Li (Tsinghua University) | Hongwen Zhang (Beijing Normal University) | Lizhen Wang (Tsinghua University) | Zerong Zheng (Tsinghua University) | Yebin Liu (Tsinghua University),2023-12-05 11:01:44+00:00,,,,,,
ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning,"Learning-based approaches to monocular motion capture have recently shown promising results by learning to regress in a data-driven manner. However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space. In this work, we introduce ProxyCap, a human-centric proxy-to-motion learning scheme to learn world-space motions from a proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy data enables us to build a learning-based network with accurate world-space supervision while also mitigating the generalization issues. For more accurate and physically plausible predictions in world space, our network is designed to learn human motions from a human-centric perspective, which enables the understanding of the same motion captured with different camera trajectories. Moreover, a contact-aware neural motion descent module is proposed in our network so that it can be aware of foot-ground contact and motion misalignment with the proxy observations. With the proposed learning-based solution, we demonstrate the first real-time monocular full-body capture system with plausible foot-ground contact in world space even using hand-held moving cameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.",http://arxiv.org/abs/2307.01200v3,,"Yuxiang Zhang (Tsinghua University) | Hongwen Zhang (Beijing Normal University) | Liangxiao Hu (Harbin Institute Of Technology) | Jiajun Zhang (Beijing University Of Posts And Telecommunications) | Hongwei Yi (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Shengping Zhang (Harbin Institute Of Technology) | Yebin Liu (Tsinghua University)",2023-07-03 17:59:45+00:00,,,,,,
GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis,"We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.",http://arxiv.org/abs/2312.02155v1,,"Shunyuan Zheng (Harbin Institute Of Technology) | Boyao ZHOU (Tsinghua University) | Ruizhi Shao (Tsinghua University) | Boning Liu (Department Of Automation, Tsinghua University) | Shengping Zhang (Harbin Institute Of Technology) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen)) | Yebin Liu (Tsinghua University)",2023-12-04 18:59:55+00:00,,,,,,
RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control,,,,Xiang Deng (Tsinghua University) | Zerong Zheng (Tsinghua University) | Yuxiang Zhang (Tsinghua University) | Jingxiang Sun (None) | Chao Xu (NNCosmos) | Xiaodong Yang (Li Auto) | Lizhen Wang (Tsinghua University) | Yebin Liu (Tsinghua University),,,,,,,
Non-autoregressive Sequence-to-Sequence Vision-Language Models,,,,"Kunyu Shi (Amazon) | Qi Dong (Amazon) | Luis Goncalves (California Institute Of Technology) | Zhuowen Tu (University Of California, San Diego) | Stefano Soatto (AWS)",,,,,,,
Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding,"Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of ""conceptual similarity"" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate ""explanations"" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.",http://arxiv.org/abs/2402.08919v1,,"Alessandro Achille (California Institute Of Technology) | Greg Ver Steeg (University Of California, Riverside) | Tian Yu Liu (University Of California, Los Angeles) | Matthew Trager (Amazon) | Carson Klingenberg (Amazon Web Services) | Stefano Soatto (AWS)",2024-02-14 03:31:17+00:00,,,,,,
CPR: Retrieval Augmented Generation for Copyright Protection,,,,"Aditya Golatkar (University Of California, Los Angeles) | Alessandro Achille (California Institute Of Technology) | Luca Zancato (AWS AI Labs) | Yu-Xiang Wang (UC Santa Barbara / Amazon) | Ashwin Swaminathan (University Of Maryland, College Park) | Stefano Soatto (AWS)",,,,,,,
THRONE: A Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models,,,,"Prannay Kaul (University Of Oxford) | Zhizhong Li (Amazon) | Hao Yang (Amazon) | Yonatan Dukler (AWS AI) | Ashwin Swaminathan (University Of Maryland, College Park) | CJ Taylor (Penn) | Stefano Soatto (AWS)",,,,,,,
Multi-Modal Hallucination Control by Visual Information Grounding,,,,"Alessandro Favero (EPFL - EPF Lausanne) | Luca Zancato (AWS AI Labs) | Matthew Trager (Amazon) | Siddharth Choudhary (Amazon AGI) | Pramuditha Perera (Amazon) | Alessandro Achille (California Institute Of Technology) | Ashwin Swaminathan (University Of Maryland, College Park) | Stefano Soatto (AWS)",,,,,,,
On the Scalability of Diffusion-based Text-to-Image Generation,,,,"Hao Li (AWS AI Labs) | Yang Zou (Amazon) | Ying Wang (Amazon) | Orchid Majumder (Amazon Web Services) | Yusheng Xie (Amazon) | R. Manmatha (Amazon) | Ashwin Swaminathan (University Of Maryland, College Park) | Zhuowen Tu (University Of California, San Diego) | Stefano Ermon (Stanford University) | Stefano Soatto (AWS)",,,,,,,
Enhancing Vision-Language Pretraining with Rich Supervisions,,,,"Yuan Gao (Computer Science Department, Stanford University) | Kunyu Shi (Amazon) | Pengkai Zhu (Boston University) | Edouard Belval (Amazon) | Oren Nuriel (Amazon) | Srikar Appalaraju (Amazon) | Shabnam Ghadar (Amazon) | Zhuowen Tu (University Of California, San Diego) | Vijay Mahadevan (Amazon) | Stefano Soatto (AWS)",,,,,,,
Neural Exposure Fusion for High-Dynamic Range Object Detection,,,,"Emmanuel Onzon (Torc Robotics) | Maximilian B??mer (Torc Robotics) | Fahim Mannan (None) | Felix Heide (Department Of Computer Science, Princeton University)",,,,,,,
Cross-spectral Gated-RGB Stereo Depth Estimation,,,,"Samuel Brucker (Mercedes Benz Research & Development) | Stefanie Walz (Mercedes-Benz AG) | Mario Bijelic (Princeton University) | Felix Heide (Department Of Computer Science, Princeton University)",,,,,,,
Flow-Guided Online Stereo Rectification for Wide Baseline Stereo,,,,"Anush Kumar (Torc Robotics) | Fahim Mannan (None) | Omid Hosseini Jafari (Torc Robotics) | Shile Li (Torc Robotics) | Felix Heide (Department Of Computer Science, Princeton University)",,,,,,,
Neural Spline Fields for Burst Image Fusion and Layer Separation,"Each photo in an image burst can be considered a sample of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant variation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a two-layer alpha-composited image plus flow model constructed with neural spline fields -- networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decompose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection suppression, and shadow removal. Validated on complex synthetic and in-the-wild captures we find that, with no post-processing steps or learned priors, our generalizable model is able to outperform existing dedicated single-image and multi-view obstruction removal approaches.",http://arxiv.org/abs/2312.14235v1,,"Ilya Chugunov (Princeton University) | David Shustin (Princeton University) | Ruyu Yan (Princeton University) | Chenyang Lei (The Hong Kong University Of Science And Technology) | Felix Heide (Department Of Computer Science, Princeton University)",2023-12-21 18:54:19+00:00,,,,,,
Polarization Wavefront Lidars: Learning to Recover Large-Scale Scene Information from Polarized Wavefronts,,,,"Dominik Scheuble (Universit??t Des Saarlandes) | Chenyang Lei (The Hong Kong University Of Science And Technology) | Mario Bijelic (Princeton University) | Seung-Hwan Baek (POSTECH) | Felix Heide (Department Of Computer Science, Princeton University)",,,,,,,
Gated Fields: Learning Scene Reconstruction from Gated Videos,,,,"Andrea Ramazzina (Saarland University, Universit??t Des Saarlandes) | Stefanie Walz (Mercedes-Benz AG) | Pragyan Dahal (Polytechnic Institute Of Milan) | Mario Bijelic (Princeton University) | Felix Heide (Department Of Computer Science, Princeton University)",,,,,,,
Global and Hierarchical Geometry Consistency Priors for Few-shot NeRFs in Indoor Scenes,,,,Xiaotian Sun (Xiamen University) | Qingshan Xu (Nanyang Technological University) | Xinjie Yang (Xiamen University) | Yu Zang (Xiamen University) | Cheng Wang (Xiamen University),,,,,,,
Commonsense Prototype for Outdoor Unsupervised 3D Object Detection,,,,Hai Wu (Xiamen University) | Shijia Zhao (Xiamen University) | Xun Huang (Xiamen University) | Chenglu Wen (Xiamen University) | Xin Li (None) | Cheng Wang (Xiamen University),,,,,,,
Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds,,,,Zhimin Yuan (School Of Informatics Xiamen University) | Wankang Zeng (Xiamen University) | Yanfei Su (Xiamen University) | Weiquan Liu (Xiamen University) | Ming Cheng (Xiamen University) | Yulan Guo (SUN YAT-SEN UNIVERSITY) | Cheng Wang (Xiamen University),,,,,,,
DiffLoc: Diffusion Model for Outdoor LiDAR Localization,,,,Wen Li (Schoold Of Informatics Xiamen University) | Yuyang Yang (Xiamen University) | Shangshu Yu (Xiamen University) | Guosheng Hu (Oosto) | Chenglu Wen (Xiamen University) | Ming Cheng (Xiamen University) | Cheng Wang (Xiamen University),,,,,,,
LiSA: LiDAR Localization with Semantic Awareness,,,,Bochun Yang (Xiamen University) | Zijun Li (Xiamen University) | Wen Li (Schoold Of Informatics Xiamen University) | Zhipeng Cai (Intel Labs) | Chenglu Wen (Xiamen University) | Yu Zang (Xiamen University) | Matthias Mueller (None) | Cheng Wang (Xiamen University),,,,,,,
HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection,,,,Qiming Xia (XMU) | Wei Ye (Xiamen University) | Hai Wu (Xiamen University) | Shijia Zhao (Xiamen University) | Leyuan Xing (Xiamen University) | Xun Huang (Xiamen University) | Jinhao Deng (Xiamen University) | Xin Li (None) | Chenglu Wen (Xiamen University) | Cheng Wang (Xiamen University),,,,,,,
RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method,,,,Ming Yan (Xiamen University) | Yan Zhang (Xiamen University) | Shuqiang Cai (Xiamen University) | Shuqi Fan (Xiamen University) | Xincheng Lin (Xiamen University) | Yudi Dai (Xiamen University) | Siqi Shen (Xiamen University) | Chenglu Wen (Xiamen University) | Lan Xu (None) | Yuexin Ma (ShanghaiTech University) | Cheng Wang (Xiamen University),,,,,,,
Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training,"The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model's performance and lead to degenerated performance (i.e., negative transfer) compared to single-dataset training. In view of this challenge, we introduce Point Prompt Training (PPT), a novel framework for multi-dataset synergistic learning in the context of 3D representation learning that supports multiple pre-training paradigms. Based on this framework, we propose Prompt-driven Normalization, which adapts the model to different datasets with domain-specific prompts and Language-guided Categorical Alignment that decently unifies the multiple-dataset label spaces by leveraging the relationship between label text. Extensive experiments verify that PPT can overcome the negative transfer associated with synergistic learning and produce generalizable representations. Notably, it achieves state-of-the-art performance on each dataset using a single weight-shared model with supervised multi-dataset training. Moreover, when served as a pre-training framework, it outperforms other pre-training approaches regarding representation quality and attains remarkable state-of-the-art performance across over ten diverse downstream tasks spanning both indoor and outdoor 3D scenarios.",http://arxiv.org/abs/2308.09718v1,,Xiaoyang Wu (The University Of Hong Kong) | Zhuotao Tian (The Chinese University Of Hong Kong) | Xin Wen (The University Of Hong Kong) | Bohao Peng (The Chinese University Of Hong Kong) | Xihui Liu (The University Of Hong Kong) | Kaicheng Yu (Alibaba Group) | Hengshuang Zhao (The University Of Hong Kong),2023-08-18 17:59:57+00:00,,,,,,
"Point Transformer V3: Simpler, Faster, Stronger","This paper is not motivated to seek innovation within the attention mechanism. Instead, it focuses on overcoming the existing trade-offs between accuracy and efficiency within the context of point cloud processing, leveraging the power of scale. Drawing inspiration from recent advances in 3D large-scale representation learning, we recognize that model performance is more influenced by scale than by intricate design. Therefore, we present Point Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the accuracy of certain mechanisms that are minor to the overall performance after scaling, such as replacing the precise neighbor search by KNN with an efficient serialized neighbor mapping of point clouds organized with specific patterns. This principle enables significant scaling, expanding the receptive field from 16 to 1024 points while remaining efficient (a 3x increase in processing speed and a 10x improvement in memory efficiency compared with its predecessor, PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that span both indoor and outdoor scenarios. Further enhanced with multi-dataset joint training, PTv3 pushes these results to a higher level.",http://arxiv.org/abs/2312.10035v1,,Xiaoyang Wu (The University Of Hong Kong) | Li Jiang (Max Planck Institute For Informatics) | Peng-Shuai Wang (Peking University) | Zhijian Liu (Massachusetts Institute Of Technology) | Xihui Liu (The University Of Hong Kong) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Wanli Ouyang (University Of Sydney) | Tong He (Shanghai AI Lab) | Hengshuang Zhao (The University Of Hong Kong),2023-12-15 18:59:59+00:00,,,,,,
UniMODE: Universal Monocular 3D Object Detection,"Realizing unified monocular 3D object detection, including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse geometry properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird's-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the geometry learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D object detection.",http://arxiv.org/abs/2402.18573v1,,Zhuoling Li (University Of Hong Kong) | Xiaogang Xu (Zhejiang Lab) | Ser-Nam Lim (Meta AI) | Hengshuang Zhao (The University Of Hong Kong),2024-02-28 18:59:31+00:00,,,,,,
AnyDoor: Zero-shot Object-level Image Customization,"This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. Project page is https://damo-vilab.github.io/AnyDoor-Page/.",http://arxiv.org/abs/2307.09481v1,,"Xi Chen (The University Of Hong Kong, University Of Hong Kong) | Lianghua Huang (Alibaba Group) | Yu Liu (Alibaba Group) | Yujun Shen (The Chinese University Of Hong Kong) | Deli Zhao (Alibaba Group) | Hengshuang Zhao (The University Of Hong Kong)",2023-07-18 17:59:02+00:00,,,,,,
Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data,"This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.",http://arxiv.org/abs/2401.10891v1,,Lihe Yang (The University Of Hong Kong) | Bingyi Kang (TikTok) | Zilong Huang (Tencent GY Lab) | Xiaogang Xu (Zhejiang Lab) | Jiashi Feng (ByteDance) | Hengshuang Zhao (The University Of Hong Kong),2024-01-19 18:59:52+00:00,,,,,,
GPT4Point: A Unified Framework for Point-Language Understanding and Generation,"Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework. GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors. To support the expansive needs of 3D object-text pairs, we develop Pyramid-XL, a point-language dataset annotation engine. It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point. A comprehensive benchmark has been proposed to evaluate 3D point-language understanding capabilities. In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation.",http://arxiv.org/abs/2312.02980v1,,Zhangyang Qi (None) | Ye Fang (None) | Zeyi Sun (Shanghai Jiao Tong University) | Xiaoyang Wu (The University Of Hong Kong) | Tong Wu (None) | Jiaqi Wang (Shanghai AI Laboratory) | Dahua Lin (The Chinese University Of Hong Kong) | Hengshuang Zhao (The University Of Hong Kong),2023-12-05 18:59:55+00:00,,,,,,
Dense Optical Tracking: Connecting the Dots,"Recent approaches to point tracking are able to recover the trajectory of any scene point through a large portion of a video despite the presence of occlusions. They are, however, too slow in practice to track every point observed in a single frame in a reasonable amount of time. This paper introduces DOT, a novel, simple and efficient method for solving this problem. It first extracts a small set of tracks from key regions at motion boundaries using an off-the-shelf point tracking algorithm. Given source and target frames, DOT then computes rough initial estimates of a dense flow field and visibility mask through nearest-neighbor interpolation, before refining them using a learnable optical flow estimator that explicitly handles occlusions and can be trained on synthetic data with ground-truth correspondences. We show that DOT is significantly more accurate than current optical flow techniques, outperforms sophisticated ""universal"" trackers like OmniMotion, and is on par with, or better than, the best point tracking algorithms like CoTracker while being at least two orders of magnitude faster. Quantitative and qualitative experiments with synthetic and real videos validate the promise of the proposed approach. Code, data, and videos showcasing the capabilities of our approach are available in the project webpage: https://16lemoing.github.io/dot .",http://arxiv.org/abs/2312.00786v3,,Guillaume Le Moing (Inria) | Jean Ponce (Ecole Normale Sup??rieure De Paris) | Cordelia Schmid (Inria / Google),2023-12-01 18:59:59+00:00,,,,,,
Sparse Global Matching for Video Frame Interpolation with Large Motion,,,,"Chunxu Liu (Nanjing University) | Guozhen Zhang (Nanjing University) | Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) | Limin Wang (Nanjing University)",,,,,,,
SUGAR: Pre-training 3D Visual Representation for Robotics,,,,Shizhe Chen (INRIA) | Ricardo Garcia Pinel (INRIA) | Ivan Laptev (INRIA Paris) | Cordelia Schmid (Inria / Google),,,,,,,
A Generative Approach for Wikipedia-Scale Visual Entity Recognition,"In this paper, we address web-scale visual entity recognition, specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual-encoder models (eg CLIP), where all the entity names and query images are embedded into a unified space, paving the way for an approximate k-NN search. Alternatively, it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast, we introduce a novel Generative Entity Recognition (GER) framework, which given an input image learns to auto-regressively decode a semantic and discriminative ``code'' identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm, showcasing state-of-the-art performance on the challenging OVEN benchmark. GER surpasses strong captioning, dual-encoder, visual matching and hierarchical classification baselines, affirming its advantage in tackling the complexities of web-scale recognition.",http://arxiv.org/abs/2403.02041v1,,Mathilde Caron (Google) | Ahmet Iscen (Google) | Alireza Fathi (Google) | Cordelia Schmid (Inria / Google),2024-03-04 13:47:30+00:00,,,,,,
Adapting Short-Term Transformers for Action Detection in Untrimmed Videos,"Vision transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pre-training. Yet, it still remains unclear how to adapt these pre-trained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective cross-snippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone. For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.0 average mAP on THUMOS14, 37.12 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.",http://arxiv.org/abs/2312.01897v1,,Min Yang (None) | Gaohuan (Inchitech Company) | Ping Guo (Intel) | Limin Wang (Nanjing University),2023-12-04 13:51:16+00:00,,,,,,
SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos,,,,Tao Wu (None) | Runyu He (Nanjing University) | Gangshan Wu (Nanjing University) | Limin Wang (Nanjing University),,,,,,,
MoReVQA: Exploring Modular Reasoning Models for Video Question Answering,,,,Juhong Min (POSTECH) | Shyamal Buch (Google) | Arsha Nagrani (Google ) | Minsu Cho (POSTECH) | Cordelia Schmid (Inria / Google),,,,,,,
Dual DETRs for Multi-Label Temporal Action Detection,,,,Yuhan Zhu (None) | Guozhen Zhang (Nanjing University) | Jing Tan (The Chinese University Of Hong Kong) | Gangshan Wu (Nanjing University) | Limin Wang (Nanjing University),,,,,,,
BIVDiff: A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models,"Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and compute overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept. Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing. Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation video editing, video inpainting and outpainting. Our project page is available at https://bivdiff.github.io.",http://arxiv.org/abs/2312.02813v1,,Fengyuan Shi (Nanjing University) | Jiaxi Gu (Huawei Noah??S Ark Lab) | Hang Xu (Huawei Noah??S Ark Lab) | Songcen Xu (Huawei Noah's Ark Lab) | Wei Zhang (Huawei Technologies Ltd.) | Limin Wang (Nanjing University),2023-12-05 14:56:55+00:00,,,,,,
Asymmetric Masked Distillation for Pre-Training Small Foundation Models,"Self-supervised foundation models have shown great potential in computer vision thanks to the pre-training paradigm of masked autoencoding. Scale is a primary factor influencing the performance of these foundation models. However, these large foundation models often result in high computational cost that might limit their deployment. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation(AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model still with high masking ratio to the original masked pre-training. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the standard pre-training.",http://arxiv.org/abs/2311.03149v1,,Zhiyu Zhao (Nanjing University) | Bingkun Huang (Nanjing University) | Sen Xing (Tsinghua University) | Gangshan Wu (Nanjing University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Limin Wang (Nanjing University),2023-11-06 14:44:34+00:00,,,,,,
Streaming dense video captioning,,,,Xingyi Zhou (Google) | Anurag Arnab (Google) | Shyamal Buch (Google) | Shen Yan (Google Research) | Austin Myers (Google) | Xuehan Xiong (Google) | Arsha Nagrani (Google ) | Cordelia Schmid (Inria / Google),,,,,,,
Pixel Aligned Language Models,"Large language models have achieved great success in recent years, so as their variants in vision. Existing vision-language models can describe images in natural languages, answer visual-related questions, or perform complex reasoning about the image. However, it is yet unclear how localization tasks, such as word grounding or referring localization, can be performed using large language models. In this work, we aim to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs. When taking locations as inputs, the model performs location-conditioned captioning, which generates captions for the indicated object or region. When generating locations as outputs, our model regresses pixel coordinates for each output word generated by the language model, and thus performs dense word grounding. Our model is pre-trained on the Localized Narrative dataset, which contains pixel-word-aligned captioning from human attention. We show our model can be applied to various location-aware vision-language tasks, including referring localization, location-conditioned captioning, and dense object captioning, archiving state-of-the-art performance on RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .",http://arxiv.org/abs/2312.09237v1,,"Jiarui Xu (University Of California, San Diego) | Xingyi Zhou (Google) | Shen Yan (Google Research) | Xiuye Gu (None) | Anurag Arnab (Google) | Chen Sun (Brown University) | Xiaolong Wang (UCSD) | Cordelia Schmid (Inria / Google)",2023-12-14 18:57:58+00:00,,,,,,
UniPTS: A Unified Framework for Proficient Post-Training Sparsity,,,,JingJing Xie (Xiamen University) | Yuxin Zhang (Xiamen University) | Mingbao Lin (Xiamen University) | ZhiHang Lin (Xiamen University) | Liujuan Cao (Xiamen University) | Rongrong Ji (Xiamen University),,,,,,,
Autoregressive Queries for Adaptive Tracking with Spatio-Temporal Transformers,,,,Jinxia Xie (Guangxi Normal University) | Bineng Zhong (Guangxi Normal University) | Zhiyi Mo (Wuzhou University) | Shengping Zhang (Harbin Institute Of Technology) | Liangtao Shi (Guangxi Normal University) | Shuxiang Song (Guangxi Normal University) | Rongrong Ji (Xiamen University),,,,,,,
Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation,"Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that combines computer vision and natural language processing, delineating specific regions in aerial images as described by textual queries. Traditional Referring Image Segmentation (RIS) approaches have been impeded by the complex spatial scales and orientations found in aerial imagery, leading to suboptimal segmentation results. To address these challenges, we introduce the Rotated Multi-Scale Interaction Network (RMSIN), an innovative approach designed for the unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction Module (IIM) to effectively address the fine-grained detail required at multiple scales and a Cross-scale Interaction Module (CIM) for integrating these details coherently across the network. Furthermore, RMSIN employs an Adaptive Rotated Convolution (ARC) to account for the diverse orientations of objects, a novel contribution that significantly enhances segmentation accuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset comprising 17,402 image-caption-mask triplets, which is unparalleled in terms of scale and variety. This dataset not only presents the model with a wide range of spatial and rotational scenarios but also establishes a stringent benchmark for the RRSIS task, ensuring a rigorous evaluation of performance. Our experimental evaluations demonstrate the exceptional performance of RMSIN, surpassing existing state-of-the-art models by a significant margin. All datasets and code are made available at https://github.com/Lsan2401/RMSIN.",http://arxiv.org/abs/2312.12470v2,,Sihan Liu (Xiamen University) | Yiwei Ma (Xiamen University) | Xiaoqing Zhang (Xiamen University) | Haowei Wang (Xiamen University) | Jiayi Ji (Xiamen University) | Xiaoshuai Sun (Xiamen University) | Rongrong Ji (Xiamen University),2023-12-19 08:14:14+00:00,,,,,,
FocSAM: Delving Deeply into Focused Objects in Segmenting Anything,,,,"You Huang (Xiamen University) | Zongyu Lan (Xiamen University) | Liujuan Cao (Xiamen University) | Xianming Lin (None) | Shengchuan Zhang (None) | Guannan Jiang (Contemporary Amperex Technology Co., Limited) | Rongrong Ji (Xiamen University)",,,,,,,
DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model,,,,Lirui Zhao (Xiamen University) | Yue Yang (Shanghai Jiao Tong University) | Kaipeng Zhang (Shanghai AI Laboratory) | Wenqi Shao (The Chinese University Of Hong Kong) | Yuxin Zhang (Xiamen University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Ping Luo (The University Of Hong Kong) | Rongrong Ji (Xiamen University),,,,,,,
Aligning and Prompting Everything All at Once for Universal Visual Perception,"Vision foundation models have been explored recently to build general-purpose vision systems. However, predominant paradigms, driven by casting instance-level tasks as an object-word alignment, bring heavy cross-modality interaction, which is not effective in prompting object detection and visual grounding. Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff, and suffers from mutual interference between foreground-object and background-class segmentation. In stark contrast to the prevailing methods, we present APE, a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks, i.e., detection, segmentation, and grounding, as an instance-level sentence-object matching paradigm. Specifically, APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection, which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion. To bridge the granularity gap of different pixel-level tasks, APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances. APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning. The extensive experiments on over 160 datasets demonstrate that, with only one-suit of weights, APE outperforms (or is on par with) the state-of-the-art models, proving that an effective yet universal perception for anything aligning and prompting is indeed feasible. Codes and trained models are released at https://github.com/shenyunhang/APE.",http://arxiv.org/abs/2312.02153v1,,"Yunhang Shen (Tencent) | Chaoyou Fu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Peixian Chen (Xiamen University) | Mengdan Zhang (Tencent Youtu Lab) | Ke Li (Tencent) | Xing Sun (Tencent YouTu Lab) | Yunsheng Wu (Tencent YouTu Lab) | Shaohui Lin (East China Normal University) | Rongrong Ji (Xiamen University)",2023-12-04 18:59:50+00:00,,,,,,
PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization,"Recent advancements in personalized image generation using diffusion models have been noteworthy. However, existing methods suffer from inefficiencies due to the requirement for subject-specific fine-tuning. This computationally intensive process hinders efficient deployment, limiting practical usability. Moreover, these methods often grapple with identity distortion and limited expression diversity. In light of these challenges, we propose PortraitBooth, an innovative approach designed for high efficiency, robust identity preservation, and expression-editable text-to-image generation, without the need for fine-tuning. PortraitBooth leverages subject embeddings from a face recognition model for personalized image generation without fine-tuning. It eliminates computational overhead and mitigates identity distortion. The introduced dynamic identity preservation strategy further ensures close resemblance to the original image identity. Moreover, PortraitBooth incorporates emotion-aware cross-attention control for diverse facial expressions in generated images, supporting text-driven expression editing. Its scalability enables efficient and high-quality image creation, including multi-subject generation. Extensive results demonstrate superior performance over other state-of-the-art methods in both single and multiple image generation scenarios.",http://arxiv.org/abs/2312.06354v1,,Xu Peng (Xiamen University) | Junwei Zhu (Tencent Youtu Lab) | Boyuan Jiang (Tencent Youtu Lab) | Ying Tai (Nanjing University) | Donghao Luo (Tencent YouTu Lab) | Jiangning Zhang (Tencent Youtu Lab) | Wei Lin (Xiamen University) | Taisong Jin (Xiamen University) | Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University) | Rongrong Ji (Xiamen University),2023-12-11 13:03:29+00:00,,,,,,
Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing,"Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.",http://arxiv.org/abs/2310.12153v1,,Jan-Nico Zaech (ETH Zurich) | Martin Danelljan (ETH Zurich) | Tolga Birdal (Imperial College London) | Luc Van Gool (ETH Zurich),2023-10-18 17:59:45+00:00,,,,,,
Real-World Mobile Image Denoising Dataset with Efficient Baselines,,,,Roman Flepp (ETH Zurich) | Andrey Ignatov (None) | Radu Timofte (University Of W??rzburg) | Luc Van Gool (ETH Zurich),,,,,,,
Continuous Pose for Monocular Cameras in Neural Implicit Representation,"In this paper, we showcase the effectiveness of optimizing monocular camera poses as a continuous function of time. The camera poses are represented using an implicit neural function which maps the given time to the corresponding camera pose. The mapped camera poses are then used for the downstream tasks where joint camera pose optimization is also required. While doing so, the network parameters -- that implicitly represent camera poses -- are optimized. We exploit the proposed method in four diverse experimental settings, namely, (1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all four settings, the proposed method performs significantly better than the compared baselines and the state-of-the-art methods. Additionally, using the assumption of continuous motion, changes in pose may actually live in a manifold that has lower than 6 degrees of freedom (DOF) is also realized. We call this low DOF motion representation as the \emph{intrinsic motion} and use the approach in vSLAM settings, showing impressive camera tracking performance.",http://arxiv.org/abs/2311.17119v3,,Qi Ma (ETHZ - ETH Zurich) | Danda Paudel (None) | Ajad Chhatkuli (Swiss Federal Institute Of Technology) | Luc Van Gool (ETH Zurich),2023-11-28 13:14:58+00:00,,,,,,
Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes,,,,Diandian Guo (Universit??t Stuttgart) | Deng-Ping Fan (ETH Zurich) | Tongyu Lu (ETHZ - ETH Zurich) | Christos Sakaridis (ETH Zurich) | Luc Van Gool (ETH Zurich),,,,,,,
Deep Equilibrium Diffusion Restoration with Parallel Sampling,"Diffusion-based image restoration (IR) methods aim to use diffusion models to recover high-quality (HQ) images from degraded images and achieve promising performance. Due to the inherent property of diffusion models, most of these methods need long serial sampling chains to restore HQ images step-by-step. As a result, it leads to expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between the restoration results and the inputs since it is hard to compute the gradients in the whole chains. In this work, we aim to rethink the diffusion-based IR models through a different perspective, i.e., a deep equilibrium (DEQ) fixed point system. Specifically, we derive an analytical solution by modeling the entire sampling chain in diffusion-based IR models as a joint multivariate fixed point system. With the help of the analytical solution, we are able to conduct single-image sampling in a parallel way and restore HQ images without training. Furthermore, we compute fast gradients in DEQ and found that initialization optimization can boost performance and control the generation direction. Extensive experiments on benchmarks demonstrate the effectiveness of our proposed method on typical IR tasks and real-world settings. The code and models will be made publicly available.",http://arxiv.org/abs/2311.11600v1,,Jiezhang Cao (ETH Zurich) | Yue Shi (Shanghai Jiao Tong University) | Kai Zhang (None) | Yulun Zhang (ETH Zurich) | Radu Timofte (University Of W??rzburg) | Luc Van Gool (ETH Zurich),2023-11-20 08:27:56+00:00,,,,,,
Equivariant Multi-Modality Image Fusion,"Multi-modality image fusion is a technique used to combine information from different sensors or modalities, allowing the fused image to retain complementary features from each modality, such as functional highlights and texture details. However, effectively training such fusion models is difficult due to the lack of ground truth fusion data. To address this issue, we propose the Equivariant Multi-Modality imAge fusion (EMMA) paradigm for end-to-end self-supervised learning. Our approach is based on the prior knowledge that natural images are equivariant to specific transformations. Thus, we introduce a novel training framework that includes a fusion module and a learnable pseudo-sensing module, which allow the network training to follow the principles of physical sensing and imaging process, and meanwhile satisfy the equivariant prior for natural images. Our extensive experiments demonstrate that our method produces high-quality fusion results for both infrared-visible and medical images, while facilitating downstream multi-modal segmentation and detection tasks. The code will be released.",http://arxiv.org/abs/2305.11443v1,,Zixiang Zhao (Xi'an Jiao Tong University) | Haowen Bai (Xi'an Jiao Tong University) | Jiangshe Zhang (Xi'an Jiao Tong University) | Yulun Zhang (ETH Zurich) | Kai Zhang (None) | Shuang Xu (Northwest Polytechnical University Xi'an) | Dongdong Chen (Heriot-Watt University) | Radu Timofte (University Of W??rzburg) | Luc Van Gool (ETH Zurich),2023-05-19 05:50:24+00:00,,,,,,
Relightable and Animatable Neural Avatar from Sparse-View Video,"This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse distances based on a parametric human model and compute fine distances by exploiting the local deformation invariance of SDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently estimate the surface intersection and light visibility. This allows us to develop the first system to recover animatable and relightable neural avatars from sparse view (or monocular) inputs. Experiments demonstrate that our approach is able to produce superior results compared to state-of-the-art methods. Our code will be released for reproducibility.",http://arxiv.org/abs/2308.07903v2,,Zhen Xu (Zhejiang University) | Sida Peng (None) | Chen Geng (Stanford University) | Linzhan Mou (Zhejiang University) | Zihan Yan (University Of Illinois Urbana-Champaign) | Jiaming Sun (Image Derivative) | Hujun Bao (Zhejiang University) | Xiaowei Zhou (None),2023-08-15 17:42:39+00:00,,,,,,
4K4D: Real-Time 4D View Synthesis at 4K Resolution,"This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution. Recently, some methods on dynamic view synthesis have shown impressive rendering quality. However, their speed is still limited when rendering high-resolution images. To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed. Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized. In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency. Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos. Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality. Our project page is available at https://zju3dv.github.io/4k4d/.",http://arxiv.org/abs/2310.11448v3,,Zhen Xu (Zhejiang University) | Sida Peng (None) | Haotong Lin (None) | Guangzhao He (Zhejiang University) | Jiaming Sun (Image Derivative) | Yujun Shen (The Chinese University Of Hong Kong) | Hujun Bao (Zhejiang University) | Xiaowei Zhou (None),2023-10-17 17:57:38+00:00,,,,,,
Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed,"We present a novel method for efficiently producing semi-dense matches across images. Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. One key observation is that performing the transformer over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. Furthermore, we find spatial variance exists in LoFTR's fine correlation module, which is adverse to matching accuracy. A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. Our efficiency optimized model is $\sim 2.5\times$ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. Project page: https://zju3dv.github.io/efficientloftr.",http://arxiv.org/abs/2403.04765v2,,Yifan Wang (None) | Xingyi He (Zhejiang University) | Sida Peng (None) | Dongli Tan (Zhejiang University) | Xiaowei Zhou (None),2024-03-07 18:58:40+00:00,,,,,,
Detector-Free Structure from Motion,,,,Xingyi He (Zhejiang University) | Jiaming Sun (Image Derivative) | Yifan Wang (None) | Sida Peng (None) | Qixing Huang (University Of Texas At Austin) | Hujun Bao (Zhejiang University) | Xiaowei Zhou (None),,,,,,,
SpatialTracker: Tracking Any 2D Pixels in 3D Space,,,,Yuxi Xiao (Zhejiang University) | Qianqian Wang (Cornell University) | Shangzhan Zhang (None) | Nan Xue (Ant Group) | Sida Peng (None) | Yujun Shen (The Chinese University Of Hong Kong) | Xiaowei Zhou (None),,,,,,,
Generating Human Motion in 3D Scenes from Text Descriptions,,,,Zhi Cen (Zhejiang University) | Huaijin Pi (Zhejiang University) | Sida Peng (None) | Zehong Shen (Zhejiang University) | Minghui Yang (Ant Group) | Shuai Zhu (Ant Group) | Hujun Bao (Zhejiang University) | Xiaowei Zhou (None),,,,,,,
When StyleGAN Meets Stable Diffusion: a W +  Adapter for Personalized Image Generation ,"Text-to-image diffusion models have remarkably excelled in producing diverse, high-quality, and photo-realistic images. This advancement has spurred a growing interest in incorporating specific identities into generated content. Most current methods employ an inversion approach to embed a target visual concept into the text embedding space using a single reference image. However, the newly synthesized faces either closely resemble the reference image in terms of facial attributes, such as expression, or exhibit a reduced capacity for identity preservation. Text descriptions intended to guide the facial attributes of the synthesized face may fall short, owing to the intricate entanglement of identity information with identity-irrelevant facial attributes derived from the reference image. To address these issues, we present the novel use of the extended StyleGAN embedding space $\mathcal{W}_+$, to achieve enhanced identity preservation and disentanglement for diffusion models. By aligning this semantically meaningful human face latent space with text-to-image diffusion models, we succeed in maintaining high fidelity in identity preservation, coupled with the capacity for semantic editing. Additionally, we propose new training objectives to balance the influences of both prompt and identity conditions, ensuring that the identity-irrelevant background remains unaffected during facial attribute modifications. Extensive experiments reveal that our method adeptly generates personalized text-to-image outputs that are not only compatible with prompt descriptions but also amenable to common StyleGAN editing directions in diverse settings. Our source code will be available at \url{https://github.com/csxmli2016/w-plus-adapter}.",http://arxiv.org/abs/2311.17461v1,,Xiaoming Li (MMLab@NTU) | Xinyu Hou (Nanyang Technological University) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY),2023-11-29 09:05:14+00:00,,,,,,
FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation,,,,Shuai Yang (Nanyang Technological University) | Yifan Zhou (Nanyang Technological University) | Ziwei Liu (Nanyang Technological University) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY),,,,,,,
Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution,"Text-based diffusion models have exhibited remarkable success in generation and editing, showing great promise for enhancing visual content with their generative prior. However, applying these models to video super-resolution remains challenging due to the high demands for output fidelity and temporal consistency, which is complicated by the inherent randomness in diffusion models. Our study introduces Upscale-A-Video, a text-guided latent diffusion framework for video upscaling. This framework ensures temporal coherence through two key mechanisms: locally, it integrates temporal layers into U-Net and VAE-Decoder, maintaining consistency within short sequences; globally, without training, a flow-guided recurrent latent propagation module is introduced to enhance overall video stability by propagating and fusing latent across the entire sequences. Thanks to the diffusion paradigm, our model also offers greater flexibility by allowing text prompts to guide texture creation and adjustable noise levels to balance restoration and generation, enabling a trade-off between fidelity and quality. Extensive experiments show that Upscale-A-Video surpasses existing methods in both synthetic and real-world benchmarks, as well as in AI-generated videos, showcasing impressive visual realism and temporal consistency.",http://arxiv.org/abs/2312.06640v1,,"Shangchen Zhou (Nanyang Technological University) | Peiqing Yang (S-Lab, Nanyang Technological University) | Jianyi Wang (Nanyang Technological University) | Yihang Luo (Nanyang Technological University) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)",2023-12-11 18:54:52+00:00,,,,,,
Learning Inclusion Matching for Animation Paint Bucket Colorization,,,,Yuekun Dai (Nanyang Technological University) | Shangchen Zhou (Nanyang Technological University) | Blake Li (Nanyang Technological University) | Chongyi Li (None) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY),,,,,,,
OMG-Seg: Is One Model Good Enough For All Segmentation?,"In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models. We propose OMG-Seg, One Model that is Good enough to efficiently and effectively handle all the segmentation tasks, including image semantic, instance, and panoptic segmentation, as well as their video counterparts, open vocabulary settings, prompt-driven, interactive segmentation like SAM, and video object segmentation. To our knowledge, this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg, a transformer-based encoder-decoder architecture with task-specific queries and outputs, can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.",http://arxiv.org/abs/2401.10229v1,,Xiangtai Li (Nanyang Technological University) | Haobo Yuan (Nanyang Technological University) | Wei Li (Nanyang Technological University) | Henghui Ding (Fudan University) | Size Wu (Nanyang Technological University) | Wenwei Zhang (None) | Yining Li (Shanghai AI Laboratory) | Kai Chen (Shanghai AI Laboratory) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY),2024-01-18 18:59:34+00:00,,,,,,
Towards Language-Driven Video Inpainting via Multimodal Large Language Models,"We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.",http://arxiv.org/abs/2401.10226v1,,Jianzong Wu (Peking University) | Xiangtai Li (Nanyang Technological University) | Chenyang Si (Nanyang Technological University Singapore) | Shangchen Zhou (Nanyang Technological University) | Jingkang Yang (Nanyang Technological University) | Jiangning Zhang (Tencent Youtu Lab) | Yining Li (Shanghai AI Laboratory) | Kai Chen (Shanghai AI Laboratory) | Yunhai Tong (Peking University) | Ziwei Liu (Nanyang Technological University) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY),2024-01-18 18:59:13+00:00,,,,,,
IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing,"We present IntrinsicAvatar, a novel approach to recovering the intrinsic properties of clothed human avatars including geometry, albedo, material, and environment lighting from only monocular videos. Recent advancements in human-based neural rendering have enabled high-quality geometry and appearance reconstruction of clothed humans from just monocular videos. However, these methods bake intrinsic properties such as albedo, material, and environment lighting into a single entangled neural representation. On the other hand, only a handful of works tackle the problem of estimating geometry and disentangled appearance properties of clothed humans from monocular videos. They usually achieve limited quality and disentanglement due to approximations of secondary shading effects via learned MLPs. In this work, we propose to model secondary shading effects explicitly via Monte-Carlo ray tracing. We model the rendering process of clothed humans as a volumetric scattering process, and combine ray tracing with body articulation. Our approach can recover high-quality geometry, albedo, material, and lighting properties of clothed humans from a single monocular video, without requiring supervised pre-training using ground truth materials. Furthermore, since we explicitly model the volumetric scattering process and ray tracing, our model naturally generalizes to novel poses, enabling animation of the reconstructed avatar in novel lighting conditions.",http://arxiv.org/abs/2312.05210v1,,Shaofei Wang (None) | Bozidar Antic (Eberhard-Karls-Universit??t T??bingen) | Andreas Geiger (University Of T??bingen) | Siyu Tang (ETH Zurich),2023-12-08 17:58:14+00:00,,,,,,
3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting,"We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.",http://arxiv.org/abs/2312.09228v2,,"Zhiyin Qian (Department Of Computer Science, ETHZ - ETH Zurich) | Shaofei Wang (None) | Marko Mihajlovic (Swiss Federal Institute Of Technology) | Andreas Geiger (University Of T??bingen) | Siyu Tang (ETH Zurich)",2023-12-14 18:54:32+00:00,,,,,,
Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation,"Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.",http://arxiv.org/abs/2401.04728v1,,Xiyi Chen (ETH Zurich) | Marko Mihajlovic (Swiss Federal Institute Of Technology) | Shaofei Wang (None) | Sergey Prokudin (ETHZ - ETH Zurich) | Siyu Tang (ETH Zurich),2024-01-09 18:59:04+00:00,,,,,,
Degree-of-Freedom Matters: Inferring Dynamics from Point Trajectories,,,,Yan Zhang (ETH Zurich) | Sergey Prokudin (ETHZ - ETH Zurich) | Marko Mihajlovic (Swiss Federal Institute Of Technology) | Qianli Ma (NVIDIA Research) | Siyu Tang (ETH Zurich),,,,,,,
Optimizing Diffusion Noise Can Serve As Universal Motion Priors,"We propose Diffusion Noise Optimization (DNO), a new method that effectively leverages existing motion diffusion models as motion priors for a wide range of motion-related tasks. Instead of training a task-specific diffusion model for each new task, DNO operates by optimizing the diffusion latent noise of an existing pre-trained text-to-motion model. Given the corresponding latent noise of a human motion, it propagates the gradient from the target criteria defined on the motion space through the whole denoising process to update the diffusion latent noise. As a result, DNO supports any use cases where criteria can be defined as a function of motion. In particular, we show that, for motion editing and control, DNO outperforms existing methods in both achieving the objective and preserving the motion content. DNO accommodates a diverse range of editing modes, including changing trajectory, pose, joint locations, or avoiding newly added obstacles. In addition, DNO is effective in motion denoising and completion, producing smooth and realistic motion from noisy and partial inputs. DNO achieves these results at inference time without the need for model retraining, offering great versatility for any defined reward or loss function on the motion representation.",http://arxiv.org/abs/2312.11994v1,,"Korrawe Karunratanakul (ETH Zurich) | Konpat Preechakul (University Of California, Berkeley) | Emre Aksan (Google) | Thabo Beeler (Google) | Supasorn Suwajanakorn (Vidyasirimedhi Institute Of Science And Technology) | Siyu Tang (ETH Zurich)",2023-12-19 09:37:25+00:00,,,,,,
EgoGen: An Egocentric Synthetic Data Generator,"Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate EgoGen's efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. EgoGen will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. Refer to our project page: https://ego-gen.github.io/.",http://arxiv.org/abs/2401.08739v1,,"Gen Li (ETH Zurich) | Kaifeng Zhao (ETHZ - ETH Zurich) | Siwei Zhang (None) | Xiaozhong Lyu (Department Of Computer Science, ETHZ - ETH Zurich) | Mihai Dusmanu (Microsoft) | Yan Zhang (ETH Zurich) | Marc Pollefeys (ETH Zurich / Microsoft) | Siyu Tang (ETH Zurich)",2024-01-16 18:55:22+00:00,,,,,,
Finsler-Laplace-Beltrami Operators with Application to Shape Analysis,,,,Simon Weber (Technische Universit??t M??nchen) | Thomas Dag??s (Technion - Israel Institute Of Technology) | Maolin Gao (None) | Daniel Cremers (Technical University Munich),,,,,,,
Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincar?? Ball,,,,Simon Weber (Technische Universit??t M??nchen) | Bar???? Z??ng??r (Technische Universit??t M??nchen) | Nikita Araslanov (TU Munich) | Daniel Cremers (Technical University Munich),,,,,,,
Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation,,,,Keonhee Han (Fraunhofer IIS) | Dominik Muhle (Technical University Of Munich) | Felix Wimbauer (Technical University Of Munich) | Daniel Cremers (Technical University Munich),,,,,,,
"Sparse views, Near light: A practical paradigm for uncalibrated point-light photometric stereo",,,,Mohammed Brahimi (Technische Universit??t M??nchen) | Bjoern Haefner (Technical University Munich) | Zhenzhang Ye (Technische Universit??t M??nchen) | Bastian Goldluecke (University Of Konstanz) | Daniel Cremers (Technical University Munich),,,,,,,
Text2Loc: 3D Point Cloud Localization from Natural Language,"We tackle the problem of 3D point cloud localization based on a few natural linguistic descriptions and introduce a novel neural network, Text2Loc, that fully interprets the semantic relationship between points and text. Text2Loc follows a coarse-to-fine localization pipeline: text-submap global place recognition, followed by fine localization. In global place recognition, relational dynamics among each textual hint are captured in a hierarchical transformer with max-pooling (HTM), whereas a balance between positive and negative pairs is maintained using text-submap contrastive learning. Moreover, we propose a novel matching-free fine localization method to further refine the location predictions, which completely removes the need for complicated text-instance matching and is lighter, faster, and more accurate than previous methods. Extensive experiments show that Text2Loc improves the localization accuracy by up to $2\times$ over the state-of-the-art on the KITTI360Pose dataset. We will make the code publicly available.",http://arxiv.org/abs/2311.15977v1,,Yan Xia (Technical University Of Munich) | Letian Shi (Technische Universit??t M??nchen) | Zifeng Ding (LMU Munich) | Jo??o F. Henriques (University Of Oxford) | Daniel Cremers (Technical University Munich),2023-11-27 16:23:01+00:00,,,,,,
Deep Imbalanced Regression via Hierarchical Classification Adjustment,"Regression tasks in computer vision, such as age estimation or counting, are often formulated into classification by quantizing the target space into classes. Yet real-world data is often imbalanced -- the majority of training samples lie in a head range of target values, while a minority of samples span a usually larger tail range. By selecting the class quantization, one can adjust imbalanced regression targets into balanced classification outputs, though there are trade-offs in balancing classification accuracy and quantization error. To improve regression performance over the entire range of data, we propose to construct hierarchical classifiers for solving imbalanced regression tasks. The fine-grained classifiers limit the quantization error while being modulated by the coarse predictions to ensure high accuracy. Standard hierarchical classification approaches, however, when applied to the regression problem, fail to ensure that predicted ranges remain consistent across the hierarchy. As such, we propose a range-preserving distillation process that can effectively learn a single classifier from the set of hierarchical classifiers. Our novel hierarchical classification adjustment (HCA) for imbalanced regression shows superior results on three diverse tasks: age estimation, crowd counting and depth estimation. We will release the source code upon acceptance.",http://arxiv.org/abs/2310.17154v1,,Haipeng Xiong (National University Of Singapore) | Angela Yao (National University Of Singapore),2023-10-26 04:54:39+00:00,,,,,,
Coherent Temporal Synthesis for Incremental Action Segmentation,"Data replay is a successful incremental learning technique for images. It prevents catastrophic forgetting by keeping a reservoir of previous data, original or synthesized, to ensure the model retains past knowledge while adapting to novel concepts. However, its application in the video domain is rudimentary, as it simply stores frame exemplars for action recognition. This paper presents the first exploration of video data replay techniques for incremental action segmentation, focusing on action temporal modeling. We propose a Temporally Coherent Action (TCA) model, which represents actions using a generative model instead of storing individual frames. The integration of a conditioning variable that captures temporal coherence allows our model to understand the evolution of action features over time. Therefore, action segments generated by TCA for replay are diverse and temporally coherent. In a 10-task incremental setup on the Breakfast dataset, our approach achieves significant increases in accuracy for up to 22% compared to the baselines.",http://arxiv.org/abs/2403.06102v1,,Guodong Ding (Natioal University Of Singapore) | Hans Golong (National University Of Singapore) | Angela Yao (National University Of Singapore),2024-03-10 06:07:06+00:00,,,,,,
KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation,,,,Fengyuan Yang (National University Of Singapore) | Kerui Gu (National University Of Singapore) | Angela Yao (National University Of Singapore),,,,,,,
Enhancing Video Super-Resolution via Implicit Resampling-based Alignment,"In video super-resolution, it is common to use a frame-wise alignment to support the propagation of information over time. The role of alignment is well-studied for low-level enhancement in video, but existing works overlook a critical step -- resampling. We show through extensive experiments that for alignment to be effective, the resampling should preserve the reference frequency spectrum while minimizing spatial distortions. However, most existing works simply use a default choice of bilinear interpolation for resampling even though bilinear interpolation has a smoothing effect and hinders super-resolution. From these observations, we propose an implicit resampling-based alignment. The sampling positions are encoded by a sinusoidal positional encoding, while the value is estimated with a coordinate network and a window-based cross-attention. We show that bilinear interpolation inherently attenuates high-frequency information while an MLP-based coordinate network can approximate more frequencies. Experiments on synthetic and real-world datasets show that alignment with our proposed implicit resampling enhances the performance of state-of-the-art frameworks with minimal impact on both compute and parameters.",http://arxiv.org/abs/2305.00163v2,,Kai Xu (National University Of Singapore) | Ziwei Yu (None) | Xin Wang (Huawei Technologies Ltd.) | Michael Bi Mi (Huawei Technologies Ltd.) | Angela Yao (National University Of Singapore),2023-04-29 03:59:36+00:00,,,,,,
Make Me a BNN: A Simple Strategy for Estimating Bayesian Uncertainty from Pre-trained Models,"Deep Neural Networks (DNNs) are powerful tools for various computer vision tasks, yet they often struggle with reliable uncertainty quantification - a critical requirement for real-world applications. Bayesian Neural Networks (BNN) are equipped for uncertainty estimation but cannot scale to large DNNs that are highly unstable to train. To address this challenge, we introduce the Adaptable Bayesian Neural Network (ABNN), a simple and scalable strategy to seamlessly transform DNNs into BNNs in a post-hoc manner with minimal computational and training overheads. ABNN preserves the main predictive properties of DNNs while enhancing their uncertainty quantification abilities through simple BNN adaptation layers (attached to normalization layers) and a few fine-tuning steps on pre-trained models. We conduct extensive experiments across multiple datasets for image classification and semantic segmentation tasks, and our results demonstrate that ABNN achieves state-of-the-art performance without the computational budget typically associated with ensemble methods.",http://arxiv.org/abs/2312.15297v1,,Gianni Franchi (ENSTA Paris) | Olivier Laurent (Universit?? Paris-Saclay) | Maxence Legu??ry (ENSTA Paris) | Andrei Bursuc (Valeo.Ai) | Andrea Pilzer (NVIDIA) | Angela Yao (National University Of Singapore),2023-12-23 16:39:24+00:00,,,,,,
CG-HOI: Contact-Guided 3D Human-Object Interaction Generation,"We propose CG-HOI, the first method to address the task of generating dynamic 3D human-object interactions (HOIs) from text. We model the motion of both human and object in an interdependent fashion, as semantically rich human motion rarely happens in isolation without any interactions. Our key insight is that explicitly modeling contact between the human body surface and object geometry can be used as strong proxy guidance, both during training and inference. Using this guidance to bridge human and object motion enables generating more realistic and physically plausible interaction sequences, where the human body and corresponding object move in a coherent manner. Our method first learns to model human motion, object motion, and contact in a joint diffusion process, inter-correlated through cross-attention. We then leverage this learned contact for guidance during inference synthesis of realistic, coherent HOIs. Extensive evaluation shows that our joint contact-based human-object interaction approach generates realistic and physically plausible sequences, and we show two applications highlighting the capabilities of our method. Conditioned on a given object trajectory, we can generate the corresponding human motion without re-training, demonstrating strong human-object interdependency learning. Our approach is also flexible, and can be applied to static real-world 3D scene scans.",http://arxiv.org/abs/2311.16097v1,,Christian Diller (Technische Universit??t M??nchen) | Angela Dai (None),2023-11-27 18:59:10+00:00,,,,,,
FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations,"We present a generative approach to forecast long-term future human behavior in 3D, requiring only weak supervision from readily available 2D human action data. This is a fundamental task enabling many downstream applications. The required ground-truth data is hard to capture in 3D (mocap suits, expensive setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our method to only require 2D RGB data while being able to generate 3D human motion sequences. We use a differentiable 2D projection scheme in an autoregressive manner for weak supervision, and an adversarial loss for 3D regularization. Our method predicts long and complex behavior sequences (e.g. cooking, assembly) consisting of multiple sub-actions. We tackle this in a semantically hierarchical manner, jointly predicting high-level coarse action labels together with their low-level fine-grained realizations as characteristic 3D human poses. We observe that these two action representations are coupled in nature, and joint prediction benefits both action and pose forecasting. Our experiments demonstrate the complementary nature of joint action and 3D pose prediction: our joint approach outperforms each task treated individually, enables robust longer-term sequence prediction, and outperforms alternative approaches to forecast actions and characteristic 3D poses.",http://arxiv.org/abs/2211.14309v2,,Christian Diller (Technische Universit??t M??nchen) | Thomas Funkhouser (Princeton University) | Angela Dai (None),2022-11-25 18:59:53+00:00,,,,,,
GenZI: Zero-Shot 3D Human-Scene Interaction Generation,"Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.",http://arxiv.org/abs/2311.17737v1,,Lei Li (Technical University Of Munich) | Angela Dai (None),2023-11-29 15:40:11+00:00,,,,,,
UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes,"3D instance segmentation is fundamental to geometric understanding of the world around us. Existing methods for instance segmentation of 3D scenes rely on supervision from expensive, manual 3D annotations. We propose UnScene3D, the first fully unsupervised 3D learning approach for class-agnostic 3D instance segmentation of indoor scans. UnScene3D first generates pseudo masks by leveraging self-supervised color and geometry features to find potential object regions. We operate on a basis of geometric oversegmentation, enabling efficient representation and learning on high-resolution 3D data. The coarse proposals are then refined through self-training our model on its predictions. Our approach improves over state-of-the-art unsupervised 3D instance segmentation methods by more than 300% Average Precision score, demonstrating effective instance segmentation even in challenging, cluttered 3D scenes.",http://arxiv.org/abs/2303.14541v1,,David Rozenberszki (None) | Or Litany (NVIDIA / Technion) | Angela Dai (None),2023-03-25 19:15:16+00:00,,,,,,
Restricted Memory Banks Improve Video Object Segmentation: A Revisit,,,,Junbao Zhou (None) | Ziqi Pang (UIUC) | Yu-Xiong Wang (None),,,,,,,
Situational Awareness Matters in 3D Vision Language Reasoning,,,,"Yunze Man (Department Of Computer Science, University Of Illinois At Urbana-Champaign) | Liang-Yan Gui (UIUC) | Yu-Xiong Wang (None)",,,,,,,
Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion,,,,Linzhan Mou (Zhejiang University) | Jun-Kun Chen (None) | Yu-Xiong Wang (None),,,,,,,
TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding,"The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates multi-modal learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language representations have been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing multi-modal 3D representation learning methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter Multi-Modal Learning (TAMM) -- a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual representations of CLIP for synthetic image-text pairs. Subsequently, our Dual Adapters decouple the 3D shape representation space into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective multi-modal pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D representations for a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the zero-shot classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \url{https://alanzhangcs.github.io/tamm-page}.",http://arxiv.org/abs/2402.18490v1,,Zhihao Zhang (Xi'an Jiao Tong University) | Shengcao Cao (University Of Illinois At Urbana-Champaign) | Yu-Xiong Wang (None),2024-02-28 17:18:38+00:00,,,,,,
ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing,,,,Jun-Kun Chen (None) | Samuel Rota Bul?? (Meta) | Norman M??ller (Meta) | Lorenzo Porzi (Facebook) | Peter Kontschieder (Meta) | Yu-Xiong Wang (None),,,,,,,
Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning,,,,Shiming Chen (Carnegie Mellon University) | Wenjin Hou (Huazhong University Of Science And Technology) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Fahad Shahbaz Khan (MBZUAI; Link??ping University),,,,,,,
Towards Open-Vocabulary Spatio-Temporal Video Grounding,"Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query. This paper addresses a critical limitation in current video grounding methodologies by introducing an Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent closed-set approaches that struggle with open-vocabulary scenarios due to limited training data and predefined vocabularies, our model leverages pre-trained representations from foundational spatial grounding models. This empowers it to effectively bridge the semantic gap between natural language and diverse visual content, achieving strong performance in closed-set and open-vocabulary settings. Our contributions include a novel spatio-temporal video grounding model, surpassing state-of-the-art results in closed-set evaluations on multiple datasets and demonstrating superior performance in open-vocabulary scenarios. Notably, the proposed model outperforms state-of-the-art methods in closed-set settings on VidSTG (Declarative and Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model surpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\%$ accuracy, demonstrating its efficacy in handling diverse linguistic and visual concepts for improved video understanding. Our codes will be released at https://github.com/TalalWasim/Video-GroundingDINO.",http://arxiv.org/abs/2401.00901v1,,Syed Talal Wasim (None) | Muzammal Naseer (MBZUAI) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Ming-Hsuan Yang (University Of California At Merced) | Fahad Shahbaz Khan (MBZUAI; Link??ping University),2023-12-31 13:53:37+00:00,,,,,,
Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery,"Recent advances in unsupervised learning have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data. Such pre-training techniques have also been explored recently in the remote sensing domain due to the availability of large amount of unlabelled data. Different from standard natural image datasets, remote sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities. Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality. In this paper, we re-visit transformers pre-training and leverage multi-scale information that is effectively utilized with multiple modalities. Our proposed approach, named SatMAE++, performs multi-scale pre-training and utilizes convolution based upsampling blocks to reconstruct the image at higher scales making it extensible to include more scales. Compared to existing works, the proposed SatMAE++ with multi-scale pre-training is equally effective for both optical as well as multi-spectral imagery. Extensive experiments on six datasets reveal the merits of proposed contributions, leading to state-of-the-art performance on all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5\% for multi-label classification task on BigEarthNet dataset. Our code and pre-trained models are available at \url{https://github.com/techmn/satmae_pp}.",http://arxiv.org/abs/2403.05419v1,,Mubashir Noman (MBZUAI) | Muzammal Naseer (MBZUAI) | Hisham Cholakkal (MBZUAI) | Rao Anwer (Mohamed Bin Zayed University Of Artificial Intelligence) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Fahad Shahbaz Khan (MBZUAI; Link??ping University),2024-03-08 16:18:04+00:00,,,,,,
GeoChat: Grounded Large Vision-Language Model for Remote Sensing,"Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic scene interpretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong backbone models for RS make it hard for the models to align their behavior with user queries. To address these limitations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Furthermore, it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. We establish a comprehensive benchmark for RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection. Our code is available at https://github.com/mbzuai-oryx/geochat.",http://arxiv.org/abs/2311.15826v1,,"Kartik Kuckreja (BITS Pilani, Birla Institute Of Technology And Science) | Muhammad Sohail Danish (Mohamed Bin Zayed University Of Artificial Intelligence) | Muzammal Naseer (MBZUAI) | Abhijit Das (BITS Pilani, Birla Institute Of Technology And Science) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Fahad Shahbaz Khan (MBZUAI; Link??ping University)",2023-11-24 18:59:10+00:00,,,,,,
Composed Video Retrieval via Enriched Context and Discriminative Embeddings,,,,Omkar Thawakar (MBZUAI) | Muzammal Naseer (MBZUAI) | Rao Anwer (Mohamed Bin Zayed University Of Artificial Intelligence) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Michael Felsberg (Link??ping University) | Mubarak Shah (University Of Central Florida) | Fahad Shahbaz Khan (MBZUAI; Link??ping University),,,,,,,
GLaMM: Grounding Large Multimodal Model,,,,Hanoona Rasheed (Mohamed Bin Zayed University Of Artificial Intelligence) | Muhammad Maaz (Mohamed Bin Zayed University Of Artificial Intelligence) | Sahal Shaji Mullappilly (Mohamed Bin Zayed University Of Artificial Intelligence) | Abdelrahman Shaker (Mohamed Bin Zayed University Of Artificial Intelligence) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Hisham Cholakkal (MBZUAI) | Rao Anwer (Mohamed Bin Zayed University Of Artificial Intelligence) | Eric P. Xing (Mohamed Bin Zayed Univeristy Of AI) | Ming-Hsuan Yang (University Of California At Merced) | Fahad Shahbaz Khan (MBZUAI; Link??ping University),,,,,,,
Dispel Darkness for Better Fusion: A Controllable Visual Enhancer based on Cross-modal Conditional Adversarial Learning,,,,Hao Zhang (Wuhan University) | Linfeng Tang (Wuhan University) | Xinyu Xiang (Wuhan University) | Xuhui Zuo (Wuhan University) | Jiayi Ma (Wuhan University),,,,,,,
MRFS: Mutually Reinforcing Image Fusion and Segmentation,,,,Hao Zhang (Wuhan University) | Xuhui Zuo (Wuhan University) | Jie Jiang (Tencent AI Lab) | Chunchao Guo (SUN YAT-SEN UNIVERSITY) | Jiayi Ma (Wuhan University),,,,,,,
ESCAPE: Encoding Super-keypoints for Category-Agnostic Pose Estimation,,,,Khoi D Nguyen (University Of Wisconsin - Madison) | Chen Li (National University Of Singapore) | Gim Hee Lee (National University Of Singapore),,,,,,,
DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF,,,,Jie Long Lee (None) | Chen Li (National University Of Singapore) | Gim Hee Lee (National University Of Singapore),,,,,,,
GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields,,,,Yunsong Wang (None) | Hanlin Chen (National University Of Singapore) | Gim Hee Lee (National University Of Singapore),,,,,,,
DeMatch: Deep Decomposition of Motion Field for Two-View Correspondence Learning,,,,Shihua Zhang (Wuhan University) | Zizhuo Li (Wuhan University) | Yuan Gao (Wuhan University) | Jiayi Ma (Wuhan University),,,,,,,
Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering,"3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at 4$\times$-128$\times$ scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splatting.",http://arxiv.org/abs/2311.17089v1,,Zhiwen Yan (National University Of Singapore) | Weng Fei Low (None) | Yu Chen (National University Of Singapore) | Gim Hee Lee (National University Of Singapore),2023-11-28 03:31:35+00:00,,,,,,
Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion,,,,Xunpeng Yi (None) | Han Xu (None) | Hao Zhang (Wuhan University) | Linfeng Tang (Wuhan University) | Jiayi Ma (Wuhan University),,,,,,,
Unmixing before Fusion: A Generalized Paradigm for Multi-modality-based Hyperspectral Image Synthesis,,,,Yang Yu (None) | Erting Pan (Wuhan University) | Xinya Wang (Wuhan University) | Yuheng Wu (Wuhan University) | Xiaoguang Mei (Wuhan University) | Jiayi Ma (Wuhan University),,,,,,,
Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption,,,,Buzhen Huang (None) | Chen Li (National University Of Singapore) | Chongyang Xu (Sichuan University) | Liang Pan (Shanghai AI Laboratory) | Yangang Wang (Southeast University) | Gim Hee Lee (National University Of Singapore),,,,,,,
WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion,"The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than single-frame methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code will be available for research purposes at http://wham.is.tue.mpg.de/",http://arxiv.org/abs/2312.07531v1,,Soyong Shin (None) | Juyong Kim (Carnegie Mellon University) | Eni Halilaj (Carnegie Mellon University) | Michael J. Black (University Of T??bingen),2023-12-12 18:57:46+00:00,,,,,,
TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation,,,,"Sai Kumar Dwivedi (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Yu Sun (Harbin Institute Of Technology) | Priyanka Patel (Max-Planck Institute) | Yao Feng (None) | Michael J. Black (University Of T??bingen)",,,,,,,
WANDR: Wrist-driven Autonomous Navigation for Data-based Goal Reaching,,,,Markos Diomataris (None) | Nikos Athanasiou (None) | Omid Taheri (None) | Xi Wang (None) | Otmar Hilliges (None) | Michael J. Black (University Of T??bingen),,,,,,,
PoseGPT: Chatting about 3D Human Pose,"We introduce PoseGPT, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively understand postures from a single image or a brief description, a process that intertwines image interpretation, world knowledge, and an understanding of body language. Traditional human pose estimation methods, whether image-based or text-based, often lack holistic scene comprehension and nuanced reasoning, leading to a disconnect between visual data and its real-world implications. PoseGPT addresses these limitations by embedding SMPL poses as a distinct signal token within a multi-modal LLM, enabling direct generation of 3D body poses from both textual and visual inputs. This approach not only simplifies pose prediction but also empowers LLMs to apply their world knowledge in reasoning about human poses, fostering two advanced tasks: speculative pose generation and reasoning about pose estimation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly accompanied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and estimation methods. Our results show that PoseGPT outperforms existing multimodal LLMs and task-sepcific methods on these newly proposed tasks. Furthermore, PoseGPT's ability to understand and generate 3D human poses based on complex reasoning opens new directions in human pose analysis.",http://arxiv.org/abs/2311.18836v1,,"Yao Feng (None) | Jing Lin (Tsinghua University) | Sai Kumar Dwivedi (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Yu Sun (Harbin Institute Of Technology) | Priyanka Patel (Max-Planck Institute) | Michael J. Black (University Of T??bingen)",2023-11-30 18:59:52+00:00,,,,,,
VAREN: Very Accurate and Realistic Horse Network,,,,"Silvia Zuffi (None) | Ylva Mellbin (Swedish University Of Agricultural Sciences) | Ci Li (None) | Markus H??schle (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Hedvig Kjellstr??m (None) | Senya Polikovsky (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Elin Hernlund (Swedish University Of Agricultural Sciences) | Michael J. Black (University Of T??bingen)",,,,,,,
EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling,"We propose EMAGE, a framework to generate full-body human gestures from audio and masked gestures, encompassing facial, local body, hands, and global movements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new mesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with FLAME head parameters and further refines the modeling of head, neck, and finger movements, offering a community-standardized, high-quality 3D motion captured dataset. EMAGE leverages masked body gesture priors during training to boost inference performance. It involves a Masked Audio Gesture Transformer, facilitating joint training on audio-to-gesture generation and masked gesture reconstruction to effectively encode audio and body gesture hints. Encoded body hints from masked gestures are then separately employed to generate facial and body movements. Moreover, EMAGE adaptively merges speech features from the audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance the results' fidelity and diversity. Experiments demonstrate that EMAGE generates holistic gestures with state-of-the-art performance and is flexible in accepting predefined spatial-temporal gesture inputs, generating complete, audio-synchronized results. Our code and dataset are available at https://pantomatrix.github.io/EMAGE/",http://arxiv.org/abs/2401.00374v2,,"Haiyang Liu (The University Of Tokyo) | Zihao Zhu (Keio University) | Giorgio Becherini (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | YICHEN PENG (Japan Advanced Institute Of Science And Technology, Tokyo Institute Of Technology) | Mingyang Su (Tsinghua University) | YOU ZHOU (Huawei Technologies Ltd.) | Xuefei Zhe (City University Of Hong Kong) | Naoya Iwamoto (Huawei Technologies Japan K.K.) | Bo Zheng (Huawei Technologies Japan) | Michael J. Black (University Of T??bingen)",2023-12-31 02:25:41+00:00,,,,,,
Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment,,,,Angchi Xu (SUN YAT-SEN UNIVERSITY) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY),,,,,,,
Unsupervised Template-assisted Point Cloud Shape Correspondence Network,,,,"Jiacheng Deng (University Of Science And Technology Of China) | Jiahao Lu (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University)",,,,,,,
BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation,,,,"Jiahao Lu (University Of Science And Technology Of China) | Jiacheng Deng (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University)",,,,,,,
NECA: Neural Customizable Human Avatar,"Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA.",http://arxiv.org/abs/2403.10335v1,,"Junjin Xiao (School Of Computer Science And Engineering, Sun Yat-Sen University) | Qing Zhang (SUN YAT-SEN UNIVERSITY) | Zhan Xu (None) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)",2024-03-15 14:23:06+00:00,,,,,,
Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation,,,,"Xiao Lin (University Of Science And Technology Of China) | Wenfei Yang (University Of Science And Technology Of China) | Yuan Gao (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University)",,,,,,,
SD2Event: Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras,,,,"Yuan Gao (University Of Science And Technology Of China) | Yuqing Zhu (University Of Science And Technology Of China) | Xinjun Li (University Of Science And Technology Of China) | Yimin Du (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University)",,,,,,,
Single-view Scene Point Cloud Human Grasp Generation,,,,Yan-Kang Wang (SUN YAT-SEN UNIVERSITY)) | Chengyi Xing (Stanford University) | Yi-Lin Wei (SUN YAT-SEN UNIVERSITY) | Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY),,,,,,,
Dexterous Grasp Transformer,,,,Guo-Hao Xu (Sun Yat-Sen University) | Yi-Lin Wei (SUN YAT-SEN UNIVERSITY) | Dian Zheng (None) | Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY),,,,,,,
Rethinking the Region Classification in Open-Vocabulary Semantic Segmentation: An Image-to-Image View,,,,"Yuan Wang (University Of Science And Technology Of China) | Rui Sun (University Of Science And Technology Of China) | Naisong Luo (University Of Science And Technology Of China) | Yuwen Pan (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University)",,,,,,,
Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model,,,,Dian Zheng (None) | Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) | Shuzhou Yang (Peking University) | Jian Zhang (None) | Jian-Fang Hu (SUN YAT-SEN UNIVERSITY) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY),,,,,,,
Prompt Learning via Meta-Regularization,,,,Jinyoung Park (Korea University) | Juyeon Ko (Korea University) | Hyunwoo J. Kim (Korea University),,,,,,,
Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers,"Vision Transformer (ViT) has emerged as a prominent backbone for computer vision. For more efficient ViTs, recent works lessen the quadratic cost of the self-attention layer by pruning or fusing the redundant tokens. However, these works faced the speed-accuracy trade-off caused by the loss of information. Here, we argue that token fusion needs to consider diverse relations between tokens to minimize information loss. In this paper, we propose a Multi-criteria Token Fusion (MCTF), that gradually fuses the tokens based on multi-criteria (e.g., similarity, informativeness, and size of fused tokens). Further, we utilize the one-step-ahead attention, which is the improved approach to capture the informativeness of the tokens. By training the model equipped with MCTF using a token reduction consistency, we achieve the best speed-accuracy trade-off in the image classification (ImageNet1K). Experimental results prove that MCTF consistently surpasses the previous reduction methods with and without training. Specifically, DeiT-T and DeiT-S with MCTF reduce FLOPs by about 44% while improving the performance (+0.5%, and +0.3%) over the base model, respectively. We also demonstrate the applicability of MCTF in various Vision Transformers (e.g., T2T-ViT, LV-ViT), achieving at least 31% speedup without performance degradation. Code is available at https://github.com/mlvlab/MCTF.",http://arxiv.org/abs/2403.10030v1,,Sanghyeok Lee (Korea University) | Joonmyung Choi (Korea University) | Hyunwoo J. Kim (Korea University),2024-03-15 05:30:29+00:00,,,,,,
Retrieval-Augmented Open-Vocabulary Object Detection,,,,Jooyeon Kim (Korea University) | Eulrang Cho (Korea University) | Sehyung Kim (Korea University) | Hyunwoo J. Kim (Korea University),,,,,,,
vid-TLDR: Training Free Token merging for Light-weight Video Transformer,,,,Joonmyung Choi (Korea University) | Sanghyeok Lee (Korea University) | Jaewon Chu (Korea University) | Minhyuk Choi (Korea University) | Hyunwoo J. Kim (Korea University),,,,,,,
Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection,,,,Jongha Kim (Korea University) | Jihwan Park (Korea University) | Jinyoung Park (Korea University) | Jinyoung Kim (Korea University) | Sehyung Kim (Korea University) | Hyunwoo J. Kim (Korea University),,,,,,,
PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation,"Comprehensive modeling of the surrounding 3D world is key to the success of autonomous driving. However, existing perception tasks like object detection, road structure segmentation, depth & elevation estimation, and open-set object localization each only focus on a small facet of the holistic 3D scene understanding task. This divide-and-conquer strategy simplifies the algorithm development procedure at the cost of losing an end-to-end unified solution to the problem. In this work, we address this limitation by studying camera-based 3D panoptic segmentation, aiming to achieve a unified occupancy representation for camera-only 3D scene understanding. To achieve this, we introduce a novel method called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal information from multi-frame and multi-view images in a coarse-to-fine scheme, integrating feature learning and scene representation into a unified occupancy representation. We have conducted extensive ablation studies to verify the effectiveness and efficiency of the proposed method. Our approach achieves new state-of-the-art results for camera-based semantic segmentation and panoptic segmentation on the nuScenes dataset. Furthermore, our method can be easily extended to dense occupancy prediction and has shown promising performance on the Occ3D benchmark. The code will be released at https://github.com/Robertwyq/PanoOcc.",http://arxiv.org/abs/2306.10013v1,,"Yuqi Wang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yuntao Chen (CAIR, HKISI, CAS) | Xingyu Liao (University Of Science And Technology Of China) | Lue Fan (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2023-06-16 17:59:33+00:00,,,,,,
Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving,"In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road. To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models. Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes. Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time. Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards. Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning.",http://arxiv.org/abs/2311.17918v1,,"Yuqi Wang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Jiawei He (Institute Of Automation, Chinese Academy Of Sciences) | Lue Fan (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Hongxin Li (Institute Of Automation, Chinese Academy Of Sciences) | Yuntao Chen (CAIR, HKISI, CAS) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2023-11-29 18:59:47+00:00,,,,,,
PatchFD: Reliable Model Patching for Unified Failure Detection,,,,"Fei Zhu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhen Cheng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xu-Yao Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Cheng-Lin Liu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",,,,,,,
MemoNav: Working Memory Model for Visual Navigation,"Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.",http://arxiv.org/abs/2402.19161v1,,"Hongxin Li (Institute Of Automation, Chinese Academy Of Sciences) | Zeyu Wang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xu Yang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yuran Yang (Tencent) | Shuqi Mei (Tencent T-Lab) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2024-02-29 13:45:13+00:00,,,,,,
Continual Forgetting for Pre-trained Vision Models,,,,"Hongbo Zhao (Institute Of Automation, Chinese Academy Of Sciences) | Bolin Ni (Institute Of Automation, Chinese Academy Of Sciences) | Junsong Fan (Centre For Artificial Intelligence And Robotics (CAIR) Hong Kong Institute Of Science & Innovation Chinese Academy Of Sciences) | Yuxi Wang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yuntao Chen (CAIR, HKISI, CAS) | Gaofeng Meng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",,,,,,,
Few-Shot Object Detection with Foundation Models,,,,Guangxing Han (Columbia University) | Ser-Nam Lim (Meta AI),,,,,,,
On the Robustness of Large Multimodal Models Against Image Adversarial Attacks,"Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thoroughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual adversarial inputs. However, our findings suggest that context provided to the model via prompts, such as questions in a QA pair helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable resilience to such attacks on the ScienceQA task with only an 8.10% drop in performance compared to their visual counterparts which dropped 99.73%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished attack effectiveness and improvements in image classification accuracy. This research highlights a previously under-explored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.",http://arxiv.org/abs/2312.03777v2,,Xuanming Cui (University Of Central Florida) | Alejandro Aparcedo (University Of Central Florida) | Young Kyun Jang (Meta AI) | Ser-Nam Lim (Meta AI),2023-12-06 04:59:56+00:00,,,,,,
Visual Delta Generator for Semi-supervised Composed Image Retrieval,,,,Young Kyun Jang (Meta AI) | Donghyun Kim (MIT-IBM Watson AI Lab) | Zihang Meng (Meta) | Dat Huynh (Meta) | Ser-Nam Lim (Meta AI),,,,,,,
Object Recognition as Next Token Prediction,"We present an approach to pose object recognition as next token prediction. The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels. To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix. This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference. To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model. This approach yields a decoder that matches the full model's performance while being notably more efficient. The code is available at https://github.com/kaiyuyue/nxtp",http://arxiv.org/abs/2312.02142v3,,"Kaiyu Yue (University Of Maryland, College Park) | Bor-Chun Chen (Facebook) | Jonas Geiping (University Of Maryland, College Park) | Hengduo Li (Meta AI) | Tom Goldstein (University Of Maryland, College Park) | Ser-Nam Lim (Meta AI)",2023-12-04 18:58:40+00:00,,,,,,
MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding,,,,Bo He (None) | Hengduo Li (Meta AI) | Young Kyun Jang (Meta AI) | Menglin Jia (Facebook) | Xuefei Cao (Meta) | Ashish Shah (Meta) | Abhinav Shrivastava (University Of Maryland) | Ser-Nam Lim (Meta AI),,,,,,,
Unsupervised Salient Instance Detection,,,,Xin Tian (Huawei Technologies Ltd.) | Ke Xu (City University Of Hong Kong) | Rynson W.H. Lau (City University Of Hong Kong),,,,,,,
Color Shift Estimation-and-Correction for Image Enhancement,,,,Yiyu Li (City University Of Hong Kong) | Ke Xu (City University Of Hong Kong) | Gerhard Hancke Hancke (None) | Rynson W.H. Lau (City University Of Hong Kong),,,,,,,
Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields,,,,Haoyuan Wang (City University Of Hong Kong) | Wenbo Hu (ByteDance) | Lei Zhu (City University Of Hong Kong) | Rynson W.H. Lau (City University Of Hong Kong),,,,,,,
Effective Video Mirror Detection with Inconsistent Motion Cues,,,,Alex Warren (Swansea University) | Ke Xu (City University Of Hong Kong) | Jiaying Lin (City University Of Hong Kong) | Gary Tam (Swansea University) | Rynson W.H. Lau (City University Of Hong Kong),,,,,,,
Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks,"Diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. However, due to the randomness in the diffusion process, they often struggle with handling diverse low-level tasks that require details preservation. To overcome this limitation, we present a new Diff-Plugin framework to enable a single pre-trained diffusion model to generate high-fidelity results across a variety of low-level tasks. Specifically, we first propose a lightweight Task-Plugin module with a dual branch design to provide task-specific priors, guiding the diffusion process in preserving image content. We then propose a Plugin-Selector that can automatically select different Task-Plugins based on the text instruction, allowing users to edit images by indicating multiple low-level tasks with natural language. We conduct extensive experiments on 8 low-level vision tasks. The results demonstrate the superiority of Diff-Plugin over existing methods, particularly in real-world scenarios. Our ablations further validate that Diff-Plugin is stable, schedulable, and supports robust training across different dataset sizes.",http://arxiv.org/abs/2403.00644v1,,Yuhao Liu (City University Of Hong Kong) | Zhanghan Ke (City University Of Hong Kong) | Fang Liu (City University Of Hong Kong) | Nanxuan Zhao (Adobe Research) | Rynson W.H. Lau (City University Of Hong Kong),2024-03-01 16:25:17+00:00,,,,,,
Adaptive Dilated Convolution from Frequency View,,,,Linwei Chen (Beijing Institute Of Technology) | Lin Gu (RIKEN / The University Of Tokyo) | Dezhi Zheng (None) | Ying Fu (None),,,,,,,
Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion,"Monocular depth estimation has experienced significant progress on terrestrial images in recent years, largely due to deep learning advancements. However, it remains inadequate for underwater scenes, primarily because of data scarcity. Given the inherent challenges of light attenuation and backscattering in water, acquiring clear underwater images or precise depth information is notably difficult and costly. Consequently, learning-based approaches often rely on synthetic data or turn to unsupervised or self-supervised methods to mitigate this lack of data. Nonetheless, the performance of these methods is often constrained by the domain gap and looser constraints. In this paper, we propose a novel pipeline for generating photorealistic underwater images using accurate terrestrial depth data. This approach facilitates the training of supervised models for underwater depth estimation, effectively reducing the performance disparity between terrestrial and underwater environments. Contrary to prior synthetic datasets that merely apply style transfer to terrestrial images without altering the scene content, our approach uniquely creates vibrant, non-existent underwater scenes by leveraging terrestrial depth data through the innovative Stable Diffusion model. Specifically, we introduce a unique Depth2Underwater ControlNet, trained on specially prepared \{Underwater, Depth, Text\} data triplets, for this generation task. Our newly developed dataset enables terrestrial depth estimation models to achieve considerable improvements, both quantitatively and qualitatively, on unseen underwater images, surpassing their terrestrial pre-trained counterparts. Moreover, the enhanced depth accuracy for underwater scenes also aids underwater image restoration techniques that rely on depth maps, further demonstrating our dataset's utility. The dataset will be available at https://github.com/zkawfanx/Atlantis.",http://arxiv.org/abs/2312.12471v1,,Fan Zhang (Beijing Institute Of Technology) | Shaodi You (Kyushu University) | Yu Li (International Digital Economy Academy) | Ying Fu (None),2023-12-19 08:56:33+00:00,,,,,,
Binarized Low-light Raw Video Enhancement,,,,Gengchen Zhang (Beijing Institute Of Technology) | Yulun Zhang (ETH Zurich) | Xin Yuan (Westlake University) | Ying Fu (None),,,,,,,
Multi-Object Tracking in the Dark,,,,Xinzhe Wang (Beijing Institute Of Technology) | Kang Ma (Beijing Institute Of Technology) | Qiankun Liu (Beijing Institute Of Technology) | Yunhao Zou (None) | Ying Fu (None),,,,,,,
Infrared Small Target Detection with Scale and Location Sensitivity,,,,Qiankun Liu (Beijing Institute Of Technology) | Rui Liu (Beijing Institute Of Technology) | Bolun Zheng (Hangzhou Dianzi University) | Hongkui Wang (Hangzhou Dianzi University) | Ying Fu (None),,,,,,,
FreeKD: Knowledge Distillation via Semantic Frequency Prompt,"Knowledge distillation (KD) has been applied to various tasks successfully, and mainstream methods typically boost the student model via spatial imitation losses. However, the consecutive downsamplings induced in the spatial domain of teacher model is a type of corruption, hindering the student from analyzing what specific information needs to be imitated, which results in accuracy degradation. To better understand the underlying pattern of corrupted feature maps, we shift our attention to the frequency domain. During frequency distillation, we encounter a new challenge: the low-frequency bands convey general but minimal context, while the high are more informative but also introduce noise. Not each pixel within the frequency bands contributes equally to the performance. To address the above problem: (1) We propose the Frequency Prompt plugged into the teacher model, absorbing the semantic frequency context during finetuning. (2) During the distillation period, a pixel-wise frequency mask is generated via Frequency Prompt, to localize those pixel of interests (PoIs) in various frequency bands. Additionally, we employ a position-aware relational frequency loss for dense prediction tasks, delivering a high-order spatial enhancement to the student model. We dub our Frequency Knowledge Distillation method as FreeKD, which determines the optimal localization and extent for the frequency distillation. Extensive experiments demonstrate that FreeKD not only outperforms spatial-based distillation methods consistently on dense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on COCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys more robustness to the student. Notably, we also validate the generalization of our approach on large-scale vision models (e.g., DINO and SAM).",http://arxiv.org/abs/2311.12079v1,,Yuan Zhang (Peking University) | Tao Huang (The University Of Sydney) | Jiaming Liu (Peking University) | Tao Jiang (Zhejiang University) | Kuan Cheng (Peking University) | Shanghang Zhang (Peking University),2023-11-20 08:06:41+00:00,,,,,,
NTO3D: Neural Target Object 3D Reconstruction with Segment Anything,,,,Xiaobao Wei (University Of The Chinese Academy Of Sciences) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Jiarui Wu (Beijing University Of Aeronautics And Astronautics) | Jiaming Liu (Peking University) | Ming Lu (Intel Labs China) | Yandong Guo (OPPO Research Institute) | Shanghang Zhang (Peking University),,,,,,,
Gradient-based Parameter Selection for Efficient Fine-Tuning,,,,"Zhi Zhang (Institute For Logic, Language And Computation, University Of Amsterdam) | Qizhe Zhang (Peking University) | Zijun Gao (Shandong University) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Ekaterina Shutova (University Of Amsterdam) | Shiji Zhou (Tsinghua University) | Shanghang Zhang (Peking University)",,,,,,,
Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation,"Continual Test-Time Adaptation (CTTA) is proposed to migrate a source pre-trained model to continually changing target distributions, addressing real-world dynamism. Existing CTTA methods mainly rely on entropy minimization or teacher-student pseudo-labeling schemes for knowledge extraction in unlabeled target domains. However, dynamic data distributions cause miscalibrated predictions and noisy pseudo-labels in existing self-supervised learning methods, hindering the effective mitigation of error accumulation and catastrophic forgetting problems during the continual adaptation process. To tackle these issues, we propose a continual self-supervised method, Adaptive Distribution Masked Autoencoders (ADMA), which enhances the extraction of target domain knowledge while mitigating the accumulation of distribution shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism to adaptively sample masked positions, followed by establishing consistency constraints between the masked target samples and the original target samples. Additionally, for masked tokens, we utilize an efficient decoder to reconstruct a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients), leveraging its invariant properties to boost task-relevant representations. Through conducting extensive experiments on four widely recognized benchmarks, our proposed method attains state-of-the-art performance in both classification and segmentation CTTA tasks.",http://arxiv.org/abs/2312.12480v1,,Jiaming Liu (Peking University) | Ran Xu (None) | Senqiao Yang (Harbin Institute Of Technology) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Qizhe Zhang (Peking University) | Zehui Chen (University Of Science And Technology Of China) | Yandong Guo (OPPO Research Institute) | Shanghang Zhang (Peking University),2023-12-19 15:34:52+00:00,,,,,,
PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought,,,,"Junyi Yao (None) | Yijiang Liu (Nanjing University) | Zhen Dong (UC Berkeley) | Mingfei Guo (Stanford University) | Helan Hu (Peking University) | Kurt Keutzer (EECS, UC Berkeley) | Li Du (Nanjing University) | Daquan Zhou (National University Of Singapore) | Shanghang Zhang (Peking University)",,,,,,,
Cloud-Device Collaborative Learning for Multimodal Large Language Models,,,,Guanqun Wang (Peking University) | Jiaming Liu (Peking University) | Chenxuan Li (Peking University) | Yuan Zhang (Peking University) | Ma Junpeng (Peking University) | Xinyu Wei (Peking University) | Kevin Zhang (Peking University) | Maurice Chong (Peking University) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Yijiang Liu (Nanjing University) | Shanghang Zhang (Peking University),,,,,,,
Selective Prediction For Open-Ended Question Answering in Black-Box Vision-Language Models,,,,Zaid Khan (Northeastern University) | Yun Fu (Northeastern University),,,,,,,
Adapting to Length Shift: FlexiLength Network for Trajectory Prediction,,,,Yi Xu (Northeastern University) | Yun Fu (Northeastern University),,,,,,,
Noise-free Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising,,,,"Haichao Zhang (Northeastern University) | Yi Xu (Northeastern University) | Hongsheng Lu (Toyota Motor North America) | Takayuki Shimizu (Toyota Motor North America,) | Yun Fu (Northeastern University)",,,,,,,
Rewrite the stars,,,,Xu Ma (Northeastern University) | Xiyang Dai (Microsoft) | Yue Bai (Northeastern University) | Yizhou Wang (Northeastern University) | Yun Fu (Northeastern University),,,,,,,
LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction,,,,"Bo Zou (Computer Science, Tsinghua University) | Chao Yang (Shanghai AI Laboratory) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Chengbin Quan (Tsinghua University) | Youjian Zhao (Tsinghua University)",,,,,,,
VideoDistill: Language-aware Vision Distillation for Video Question Answering,,,,"Bo Zou (Computer Science, Tsinghua University) | Chao Yang (Shanghai AI Laboratory) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Chengbin Quan (Tsinghua University) | Youjian Zhao (Tsinghua University)",,,,,,,
Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge,,,,"Bo Zou (Computer Science, Tsinghua University) | Shaofeng Wang (Capital Medical Universty) | Hao Liu (, Tsinghua University) | Gaoyue Sun (Imperial College London) | Yajie Wang (Tsinghua University) | Zuo FeiFei (LargeV .Inc) | Chengbin Quan (Tsinghua University) | Youjian Zhao (Tsinghua University)",,,,,,,
VCoder: Versatile Visual Encoder for Accurate Object-Level Perception with Large Language Models,,,,Jitesh Jain (Georgia Institute Of Technology) | Jianwei Yang (Microsoft Research) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR),,,,,,,
Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis,,,,Marianna Ohanyan (Picsart) | Hayk Manukyan (Picsart AI Research) | Zhangyang Wang (University Of Texas At Austin) | Shant Navasardyan (Picsart AI Research) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR),,,,,,,
Prompt-Free Diffusion: Taking ??Text?? out of Text-to-Image Diffusion Models,,,,"Xingqian Xu (University Of Illinois, Urbana Champaign) | Jiayi Guo (Tsinghua University) | Zhangyang Wang (University Of Texas At Austin) | Gao Huang (Tsinghua University) | Irfan Essa (Georgia Institute Of Technology) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)",,,,,,,
Brush2Prompt: Contextual Prompt Generator for Object Inpainting,,,,"Mang Tik Chiu (University Of Illinois, Urbana Champaign) | Yuqian Zhou (University Of Illinois, Urbana-Champaign) | Lingzhi Zhang (School Of Engineering And Applied Science, University Of Pennsylvania) | Zhe Lin (Adobe Research) | Connelly Barnes (Adobe Systems) | Sohrab Amirghodsi (Adobe) | Eli Shechtman (Adobe) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)",,,,,,,
PAIR Diffusion: A Comprehensive Multimodal Object-Level Image Editor,"Generative image editing has recently witnessed extremely fast-paced growth. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e.\,object-level image editing. In this work, we tackle the task by perceiving the images as an amalgamation of various objects and aim to control the properties of each object in a fine-grained manner. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose \textbf{PAIR} Diffusion, a generic framework that can enable a diffusion model to control the structure and appearance properties of each object in the image. We show that having control over the properties of each object in an image leads to comprehensive editing capabilities. Our framework allows for various object-level editing operations on real images such as reference image-based appearance editing, free-form shape editing, adding objects, and variations. Thanks to our design, we do not require any inversion step. Additionally, we propose multimodal classifier-free guidance which enables editing images using both reference images and text when using our approach with foundational diffusion models. We validate the above claims by extensively evaluating our framework on both unconditional and foundational diffusion models. Please refer to https://vidit98.github.io/publication/conference-paper/pair_diff.html for code and model release.",http://arxiv.org/abs/2303.17546v2,,"Vidit Goel (Snap) | Elia Peruzzo (University Of Trento) | Yifan Jiang (University Of Texas At Austin) | Dejia Xu (University Of Texas At Austin) | Xingqian Xu (University Of Illinois, Urbana Champaign) | Nicu Sebe (University Of Trento) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Zhangyang Wang (University Of Texas At Austin) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)",2023-03-30 17:13:56+00:00,,,,,,
Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models,"Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",http://arxiv.org/abs/2312.04410v1,,"Jiayi Guo (Tsinghua University) | Xingqian Xu (University Of Illinois, Urbana Champaign) | Yifan Pu (Tsinghua University) | Zanlin Ni (Tsinghua University) | Chaofei Wang (Tsinghua University) | Manushree Vasu (Georgia Institute Of Technology) | Shiji Song (Tsinghua University) | Gao Huang (Tsinghua University) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)",2023-12-07 16:26:23+00:00,,,,,,
Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction,,,,Inhwan Bae (GIST) | Junoh Lee (Gwangju Institute Of Science And Technology) | Hae-Gon Jeon (GIST),,,,,,,
SingularTrajectory: Universal Trajectory Predictor using Diffusion Model,,,,Inhwan Bae (GIST) | Young-Jae Park (GIST) | Hae-Gon Jeon (GIST),,,,,,,
Depth Prompting for Sensor-agnostic Depth Estimation,,,,Jin-Hwi Park (GIST) | Chanhwi Jeong (Gwangju Institute Of Science And Technology) | Junoh Lee (Gwangju Institute Of Science And Technology) | Hae-Gon Jeon (GIST),,,,,,,
Close Imitation of Expert Retouching for Black-and-White Photography,,,,"Seunghyun Shin (None) | Jisu Shin (None) | Jihwan Bae (CHA University, School Of Medicine) | Inwook Shim (Inha University) | Hae-Gon Jeon (GIST)",,,,,,,
Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder,"Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage.",http://arxiv.org/abs/2403.10255v1,,Jinseok Kim (KAIST / LG Electronics) | Tae-Kyun Kim (Imperial College London),2024-03-15 12:45:40+00:00,,,,,,
BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image,"Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms. While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes. However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input. In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1)\ bi-directional (left $\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3)\ the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands. BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor. In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively. The code is available at https://github.com/yunminjin2/BiTT",http://arxiv.org/abs/2403.08262v1,,Minje Kim (KAIST) | Tae-Kyun Kim (Imperial College London),2024-03-13 05:25:49+00:00,,,,,,
Prompt Augmentation for Self-supervised Text-guided Image Manipulation,,,,Rumeysa Bodur (Imperial College London) | Binod Bhattarai (University Of Aberdeen) | Tae-Kyun Kim (Imperial College London),,,,,,,
InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion,,,,Jihyun Lee (KAIST) | Shunsuke Saito (Reality Labs Research) | Giljoo Nam (Meta) | Minhyuk Sung (KAIST) | Tae-Kyun Kim (Imperial College London),,,,,,,
Mo n o D i f f : Monocular 3D Object Detection and Pose Estimation with Diffusion Models ,,,,Yasiru Ranasinghe (Johns Hopkins University) | Deepti Hegde (Johns Hopkins University) | Vishal M. Patel (Johns Hopkins University),,,,,,,
Cr o w d D i f f : Multi-hypothesis Crowd Density Estimation using Diffusion Models ,,,,Yasiru Ranasinghe (Johns Hopkins University) | Nithin Gopalakrishnan Nair (Johns Hopkins University) | Wele Gedara Chaminda Bandara (Johns Hopkins University) | Vishal M. Patel (Johns Hopkins University),,,,,,,
LQ M F o r m e r :~Language-aware Query Mask Transformer for Referring Image Segmentation ,,,,Nisarg Shah (Johns Hopkins University) | Vibashan VS (Johns Hopkins University) | Vishal M. Patel (Johns Hopkins University),,,,,,,
Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image,"At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D GAN (EG3D) to reconstruct geometry and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.",http://arxiv.org/abs/2403.09632v1,,Yiqun Mei (None) | Yu Zeng (None) | He Zhang (Adobe Systems) | Zhixin Shu (Adobe Systems) | Xuaner Zhang (Adobe) | Sai Bi (Adobe Systems) | Jianming Zhang (Adobe Systems) | HyunJoon Jung (Adobe Systems) | Vishal M. Patel (Johns Hopkins University),2024-03-14 17:58:56+00:00,,,,,,
Rethinking Multi-view Representation Learning via Distilled Disentangling,,,,Guanzhou Ke (Beijing Jiao Tong University) | Bo Wang (Peking University) | Xiao-Li Wang (Nanjing University Of Science And Technology) | Shengfeng He (Singapore Management University),,,,,,,
Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation,,,,Haofeng Liu (South China Normal University) | Chenshu Xu (Singapore Management University) | Yifei Yang (Singapore Management University) | Lihua Zeng (South China Normal University) | Shengfeng He (Singapore Management University),,,,,,,
Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency,,,,"Xu Yingjie (None) | Bangzhen Liu (South China University Of Technology) | Hao Tang (School Of Computer Science And Engineering, Nanjing University Of Science And Technology) | Bailin Deng (Cardiff University) | Shengfeng He (Singapore Management University)",,,,,,,
Beyond Textual Constraints: Learning Novel Diffusion Conditions with Fewer Examples,,,,Yuyang Yu (South China University Of Technology) | Bangzhen Liu (South China University Of Technology) | Chenxi Zheng (None) | Xuemiao Xu (South China University Of Technology) | Huaidong Zhang (South China University Of Technology) | Shengfeng He (Singapore Management University),,,,,,,
D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval,,,,Yi Xie (South China University Of Technology) | Yihong Lin (South China University Of Technology) | Wenjie Cai (None) | Xuemiao Xu (South China University Of Technology) | Huaidong Zhang (South China University Of Technology) | Yong Du (Ocean University Of China) | Shengfeng He (Singapore Management University),,,,,,,
3D Human Pose Perception from Egocentric Stereo Videos,"While head-mounted devices are becoming more compact, they provide egocentric views with significant self-occlusions of the device user. Hence, existing methods often fail to accurately estimate complex 3D poses from egocentric views. In this work, we propose a new transformer-based framework to improve egocentric stereo 3D human pose estimation, which leverages the scene information and temporal context of egocentric stereo videos. Specifically, we utilize 1) depth features from our 3D scene reconstruction module with uniformly sampled windows of egocentric stereo frames, and 2) human joint queries enhanced by temporal features of the video inputs. Our method is able to accurately estimate human poses even in challenging scenarios, such as crouching and sitting. Furthermore, we introduce two new benchmark datasets, i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer a much larger number of egocentric stereo views with a wider variety of human motions than the existing datasets, allowing comprehensive evaluation of existing and upcoming methods. Our extensive experiments show that the proposed approach significantly outperforms previous methods. We will release UnrealEgo2, UnrealEgo-RW, and trained models on our project page.",http://arxiv.org/abs/2401.00889v1,,Hiroyasu Akada (Max Planck Institute For Informatics) | Jian Wang (Max Planck Institute For Informatics) | Vladislav Golyanik (MPI For Informatics) | Christian Theobalt (MPI Informatik),2023-12-30 21:21:54+00:00,,,,,,
VINECS: Video-based Neural Character Skinning,"Rigging and skinning clothed human avatars is a challenging task and traditionally requires a lot of manual work and expertise. Recent methods addressing it either generalize across different characters or focus on capturing the dynamics of a single character observed under different pose configurations. However, the former methods typically predict solely static skinning weights, which perform poorly for highly articulated poses, and the latter ones either require dense 3D character scans in different poses or cannot generate an explicit mesh with vertex correspondence over time. To address these challenges, we propose a fully automated approach for creating a fully rigged character with pose-dependent skinning weights, which can be solely learned from multi-view video. Therefore, we first acquire a rigged template, which is then statically skinned. Next, a coordinate-based MLP learns a skinning weights field parameterized over the position in a canonical pose space and the respective pose. Moreover, we introduce our pose- and view-dependent appearance field allowing us to differentiably render and supervise the posed mesh using multi-view imagery. We show that our approach outperforms state-of-the-art while not relying on dense 4D scans.",http://arxiv.org/abs/2307.00842v1,,"Zhouyingcheng Liao (The University Of Hong Kong, University Of Hong Kong) | Vladislav Golyanik (MPI For Informatics) | Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) | Christian Theobalt (MPI Informatik)",2023-07-03 08:35:53+00:00,,,,,,
Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras,"We present the first approach to render highly realistic free-viewpoint videos of a human actor in general apparel, from sparse multi-view recording to display, in real-time at an unprecedented 4K resolution. At inference, our method only requires four camera views of the moving actor and the respective 3D skeletal pose. It handles actors in wide clothing, and reproduces even fine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand gestures. At training time, our learning-based approach expects dense multi-view video and a rigged static surface scan of the actor. Our method comprises three main stages. Stage 1 is a skeleton-driven neural approach for high-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel solution to create a view-dependent texture using four test-time camera views as input. Finally, stage 3 comprises a new image-based refinement network rendering the final 4K image given the output from the previous stages. Our approach establishes a new benchmark for real-time rendering resolution and quality using sparse input camera views, unlocking possibilities for immersive telepresence.",http://arxiv.org/abs/2312.07423v1,,"Ashwath Shetty (Saarland Informatics Campus, Max-Planck Institute) | Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) | Guoxing Sun (Max Planck Institute For Informatics) | Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) | Vladislav Golyanik (MPI For Informatics) | Christian Theobalt (MPI Informatik)",2023-12-12 16:45:52+00:00,,,,,,
ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis,,,,"Muhammad Hamza Mughal (Max-Planck Institute For Informatics) | Rishabh Dabral (Saarland Informatics Campus, Max-Planck Institute) | Ikhsanul Habibie (Saarland Informatics Campus, Max-Planck Institute) | Lucia Donatelli (Vrije Universiteit Amsterdam) | Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) | Christian Theobalt (MPI Informatik)",,,,,,,
Egocentric Full Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement,"In this work, we explore egocentric whole-body motion capture using a single fisheye camera, which simultaneously estimates human body and hand motion. This task presents significant challenges due to three factors: the lack of high-quality datasets, fisheye camera distortion, and human body self-occlusion. To address these challenges, we propose a novel approach that leverages FisheyeViT to extract fisheye image features, which are subsequently converted into pixel-aligned 3D heatmap representations for 3D human body pose prediction. For hand tracking, we incorporate dedicated hand detection and hand pose estimation networks for regressing 3D hand poses. Finally, we develop a diffusion-based whole-body motion prior model to refine the estimated whole-body motion while accounting for joint uncertainties. To train these networks, we collect a large synthetic dataset, EgoWholeBody, comprising 840,000 high-quality egocentric images captured across a diverse range of whole-body motion sequences. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in producing high-quality whole-body motion estimates from a single egocentric camera.",http://arxiv.org/abs/2311.16495v2,,"Jian Wang (Max Planck Institute For Informatics) | Zhe Cao (None) | Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) | Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) | Kripasindhu Sarkar (Google) | Danhang Tang (Google) | Thabo Beeler (Google) | Christian Theobalt (MPI Informatik)",2023-11-28 07:13:47+00:00,,,,,,
Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer,"Unsupervised Domain Adaptation (UDA) can effectively address domain gap issues in real-world image Super-Resolution (SR) by accessing both the source and target data. Considering privacy policies or transmission restrictions of source data in practical scenarios, we propose a SOurce-free Domain Adaptation framework for image SR (SODA-SR) to address this issue, i.e., adapt a source-trained model to a target domain with only unlabeled target data. SODA-SR leverages the source-trained model to generate refined pseudo-labels for teacher-student learning. To better utilize pseudo-labels, we propose a novel wavelet-based augmentation method, named Wavelet Augmentation Transformer (WAT), which can be flexibly incorporated with existing networks, to implicitly produce useful augmented data. WAT learns low-frequency information of varying levels across diverse samples, which is aggregated efficiently via deformable attention. Furthermore, an uncertainty-aware self-training mechanism is proposed to improve the accuracy of pseudo-labels, with inaccurate predictions being rectified by uncertainty estimation. To acquire better SR results and avoid overfitting pseudo-labels, several regularization losses are proposed to constrain target LR and SR images in the frequency domain. Experiments show that without accessing source data, SODA-SR outperforms state-of-the-art UDA methods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptation settings, and is not constrained by specific network architectures.",http://arxiv.org/abs/2303.17783v4,,"Yuang Ai (Institute Of Automation, Chinese Academy Of Sciences) | Xiaoqiang Zhou (University Of Science And Technology Of China) | Huaibo Huang (None) | Lei Zhang (The Hong Kong Polytechnic University) | Ran He (None)",2023-03-31 03:14:44+00:00,,,,,,
"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration","Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations. This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration. Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation. Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations. Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation. To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks. Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive experiments on 16 IR tasks and 26 benchmarks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity.",http://arxiv.org/abs/2312.02918v1,,"Yuang Ai (Institute Of Automation, Chinese Academy Of Sciences) | Huaibo Huang (None) | Xiaoqiang Zhou (University Of Science And Technology Of China) | Jiexiang Wang (University Of Science And Technology Of China) | Ran He (None)",2023-12-05 17:47:11+00:00,,,,,,
Test-Time Backdoor Defense via Detecting and Repairing,"Deep neural networks have played a crucial part in many critical domains, such as autonomous driving, face recognition, and medical diagnosis. However, deep neural networks are facing security threats from backdoor attacks and can be manipulated into attacker-decided behaviors by the backdoor attacker. To defend the backdoor, prior research has focused on using clean data to remove backdoor attacks before model deployment. In this paper, we investigate the possibility of defending against backdoor attacks at test time by utilizing partially poisoned data to remove the backdoor from the model. To address the problem, a two-stage method Test-Time Backdoor Defense (TTBD) is proposed. In the first stage, we propose a backdoor sample detection method DDP to identify poisoned samples from a batch of mixed, partially poisoned samples. Once the poisoned samples are detected, we employ Shapley estimation to calculate the contribution of each neuron's significance in the network, locate the poisoned neurons, and prune them to remove backdoor in the models. Our experiments demonstrate that TTBD removes the backdoor successfully with only a batch of partially poisoned data across different model architectures and datasets against different types of backdoor attacks.",http://arxiv.org/abs/2308.06107v2,,"Jiyang Guan (Institute Of Automation, Chinese Academy Of Sciences) | Jian Liang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Ran He (None)",2023-08-11 12:46:05+00:00,,,,,,
RMT: Retentive Networks Meet Vision Transformers,"Vision Transformer (ViT) has gained increasing attention in the computer vision community in recent years. However, the core component of ViT, Self-Attention, lacks explicit spatial priors and bears a quadratic computational complexity, thereby constraining the applicability of ViT. To alleviate these issues, we draw inspiration from the recent Retentive Network (RetNet) in the field of NLP, and propose RMT, a strong vision backbone with explicit spatial prior for general purposes. Specifically, we extend the RetNet's temporal decay mechanism to the spatial domain, and propose a spatial decay matrix based on the Manhattan distance to introduce the explicit spatial prior to Self-Attention. Additionally, an attention decomposition form that adeptly adapts to explicit spatial prior is proposed, aiming to reduce the computational burden of modeling global information without disrupting the spatial decay matrix. Based on the spatial decay matrix and the attention decomposition form, we can flexibly integrate explicit spatial prior into the vision backbone with linear complexity. Extensive experiments demonstrate that RMT exhibits exceptional performance across various vision tasks. Specifically, without extra training data, RMT achieves **84.8%** and **86.1%** top-1 acc on ImageNet-1k with **27M/4.5GFLOPs** and **96M/18.2GFLOPs**. For downstream tasks, RMT achieves **54.5** box AP and **47.2** mask AP on the COCO detection task, and **52.8** mIoU on the ADE20K semantic segmentation task. Code is available at https://github.com/qhfan/RMT",http://arxiv.org/abs/2309.11523v5,,"Qihang Fan (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Huaibo Huang (None) | Mingrui Chen (Institute Of Automation, Chinese Academy Of Sciences (CASIA)) | Hongmin Liu (University Of Science And Technology Beijing) | Ran He (None)",2023-09-20 00:57:48+00:00,,,,,,
VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation,"Existing approaches to unsupervised video instance segmentation typically rely on motion estimates and experience difficulties tracking small or divergent motions. We present VideoCutLER, a simple method for unsupervised multi-instance video segmentation without using motion-based learning signals like optical flow or training on natural videos. Our key insight is that using high-quality pseudo masks and a simple video synthesis method for model training is surprisingly sufficient to enable the resulting video model to effectively segment and track multiple instances across video frames. We show the first competitive unsupervised learning results on the challenging YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous state-of-the-art by a large margin. VideoCutLER can also serve as a strong pretrained model for supervised video instance segmentation tasks, exceeding DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",http://arxiv.org/abs/2308.14710v1,,"Xudong Wang (Electrical Engineering & Computer Science Department, University Of California Berkeley) | Ishan Misra (Facebook) | Ziyun Zeng (UCB) | Rohit Girdhar (Meta) | Trevor Darrell (Electrical Engineering & Computer Science Department)",2023-08-28 17:10:12+00:00,,,,,,
Unsupervised Universal Image Segmentation,"Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation). We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework. U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels. We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP$^{\text{box}}$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff. Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored. U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0 AP$^{\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO labels. We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation.",http://arxiv.org/abs/2312.17243v1,,"Xudong Wang (Electrical Engineering & Computer Science Department, University Of California Berkeley) | Dantong Niu (University Of California, Berkeley) | Xinyang Han (None) | Long Lian (University Of California, Berkeley) | Roei Herzig (Tel Aviv University) | Trevor Darrell (Electrical Engineering & Computer Science Department)",2023-12-28 18:59:04+00:00,,,,,,
Self-correcting LLM-controlled Diffusion,,,,"Tsung-Han Wu (University Of California, Berkeley) | Long Lian (University Of California, Berkeley) | Joseph Gonzalez (University Of California - Berkeley) | Boyi Li (UC Berkeley / NVIDIA) | Trevor Darrell (Electrical Engineering & Computer Science Department)",,,,,,,
Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera,"We present a lightweight and affordable motion capture method based on two smartwatches and a head-mounted camera. In contrast to the existing approaches that use six or more expert-level IMU devices, our approach is much more cost-effective and convenient. Our method can make wearable motion capture accessible to everyone everywhere, enabling 3D full-body motion capture in diverse environments. As a key idea to overcome the extreme sparsity and ambiguities of sensor inputs, we integrate 6D head poses obtained from the head-mounted cameras for motion estimation. To enable capture in expansive indoor and outdoor scenes, we propose an algorithm to track and update floor level changes to define head poses, coupled with a multi-stage Transformer-based regression module. We also introduce novel strategies leveraging visual cues of egocentric images to further enhance the motion capture quality while reducing ambiguities. We demonstrate the performance of our method on various challenging scenarios, including complex outdoor environments and everyday motions including object interactions and social interactions among multiple individuals.",http://arxiv.org/abs/2401.00847v1,,Jiye Lee (Seoul National University) | Hanbyul Joo (Seoul National University),2024-01-01 18:56:54+00:00,,,,,,
Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses,,,,Inhee Lee (Seoul National University) | Byungjun Kim (Seoul National University) | Hanbyul Joo (Seoul National University),,,,,,,
PEGASUS: Personalized Generative 3D Avatars with Composable Attributes,"We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources. As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity. We present two key approaches to achieve this goal. First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos. Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism. Subsequently, we introduce a zero-shot approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model.",http://arxiv.org/abs/2402.10636v1,,Hyunsoo Cha (Seoul National University) | Byungjun Kim (Seoul National University) | Hanbyul Joo (Seoul National University),2024-02-16 12:35:35+00:00,,,,,,
GALA: Generating Animatable Layered Assets from a Single Scan,"We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.",http://arxiv.org/abs/2401.12979v1,,Taeksoo Kim (Seoul National University) | Byungjun Kim (Seoul National University) | Shunsuke Saito (Reality Labs Research) | Hanbyul Joo (Seoul National University),2024-01-23 18:59:59+00:00,,,,,,
Can I Trust Your Answer? Visually Grounded Video Question Answering,"We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a variety of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are weak in substantiating the answers despite their strong QA performance. This exposes a severe limitation of these models in making reliable predictions. As a remedy, we further explore and suggest a video grounding mechanism via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both video grounding and QA. Our dataset and code are released. With these efforts, we aim to push towards the reliability of deploying VLMs in VQA systems.",http://arxiv.org/abs/2309.01327v1,,"Junbin Xiao (None) | Angela Yao (National University Of Singapore) | Yicong Li (National University Of Singaore, National University Of Singapore) | Tat-Seng Chua (National University Of Singapore)",2023-09-04 03:06:04+00:00,,,,,,
Empowering Dynamics-aware Text-to-Video Diffusion with LLMs,,,,Hao Fei (National University Of Singapore) | Shengqiong Wu (National University Of Singapore) | Wei Ji (None) | Hanwang Zhang (Nanyang Technological University) | Tat-Seng Chua (National University Of Singapore),,,,,,,
Discriminative Probing and Tuning for Text-to-Image Generation,"Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.",http://arxiv.org/abs/2403.04321v2,,Leigang Qu (National University Of Singapore) | Wenjie Wang (National University Of Singapore) | Yongqi Li (Hong Kong Polytechnic University) | Hanwang Zhang (Nanyang Technological University) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen)) | Tat-Seng Chua (National University Of Singapore),2024-03-07 08:37:33+00:00,,,,,,
LASO: Language-guided Affordance Segmentation on 3D Object,,,,"Yicong Li (National University Of Singaore, National University Of Singapore) | Na Zhao (Singapore University Of Technology And Design) | Junbin Xiao (None) | Chun Feng (University Of Science And Technology Of China) | Xiang Wang (University Of Science And Technology Of China) | Tat-Seng Chua (National University Of Singapore)",,,,,,,
Abductive Ego-View Accident Video Understanding for Safe Driving Perception,"We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the causal region learning while fixing the content of the original frame background in video generation, to find the dominant cause-effect chain for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art diffusion models. Additionally, we provide careful benchmark evaluations for object detection and accident reason answering since AdVersa-SD relies on precise object and accident reason information.",http://arxiv.org/abs/2403.00436v1,,"Jianwu Fang (Xi'an Jiao Tong University) | Lei-Lei Li (Chang'an University) | Junfei Zhou (Chang'an University) | Junbin Xiao (None) | Hongkai Yu (Cleveland State University) | Chen Lv (Nanyang Technological University) | Jianru Xue (Xi'an Jiao Tong University, Tsinghua University) | Tat-Seng Chua (National University Of Singapore)",2024-03-01 10:42:52+00:00,,,,,,
Orthogonal Adaptation for Modular Customization of Diffusion Models,"Customization techniques for text-to-image models have paved the way for a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods facilitate high-fidelity customization for individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs.   To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference.   Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.",http://arxiv.org/abs/2312.02432v1,,Ryan Po (Stanford University) | Guandao Yang (None) | Kfir Aberman (Google) | Gordon Wetzstein (Stanford University),2023-12-05 02:17:48+00:00,,,,,,
PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors,"Conventional image sensors digitize high-resolution images at fast frame rates, producing a large amount of data that needs to be transmitted off the sensor for further processing. This is challenging for perception systems operating on edge devices, because communication is power inefficient and induces latency. Fueled by innovations in stacked image sensor fabrication, emerging sensor-processors offer programmability and minimal processing capabilities directly on the sensor. We exploit these capabilities by developing an efficient recurrent neural network architecture, PixelRNN, that encodes spatio-temporal features on the sensor using purely binary operations. PixelRNN reduces the amount of data to be transmitted off the sensor by a factor of 64x compared to conventional systems while offering competitive accuracy for hand gesture recognition and lip reading tasks. We experimentally validate PixelRNN using a prototype implementation on the SCAMP-5 sensor-processor platform.",http://arxiv.org/abs/2304.05440v1,,Haley So (Stanford University) | Laurie Bose (None) | Piotr Dudek (University Of Manchester) | Gordon Wetzstein (Stanford University),2023-04-11 18:16:47+00:00,,,,,,
Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models,"Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene's geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerging text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth correspondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image generation model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path.",http://arxiv.org/abs/2312.01409v1,,Shengqu Cai (ETH Zurich & Stanford University) | Duygu Ceylan (Adobe Systems) | Matheus Gadelha (Adobe Systems) | Chun-Hao P. Huang (Adobe Systems) | Tuanfeng Y. Wang (None) | Gordon Wetzstein (Stanford University),2023-12-03 14:17:11+00:00,,,,,,
GPT-4V(ision) is a Versatile and Human-Aligned Evaluator for Text-to-3D Generation,,,,Tong Wu (None) | Guandao Yang (None) | Zhibing Li (The Chinese University Of Hong Kong) | Kai Zhang (Adobe Systems) | Ziwei Liu (Nanyang Technological University) | Leonidas Guibas (Stanford University) | Dahua Lin (The Chinese University Of Hong Kong) | Gordon Wetzstein (Stanford University),,,,,,,
Gaussian Shell Maps for Efficient 3D Human Generation,"Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of $512 \times 512$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",http://arxiv.org/abs/2311.17857v1,,Rameen Abdal (Stanford University) | Wang Yifan (Stanford University) | Zifan Shi (HKUST) | Yinghao Xu (Chinese University Of Hong Kong) | Ryan Po (Stanford University) | Zhengfei Kuang (Stanford University) | Qifeng Chen (Hong Kong University Of Science And Technology) | Dit-Yan Yeung (Hong Kong University Of Science And Technology) | Gordon Wetzstein (Stanford University),2023-11-29 18:04:07+00:00,,,,,,
BilevelPruning: Unified Dynamic and Static Channel Pruning for Convolutional Neural Networks,,,,Shangqian Gao (University Of Pittsburgh) | Yanfu Zhang (College Of William And Mary) | Feihu Huang (Nanjing University Of Aeronautics And Astronautics) | Heng Huang (University Of Pittsburgh),,,,,,,
Device-Wise Federated Network Pruning,,,,"Shangqian Gao (University Of Pittsburgh) | Junyi Li (University Of Maryland, College Park) | Zeyu Zhang (Amazon AGI) | Yanfu Zhang (College Of William And Mary) | Weidong Cai (The University Of Sydney) | Heng Huang (University Of Pittsburgh)",,,,,,,
Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment,,,,"Alireza Ganjdanesh (University Of Maryland, College Park) | Shangqian Gao (University Of Pittsburgh) | Heng Huang (University Of Pittsburgh)",,,,,,,
Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch,,,,Xidong Wu (University Of Pittsburgh) | Shangqian Gao (University Of Pittsburgh) | Zeyu Zhang (Amazon AGI) | Zhenzhen Li (Bosch) | Runxue Bao (NEC Labs America) | Yanfu Zhang (College Of William And Mary) | Xiaoqian Wang (Purdue University) | Heng Huang (University Of Pittsburgh),,,,,,,
SPECAT: SPatial-spEctral Cumulative-Attention Transformer for High-Resolution Hyperspectral Image Reconstruction,,,,"Zhiyang Yao (Department Of Electronic Engineering, Tsinghua University) | Shuyang Liu (Tsinghua University) | Xiaoyun Yuan (Tsinghua University) | Lu Fang (Tsinghua University)",,,,,,,
XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold,,,,"Guangyu Wang (Tsinghua University) | Jinzhi Zhang (Electronic Engineering, Tsinghua University) | Fan Wang (Alibaba Group) | Ruqi Huang (Tsinghua Shenzhen International Graduate School/Tsinghua Berkeley Shenzhen Institute ) | Lu Fang (Tsinghua University)",,,,,,,
When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach,,,,TAO MA (Peking University) | Bing Bai (Qiyuan Lab) | Haozhe Lin (None) | Heyuan Wang (Peking University) | Yu Wang (Qiyuan Lab) | Lin Luo (Peking University) | Lu Fang (Tsinghua University),,,,,,,
GigaTraj: Predicting Long-term Trajectories of Hundreds of Pedestrians in Gigapixel Complex Scenes,,,,Haozhe Lin (None) | Chunyu Wei (Tsinghua University) | Li He (Qiyuan Lab) | Yuchen Guo (Tsinghua University) | Yuchy Zhao (Tsinghua University) | Shanglong Li (Tsinghua University) | Lu Fang (Tsinghua University),,,,,,,
OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning,"Towards holistic understanding of 3D scenes, a general 3D segmentation method is needed that can segment diverse objects without restrictions on object quantity or categories, while also reflecting the inherent hierarchical structure. To achieve this, we propose OmniSeg3D, an omniversal segmentation method aims for segmenting anything in 3D all at once. The key insight is to lift multi-view inconsistent 2D segmentations into a consistent 3D feature field through a hierarchical contrastive learning framework, which is accomplished by two steps. Firstly, we design a novel hierarchical representation based on category-agnostic 2D segmentations to model the multi-level relationship among pixels. Secondly, image features rendered from the 3D feature field are clustered at different levels, which can be further drawn closer or pushed apart according to the hierarchical relationship between different levels. In tackling the challenges posed by inconsistent 2D segmentations, this framework yields a global consistent 3D feature field, which further enables hierarchical segmentation, multi-object selection, and global discretization. Extensive experiments demonstrate the effectiveness of our method on high-quality 3D segmentation and accurate hierarchical structure understanding. A graphical user interface further facilitates flexible interaction for omniversal 3D segmentation.",http://arxiv.org/abs/2311.11666v1,,"Haiyang Ying (None) | Yixuan Yin (Tsinghua University) | Jinzhi Zhang (Electronic Engineering, Tsinghua University) | Fan Wang (Alibaba Group) | Tao Yu (Tsinghua University) | Ruqi Huang (Tsinghua Shenzhen International Graduate School/Tsinghua Berkeley Shenzhen Institute ) | Lu Fang (Tsinghua University)",2023-11-20 11:04:59+00:00,,,,,,
AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution,,,,Cheeun Hong (Seoul National University) | Kyoung Mu Lee (Seoul National University),,,,,,,
Semantic Line Combination Detector,,,,"JINWON KO (Korea University, Seoul) | Dongkwon Jin (Korea University) | Chang-Su Kim (Korea University)",,,,,,,
Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement,,,,Jinyoung Jun (None) | Jae-Han Lee (Gauss Labs) | Chang-Su Kim (Korea University),,,,,,,
Blind Image Quality Assessment Based on Geometric Order Learning,,,,Nyeong-Ho Shin (None) | Seon-Ho Lee (None) | Chang-Su Kim (Korea University),,,,,,,
Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss,,,,Jaeha Kim (Seoul National University) | Junghun Oh (None) | Kyoung Mu Lee (Seoul National University),,,,,,,
MFP: Making Full use of Probability Maps for Interactive Image Segmentation,,,,Chaewon Lee (None) | Seon-Ho Lee (None) | Chang-Su Kim (Korea University),,,,,,,
CNC-Net: Self-Supervised Learning for CNC Machining Operations,"CNC manufacturing is a process that employs computer numerical control (CNC) machines to govern the movements of various industrial tools and machinery, encompassing equipment ranging from grinders and lathes to mills and CNC routers. However, the reliance on manual CNC programming has become a bottleneck, and the requirement for expert knowledge can result in significant costs. Therefore, we introduce a pioneering approach named CNC-Net, representing the use of deep neural networks (DNNs) to simulate CNC machines and grasp intricate operations when supplied with raw materials. CNC-Net constitutes a self-supervised framework that exclusively takes an input 3D model and subsequently generates the essential operation parameters required by the CNC machine to construct the object. Our method has the potential to transformative automation in manufacturing by offering a cost-effective alternative to the high costs of manual CNC programming while maintaining exceptional precision in 3D object production. Our experiments underscore the effectiveness of our CNC-Net in constructing the desired 3D objects through the utilization of CNC operations. Notably, it excels in preserving finer local details, exhibiting a marked enhancement in precision compared to the state-of-the-art 3D CAD reconstruction approaches.",http://arxiv.org/abs/2312.09925v1,,Mohsen Yavartanoo (None) | Sangmin Hong (Seoul National University) | Reyhaneh Neshatavar (None) | Kyoung Mu Lee (Seoul National University),2023-12-15 16:31:17+00:00,,,,,,
Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer,,,,Hyeongjin Nam (None) | Daniel Jung (Seoul National University) | Gyeongsik Moon (None) | Kyoung Mu Lee (Seoul National University),,,,,,,
Classification-Free 3D Object Grounding with Regularized Concept Learners,,,,Chun Feng (University Of Science And Technology Of China) | Joy Hsu (Stanford University) | Weiyu Liu (Stanford University) | Jiajun Wu (Stanford University),,,,,,,
BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation,,,,"Yunhao Ge (University Of Southern California) | Yihe Tang (Stanford University) | Jiashu Xu (University Of Southern California) | Cem Gokmen (Stanford University) | Chengshu Li (Stanford University) | Wensi Ai (Stanford University) | Benjamin Martinez (Stanford University) | Arman Aydin (Stanford University) | Mona Anvari (Computer Science Department, Stanford University) | Ayush Chakravarthy (Stanford University) | Hong-Xing Yu (Computer Science Department, Stanford University) | Josiah Wong (Stanford University) | Sanjana Srivastava (Stanford University) | Sharon Lee (Stanford University) | Shengxin Zha (Meta GenAI) | Laurent Itti (USC) | Yunzhu Li (University Of Illinois Urbana-Champaign) | Roberto Mart??n-Mart??n (University Of Texas At Austin) | Miao Liu (META AI) | Pengchuan Zhang (Meta AI) | Ruohan Zhang (Stanford University) | Li Fei-Fei (Stanford University) | Jiajun Wu (Stanford University)",,,,,,,
Hearing Anything Anywhere,,,,Mason Wang (Stanford University) | Ryosuke Sawata (Sony Research) | Samuel Clarke (Stanford University) | Ruohan Gao (Stanford University) | Shangzhe Wu (Stanford University) | Jiajun Wu (Stanford University),,,,,,,
Learning the 3D Fauna of the Web,"Learning 3D models of all animals on the Earth requires massively scaling up existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an approach that learns a pan-category deformable 3D animal model for more than 100 animal species jointly. One crucial bottleneck of modeling animals is the limited availability of training data, which we overcome by simply learning from 2D Internet images. We show that prior category-specific attempts fail to generalize to rare species with limited training images. We address this challenge by introducing the Semantic Bank of Skinned Models (SBSM), which automatically discovers a small set of base animal shapes by combining geometric inductive priors with semantic knowledge implicitly captured by an off-the-shelf self-supervised feature extractor. To train such a model, we also contribute a new large-scale dataset of diverse animal species. At inference time, given a single image of any quadruped animal, our model reconstructs an articulated 3D mesh in a feed-forward fashion within seconds.",http://arxiv.org/abs/2401.02400v1,,Zizhang Li (Zhejiang University) | Dor Litvak (University Of Texas At Austin) | Ruining Li (University Of Oxford) | Yunzhi Zhang (Stanford University) | Tomas Jakab (University Of Oxford) | Christian Rupprecht (University Of Oxford) | Shangzhe Wu (Stanford University) | Andrea Vedaldi (University Of Oxford) | Jiajun Wu (Stanford University),2024-01-04 18:32:48+00:00,,,,,,
ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image,"We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view synthesis for in-the-wild scenes. While existing methods are designed for single objects with masked backgrounds, we propose new techniques to address challenges introduced by in-the-wild multi-object scenes with complex backgrounds. Specifically, we train a generative prior on a mixture of data sources that capture object-centric, indoor, and outdoor scenes. To address issues from data mixture such as depth-scale ambiguity, we propose a novel camera conditioning parameterization and normalization scheme. Further, we observe that Score Distillation Sampling (SDS) tends to truncate the distribution of complex backgrounds during distillation of 360-degree scenes, and propose ""SDS anchoring"" to improve the diversity of synthesized novel views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset in the zero-shot setting, even outperforming methods specifically trained on DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark for single-image novel view synthesis, and demonstrate strong performance in this setting. Our code and data are at http://kylesargent.github.io/zeronvs/",http://arxiv.org/abs/2310.17994v1,,"Kyle Sargent (Computer Science Department, Stanford University) | Zizhang Li (Zhejiang University) | Tanmay Shah (Google) | Charles Herrmann (Google) | Hong-Xing Yu (Computer Science Department, Stanford University) | Yunzhi Zhang (Stanford University) | Eric Ryan Chan (Stanford University) | Dmitry Lagun (Google) | Li Fei-Fei (Stanford University) | Deqing Sun (Google) | Jiajun Wu (Stanford University)",2023-10-27 09:06:43+00:00,,,,,,
Exploring Efficient Asymmetric Blind-Spots for Self-supervised Denoising in Real-World Scenarios,,,,Shiyan Chen (Peking University) | Jiyuan Zhang (Peking University) | Zhaofei Yu (Peking University) | Tiejun Huang (Peking University),,,,,,,
Spike-guided Motion Deblurring with Unknown Modal Spatiotemporal Alignment,,,,Jiyuan Zhang (Peking University) | Shiyan Chen (Peking University) | Yajing Zheng (Peking University) | Zhaofei Yu (Peking University) | Tiejun Huang (Peking University),,,,,,,
Super-Resolution Reconstruction from Bayer-Pattern Spike Streams,,,,Yanchen Dong (Peking University) | Ruiqin Xiong (Peking University) | Jian Zhang (None) | Zhaofei Yu (Peking University) | Xiaopeng Fan (Harbin Institute Of Technology) | Shuyuan Zhu (University Of Electronic Science And Technology Of China) | Tiejun Huang (Peking University),,,,,,,
Boosting Spike Camera Image Reconstruction from a Perspective of Dealing with Spike Fluctuations,,,,Rui Zhao (None) | Ruiqin Xiong (Peking University) | Jing Zhao (Cncert) | Jian Zhang (None) | Xiaopeng Fan (Harbin Institute Of Technology) | Zhaofei Yu (Peking University) | Tiejun Huang (Peking University),,,,,,,
Intensity-Robust Autofocus for Spike Camera,,,,Changqing Su (Peking University) | Ye Zhiyuan (None) | Yongsheng Xiao (Nanchang Hangkong University) | You Zhou (Nanjing University) | Zhen Cheng (Tsinghua University) | Bo Xiong (Peking University) | Zhaofei Yu (Peking University) | Tiejun Huang (Peking University),,,,,,,
Active Object Detection with Knowledge Aggregation and Distillation,,,,Dejie Yang (Peking University) | Yang Liu (Peking University),,,,,,,
Torwards Open-Vocabulary HOI Detection via Conditional Multi-level Decoding and Fine-grained Semantic Enhancement,,,,Ting Lei (Peking University) | Shaofeng Yin (Peking University) | Yang Liu (Peking University),,,,,,,
OED: Towards One-stage End-to-End Dynamic Scene Graph Generation,,,,Guan Wang (Peking University) | Zhimin Li (Tencent Data Platform) | Qingchao Chen (Peking University) | Yang Liu (Peking University),,,,,,,
Diff-BGM: A Diffusion Model for Video Background Music Generation,,,,Sizhe Li (Peking University) | Yiming Qin (Peking University) | Minghang Zheng (Peking University) | Xin Jin (Beijing Electronic Science And Technology Institute) | Yang Liu (Peking University),,,,,,,
MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation,,,,Zhicheng Zhang (Nankai University) | Pancheng Zhao (Nankai University) | Eunil Park (Sungkyunkwan University) | Jufeng Yang (None),,,,,,,
Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM,,,,Pingping Zhang (Dalian University Of Technology) | Tianyu Yan (Dalian University Of Technology) | Yang Liu (Dalian University Of Technology) | Huchuan Lu (Dalian University Of Technology),,,,,,,
Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification,"Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities. Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR). They are formulated as two new loss functions, which improve the feature discrimination with background suppression. As a result, our framework can generate more discriminative features for multi-modal object ReID. Extensive experiments on three multi-modal ReID benchmarks verify the effectiveness of our methods. The code is available at https://github.com/924973292/EDITOR.",http://arxiv.org/abs/2403.10254v1,,"Pingping Zhang (Dalian University Of Technology) | Yuhao Wang (Dalian University Of Technology) | Yang Liu (Dalian University Of Technology) | Zhengzheng Tu (Key Laboratory Of Intelligent Computing And Signal Processing Of Ministry Of Education, Anhui University) | Huchuan Lu (Dalian University Of Technology)",2024-03-15 12:44:35+00:00,,,,,,
ExtDM: Dual Distribution Extrapolation Diffusion Model for Video Prediction,,,,Zhicheng Zhang (Nankai University) | Junyao Hu (Nankai University) | Wentao Cheng (Nankai University) | Danda Paudel (None) | Jufeng Yang (None),,,,,,,
Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration,,,,Shihao Zhou (Nankai University) | Duosheng Chen (Nankai University) | Jinshan Pan (Nanjing University Of Science And Technology) | Jinglei Shi (Nankai University) | Jufeng Yang (None),,,,,,,
Multi-view Aggregation Network for Dichotomous Image Segmentation,,,,Qian Yu (Dalian University Of Technology) | Xiaoqi Zhao (Dalian University Of Technology) | Youwei Pang (Dalian University Of Technology) | Lihe Zhang (Dalian University Of Technology) | Huchuan Lu (Dalian University Of Technology),,,,,,,
LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion,,,,Pancheng Zhao (Nankai University) | Peng Xu (Tsinghua University) | Pengda Qin (Alibaba Group) | Deng-Ping Fan (ETH Zurich) | Zhicheng Zhang (Nankai University) | Guoli Jia (None) | Bowen Zhou (Tsinghua University) | Jufeng Yang (None),,,,,,,
"Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline","We conduct a comprehensive study on a new task named power battery detection (PBD), which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD, which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task, we first elaborately collect a dataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from thousands of power batteries of $5$ manufacturers, with $7$ different visual interference. Then, we propose a novel segmentation-based solution for PBD, termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors, the representation of the point segmentation branch can be improved at both semantic and detail aspects.Besides, we design an effective distance-adaptive mask generation strategy, which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles, our segmentation-based MDCNet consistently outperforms various other corner detection, crowd counting and general/tiny object detection-based solutions, making it a strong baseline that can help facilitate future research in PBD. Finally, we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at \href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{X-ray PBD}.",http://arxiv.org/abs/2312.02528v2,,"Xiaoqi Zhao (Dalian University Of Technology) | Youwei Pang (Dalian University Of Technology) | Zhenyu Chen (Dalian University Of Technology) | Qian Yu (Dalian University Of Technology) | Lihe Zhang (Dalian University Of Technology) | Hanqi Liu (Ohio State University, Columbus) | Jiaming Zuo (University Of Southern California) | Huchuan Lu (Dalian University Of Technology)",2023-12-05 06:18:38+00:00,,,,,,
SimDA: Simple Diffusion Adapter for Efficient Video Generation,"The recent wave of AI-generated content has witnessed the great development and success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video (T2V) still falls short of expectations though attracting increasing interests. Existing works either train from scratch or adapt large T2I model to videos, both of which are computation and resource expensive. In this work, we propose a Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B parameters of a strong T2I model, adapting it to video generation in a parameter-efficient way. In particular, we turn the T2I model for T2V by designing light-weight spatial and temporal adapters for transfer learning. Besides, we change the original spatial attention to the proposed Latent-Shift Attention (LSA) for temporal consistency. With similar model architecture, we further train a video super-resolution model to generate high-definition (1024x1024) videos. In addition to T2V generation in the wild, SimDA could also be utilized in one-shot video editing with only 2 minutes tuning. Doing so, our method could minimize the training effort with extremely few tunable parameters for model adaptation.",http://arxiv.org/abs/2308.09710v1,,Zhen Xing (Fudan University) | Qi Dai (Microsoft Research Asia) | Han Hu (Microsft Research Asia) | Zuxuan Wu (Fudan University) | Yu-Gang Jiang (Fudan University),2023-08-18 17:58:44+00:00,,,,,,
Doubly Abductive Counterfactual Inference for Text-based Image Editing,"We study text-based image editing (TBIE) of a single image by counterfactual inference because it is an elegant formulation to precisely address the requirement: the edited image should retain the fidelity of the original one. Through the lens of the formulation, we find that the crux of TBIE is that existing techniques hardly achieve a good trade-off between editability and fidelity, mainly due to the overfitting of the single-image fine-tuning. To this end, we propose a Doubly Abductive Counterfactual inference framework (DAC). We first parameterize an exogenous variable as a UNet LoRA, whose abduction can encode all the image details. Second, we abduct another exogenous variable parameterized by a text encoder LoRA, which recovers the lost editability caused by the overfitted first abduction. Thanks to the second abduction, which exclusively encodes the visual transition from post-edit to pre-edit, its inversion -- subtracting the LoRA -- effectively reverts pre-edit back to post-edit, thereby accomplishing the edit. Through extensive experiments, our DAC achieves a good trade-off between editability and fidelity. Thus, we can support a wide spectrum of user editing intents, including addition, removal, manipulation, replacement, style transfer, and facial change, which are extensively validated in both qualitative and quantitative evaluations. Codes are in https://github.com/xuesong39/DAC.",http://arxiv.org/abs/2403.02981v1,,Xue Song (Fudan University) | Jiequan Cui (Nanyang Technological University) | Hanwang Zhang (Nanyang Technological University) | Jingjing Chen (Fudan University) | Richang Hong (Hefei University Of Technology) | Yu-Gang Jiang (Fudan University),2024-03-05 13:59:21+00:00,,,,,,
Learning to Rank Patches for Unbiased Image Redundancy Reduction,,,,Yang Luo (Fudan University) | Zhineng Chen (Fudan University) | Peng Zhou (Amazon) | Zuxuan Wu (Fudan University) | Xieping Gao (None) | Yu-Gang Jiang (Fudan University),,,,,,,
MotionEditor: Editing Video Motion via Content-Aware Diffusion,"Existing diffusion-based video editing models have made gorgeous advances for editing attributes of a source video over time but struggle to manipulate the motion information while preserving the original protagonist's appearance and background. To address this, we propose MotionEditor, a diffusion model for video motion editing. MotionEditor incorporates a novel content-aware motion adapter into ControlNet to capture temporal motion correspondence. While ControlNet enables direct generation based on skeleton poses, it encounters challenges when modifying the source motion in the inverted noise due to contradictory signals between the noise (source) and the condition (reference). Our adapter complements ControlNet by involving source content to transfer adapted control signals seamlessly. Further, we build up a two-branch architecture (a reconstruction branch and an editing branch) with a high-fidelity attention injection mechanism facilitating branch interaction. This mechanism enables the editing branch to query the key and value from the reconstruction branch in a decoupled manner, making the editing branch retain the original background and protagonist appearance. We also propose a skeleton alignment algorithm to address the discrepancies in pose size and position. Experiments demonstrate the promising motion editing ability of MotionEditor, both qualitatively and quantitatively.",http://arxiv.org/abs/2311.18830v1,,Shuyuan Tu (Fudan University) | Qi Dai (Microsoft Research Asia) | Zhi-Qi Cheng (Carnegie Mellon University) | Han Hu (Microsft Research Asia) | Xintong Han (Huya Inc) | Zuxuan Wu (Fudan University) | Yu-Gang Jiang (Fudan University),2023-11-30 18:59:33+00:00,,,,,,
OmniVid: A Generative Framework for Universal Video Understanding,,,,Junke Wang (None) | Dongdong Chen (Microsoft Research) | Chong Luo (Microsoft Research Asia) | Bo He (None) | Lu Yuan (Microsoft) | Zuxuan Wu (Fudan University) | Yu-Gang Jiang (Fudan University),,,,,,,
"Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras","The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.",http://arxiv.org/abs/2311.16728v1,,Huajian Huang (The Hong Kong University Of Science And Technology) | Longwei Li (SUN YAT-SEN UNIVERSITY) | Hui Cheng (SUN YAT-SEN UNIVERSITY) | Sai-Kit Yeung (The Hong Kong University Of Science And Technology (HKUST)),2023-11-28 12:19:00+00:00,,,,,,
360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries,"Portable 360$^\circ$ cameras are becoming a cheap and efficient tool to establish large visual databases. By capturing omnidirectional views of a scene, these cameras could expedite building environment models that are essential for visual localization. However, such an advantage is often overlooked due to the lack of valuable datasets. This paper introduces a new benchmark dataset, 360Loc, composed of 360$^\circ$ images with ground truth poses for visual localization. We present a practical implementation of 360$^\circ$ mapping combining 360$^\circ$ images with lidar data to generate the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that explores the challenge of cross-device visual positioning, involving 360$^\circ$ reference frames, and query frames from pinhole, ultra-wide FoV fisheye, and 360$^\circ$ cameras. We propose a virtual camera approach to generate lower-FoV query frames from 360$^\circ$ images, which ensures a fair comparison of performance among different query types in visual localization tasks. We also extend this virtual camera approach to feature matching-based and pose regression-based methods to alleviate the performance loss caused by the cross-device domain gap, and evaluate its effectiveness against state-of-the-art baselines. We demonstrate that omnidirectional visual localization is more robust in challenging large-scale scenes with symmetries and repetitive structures. These results provide new insights into 360-camera mapping and omnidirectional visual localization with cross-device queries.",http://arxiv.org/abs/2311.17389v1,,Huajian Huang (The Hong Kong University Of Science And Technology) | Changkun Liu (Hong Kong University Of Science And Technology) | Yipeng Zhu (Hong Kong University Of Science And Technology) | Hui Cheng (SUN YAT-SEN UNIVERSITY) | Tristan Braud (Hong Kong University Of Science And Technology) | Sai-Kit Yeung (The Hong Kong University Of Science And Technology (HKUST)),2023-11-29 06:42:12+00:00,,,,,,
Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates,"Neural radiance field is an emerging rendering method that generates high-quality multi-view consistent images from a neural scene representation and volume rendering. Although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.",http://arxiv.org/abs/2309.11281v2,,Ka Chun SHUM (The Hong Kong University Of Science And Technology) | Jaeyeon Kim (Hong Kong University Of Science And Technology) | Binh-Son Hua (Trinity College Dublin) | Thanh Nguyen (Deakin University) | Sai-Kit Yeung (The Hong Kong University Of Science And Technology (HKUST)),2023-09-20 13:05:42+00:00,,,,,,
CoralSCOP: Segment any COral Image on this Planet,,,,"Zheng Ziqiang (Hong Kong University Of Science And Technology) | Liang Haixin (None) | Binh-Son Hua (Trinity College Dublin) | Tim, Yue Him Wong (Shenzhen University) | Put ANG (The Chinese University Of Hong Kong) | Apple CHUI (Chinese University Of Hong Kong) | Sai-Kit Yeung (The Hong Kong University Of Science And Technology (HKUST))",,,,,,,
Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes,,,,"Gaurav Shrivastava (Department Of Computer Science, University Of Maryland, College Park) | Abhinav Shrivastava (University Of Maryland)",,,,,,,
Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation,,,,Ming Xu (Australian National University) | Stephen Gould (Australian National University),,,,,,,
Rethinking Inductive Biases for Surface Normal Estimation,"Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) encode the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp - yet, piecewise smooth - predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state-of-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset. The code is available at https://github.com/baegwangbin/DSINE.",http://arxiv.org/abs/2403.00712v1,,Gwangbin Bae (Imperial College London) | Andrew J. Davison (Imperial College London),2024-03-01 17:54:37+00:00,,,,,,
SuperPrimitive: Scene Reconstruction at a Primitive Level,"Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.",http://arxiv.org/abs/2312.05889v1,,Kirill Mazur (Imperial College London) | Gwangbin Bae (Imperial College London) | Andrew J. Davison (Imperial College London),2023-12-10 13:44:03+00:00,,,,,,
Beyond Seen Primitive Concepts and Attribute-Object Compositional Learning,,,,"Nirat Saini (None) | Khoi Pham (University Of Maryland, College Park) | Abhinav Shrivastava (University Of Maryland)",,,,,,,
3DInAction: Understanding Human Actions in 3D Point Clouds,"We propose a novel method for 3D point cloud action recognition. Understanding human actions in RGB videos has been widely studied in recent years, however, its 3D point cloud counterpart remains under-explored. This is mostly due to the inherent limitation of the point cloud data modality -- lack of structure, permutation invariance, and varying number of points -- which makes it difficult to learn a spatio-temporal representation. To address this limitation, we propose the 3DinAction pipeline that first estimates patches moving in time (t-patches) as a key building block, alongside a hierarchical architecture that learns an informative spatio-temporal representation. We show that our method achieves improved performance on existing datasets, including DFAUST and IKEA ASM.",http://arxiv.org/abs/2303.06346v1,,"Yizhak Ben-Shabat (Technion, Israel Institute Of Technology) | Oren Shrout (Faculty Of Electrical And Computer Engineering - Technion, Israel) | Stephen Gould (Australian National University)",2023-03-11 08:42:54+00:00,,,,,,
Gaussian Splatting SLAM,,,,Hidenobu Matsuki (Imperial College London) | Riku Murai (Imperial College London) | Paul Kelly (Imperial College London) | Andrew J. Davison (Imperial College London),,,,,,,
Composing Object Relations and Attributes for Image-Text Matching,,,,"Khoi Pham (University Of Maryland, College Park) | Chuong Huynh (University Of Maryland, College Park) | Ser-Nam Lim (Meta AI) | Abhinav Shrivastava (University Of Maryland)",,,,,,,
Small Steps and Level Sets: Fitting Neural Surface Models with Point Guidance,,,,"Chamin Hewa Koneputugodage (Australian National University) | Yizhak Ben-Shabat (Technion, Israel Institute Of Technology) | Dylan Campbell (Australian National University) | Stephen Gould (Australian National University)",,,,,,,
EscherNet: A Generative Model for Scalable View Synthesis,"We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{https://kxhit.github.io/EscherNet}.",http://arxiv.org/abs/2402.03908v1,,Xin Kong (Imperial College London) | Shikun Liu (Imperial College London) | Xiaoyang Lyu (University Of Hong Kong) | Marwan Taher (The University Of Sheffield) | Xiaojuan Qi (University Of Oxford) | Andrew J. Davison (Imperial College London),2024-02-06 11:21:58+00:00,,,,,,
Differentiable Neural Surface Refinement for Transparent Objects,,,,Weijian Deng (Australian National University) | Dylan Campbell (Australian National University) | Chunyi Sun (Australian National University) | Shubham Kanitkar (RIOS Intelligent Machines) | Matthew Shaffer (RIOS Intelligent Machines) | Stephen Gould (Australian National University),,,,,,,
Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions,"The many variations of Implicit Neural Representations (INRs), where a neural network is trained as a continuous representation of a signal, have tremendous practical utility for downstream tasks including novel view synthesis, video compression, and image superresolution. Unfortunately, the inner workings of these networks are seriously under-studied. Our work, eXplaining the Implicit Neural Canvas (XINC), is a unified framework for explaining properties of INRs by examining the strength of each neuron's contribution to each output pixel. We call the aggregate of these contribution maps the Implicit Neural Canvas and we use this concept to demonstrate that the INRs which we study learn to ''see'' the frames they represent in surprising ways. For example, INRs tend to have highly distributed representations. While lacking high-level object semantics, they have a significant bias for color and edges, and are almost entirely space-agnostic. We arrive at our conclusions by examining how objects are represented across time in video INRs, using clustering to visualize similar neurons across layers and architectures, and show that this is dominated by motion. These insights demonstrate the general usefulness of our analysis framework. Our project page is available at https://namithap10.github.io/xinc.",http://arxiv.org/abs/2401.10217v1,,"Namitha Padmanabhan (University Of Maryland) | Matthew A Gwilliam (University Of Maryland, College Park) | Pulkit Kumar (None) | Shishira R Maiya (University Of Maryland) | Max Ehrlich (University Of Maryland, College Park) | Abhinav Shrivastava (University Of Maryland)",2024-01-18 18:57:40+00:00,,,,,,
Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images,,,,Chaoqin Huang (Shanghai Jiao Tong University) | Aofan Jiang (Shanghai Jiao Tong University) | Jinghao Feng (Shanghai Jiao Tong University) | Ya Zhang (Shanghai Jiao Tong University) | Xinchao Wang (National University Of Singapore) | Yanfeng Wang (Shanghai Jiao Tong University),,,,,,,
Audio-Visual Segmentation via Unlabeled Frame Exploitation,,,,Jinxiang Liu (Shanghai Jiao Tong University) | Yikun Liu (Shanghai Jiao Tong University) | Ferenas (None) | Chen Ju (None) | Ya Zhang (Shanghai Jiao Tong University) | Yanfeng Wang (Shanghai Jiao Tong University),,,,,,,
Low-Rank Knowledge Decomposition for Medical Foundation Models,,,,Yuhang Zhou (None) | Haolin Li (Fudan University) | Siyuan Du (Fudan University) | Jiangchao Yao (Shanghai Jiao Tong University) | Ya Zhang (Shanghai Jiao Tong University) | Yanfeng Wang (Shanghai Jiao Tong University),,,,,,,
Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning,,,,Zihua Zhao (Shanghai Jiao Tong University) | Mengxi Chen (Shanghai Jiao Tong University) | Tianjie Dai (Shanghai Jiao Tong University) | Jiangchao Yao (Shanghai Jiao Tong University) | Bo Han (HKBU) | Ya Zhang (Shanghai Jiao Tong University) | Yanfeng Wang (Shanghai Jiao Tong University),,,,,,,
Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration,"Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.",http://arxiv.org/abs/2402.05746v2,,"Yuxi Wei (None) | Zi Wang (CMU, Carnegie Mellon University) | Yifan Lu (Shanghai Jiao Tong University) | Chenxin Xu (Shanghai Jiao Tong University & National University Of Singapore) | Changxing Liu (Shanghai Jiao Tong University) | Hao Zhao (Tsinghua University) | Siheng Chen (Shanghai Jiao Tong University) | Yanfeng Wang (Shanghai Jiao Tong University)",2024-02-08 15:26:28+00:00,,,,,,
Free3D: Consistent Novel View Synthesis without 3D Representation,"We introduce Free3D, a simple approach designed for open-set novel view synthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from a pre-trained 2D image generator for generalization, and fine-tune it for NVS. Compared to recent and concurrent works, we obtain significant improvements without resorting to an explicit 3D representation, which is slow and memory-consuming or training an additional 3D network. We do so by encoding better the target camera pose via a new per-pixel ray conditioning normalization (RCN) layer. The latter injects pose information in the underlying 2D image generator by telling each pixel its specific viewing direction. We also improve multi-view consistency via a light-weight multi-view attention layer and multi-view noise sharing. We train Free3D on the Objaverse dataset and demonstrate excellent generalization to various new categories in several new datasets, including OminiObject3D and GSO. We hope our simple and effective approach will serve as a solid baseline and help future research in NVS with more accuracy pose. The project page is available at https://chuanxiaz.com/free3d/.",http://arxiv.org/abs/2312.04551v1,,Chuanxia Zheng (University Of Oxford) | Andrea Vedaldi (University Of Oxford),2023-12-07 18:59:18+00:00,,,,,,
Splatter Image: Ultra-Fast Single-View 3D Reconstruction,"We introduce the Splatter Image, an ultra-fast approach for monocular 3D object reconstruction which operates at 38 FPS. Splatter Image is based on Gaussian Splatting, which has recently brought real-time rendering, fast training, and excellent scaling to multi-view reconstruction. For the first time, we apply Gaussian Splatting in a monocular reconstruction setting. Our approach is learning-based, and, at test time, reconstruction only requires the feed-forward evaluation of a neural network. The main innovation of Splatter Image is the surprisingly straightforward design: it uses a 2D image-to-image network to map the input image to one 3D Gaussian per pixel. The resulting Gaussians thus have the form of an image, the Splatter Image. We further extend the method to incorporate more than one image as input, which we do by adding cross-view attention. Owning to the speed of the renderer (588 FPS), we can use a single GPU for training while generating entire images at each iteration in order to optimize perceptual metrics like LPIPS. On standard benchmarks, we demonstrate not only fast reconstruction but also better results than recent and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.",http://arxiv.org/abs/2312.13150v1,,Stanislaw Szymanowicz (University Of Oxford) | Christian Rupprecht (University Of Oxford) | Andrea Vedaldi (University Of Oxford),2023-12-20 16:14:58+00:00,,,,,,
SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds,"We propose a novel feed-forward 3D editing framework called Shap-Editor. Prior research on editing 3D objects primarily concentrated on editing individual objects by leveraging off-the-shelf 2D image editing networks. This is achieved via a process called distillation, which transfers knowledge from the 2D network to 3D assets. Distillation necessitates at least tens of minutes per asset to attain satisfactory editing results, and is thus not very practical. In contrast, we ask whether 3D editing can be carried out directly by a feed-forward network, eschewing test-time optimisation. In particular, we hypothesise that editing can be greatly simplified by first encoding 3D objects in a suitable latent space. We validate this hypothesis by building upon the latent space of Shap-E. We demonstrate that direct 3D editing in this space is possible and efficient by building a feed-forward editor network that only requires approximately one second per edit. Our experiments show that Shap-Editor generalises well to both in-distribution and out-of-distribution 3D assets with different prompts, exhibiting comparable performance with methods that carry out test-time optimisation for each edited instance.",http://arxiv.org/abs/2312.09246v1,,Minghao Chen (University Of Oxford) | Junyu Xie (University Of Oxford) | Iro Laina (University Of Oxford) | Andrea Vedaldi (University Of Oxford),2023-12-14 18:59:06+00:00,,,,,,
GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering,"Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .",http://arxiv.org/abs/2402.10128v1,,"Abdullah J Hamdi (University Of Oxford) | Luke Melas-Kyriazi (VGG, University Of Oxford) | Jinjie Mai (KAUST) | Guocheng Qian (KAUST) | Ruoshi Liu (Columbia University) | Carl Vondrick (Columbia University) | Bernard Ghanem (KAUST) | Andrea Vedaldi (University Of Oxford)",2024-02-15 17:32:50+00:00,,,,,,
SpiderMatch: 3D Shape Matching with Global Optimality and Geometric Consistency,,,,Paul Roetzer (University Of Bonn) | Florian Bernard (University Of Bonn),,,,,,,
Unsupervised 3D Structure Inference from Category-Specific Image Collections,,,,Weikang Wang (Rheinische Friedrich-Wilhelms Universit??t Bonn) | Dongliang Cao (University Of Bonn) | Florian Bernard (University Of Bonn),,,,,,,
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation,"Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches.",http://arxiv.org/abs/2402.18920v4,,Dongliang Cao (University Of Bonn) | Marvin Eisenberger (Technical University Munich) | Nafie El Amrani (Rheinische Friedrich-Wilhelms Universit??t Bonn) | Daniel Cremers (Technical University Munich) | Florian Bernard (University Of Bonn),2024-02-29 07:26:23+00:00,,,,,,
Partial-to-Partial Shape Matching with Geometric Consistency,,,,Viktoria Ehm (Technische Universit??t M??nchen) | Maolin Gao (None) | Paul Roetzer (University Of Bonn) | Marvin Eisenberger (Technical University Munich) | Daniel Cremers (Technical University Munich) | Florian Bernard (University Of Bonn),,,,,,,
A Unified Framework for Human-centric Point Cloud Video Understanding,,,,Yiteng Xu (None) | Kecheng Ye (None) | Xiao Han (ShanghaiTech University) | Yiming Ren (None) | Xinge Zhu (The Chinese University Of Hong Kong) | Yuexin Ma (ShanghaiTech University),,,,,,,
LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment,"For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.",http://arxiv.org/abs/2402.17171v1,,Yiming Ren (None) | Xiao Han (ShanghaiTech University) | Chengfeng Zhao (ShanghaiTech University) | Jingya Wang (ShanghaiTech University) | Lan Xu (None) | Jingyi Yu (Shanghai Tech University) | Yuexin Ma (ShanghaiTech University),2024-02-27 03:08:44+00:00,,,,,,
HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes,"Human-centric 3D scene understanding has recently drawn increasing attention, driven by its critical impact on robotics. However, human-centric real-life scenarios are extremely diverse and complicated, and humans have intricate motions and interactions. With limited labeled data, supervised methods are difficult to generalize to general scenarios, hindering real-life applications. Mimicking human intelligence, we propose an unsupervised 3D detection method for human-centric scenarios by transferring the knowledge from synthetic human instances to real scenes. To bridge the gap between the distinct data representations and feature distributions of synthetic models and real point clouds, we introduce novel modules for effective instance-to-scene representation transfer and synthetic-to-real feature alignment. Remarkably, our method exhibits superior performance compared to current state-of-the-art techniques, achieving 87.8% improvement in mAP and closely approaching the performance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLife Dataset.",http://arxiv.org/abs/2403.02769v2,,"Yichen Yao (ShanghaiTech University) | Zimo Jiang (ShanghaiTech University) | YUJING SUN (The University Of Hong Kong, University Of Hong Kong) | Zhencai Zhu (Innovation Academy For Microsatellites) | Xinge Zhu (The Chinese University Of Hong Kong) | Runnan Chen (None) | Yuexin Ma (ShanghaiTech University)",2024-03-05 08:37:05+00:00,,,,,,
GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces,"The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds. However, the explicit and discrete representation encounters challenges when applied to scenes featuring reflective surfaces. In this paper, we present GaussianShader, a novel method that applies a simplified shading function on 3D Gaussians to enhance the neural rendering in scenes with reflective surfaces while preserving the training and rendering efficiency. The main challenge in applying the shading function lies in the accurate normal estimation on discrete 3D Gaussians. Specifically, we proposed a novel normal estimation framework based on the shortest axis directions of 3D Gaussians with a delicately designed loss to make the consistency between the normals and the geometries of Gaussian spheres. Experiments show that GaussianShader strikes a commendable balance between efficiency and visual quality. Our method surpasses Gaussian Splatting in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When compared to prior works handling reflective surfaces, such as Ref-NeRF, our optimization time is significantly accelerated (23h vs. 0.58h). Please click on our project website to see more results.",http://arxiv.org/abs/2311.17977v1,,Yingwenqi Jiang (None) | Jiadong Tu (None) | Yuan Liu (The University Of Hong Kong) | Xifeng Gao (Tencent America) | Xiaoxiao Long (The University Of Hong Kong) | Wenping Wang (Texas A&M University - College Station) | Yuexin Ma (ShanghaiTech University),2023-11-29 17:22:26+00:00,,,,,,
Multi-Space Alignments Towards Universal LiDAR Segmentation,,,,Youquan Liu (Hochschule Bremerhaven) | Lingdong Kong (National University Of Singapore) | Xiaoyang Wu (The University Of Hong Kong) | Runnan Chen (None) | Xin Li (East China Normal University) | Liang Pan (Shanghai AI Lab) | Ziwei Liu (Nanyang Technological University) | Yuexin Ma (ShanghaiTech University),,,,,,,
CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow,"Autonomous driving stands as a pivotal domain in computer vision, shaping the future of transportation. Within this paradigm, the backbone of the system plays a crucial role in interpreting the complex environment. However, a notable challenge has been the loss of clear supervision when it comes to Bird's Eye View elements. To address this limitation, we introduce CLIP-BEVFormer, a novel approach that leverages the power of contrastive learning techniques to enhance the multi-view image-derived BEV backbones with ground truth information flow. We conduct extensive experiments on the challenging nuScenes dataset and showcase significant and consistent improvements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive 8.5\% and 9.2\% enhancement in terms of NDS and mAP, respectively, over the previous best BEV model on the 3D object detection task.",http://arxiv.org/abs/2403.08919v1,,Chenbin Pan (Syracuse University) | Burhaneddin Yaman (Bosch Center For Artificial Intelligence) | Senem Velipasalar (Syracuse University) | Liu Ren (Bosch Research),2024-03-13 19:21:03+00:00,,,,,,
VLP: Vision Language Planning for Autonomous Driving,"Autonomous driving is a complex and challenging task that aims at safe motion planning through scene understanding and reasoning. While vision-only autonomous driving methods have recently achieved notable performance, through enhanced scene understanding, several key issues, including lack of reasoning, low generalization performance and long-tail scenarios, still need to be addressed. In this paper, we present VLP, a novel Vision-Language-Planning framework that exploits language models to bridge the gap between linguistic understanding and autonomous driving. VLP enhances autonomous driving systems by strengthening both the source memory foundation and the self-driving car's contextual understanding. VLP achieves state-of-the-art end-to-end planning performance on the challenging NuScenes dataset by achieving 35.9\% and 60.5\% reduction in terms of average L2 error and collision rates, respectively, compared to the previous best method. Moreover, VLP shows improved performance in challenging long-tail scenarios and strong generalization capabilities when faced with new urban environments.",http://arxiv.org/abs/2401.05577v3,,Chenbin Pan (Syracuse University) | Burhaneddin Yaman (Bosch Center For Artificial Intelligence) | Tommaso Nesti (None) | Abhirup Mallik (Bosch) | Alessandro G Allievi (Bosch / University Of Texas At Austin) | Senem Velipasalar (Syracuse University) | Liu Ren (Bosch Research),2024-01-10 23:00:40+00:00,,,,,,
Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion,,,,Su Sun (Purdue University) | Henry Zhao (Bosch Research) | Yuliang Guo (Bosch US Research) | Ruoyu Wang (Bosch) | Xinyu Huang (Robert Bosch Research NA) | Yingjie Victor Chen (Purdue University) | Liu Ren (Bosch Research),,,,,,,
USE: Universal Segment Embeddings for Open-Vocabulary Image Segmentation,,,,"Xiaoqi Wang (Ohio State University, Columbus) | Wenbin He (Bosch) | Xiwei Xuan (Bosch) | Clint Sebastian (Bosch) | Jorge Piazentin Ono (Bosch) | Xin Li (Bosch Reserach) | Sima Behpour (Bosch Center For Artificial Intelligence (BCAI)) | Thang Doan (McGill) | Liang Gou (Bosch) | Shen (Ohio State University) | Liu Ren (Bosch Research)",,,,,,,
Open-Vocabulary Segmentation with Semantic-Assisted Calibration,,,,Yong Liu (None) | Sule Bai (Tsinghua University) | Guanbin Li (Sun Yat-Sen University) | Yitong Wang (ByteDance Inc) | Yansong Tang (Tsinghua University),,,,,,,
Universal Segmentation at Arbitrary Granularity with Language Instruction,"This paper aims to achieve universal segmentation of arbitrary semantic level. Despite significant progress in recent years, specialist segmentation approaches are limited to specific tasks and data distribution. Retraining a new model for adaptation to new scenarios or settings takes expensive computation and time cost, which raises the demand for versatile and universal segmentation model that can cater to various granularity. Although some attempts have been made for unifying different segmentation tasks or generalization to various scenarios, limitations in the definition of paradigms and input-output spaces make it difficult for them to achieve accurate understanding of content at arbitrary granularity. To this end, we present UniLSeg, a universal segmentation model that can perform segmentation at any semantic level with the guidance of language instructions. For training UniLSeg, we reorganize a group of tasks from original diverse distributions into a unified data format, where images with texts describing segmentation targets as input and corresponding masks are output. Combined with a automatic annotation engine for utilizing numerous unlabeled data, UniLSeg achieves excellent performance on various tasks and settings, surpassing both specialist and unified segmentation models.",http://arxiv.org/abs/2312.01623v3,,Yong Liu (None) | Cairong Zhang (ByteDance) | Yitong Wang (ByteDance Inc) | Jiahao Wang (Shanghai AI Lab) | Yujiu Yang (Tsinghua University) | Yansong Tang (Tsinghua University),2023-12-04 04:47:48+00:00,,,,,,
Narrative Action Evaluation with Prompt-Guided Multimodal Interaction,,,,"Shiyi Zhang (None) | Sule Bai (Tsinghua University) | Guangyi Chen (MBZUAI, CMU) | Lei Chen (Beijing University Of Science And Technology) | Jiwen Lu (Tsinghua University) | Junle Wang (Tencent) | Yansong Tang (Tsinghua University)",,,,,,,
PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild,,,,"Kun Yuan (Kuaishou Technology) | Hongbo Liu (Tsinghua University) | Mading Li (Kuaishou Technology) | Muyi Sun (Institute Of Automation, Chinese Academy Of Sciences) | Ming Sun (Kuaishou Tech) | Jiachao Gong (Beijing Kuaishou ) | Jinhua Hao (Kuaishou Tech) | Chao Zhou (Peking University) | Yansong Tang (Tsinghua University)",,,,,,,
Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos,"We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos. Our method uses a masked auto-encoding framework to synthesize masked binaural (multi-channel) audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities. We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising. Through extensive experiments, we show that our features are generic enough to improve over multiple state-of-the-art baselines on both tasks on two challenging egocentric video datasets that offer binaural audio, EgoCom and EasyCom. Project: http://vision.cs.utexas.edu/projects/ego_av_corr.",http://arxiv.org/abs/2307.04760v2,,Sagnik Majumder (UT Austin & Meta AI) | Ziad Al-Halah (University Of Utah) | Kristen Grauman (University Of Texas At Austin),2023-07-10 17:58:17+00:00,,,,,,
Learning Object State Changes in Videos: An Open-World Perspective,"Object State Changes (OSCs) are pivotal for video understanding. While humans can effortlessly generalize OSC understanding from familiar to unknown objects, current approaches are confined to a closed vocabulary. Addressing this gap, we introduce a novel open-world formulation for the video OSC problem. The goal is to temporally localize the three stages of an OSC -- the object's initial state, its transitioning state, and its end state -- whether or not the object has been observed during training. Towards this end, we develop VidOSC, a holistic learning approach that: (1) leverages text and vision-language models for supervisory signals to obviate manually labeling OSC training data, and (2) abstracts fine-grained shared state representations from objects to enhance generalization. Furthermore, we present HowToChange, the first open-world benchmark for video OSC localization, which offers an order of magnitude increase in the label space and annotation volume compared to the best existing benchmark. Experimental results demonstrate the efficacy of our approach, in both traditional closed-world and open-world scenarios.",http://arxiv.org/abs/2312.11782v1,,Zihui Xue (None) | Kumar Ashutosh (None) | Kristen Grauman (University Of Texas At Austin),2023-12-19 01:33:46+00:00,,,,,,
Detours for Navigating Instructional Videos,"We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.",http://arxiv.org/abs/2401.01823v1,,Kumar Ashutosh (None) | Zihui Xue (None) | Tushar Nagarajan (Meta) | Kristen Grauman (University Of Texas At Austin),2024-01-03 16:38:56+00:00,,,,,,
SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos,,,,"Changan Chen (University Of Texas At Austin) | Kumar Ashutosh (None) | Rohit Girdhar (Meta) | David Harwath (University Of Texas, Austin) | Kristen Grauman (University Of Texas At Austin)",,,,,,,
LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset,"Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale, high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned, restricting the performance of data-driven methods on real data. To this end, we introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising 10,412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes, created manually by professional artists. On this top, we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides, we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA, extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks.",http://arxiv.org/abs/2312.12418v1,,"Haolin Liu (The Chinese University Of Hong Kong, Shenzhen) | Chongjie Ye (The Chinese University Of Hong Kong, Shenzhen) | Yinyu Nie (Huawei Technologies Ltd.) | Yingfan He (Chinese University Of Hong Kong, Shenzhen) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen)",2023-12-19 18:50:10+00:00,,,,,,
PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns,"In this paper, we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human images. Unlike prior arts constrained by specific input types, our method allows flexible specification of style (text or image) and texture (full garment, cropped sections, or texture patches) conditions. To address the entanglement challenge when using full garment images as conditions, we develop a two-stage pipeline with explicit disentanglement of style and texture. In the first stage, we generate a human parsing map reflecting the desired style conditioned on the input. In the second stage, we composite textures onto the parsing map areas based on the texture input. To represent complex and non-stationary textures that have never been achieved in previous fashion editing works, we first propose extracting hierarchical and balanced CLIP features and applying position encoding in VTON. Experiments demonstrate superior synthesis quality and personalization enabled by our method. The flexible control over style and texture mixing brings virtual try-on to a new level of user experience for online shopping and fashion design.",http://arxiv.org/abs/2312.04534v1,,"Shuliang Ning (The Chinese University Of HongKong, ShenZhen) | Duomin Wang (None) | Yipeng Qin (Cardiff University) | Zirong Jin (None) | Baoyuan Wang (Xiaobing.Ai) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen)",2023-12-07 18:53:18+00:00,,,,,,
IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images,,,,"Yushuang Wu (The Chinese University Of Hong Kong (Shenzhen)) | Luyue Shi (The Chinese University Of Hong Kong, Shenzhen) | Junhao Cai (Hong Kong University Of Science And Technology) | Weihao Yuan (Alibaba Group) | Lingteng Qiu (None) | Zilong Dong (Alibaba Group) | Liefeng Bo (None) | Shuguang Cui (The Chinese University Of Hong Kong, Shenzhen) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen)",,,,,,,
RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D,"Lifting 2D diffusion for 3D generation is a challenging problem due to the lack of geometric prior and the complex entanglement of materials and lighting in natural images. Existing methods have shown promise by first creating the geometry through score-distillation sampling (SDS) applied to rendered surface normals, followed by appearance modeling. However, relying on a 2D RGB diffusion model to optimize surface normals is suboptimal due to the distribution discrepancy between natural images and normals maps, leading to instability in optimization. In this paper, recognizing that the normal and depth information effectively describe scene geometry and be automatically estimated from images, we propose to learn a generalizable Normal-Depth diffusion model for 3D generation. We achieve this by training on the large-scale LAION dataset together with the generalizable image-to-depth and normal prior models. In an attempt to alleviate the mixed illumination effects in the generated materials, we introduce an albedo diffusion model to impose data-driven constraints on the albedo component. Our experiments show that when integrated into existing text-to-3D pipelines, our models significantly enhance the detail richness, achieving state-of-the-art results. Our project page is https://aigc3d.github.io/richdreamer/.",http://arxiv.org/abs/2311.16918v2,,"Lingteng Qiu (None) | Guanying Chen (The Chinese University Of Hong Kong, Shenzhen) | Xiaodong Gu (Alibaba Group) | Qi Zuo (Alibaba Group) | Mutian Xu (None) | Yushuang Wu (The Chinese University Of Hong Kong (Shenzhen)) | Weihao Yuan (Alibaba Group) | Zilong Dong (Alibaba Group) | Liefeng Bo (None) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen)",2023-11-28 16:22:33+00:00,,,,,,
MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures,"In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.",http://arxiv.org/abs/2312.02963v1,,"Zhangyang Xiong (None) | Chenghong Li (The Chinese University Of Hong Kong, Shenzhen) | Kenkun Liu (The Chinese University Of Hong Kong (Shenzhen???) | Hongjie Liao (Chinese University Of Hong Kong, Shenzhen) | Jianqiao HU (The Chinese University Of Hong Kong, Shenzhen) | Junyi Zhu (The Chinese University Of Hongkong, Shenzhen) | Shuliang Ning (The Chinese University Of HongKong, ShenZhen) | Lingteng Qiu (None) | Chongjie Wang (The Chinese University Of Hong Kong ???Shenzhen) | Shijie Wang (The Chinese University Of Hong Kong, Shenzhen) | Shuguang Cui (The Chinese University Of Hong Kong, Shenzhen) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen)",2023-12-05 18:50:12+00:00,,,,,,
NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis,"We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.",http://arxiv.org/abs/2307.07511v1,,Nilesh Kulkarni (None) | Davis Rempe (NVIDIA) | Kyle Genova (Google) | Abhijit Kundu (Google) | Justin Johnson (University Of Michigan) | David Fouhey (New York University) | Leonidas Guibas (Stanford University),2023-07-14 17:59:38+00:00,,,,,,
CurveCloudNet: Processing Point Clouds with 1D Structure,"Modern depth sensors such as LiDAR operate by sweeping laser-beams across the scene, resulting in a point cloud with notable 1D curve-like structures. In this work, we introduce a new point cloud processing scheme and backbone, called CurveCloudNet, which takes advantage of the curve-like structure inherent to these sensors. While existing backbones discard the rich 1D traversal patterns and rely on generic 3D operations, CurveCloudNet parameterizes the point cloud as a collection of polylines (dubbed a ""curve cloud""), establishing a local surface-aware ordering on the points. By reasoning along curves, CurveCloudNet captures lightweight curve-aware priors to efficiently and accurately reason in several diverse 3D environments. We evaluate CurveCloudNet on multiple synthetic and real datasets that exhibit distinct 3D size and structure. We demonstrate that CurveCloudNet outperforms both point-based and sparse-voxel backbones in various segmentation settings, notably scaling to large scenes better than point-based alternatives while exhibiting improved single-object performance over sparse-voxel alternatives. In all, CurveCloudNet is an efficient and accurate backbone that can handle a larger variety of 3D environments than past works.",http://arxiv.org/abs/2303.12050v2,,Colton Stearns (None) | Alex Fu (Illumix) | Jiateng Liu (Department Of Computer Science) | Jeong Joon Park (Stanford University) | Davis Rempe (NVIDIA) | Despoina Paschalidou (Stanford) | Leonidas Guibas (Stanford University),2023-03-21 17:41:36+00:00,,,,,,
Category-Level Multi-Part Multi-Joint 3D Shape Assembly,,,,Yichen Li (Massachusetts Institute Of Technology) | Kaichun Mo (NVIDIA Research) | Yueqi Duan (None) | He Wang (None) | Jiequan Zhang (None) | Lin Shao (National University Of Singapore) | Wojciech Matusik (Massachusetts Institute Of Technology) | Leonidas Guibas (Stanford University),,,,,,,
MultiPhys: Multi-Person Physics-aware 3D Motion Estimation,,,,Nicol??s Ugrinovic (Universitat Polit??cnica De Catalunya) | Boxiao Pan (Stanford University) | Georgios Pavlakos (University Of Texas At Austin) | Despoina Paschalidou (Stanford) | Bokui Shen (Stanford University) | Jordi Sanchez-Riera (IRI-CSIC - Institut De Rob??tica I Inform??tica Industrial) | Francesc Moreno-Noguer (Universidad Polit??cnica De Cataluna) | Leonidas Guibas (Stanford University),,,,,,,
CAD: Photorealistic 3D Generation via Adversarial Distillation,"The increased demand for 3D data in AR/VR, robotics and gaming applications, gave rise to powerful generative pipelines capable of synthesizing high-quality 3D objects. Most of these models rely on the Score Distillation Sampling (SDS) algorithm to optimize a 3D representation such that the rendered image maintains a high likelihood as evaluated by a pre-trained diffusion model. However, finding a correct mode in the high-dimensional distribution produced by the diffusion model is challenging and often leads to issues such as over-saturation, over-smoothing, and Janus-like artifacts. In this paper, we propose a novel learning paradigm for 3D synthesis that utilizes pre-trained diffusion models. Instead of focusing on mode-seeking, our method directly models the distribution discrepancy between multi-view renderings and diffusion priors in an adversarial manner, which unlocks the generation of high-fidelity and photorealistic 3D content, conditioned on a single image and prompt. Moreover, by harnessing the latent space of GANs and expressive diffusion model priors, our method facilitates a wide variety of 3D applications including single-view reconstruction, high diversity generation and continuous 3D interpolation in the open domain. The experiments demonstrate the superiority of our pipeline compared to previous works in terms of generation quality and diversity.",http://arxiv.org/abs/2312.06663v1,,"Ziyu Wan (City University Of Hong Kong) | Despoina Paschalidou (Stanford) | Ian Huang (Computer Science Department, Stanford University) | Hongyu Liu (Hong Kong University Of Science And Technology) | Bokui Shen (Stanford University) | Xiaoyu Xiang (Meta) | Jing Liao (City University Of Hong Kong) | Leonidas Guibas (Stanford University)",2023-12-11 18:59:58+00:00,,,,,,
Spatial-Aware Regression for Keypoint Localization,,,,Dongkai Wang (Peking University) | Shiliang Zhang (Peking University),,,,,,,
LocLLM: Exploiting Generalizable Human Keypoint Localization via Large Language Model,,,,Dongkai Wang (Peking University) | Shiyu Xuan (Peking University) | Shiliang Zhang (Peking University),,,,,,,
Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs,"Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in various multi-modal tasks. Nevertheless, their performance in fine-grained image understanding tasks is still limited. To address this issue, this paper proposes a new framework to enhance the fine-grained image understanding abilities of MLLMs. Specifically, we present a new method for constructing the instruction tuning dataset at a low cost by leveraging annotations in existing datasets. A self-consistent bootstrapping method is also introduced to extend existing dense object annotations into high-quality referring-expression-bounding-box pairs. These methods enable the generation of high-quality instruction data which includes a wide range of fundamental abilities essential for fine-grained image perception. Moreover, we argue that the visual encoder should be tuned during instruction tuning to mitigate the gap between full image perception and fine-grained image perception. Experimental results demonstrate the superior performance of our method. For instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We have also attained the top rank on the leaderboard of MMBench. This promising performance is achieved by training on only publicly available data, making it easily reproducible. The models, datasets, and codes are publicly available at https://github.com/SY-Xuan/Pink.",http://arxiv.org/abs/2310.00582v3,,Shiyu Xuan (Peking University) | Qingpei Guo (Ant Group) | Ming Yang (Ant Group) | Shiliang Zhang (Peking University),2023-10-01 05:53:15+00:00,,,,,,
DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data,,,,"Hanrong Ye (None) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology)",,,,,,,
CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs,,,,"Yingji Zhong (None) | Lanqing Hong (Huawei Technologies Ltd.) | Zhenguo Li (Huawei) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology)",,,,,,,
Interactive3D: Create What You Want by Interactive 3D Generation,,,,"Shaocong Dong (Hong Kong University Of Science And Technology) | Lihe Ding (The Chinese University Of Hong Kong) | Zhanpeng Huang (SenseTime Research) | Zibin Wang (Sensetime Group Limited) | Tianfan Xue (The Chinese University Of Hong Kong) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology)",,,,,,,
DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection,,,,"Lewei Yao (Harbin Institute Of Technology) | Renjie Pi (None) | Jianhua Han (Huawei Technologies Ltd.) | Xiaodan Liang (Sun Yat-Sen University) | Hang Xu (Huawei Noah??S Ark Lab) | Wei Zhang (Huawei Technologies Ltd.) | Zhenguo Li (Huawei) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology)",,,,,,,
Communication-Efficient Federated Learning with Accelerated Client Gradient,"Federated learning often suffers from unstable and slow convergence due to heterogeneous characteristics of participating clients. Such tendency is aggravated when the client participation ratio is low since the information collected from the clients at each round is prone to be more inconsistent. To tackle the challenge, we propose a novel federated learning framework, which improves the stability of the server-side aggregation step, which is achieved by sending the clients an accelerated model estimated with the global gradient to guide the local gradient updates. Our algorithm naturally aggregates and conveys the global update information to participants with no additional communication cost and does not require to store the past models in the clients. We also regularize local update to further reduce the bias and improve the stability of local updates. We perform comprehensive empirical studies on real data under various settings and demonstrate the remarkable performance of the proposed method in terms of accuracy and communication-efficiency compared to the state-of-the-art methods, especially with low client participation rates. Our code is available at https://github.com/ ninigapa0/FedAGM",http://arxiv.org/abs/2201.03172v1,,Geeho Kim (Seoul National University) | Jinkyu Kim (Seoul National University) | Bohyung Han (Seoul National University),2022-01-10 05:31:07+00:00,,,,,,
Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering,"We present Paint-it, a text-driven high-fidelity texture map synthesis method for 3D meshes via neural re-parameterized texture optimization. Paint-it synthesizes texture maps from a text description by synthesis-through-optimization, exploiting the Score-Distillation Sampling (SDS). We observe that directly applying SDS yields undesirable texture quality due to its noisy gradients. We reveal the importance of texture parameterization when using SDS. Specifically, we propose Deep Convolutional Physically-Based Rendering (DC-PBR) parameterization, which re-parameterizes the physically-based rendering (PBR) texture maps with randomly initialized convolution-based neural kernels, instead of a standard pixel-based parameterization. We show that DC-PBR inherently schedules the optimization curriculum according to texture frequency and naturally filters out the noisy signals from SDS. In experiments, Paint-it obtains remarkable quality PBR texture maps within 15 min., given only a text description. We demonstrate the generalizability and practicality of Paint-it by synthesizing high-quality texture maps for large-scale mesh datasets and showing test-time applications such as relighting and material control using a popular graphics engine. Project page: https://kim-youwang.github.io/paint-it",http://arxiv.org/abs/2312.11360v1,,Kim Youwang (Pohang University Of Science And Technology) | Tae-Hyun Oh (None) | Gerard Pons-Moll (University Of T??bingen),2023-12-18 17:17:08+00:00,,,,,,
Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective,,,,Jinjing Zhao (The University Of Sydney) | Fangyun Wei (None) | Chang Xu (University Of Sydney),,,,,,,
Towards Memorization-Free Diffusion Models,,,,Chen Chen (University Of Sydney) | Daochang Liu (University Of Sydney) | Chang Xu (University Of Sydney),,,,,,,
Relaxed Contrastive Learning for Federated Learning,"We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\""ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge margins on the standard benchmarks through extensive experimental results.",http://arxiv.org/abs/2401.04928v1,,Seonguk Seo (Seoul National University) | Jinkyu Kim (Seoul National University) | Geeho Kim (Seoul National University) | Bohyung Han (Seoul National University),2024-01-10 04:55:24+00:00,,,,,,
Observation-Guided Diffusion Probabilistic Models,,,,Junoh Kang (Seoul National University) | Jinyoung Choi (Seoul National University) | Sungik Choi (LG AI Research) | Bohyung Han (Seoul National University),,,,,,,
GEARS: Local Geometry-aware Hand-object Interaction Synthesis,,,,"Keyang Zhou (Eberhard-Karls-Universit??t T??bingen) | Bharat Lal Bhatnagar (Eberhard-Karls-Universit??t T??bingen) | Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) | Gerard Pons-Moll (University Of T??bingen)",,,,,,,
Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation,"Reconstructing human-object interaction in 3D from a single RGB image is a challenging task and existing data driven methods do not generalize beyond the objects present in the carefully curated 3D interaction datasets. Capturing large-scale real data to learn strong interaction and 3D shape priors is very expensive due to the combinatorial nature of human-object interactions. In this paper, we propose ProciGen (Procedural interaction Generation), a method to procedurally generate datasets with both, plausible interaction and diverse object variation. We generate 1M+ human-object interaction pairs in 3D and leverage this large-scale data to train our HDM (Hierarchical Diffusion Model), a novel method to reconstruct interacting human and unseen objects, without any templates. Our HDM is an image-conditioned diffusion model that learns both realistic interaction and highly accurate human and object shapes. Experiments show that our HDM trained with ProciGen significantly outperforms prior methods that requires template meshes and that our dataset allows training methods with strong generalization ability to unseen object instances. Our code and data will be publicly released at: https://virtualhumans.mpi-inf.mpg.de/procigen-hdm.",http://arxiv.org/abs/2312.07063v2,,"Xianghui Xie (University Of T??bingen) | Bharat Lal Bhatnagar (Eberhard-Karls-Universit??t T??bingen) | Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) | Gerard Pons-Moll (University Of T??bingen)",2023-12-12 08:32:55+00:00,,,,,,
NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors,"Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.",http://arxiv.org/abs/2403.03122v1,,"Yannan He (University Of T??bingen) | Garvita Tiwari (University Of Tuebingen And MPI-Saarbrucken) | Tolga Birdal (Imperial College London) | Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) | Gerard Pons-Moll (University Of T??bingen)",2024-03-05 17:07:29+00:00,,,,,,
Robust Image Denoising through Adversarial Frequency Mixup,,,,Donghun Ryou (Seoul National University) | Inju Ha (None) | Hyewon Yoo (Seoul National University) | Dongwan Kim (Seoul National University) | Bohyung Han (Seoul National University),,,,,,,
Residual Learning in Diffusion Models,,,,Zhang Junyu (Central South University) | Daochang Liu (University Of Sydney) | Eunbyung Park (SKKU) | Shichao Zhang (Central South University) | Chang Xu (University Of Sydney),,,,,,,
Random Entangled Tokens for Adversarially Robust Vision Transformer,,,,"Huihui Gong (University Of Sydney) | Minjing Dong (City University Of Hong Kong) | Siqi Ma (University Of New South Wales) | Seyit Camtepe (CSIRO) | Surya Nepal (, CSIRO) | Chang Xu (University Of Sydney)",,,,,,,
Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning,,,,"Rongjie Li (SIST ,ShanghaiTech University) | Yu Wu (ShanghaiTech University) | Xuming He (ShanghaiTech University)",,,,,,,
From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models,,,,"Rongjie Li (SIST ,ShanghaiTech University) | Songyang Zhang (Shanghai AI Laboratory) | Dahua Lin (The Chinese University Of Hong Kong) | Kai Chen (Shanghai AI Laboratory) | Xuming He (ShanghaiTech University)",,,,,,,
DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation,,,,Zeeshan Hayder (CSIRO) | Xuming He (ShanghaiTech University),,,,,,,
Towards Automated Movie Trailer Generation,,,,Dawit Argaw Argaw (None) | Mattia Soldan (None) | Alejandro Pardo (KAUST) | Chen Zhao (King Abdullah University Of Science And Technology (KAUST)) | Fabian Caba Heilbron (Adobe Research) | Joon Chung (KAIST) | Bernard Ghanem (KAUST),,,,,,,
Privacy-preserving Optics for Enhancing Protection in Face De-identification,,,,Jhon Lopez (Universidad Industrial De Santander) | Carlos Hinojosa (KAUST) | Henry Arguello (Universidad Industrial De Santander) | Bernard Ghanem (KAUST),,,,,,,
End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames,"Recently, temporal action detection (TAD) has seen significant performance improvement with end-to-end training. However, due to the memory bottleneck, only models with limited scales and limited data volumes can afford end-to-end training, which inevitably restricts TAD performance. In this paper, we reduce the memory consumption for end-to-end training, and manage to scale up the TAD backbone to 1 billion parameters and the input video to 1,536 frames, leading to significant detection performance. The key to our approach lies in our proposed temporal-informative adapter (TIA), which is a novel lightweight module that reduces training memory. Using TIA, we free the humongous backbone from learning to adapt to the TAD task by only updating the parameters in TIA. TIA also leads to better TAD representation by temporally aggregating context from adjacent frames throughout the backbone. We evaluate our model across four representative datasets. Owing to our efficient design, we are able to train end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the first end-to-end model to outperform the best feature-based methods.",http://arxiv.org/abs/2311.17241v1,,"Shuming Liu (KAUST) | Chenlin Zhang (Moonshot AI, Ltd) | Chen Zhao (King Abdullah University Of Science And Technology (KAUST)) | Bernard Ghanem (KAUST)",2023-11-28 21:31:04+00:00,,,,,,
Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning,"Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic training strategy that seamlessly transitions the pretrained model to a reversible network with much higher numerical precision. We evaluate Dr$^2$Net on various pretrained models and various tasks, and show that it can reach comparable performance to conventional finetuning but with significantly less memory usage.",http://arxiv.org/abs/2401.04105v1,,"Chen Zhao (King Abdullah University Of Science And Technology (KAUST)) | Shuming Liu (KAUST) | Karttikeya Mangalam (University Of California Berkeley) | Guocheng Qian (KAUST) | Fatimah Zohra (King Abdullah University Of Science And Technology) | Abdulmohsen Alghannam (University Of Virginia, Charlottesville) | Jitendra Malik (University Of California At Berkeley) | Bernard Ghanem (KAUST)",2024-01-08 18:59:31+00:00,,,,,,
Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement,,,,Zaid Khan (Northeastern University) | Vijay Kumar BG (NEC Laboratories America) | Samuel Schulter (None) | Yun Fu (Northeastern University) | Manmohan Chandraker (UC San Diego),,,,,,,
Instantaneous Perception of 3D Motion for Vehicles,,,,"Di Liu (Rutgers University, New Brunswick) | Bingbing Zhuang (NEC Labs America) | Dimitris N. Metaxas (Rutgers) | Manmohan Chandraker (UC San Diego)",,,,,,,
Delving into Lidar for Neural Radiance Field on Road Scenes,,,,"Shanlin Sun (University Of California, Irvine) | Bingbing Zhuang (NEC Labs America) | Ziyu Jiang (Texas A&M) | Buyu Liu (NEC-Labs) | Xiaohui Xie (University Of California, Irvine) | Manmohan Chandraker (UC San Diego)",,,,,,,
AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving,,,,"Mingfu Liang (Northwestern University) | Jong-Chyi Su (None) | Samuel Schulter (None) | Sparsh Garg (NEC Laboratories America) | Shiyu Zhao (Rutgers University, New Brunswick) | Ying Wu (Northwestern University) | Manmohan Chandraker (UC San Diego)",,,,,,,
The Manga Whisperer: Automatically Generating Transcriptions for Comics,"In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way.   To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages. The code, evaluation datasets and the pre-trained model can be found at: https://github.com/ragavsachdeva/magi.",http://arxiv.org/abs/2401.10224v1,,Ragav Sachdeva (University Of Oxford) | Andrew Zisserman (University Of Oxford),2024-01-18 18:59:09+00:00,,,,,,
Amodal Ground Truth and Completion in the Wild,"The problem we study in this paper is amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts. In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective. In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images. This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels. To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories. Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset. The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/.",http://arxiv.org/abs/2312.17247v1,,"Guanqi Zhan (VGG, University Of Oxford) | Chuanxia Zheng (University Of Oxford) | Weidi Xie (Shanghai Jiao Tong University) | Andrew Zisserman (University Of Oxford)",2023-12-28 18:59:41+00:00,,,,,,
AutoAD III: The Prequel -- Back to the Pixels,,,,"Tengda Han (University Of Oxford) | Max Bain (VGG, University Of Oxford) | Arsha Nagrani (Google ) | G??l Varol (Ecole Des Ponts ParisTech) | Weidi Xie (Shanghai Jiao Tong University) | Andrew Zisserman (University Of Oxford)",,,,,,,
Learning from One Continuous Video Stream,,,,Joao Carreira (DeepMind) | Michael King (Fit) | Viorica Patraucean (DeepMind) | Dilara Gokay (Google DeepMind) | Catalin Ionescu (Google) | Yi Yang (DeepMind) | Daniel Zoran (DeepMind) | Joseph Heyward (Google) | Carl Doersch (DeepMind) | Yusuf Aytar (Google DeepMind) | Dima Damen (None) | Andrew Zisserman (University Of Oxford),,,,,,,
Versatile Neural Video Codec,,,,Jiahao Li (Microsoft Research Asia) | Bin Li (Microsoft) | Yan Lu (Microsoft Research Asia),,,,,,,
Implicit Motion Function,,,,Yue Gao (Microsoft Research) | Jiahao Li (Microsoft Research Asia) | Lei Chu (Microsoft Research Asia) | Yan Lu (Microsoft Research Asia),,,,,,,
Hierarchical Intra-modal Correlation Learning for Label-free 3D Semantic Segmentation,,,,Xin Kang (None) | Lei Chu (Microsoft Research Asia) | Jiahao Li (Microsoft Research Asia) | Xuejin Chen (University Of Science And Technology Of China) | Yan Lu (Microsoft Research Asia),,,,,,,
Generative Latent Coding for Ultra-Low Bitrate Image Compression,,,,Zhaoyang Jia (University Of Science And Technology Of China) | Jiahao Li (Microsoft Research Asia) | Bin Li (Microsoft) | Houqiang Li (University Of Science And Technology Of China) | Yan Lu (Microsoft Research Asia),,,,,,,
Motion Blur Decomposition with Cross-shutter Guidance,,,,Xiang Ji (The University Of Tokyo) | Haiyang Jiang (None) | Yinqiang Zheng (None),,,,,,,
Rolling Shutter Correction with Intermediate Distortion Flow Estimation,,,,"Mingdeng Cao (The University Of Tokyo) | Sidi Yang (Shenzhen International Graduate School, Tsinghua University) | Yujiu Yang (Tsinghua University) | Yinqiang Zheng (None)",,,,,,,
Fooling Polarization-based Vision using Locally Controllable Polarizing Projection,"Polarization is a fundamental property of light that encodes abundant information regarding surface shape, material, illumination and viewing geometry. The computer vision community has witnessed a blossom of polarization-based vision applications, such as reflection removal, shape-from-polarization, transparent object segmentation and color constancy, partially due to the emergence of single-chip mono/color polarization sensors that make polarization data acquisition easier than ever. However, is polarization-based vision vulnerable to adversarial attacks? If so, is that possible to realize these adversarial attacks in the physical world, without being perceived by human eyes? In this paper, we warn the community of the vulnerability of polarization-based vision, which can be more serious than RGB-based vision. By adapting a commercial LCD projector, we achieve locally controllable polarizing projection, which is successfully utilized to fool state-of-the-art polarization-based vision algorithms for glass segmentation and color constancy. Compared with existing physical attacks on RGB-based vision, which always suffer from the trade-off between attack efficacy and eye conceivability, the adversarial attackers based on polarizing projection are contact-free and visually imperceptible, since naked human eyes can rarely perceive the difference of viciously manipulated polarizing light and ordinary illumination. This poses unprecedented risks on polarization-based vision, both in the monochromatic and trichromatic domain, for which due attentions should be paid and counter measures be considered.",http://arxiv.org/abs/2303.17890v1,,Zhuoxiao Li (Univerisity Of Tokyo) | Zhihang Zhong (Shanghai AI Lab) | Shohei Nobuhara (Kyoto Institute Of Technology) | Ko Nishino (Kyoto University) | Yinqiang Zheng (None),2023-03-31 08:48:57+00:00,,,,,,
IQ-VFI: Implicit Quadratic Motion Estimation for Video Frame Interpolation,,,,Mengshun Hu (None) | Kui Jiang (Harbin Institute Of Technology) | Zhihang Zhong (Shanghai AI Lab) | Zheng Wang (Wuhan University) | Yinqiang Zheng (None),,,,,,,
RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos,"We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/.",http://arxiv.org/abs/2401.12592v2,,Hongchi Xia (Shanghai Jiao Tong University) | Yang Fu (University Of California San Diego) | Sifei Liu (NVIDIA) | Xiaolong Wang (UCSD),2024-01-23 09:47:13+00:00,,,,,,
HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data,,,,"Mengqi Zhang (University Of California, San Diego) | Yang Fu (University Of California San Diego) | Zheng Ding (University Of California, San Diego) | Sifei Liu (NVIDIA) | Zhuowen Tu (University Of California, San Diego) | Xiaolong Wang (UCSD)",,,,,,,
COLMAP-Free 3D Gaussian Splatting,,,,Yang Fu (University Of California San Diego) | Sifei Liu (NVIDIA) | Amey Kulkarni (NVIDIA) | Jan Kautz (NVIDIA) | Alexei A. Efros (UC Berkeley) | Xiaolong Wang (UCSD),,,,,,,
CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation,"We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io",http://arxiv.org/abs/2402.14795v2,,"Jun Wang (University Of California, San Diego) | Yuzhe Qin (University Of California, San Diego) | Kaiming Kuang (University Of California, San Diego) | Yigit Korkmaz (University Of Southern California) | Akhilan Gurumoorthy (University Of California, San Diego) | Hao Su (UCSD) | Xiaolong Wang (UCSD)",2024-02-22 18:54:32+00:00,,,,,,
Imagine Before Go: Self-Supervised Generative Map for Object Goal Navigation,,,,"Sixian Zhang (None) | Xinyao Yu (University Of The Chinese Academy Of Sciences) | Xinhang Song (None) | XIAOHAN Wang (Xi'an Jiao Tong University) | Shuqiang Jiang (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
A Category Agnostic Model for Visual Rearrangement,,,,"Yuyi Liu (Institute Of Computing Technology,University Of The Chinese Academy Of Sciences) | Xinhang Song (None) | Weijie Li (Alibaba Group) | XIAOHAN Wang (Xi'an Jiao Tong University) | Shuqiang Jiang (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
An Interactive Navigation Method with Effect-oriented Affordance,,,,"XIAOHAN Wang (Xi'an Jiao Tong University) | Yuehu LIU (College Of Artificial Intelligence, Xi'an Jiao Tong University) | Xinhang Song (None) | Yuyi Liu (Institute Of Computing Technology,University Of The Chinese Academy Of Sciences) | Sixian Zhang (None) | Shuqiang Jiang (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation,,,,"Zihan Wang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Xiangyang Li (Institue Of Computing Technology, Chinese Academy Of Sciences) | Jiahao Yang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Yeqi Liu (Institute Of Computing Technology, Chinese Academy Of Sciences) | Junjie Hu (University Of Wisconsin, Madison) | Ming Jiang (Indiana University) | Shuqiang Jiang (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
Grounded Text-to-Image Synthesis with Attention Refocusing,"Driven by the scalable diffusion models trained on large-scale datasets, text-to-image synthesis methods have shown compelling results. However, these models still fail to precisely follow the text prompt involving multiple objects, attributes, or spatial compositions. In this paper, we reveal the potential causes in the diffusion model's cross-attention and self-attention layers. We propose two novel losses to refocus attention maps according to a given spatial layout during sampling. Creating the layouts manually requires additional effort and can be tedious. Therefore, we explore using large language models (LLM) to produce these layouts for our method. We conduct extensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate our proposed method. We show that our proposed attention refocusing effectively improves the controllability of existing approaches.",http://arxiv.org/abs/2306.05427v2,,"Quynh Phung (University Of Maryland, College Park) | Songwei Ge (University Of Maryland, College Park) | Jia-Bin Huang (University Of Maryland, College Park)",2023-06-08 17:59:59+00:00,,,,,,
Seeing the World through Your Eyes,"The reflective nature of the human eye is an underappreciated source of information about what the world around us looks like. By imaging the eyes of a moving person, we can collect multiple views of a scene outside the camera's direct line of sight through the reflections in the eyes. In this paper, we reconstruct a 3D scene beyond the camera's line of sight using portrait images containing eye reflections. This task is challenging due to 1) the difficulty of accurately estimating eye poses and 2) the entangled appearance of the eye iris and the scene reflections. Our method jointly refines the cornea poses, the radiance field depicting the scene, and the observer's eye iris texture. We further propose a simple regularization prior on the iris texture pattern to improve reconstruction quality. Through various experiments on synthetic and real-world captures featuring people with varied eye colors, we demonstrate the feasibility of our approach to recover 3D scenes using eye reflections.",http://arxiv.org/abs/2306.09348v2,,"Hadi Alzayer (University Of Maryland) | Kevin Zhang (University Of Maryland, College Park) | Brandon Y. Feng (Massachusetts Institute Of Technology) | Christopher Metzler (University Of Maryland, College Park) | Jia-Bin Huang (University Of Maryland, College Park)",2023-06-15 17:59:59+00:00,,,,,,
On the Content Bias in Frechet Video Distance,,,,"Songwei Ge (University Of Maryland, College Park) | Aniruddha Mahapatra (CMU, Carnegie Mellon University) | Gaurav Parmar (Carnegie Mellon University) | Jun-Yan Zhu (Carnegie Mellon University) | Jia-Bin Huang (University Of Maryland, College Park)",,,,,,,
In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing,"3D-aware GANs offer new capabilities for view synthesis while preserving the editing functionalities of their 2D counterparts. GAN inversion is a crucial step that seeks the latent code to reconstruct input images or videos, subsequently enabling diverse editing tasks through manipulation of this latent code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often has difficulty reconstructing images with out-of-distribution (OOD) objects such as faces with heavy make-up or occluding objects. We address this issue by explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea is to represent the image using two individual neural radiance fields: one for the in-distribution content and the other for the out-of-distribution object. The final reconstruction is achieved by optimizing the composition of these two radiance fields with carefully designed regularization. We demonstrate that our explicit decomposition alleviates the inherent trade-off between reconstruction fidelity and editability. We evaluate reconstruction accuracy and editability of our method on challenging real face images and videos and showcase favorable results against other baselines.",http://arxiv.org/abs/2302.04871v3,,"Yiran Xu (None) | Zhixin Shu (Adobe Systems) | Cameron Smith (Adobe Systems) | Seoung Wug Oh (Adobe Systems) | Jia-Bin Huang (University Of Maryland, College Park)",2023-02-09 18:59:56+00:00,,,,,,
Differentiable Point-based Inverse Rendering,"We present differentiable point-based inverse rendering, DPIR, an analysis-by-synthesis method that processes images captured under diverse illuminations to estimate shape and spatially-varying BRDF. To this end, we adopt point-based rendering, eliminating the need for multiple samplings per ray, typical of volumetric rendering, thus significantly enhancing the speed of inverse rendering. To realize this idea, we devise a hybrid point-volumetric representation for geometry and a regularized basis-BRDF representation for reflectance. The hybrid geometric representation enables fast rendering through point-based splatting while retaining the geometric details and stability inherent to SDF-based representations. The regularized basis-BRDF mitigates the ill-posedness of inverse rendering stemming from limited light-view angular samples. We also propose an efficient shadow detection method using point-based shadow map rendering. Our extensive evaluations demonstrate that DPIR outperforms prior works in terms of reconstruction accuracy, computational efficiency, and memory footprint. Furthermore, our explicit point-based representation and rendering enables intuitive geometry and reflectance editing. The code will be publicly available.",http://arxiv.org/abs/2312.02480v1,,Hoon-Gyu Chung (POSTECH) | Seokjun Choi (Pohang University Of Science And Technology) | Seung-Hwan Baek (POSTECH),2023-12-05 04:13:31+00:00,,,,,,
Dispersed Structured Light for Hyperspectral 3D Imaging,"Hyperspectral 3D imaging aims to acquire both depth and spectral information of a scene. However, existing methods are either prohibitively expensive and bulky or compromise on spectral and depth accuracy. In this work, we present Dispersed Structured Light (DSL), a cost-effective and compact method for accurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera system by placing a sub-millimeter thick diffraction grating film front of the projector. The grating disperses structured light based on light wavelength. To utilize the dispersed structured light, we devise a model for dispersive projection image formation and a per-pixel hyperspectral 3D reconstruction method. We validate DSL by instantiating a compact experimental prototype. DSL achieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth error of 1mm. We demonstrate that DSL outperforms prior work on practical hyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D imaging for diverse application domains, including computer vision and graphics, cultural heritage, geology, and biology.",http://arxiv.org/abs/2311.18287v1,,"Suhyun Shin (Pohang University Of Science And Technology) | Seokjun Choi (Pohang University Of Science And Technology) | Felix Heide (Department Of Computer Science, Princeton University) | Seung-Hwan Baek (POSTECH)",2023-11-30 06:45:52+00:00,,,,,,
Differentiable Display Photometric Stereo,"Photometric stereo leverages variations in illumination conditions to reconstruct surface normals. Display photometric stereo, which employs a conventional monitor as an illumination source, has the potential to overcome limitations often encountered in bulky and difficult-to-use conventional setups. In this paper, we present differentiable display photometric stereo (DDPS), addressing an often overlooked challenge in display photometric stereo: the design of display patterns. Departing from using heuristic display patterns, DDPS learns the display patterns that yield accurate normal reconstruction for a target system in an end-to-end manner. To this end, we propose a differentiable framework that couples basis-illumination image formation with analytic photometric-stereo reconstruction. The differentiable framework facilitates the effective learning of display patterns via auto-differentiation. Also, for training supervision, we propose to use 3D printing for creating a real-world training dataset, enabling accurate reconstruction on the target real-world setup. Finally, we exploit that conventional LCD monitors emit polarized light, which allows for the optical separation of diffuse and specular reflections when combined with a polarization camera, leading to accurate normal reconstruction. Extensive evaluation of DDPS shows improved normal-reconstruction accuracy compared to heuristic patterns and demonstrates compelling properties such as robustness to pattern initialization, calibration errors, and simplifications in image formation and reconstruction.",http://arxiv.org/abs/2306.13325v4,,Seokjun Choi (Pohang University Of Science And Technology) | Seungwoo Yoon (POSTECH) | Giljoo Nam (Meta) | Seungyong Lee (POSTECH) | Seung-Hwan Baek (POSTECH),2023-06-23 07:05:08+00:00,,,,,,
Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset,"Image datasets are essential not only in validating existing methods in computer vision but also in developing new methods. Most existing image datasets focus on trichromatic intensity images to mimic human vision. However, polarization and spectrum, the wave properties of light that animals in harsh environments and with limited brain capacity often rely on, remain underrepresented in existing datasets. Although spectro-polarimetric datasets exist, these datasets have insufficient object diversity, limited illumination conditions, linear-only polarization data, and inadequate image count. Here, we introduce two spectro-polarimetric datasets: trichromatic Stokes images and hyperspectral Stokes images. These novel datasets encompass both linear and circular polarization; they introduce multiple spectral channels; and they feature a broad selection of real-world scenes. With our dataset in hand, we analyze the spectro-polarimetric image statistics, develop efficient representations of such high-dimensional data, and evaluate spectral dependency of shape-from-polarization methods. As such, the proposed dataset promises a foundation for data-driven spectro-polarimetric imaging and vision research. Dataset and code will be publicly available.",http://arxiv.org/abs/2311.17396v2,,"Yujin Jeon (Pohang University Of Science And Technology) | Eunsue Choi (Pohang University Of Science And Technology) | Youngchan Kim (Pohang University Of Science And Technology) | Yunseong Moon (Pohang University Of Science And Technology) | Khalid Omer (Meta Reality Labs) | Felix Heide (Department Of Computer Science, Princeton University) | Seung-Hwan Baek (POSTECH)",2023-11-29 06:53:23+00:00,,,,,,
GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence,"We present GigaPose, a fast, robust, and accurate method for CAD-based novel object pose estimation in RGB images. GigaPose first leverages discriminative ""templates"", rendered images of the CAD models, to recover the out-of-plane rotation and then uses patch correspondences to estimate the four remaining parameters. Our approach samples templates in only a two-degrees-of-freedom space instead of the usual three and matches the input image to the templates using fast nearest-neighbor search in feature space, results in a speedup factor of 35x compared to the state of the art. Moreover, GigaPose is significantly more robust to segmentation errors. Our extensive evaluation on the seven core datasets of the BOP challenge demonstrates that it achieves state-of-the-art accuracy and can be seamlessly integrated with existing refinement methods. Additionally, we show the potential of GigaPose with 3D models predicted by recent work on 3D reconstruction from a single image, relaxing the need for CAD models and making 6D pose object estimation much more convenient. Our source code and trained models are publicly available at https://github.com/nv-nguyen/gigaPose",http://arxiv.org/abs/2311.14155v2,,Van Nguyen Nguyen (Ecole Des Ponts ParisTech) | Thibault Groueix (Adobe Systems) | Mathieu Salzmann (EPFL) | Vincent Lepetit (Ecole Des Ponts ParisTech),2023-11-23 18:55:03+00:00,,,,,,
SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering,"We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/",http://arxiv.org/abs/2311.12775v3,,Antoine Gu??don (Ecole Des Ponts ParisTech) | Vincent Lepetit (Ecole Des Ponts ParisTech),2023-11-21 18:38:03+00:00,,,,,,
NOPE: Novel Object Pose Estimation from a Single Image,"The practicality of 3D object pose estimation remains limited for many applications due to the need for prior knowledge of a 3D model and a training period for new objects. To address this limitation, we propose an approach that takes a single image of a new object as input and predicts the relative pose of this object in new images without prior knowledge of the object's 3D model and without requiring training time for new objects and categories. We achieve this by training a model to directly predict discriminative embeddings for viewpoints surrounding the object. This prediction is done using a simple U-Net architecture with attention and conditioned on the desired pose, which yields extremely fast inference. We compare our approach to state-of-the-art methods and show it outperforms them both in terms of accuracy and robustness. Our source code is publicly available at https://github.com/nv-nguyen/nope",http://arxiv.org/abs/2303.13612v1,,"Van Nguyen Nguyen (Ecole Des Ponts ParisTech) | Thibault Groueix (Adobe Systems) | Georgy Ponimatkin (CIIRC, Czech Technical University, Czech Technical University Of Prague) | Yinlin Hu (Magic Leap) | Renaud Marlet (INRIA) | Mathieu Salzmann (EPFL) | Vincent Lepetit (Ecole Des Ponts ParisTech)",2023-03-23 18:55:43+00:00,,,,,,
Deformable One-shot Face Stylization via DINO Semantic Guidance,"This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a self-supervised vision transformer, specifically DINO-ViT, to establish a robust and consistent facial structure representation across both real and style domains. Our stylization process begins by adapting the StyleGAN generator to be deformation-aware through the integration of spatial transformers (STN). We then introduce two innovative constraints for generator fine-tuning under the guidance of DINO semantics: i) a directional deformation loss that regulates directional vectors in DINO space, and ii) a relative structural consistency constraint based on DINO token self-similarities, ensuring diverse generation. Additionally, style-mixing is employed to align the color generation with the reference, minimizing inconsistent correspondences. This framework delivers enhanced deformability for general one-shot face stylization, achieving notable efficiency with a fine-tuning duration of approximately 10 minutes. Extensive qualitative and quantitative comparisons demonstrate our superiority over state-of-the-art one-shot face stylization methods. Code is available at https://github.com/zichongc/DoesFS",http://arxiv.org/abs/2403.00459v2,,Yang Zhou (Shenzhen University) | Zichong Chen (Shenzhen University) | Hui Huang (Shenzhen University),2024-03-01 11:30:55+00:00,,,,,,
Generating Non-Stationary Textures using Self-Rectification,,,,"Yang Zhou (Shenzhen University) | Rongjun Xiao (None) | Dani Lischinski (The Hebrew University Of Jerusalem, Israel) | Daniel Cohen-Or (Google) | Hui Huang (Shenzhen University)",,,,,,,
Distilling CLIP with Dual Guidance for Learning Discriminative Human Body Shape Representation,,,,Feng Liu (Michigan State University) | Minchul Kim (Michigan State University) | Zhiyuan Ren (Michigan State University) | Xiaoming Liu (None),,,,,,,
Robust Noisy Correspondence Learning with Equivariant Similarity Consistency,,,,Yuchen Yang (Xi'an University Of Electronic Science And Technology) | Erkun Yang (None) | Likai Wang (Xi'an University Of Electronic Science And Technology) | Cheng Deng (Xidian University),,,,,,,
TIGER: Time-Varying Denoising Model for 3D Point Cloud Generation with Diffusion Process,,,,Zhiyuan Ren (Michigan State University) | Minchul Kim (Michigan State University) | Feng Liu (Michigan State University) | Xiaoming Liu (None),,,,,,,
EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models,"Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.",http://arxiv.org/abs/2401.04608v1,,Jingyuan Yang (Shenzhen University) | Jiawei Feng (Shenzhen University) | Hui Huang (Shenzhen University),2024-01-09 15:23:21+00:00,,,,,,
Unveiling the Unknown: Unleashing the Power of Unknown to Known in Open-Set Source-Free Domain Adaptation,,,,Fuli Wan (Xi'an University Of Electronic Science And Technology) | Han Zhao (Xidian University) | Xu Yang (Xi'an University Of Electronic Science And Technology) | Cheng Deng (Xidian University),,,,,,,
SeaBird: Segmentation in Bird??s View with Dice Loss Improves Monocular 3D Detection of Large Objects,,,,Abhinav Kumar (Michigan State University) | Yuliang Guo (Bosch US Research) | Xinyu Huang (Robert Bosch Research NA) | Liu Ren (Bosch Research) | Xiaoming Liu (None),,,,,,,
Long-Tail Class Incremental Learning via Independent Sub-prototype Construction,,,,Xi Wang (Xidian University) | Xu Yang (Xi'an University Of Electronic Science And Technology) | Jie Yin (None) | Kun Wei (Xidian University) | Cheng Deng (Xidian University),,,,,,,
Capture Now and Embrace Future: A Versatile Framework for Continual Test-Time Domain Adaptation,,,,Xu Yang (Xi'an University Of Electronic Science And Technology) | Xuan Chen (Xi'an University Of Electronic Science And Technology) | Moqi Li (Xi'an University Of Electronic Science And Technology) | Kun Wei (Xidian University) | Cheng Deng (Xidian University),,,,,,,
KeyPoint Relative Position Encoding for Face Recognition,,,,"Minchul Kim (Michigan State University) | Feng Liu (Michigan State University) | Yiyang Su (None) | Anil Jain (, Michigan State University) | Xiaoming Liu (None)",,,,,,,
Grounding and Enhancing Grid-based Models for Neural Fields,,,,Zelin Zhao (SJTU) | FENGLEI FAN (The Chinese University Of Hong Kong) | Wenlong Liao (Shanghai Jiao Tong University) | Junchi Yan (Shanghai Jiao Tong University),,,,,,,
Circuit Design and Efficient Simulation of Quantum Inner Product and Empirical Studies of Its Effect on Near-Term Hybrid Quantum-Classic Machine Learning,,,,Hao Xiong (Shanghai Jiao Tong University) | Yehui Tang (Shanghai Jiao Tong University) | Xinyu Ye (Shanghai Jiao Tong University) | Junchi Yan (Shanghai Jiao Tong University),,,,,,,
Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach,,,,Beichen Zhang (Shanghai Jiao Tong University) | Xiaoxing Wang (Shanghai Jiao Tong University) | Xiaohan Qin (None) | Junchi Yan (Shanghai Jiao Tong University),,,,,,,
Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision,"With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning rotated box (RBox) from the horizontal box (HBox) has attracted more and more attention. In this paper, we explore a more challenging yet label-efficient setting, namely single point-supervised OOD, and present our approach called Point2RBox. Specifically, we propose to leverage two principles: 1) Synthetic pattern knowledge combination: By sampling around each labelled point on the image, we transfer the object feature to synthetic visual patterns with the known bounding box to provide the knowledge for box regression. 2) Transform self-supervision: With a transformed input image (e.g. scaled/rotated), the output RBoxes are trained to follow the same transformation so that the network can perceive the relative size/rotation between objects. The detector is further enhanced by a few devised techniques to cope with peripheral issues, e.g. the anchor/layer assignment as the size of the object is not available in our point supervision setting. To our best knowledge, Point2RBox is the first end-to-end solution for point-supervised OOD. In particular, our method uses a lightweight paradigm, yet it achieves a competitive performance among point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.",http://arxiv.org/abs/2311.14758v1,,Yi Yu (Southeast University) | Xue Yang (Shanghai AI Laboratory) | Qingyun Li (Harbin Institute Of Technology) | Feipeng Da (Southeast University) | Jifeng Dai (Tsinghua University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Junchi Yan (Shanghai Jiao Tong University),2023-11-23 15:57:41+00:00,,,,,,
On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation,,,,"Agneet Chatterjee (Arizona State University) | Tejas Gokhale (University Of Maryland, Baltimore County) | Chitta Baral (Arizona State University) | 'YZ' Yezhou Yang (Arizona State University)",,,,,,,
Continual Segmentation with Disentangled Objectness Learning and Class Recognition,"Most continual segmentation methods tackle the problem as a per-pixel classification task. However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance. Based on these findings, we propose CoMasTRe by disentangling continual segmentation into two stages: forgetting-resistant continual objectness learning and well-researched continual classification. CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage. During continual learning, a simple but effective distillation is adopted to strengthen objectness. To further mitigate the forgetting of old classes, we design a multi-label class distillation strategy suited for segmentation. We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets. Code will be available at https://github.com/jordangong/CoMasTRe.",http://arxiv.org/abs/2403.03477v2,,Yizheng Gong (Xi'an Jiaotong-Liverpool University) | Siyue Yu (Xi'an Jiaotong-Liverpool University) | Xiaoyang Wang (None) | Jimin Xiao (Xi'an Jiaotong-Liverpool University),2024-03-06 05:33:50+00:00,,,,,,
Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation,,,,Bingfeng Zhang (China University Of Petroleum (East China)) | Siyue Yu (Xi'an Jiaotong-Liverpool University) | Yunchao Wei (Beijing Jiao Tong University) | Yao Zhao (Beijing Jiao Tong University) | Jimin Xiao (Xi'an Jiaotong-Liverpool University),,,,,,,
ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations,"Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g., DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional T2I benchmarks, at the cost of significant computational resources. The unCLIP stack comprises T2I prior and diffusion image decoder. The T2I prior model alone adds a billion parameters compared to the Latent Diffusion Models, which increases the computational and high-quality data requirements. We introduce ECLIPSE, a novel contrastive learning method that is both parameter and data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g., CLIP) to distill the knowledge into the prior model. We demonstrate that the ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere 2.8% of the data, surpasses the baseline T2I priors with an average of 71.6% preference score under resource-limited setting. It also attains performance on par with SOTA big models, achieving an average of 63.36% preference score in terms of the ability to follow the text compositions. Extensive experiments on two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE priors consistently deliver high performance while significantly reducing resource dependency.",http://arxiv.org/abs/2312.04655v1,,Maitreya Patel (Arizona State University) | Changhoon Kim (Arizona State University) | Sheng Cheng (Arizona State University) | Chitta Baral (Arizona State University) | 'YZ' Yezhou Yang (Arizona State University),2023-12-07 19:32:39+00:00,,,,,,
WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models,"The rapid advancement of generative models, facilitating the creation of hyper-realistic images from textual descriptions, has concurrently escalated critical societal concerns such as misinformation. Although providing some mitigation, traditional fingerprinting mechanisms fall short in attributing responsibility for the malicious use of synthetic images. This paper introduces a novel approach to model fingerprinting that assigns responsibility for the generated images, thereby serving as a potential countermeasure to model misuse. Our method modifies generative models based on each user's unique digital fingerprint, imprinting a unique identifier onto the resultant content that can be traced back to the user. This approach, incorporating fine-tuning into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates near-perfect attribution accuracy with a minimal impact on output quality. Through extensive evaluation, we show that our method outperforms baseline methods with an average improvement of 11\% in handling image post-processes. Our method presents a promising and novel avenue for accountable model distribution and responsible use.",http://arxiv.org/abs/2306.04744v2,,Changhoon Kim (Arizona State University) | Kyle Min (Intel Labs) | Maitreya Patel (Arizona State University) | Sheng Cheng (Arizona State University) | 'YZ' Yezhou Yang (Arizona State University),2023-06-07 19:44:14+00:00,,,,,,
PSDPM: Prototype-based Secondary Discriminative Pixels Mining for Weakly Supervised Semantic Segmentation,,,,Xinqiao Zhao (Xi??an Jiaotong-Liverpool University) | Yang (None) | Tianhong Dai (University Of Aberdeen) | Bingfeng Zhang (China University Of Petroleum (East China)) | Jimin Xiao (Xi'an Jiaotong-Liverpool University),,,,,,,
Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation,"Semi-supervised semantic segmentation allows model to mine effective supervision from unlabeled data to complement label-guided training. Recent research has primarily focused on consistency regularization techniques, exploring perturbation-invariant training at both the image and feature levels. In this work, we proposed a novel feature-level consistency learning framework named Density-Descending Feature Perturbation (DDFP). Inspired by the low-density separation assumption in semi-supervised learning, our key insight is that feature density can shed a light on the most promising direction for the segmentation classifier to explore, which is the regions with lower density. We propose to shift features with confident predictions towards lower-density regions by perturbation injection. The perturbed features are then supervised by the predictions on the original features, thereby compelling the classifier to explore less dense regions to effectively regularize the decision boundary. Central to our method is the estimation of feature density. To this end, we introduce a lightweight density estimator based on normalizing flow, allowing for efficient capture of the feature density distribution in an online manner. By extracting gradients from the density estimator, we can determine the direction towards less dense regions for each feature. The proposed DDFP outperforms other designs on feature-level perturbations and shows state of the art performances on both Pascal VOC and Cityscapes dataset under various partition protocols. The project is available at https://github.com/Gavinwxy/DDFP.",http://arxiv.org/abs/2403.06462v2,,Xiaoyang Wang (None) | Huihui Bai (Beijing Jiaotong University) | Limin Yu (Xi'an Jiaotong-Liverpool University) | Yao Zhao (Beijing Jiao Tong University) | Jimin Xiao (Xi'an Jiaotong-Liverpool University),2024-03-11 06:59:05+00:00,,,,,,
eTraM: Event-based Traffic Monitoring Dataset,,,,Aayush Verma (None) | Bharatesh Chakravarthi (Arizona State University) | Arpitsinh Vaghela (None) | Hua Wei (Arizona State University) | 'YZ' Yezhou Yang (Arizona State University),,,,,,,
Physics-aware Hand-object Interaction Denoising,,,,Haowen Luo (Tsinghua University) | Yunze Liu (None) | Li Yi (None),,,,,,,
GenN2N: Generative NeRF2NeRF Translation,,,,Xiangyue Liu (None) | Han Xue (Tsinghua University) | Kunming Luo (Hong Kong University Of Science And Technology) | Ping Tan (Hong Kong University Of Science And Technology) | Li Yi (None),,,,,,,
"GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation","This paper presents GenH2R, a framework for learning generalizable vision-based human-to-robot (H2R) handover skills. The goal is to equip robots with the ability to reliably receive objects with unseen geometry handed over by humans in various complex trajectories. We acquire such generalizability by learning H2R handover at scale with a comprehensive solution including procedural simulation assets creation, automated demonstration generation, and effective imitation learning. We leverage large-scale 3D model repositories, dexterous grasp generation methods, and curve-based 3D animation to create an H2R handover simulation environment named \simabbns, surpassing the number of scenes in existing simulators by three orders of magnitude. We further introduce a distillation-friendly demonstration generation method that automatically generates a million high-quality demonstrations suitable for learning. Finally, we present a 4D imitation learning method augmented by a future forecasting objective to distill demonstrations into a visuo-motor handover policy. Experimental evaluations in both simulators and the real world demonstrate significant improvements (at least +10\% success rate) over baselines in all cases. The project page is https://GenH2R.github.io/.",http://arxiv.org/abs/2401.00929v1,,"Zifan Wang (Tsinghua University) | Junyu Chen (Tsinghua University) | Ziqing Chen (Tsinghua University) | Pengwei Xie (Electronic Engineering, Tsinghua University) | Rui Chen (Tsinghua University) | Li Yi (None)",2024-01-01 18:20:43+00:00,,,,,,
TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding,"Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities. However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support. To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities. TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels. To rapidly expand the data scale, we present a fully-automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system. With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis. Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis. Our data and code are available at https://taco2024.github.io.",http://arxiv.org/abs/2401.08399v1,,Yun Liu (None) | Haolin Yang (Beijing University Of Posts And Telecommunications) | Xu Si (Tsinghua University) | Ling Liu (Beijing Institute Of Technology) | Zipeng Li (Tsinghua University) | Yuxiang Zhang (Tsinghua University) | Yebin Liu (Tsinghua University) | Li Yi (None),2024-01-16 14:41:42+00:00,,,,,,
NAPGuard: Towards Detecting Naturalistic Adversarial Patches,,,,Siyang Wu (None) | Jiakai Wang (Zhongguancun Laboratory) | Jiejie Zhao (Zhongguancun Laboratory) | Yazhe Wang (Zhongguancun Laboratory) | Xianglong Liu (BUAA),,,,,,,
TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models,"The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TFMQ) framework building upon a Temporal Information Block which is just related to the time-step $t$ and unrelated to the sampling data. Powered by the pioneering block design, we devise temporal information aware reconstruction (TIAR) and finite set calibration (FSC) to align the full-precision temporal features in a limited time. Equipped with the framework, we can maintain the most temporal information and ensure the end-to-end generation quality. Extensive experiments on various datasets and diffusion models prove our state-of-the-art results. Remarkably, our quantization approach, for the first time, achieves model performance nearly on par with the full-precision model under 4-bit weight quantization. Additionally, our method incurs almost no extra computational cost and accelerates quantization time by $2.0 \times$ on LSUN-Bedrooms $256 \times 256$ compared to previous works. Our code is publicly available at https://github.com/ModelTC/TFMQ-DM.",http://arxiv.org/abs/2311.16503v3,,Yushi Huang (SenseTime) | Ruihao Gong (SenseTime) | Jing Liu (None) | Tianlong Chen (Massachusetts Institute Of Technology) | Xianglong Liu (BUAA),2023-11-27 12:59:52+00:00,,,,,,
Reg-PTQ: Regression-specialized Post-training Quantization for Fully Quantized Object Detector,,,,Yifu Ding (None) | Weilun Feng (Beijing University Of Aeronautics And Astronautics) | Chuyan Chen (Beijing University Of Aeronautics And Astronautics) | Jinyang Guo (Beijing University Of Aeronautics And Astronautics) | Xianglong Liu (BUAA),,,,,,,
PTQ4SAM: Post-Training Quantization for Segment Anything,,,,Chengtao Lv (None) | Hong Chen (Beijing University Of Aeronautics And Astronautics) | Jinyang Guo (Beijing University Of Aeronautics And Astronautics) | Yifu Ding (None) | Xianglong Liu (BUAA),,,,,,,
OTE: Exploring Accurate Scene Text Recognition Using One Token,,,,Jianjun Xu (University Of Science And Technology Of China) | Yuxin Wang (University Of Science And Technology Of China) | Hongtao Xie (University Of Science And Technology Of China) | Yongdong Zhang (University Of Science And Technology Of China),,,,,,,
\emph{RealCustom}: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization,,,,"Mengqi Huang (University Of Science And Technology Of China) | Zhendong Mao (None) | Mingcong Liu (ByteDance) | Qian HE (Institute Of Remote Sensing Application, Chinese Academic Of Sciences) | Yongdong Zhang (University Of Science And Technology Of China)",,,,,,,
DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection,,,,Yuhao Sun (University Of Science And Technology Of China) | Lingyun Yu (University Of Science And Technology Of China) | Hongtao Xie (University Of Science And Technology Of China) | Jiaming Li (University Of Science And Technology Of China) | Yongdong Zhang (University Of Science And Technology Of China),,,,,,,
DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations,"The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is https://tianhao-qi.github.io/DEADiff/.",http://arxiv.org/abs/2403.06951v2,,"Tianhao Qi (University Of Science And Technology Of China) | Shancheng Fang (University Of Science And Technology Of China) | Yanze Wu (ByteDance) | Hongtao Xie (University Of Science And Technology Of China) | Jiawei Liu (ByteDance) | Lang Chen (ByteDance) | Qian HE (Institute Of Remote Sensing Application, Chinese Academic Of Sciences) | Yongdong Zhang (University Of Science And Technology Of China)",2024-03-11 17:35:23+00:00,,,,,,
UniVS: Unified and Universal Video Segmentation with Prompts as Queries,"Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \url{https://github.com/MinghanLi/UniVS}.",http://arxiv.org/abs/2402.18115v1,,"Minghan Li (The Hong Kong Polytechnic University ) | Shuai Li (The Hong Kong Polytechnic University) | Xindong Zhang (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Lei Zhang (The Hong Kong Polytechnic University)",2024-02-28 07:05:27+00:00,,,,,,
Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models,,,,Yabin Zhang (The Hong Kong Polytechnic University) | Wenjie Zhu (None) | Hui Tang (Hong Kong University Of Science And Technology) | Zhiyuan Ma (None) | Kaiyang Zhou (Hong Kong Baptist University) | Lei Zhang (The Hong Kong Polytechnic University),,,,,,,
SeeSR: Towards Semantics-Aware Real-World Image Super-Resolution,"Owe to the powerful generative priors, the pre-trained text-to-image (T2I) diffusion models have become increasingly popular in solving the real-world image super-resolution problem. However, as a consequence of the heavy quality degradation of input low-resolution (LR) images, the destruction of local structures can lead to ambiguous image semantics. As a result, the content of reproduced high-resolution image may have semantic errors, deteriorating the super-resolution performance. To address this issue, we present a semantics-aware approach to better preserve the semantic fidelity of generative real-world image super-resolution. First, we train a degradation-aware prompt extractor, which can generate accurate soft and hard semantic prompts even under strong degradation. The hard semantic prompts refer to the image tags, aiming to enhance the local perception ability of the T2I model, while the soft semantic prompts compensate for the hard ones to provide additional representation information. These semantic prompts can encourage the T2I model to generate detailed and semantically accurate results. Furthermore, during the inference process, we integrate the LR images into the initial sampling noise to mitigate the diffusion model's tendency to generate excessive random details. The experiments show that our method can reproduce more realistic image details and hold better the semantics.",http://arxiv.org/abs/2311.16518v1,,"Rongyuan Wu (Hong Kong Polytechnic University) | Tao Yang (Tsinghua University) | Lingchen Sun (Hong Kong Polytechnic University) | Zhengqiang ZHANG (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Shuai Li (The Hong Kong Polytechnic University) | Lei Zhang (The Hong Kong Polytechnic University)",2023-11-27 18:11:19+00:00,,,,,,
Neural Super-Resolution for Real-time Rendering with Radiance Demodulation,"It is time-consuming to render high-resolution images in applications such as video games and virtual reality, and thus super-resolution technologies become increasingly popular for real-time rendering. However, it is challenging to preserve sharp texture details, keep the temporal stability and avoid the ghosting artifacts in real-time super-resolution rendering. To address this issue, we introduce radiance demodulation to separate the rendered image or radiance into a lighting component and a material component, considering the fact that the light component is smoother than the rendered image so that the high-resolution material component with detailed textures can be easily obtained. We perform the super-resolution on the lighting component only and re-modulate it with the high-resolution material component to obtain the final super-resolution image with more texture details. A reliable warping module is proposed by explicitly marking the occluded regions to avoid the ghosting artifacts. To further enhance the temporal stability, we design a frame-recurrent neural network and a temporal loss to aggregate the previous and current frames, which can better capture the spatial-temporal consistency among reconstructed frames. As a result, our method is able to produce temporally stable results in real-time rendering with high-quality details, even in the challenging 4 $\times$ 4 super-resolution scenarios.",http://arxiv.org/abs/2308.06699v2,,Jia Li (Shandong University) | Ziling Chen (Shandong University) | Xiaolong Wu (None) | Lu Wang (Shandong University) | Beibei Wang (Nankai University) | Lei Zhang (The Hong Kong Polytechnic University),2023-08-13 06:40:41+00:00,,,,,,
Distilling ODE Solvers of Diffusion Models into Smaller Steps,"Distillation techniques have substantially improved the sampling speed of diffusion models, allowing of the generation within only one step or a few steps. However, these distillation methods require extensive training for each dataset, sampler, and network, which limits their practical applicability. To address this limitation, we propose a straightforward distillation approach, Distilled-ODE solvers (D-ODE solvers), that optimizes the ODE solver rather than training the denoising network. D-ODE solvers are formulated by simply applying a single parameter adjustment to existing ODE solvers. Subsequently, D-ODE solvers with smaller steps are optimized by ODE solvers with larger steps through distillation over a batch of samples. Our comprehensive experiments indicate that D-ODE solvers outperform existing ODE solvers, including DDIM, PNDM, DPM-Solver, DEIS, and EDM, especially when generating samples with fewer steps. Our method incur negligible computational overhead compared to previous distillation techniques, enabling simple and rapid integration with previous samplers. Qualitative analysis further shows that D-ODE solvers enhance image quality while preserving the sampling trajectory of ODE solvers.",http://arxiv.org/abs/2309.16421v1,,Sanghwan Kim (ETHZ - ETH Zurich) | Hao Tang (ETH Zurich And CMU) | Fisher Yu (ETH Zurich),2023-09-28 13:12:18+00:00,,,,,,
UniDepth: Universal Monocular Metric Depth Estimation,,,,Luigi Piccinelli (ETH Zurich) | Yung-Hsu Yang (None) | Christos Sakaridis (ETH Zurich) | Mattia Segu (ETH Zurich - Swiss Federal Institute Of Technology) | Siyuan Li (None) | Luc Van Gool (ETH Zurich) | Fisher Yu (ETH Zurich),,,,,,,
Matching Anything by Segmenting Anything,,,,Siyuan Li (None) | Lei Ke (HKUST & ETH Zurich) | Martin Danelljan (ETH Zurich) | Luigi Piccinelli (ETH Zurich) | Mattia Segu (ETH Zurich - Swiss Federal Institute Of Technology) | Luc Van Gool (ETH Zurich) | Fisher Yu (ETH Zurich),,,,,,,
MuRF: Multi-Baseline Radiance Fields,"We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward approach to solving sparse view synthesis under multiple different baseline settings (small and large baselines, and different number of input views). To render a target novel view, we discretize the 3D space into planes parallel to the target image plane, and accordingly construct a target view frustum volume. Such a target volume representation is spatially aligned with the target view, which effectively aggregates relevant information from the input views for high-quality rendering. It also facilitates subsequent radiance field regression with a convolutional network thanks to its axis-aligned nature. The 3D context modeled by the convolutional network enables our method to synthesis sharper scene structures than prior works. Our MuRF achieves state-of-the-art performance across multiple different baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K and LLFF). We also show promising zero-shot generalization abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability of MuRF.",http://arxiv.org/abs/2312.04565v1,,"Haofei Xu (Department Of Computer Science, ETHZ - ETH Zurich) | Anpei Chen (Department Of Computer Science, ETHZ - ETH Zurich) | Yuedong Chen (Monash University) | Christos Sakaridis (ETH Zurich) | Yulun Zhang (ETH Zurich) | Marc Pollefeys (ETH Zurich / Microsoft) | Andreas Geiger (University Of T??bingen) | Fisher Yu (ETH Zurich)",2023-12-07 18:59:56+00:00,,,,,,
UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather,,,,Haimei Zhao (The University Of Sydney) | Jing Zhang (The University Of Sydney) | Zhuo Chen (Tsinghua University) | Shanshan Zhao (JD Explore Academy) | Dacheng Tao (None),,,,,,,
Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis,,,,Yiyang Chen (South China University Of Technology) | Lunhao Duan (Wuhan University) | Shanshan Zhao (JD Explore Academy) | Changxing Ding (South China University Of Technology) | Dacheng Tao (None),,,,,,,
A Free Lunch for Faster and Better Data-Free Meta-Learning,,,,"Yongxian Wei (Tsinghua University) | Zixuan Hu (Tsinghua University) | Zhenyi Wang (University Of Maryland, College Park) | Li Shen (JD Explore Academy) | Chun Yuan (Tsinghua University) | Dacheng Tao (None)",,,,,,,
Sheared Backpropagation for Finetuning Foundation Models,,,,"Zhiyuan Yu (None) | Li Shen (JD Explore Academy) | Liang Ding (Zhejiang University) | Xinmei Tian (University Of Science And Technology Of China) | Yixin Chen (Washington University, Saint Louis) | Dacheng Tao (None)",,,,,,,
How Far Can We Compress Instant NGP-Based NeRF?,,,,Yihang Chen (Shanghai Jiao Tong University) | Qianyi Wu (Monash University) | Mehrtash Harandi (Monash University) | Jianfei Cai (Monash University),,,,,,,
Generative Region-Language Pretraining for Open-Ended Object Detection,,,,Chuang Lin (None) | Yi Jiang (Bytedance) | Lizhen Qu (Monash University) | Zehuan Yuan (Nanjing University) | Jianfei Cai (Monash University),,,,,,,
Taming Stable Diffusion for Text to 360???  Panorama Image Generation ,,,,Cheng Zhang (None) | Qianyi Wu (Monash University) | Camilo Cruz Gambardella (Monash University) | Xiaoshui Huang (Shanghai AI Laboratory) | Dinh Phung (Monash University) | Wanli Ouyang (University Of Sydney) | Jianfei Cai (Monash University),,,,,,,
Diversified and Personalized Multi-rater Medical Image Segmentation,,,,Yicheng Wu (Monash University) | Xiangde Luo (University Of Electronic Science And Technology Of China) | Zhe Xu (The Chinese University Of Hong Kong; Harvard Medical School) | Xiaoqing Guo (University Of Oxford) | Lie Ju (Monash University) | Zongyuan Ge (Monash University) | Wenjun Liao (University Of Electronic Science And Technology Of China) | Jianfei Cai (Monash University),,,,,,,
"HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild",,,,"Supreeth Narasimhaswamy (Stony Brook University, New York) | Huy Nguyen (, State University Of New York At Stony Brook) | Lihan Huang (University Of Science And Technology Of China) | Minh Hoai (State University Of New York, Stony Brook)",,,,,,,
HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances,"Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We conduct extensive quantitative and qualitative experiments and perform user studies to demonstrate the efficacy of our method in generating images with high-quality hands.",http://arxiv.org/abs/2403.01693v1,,"Supreeth Narasimhaswamy (Stony Brook University, New York) | Uttaran Bhattacharya (Adobe) | Xiang Chen (Adobe Research) | Ishita Dasgupta (Department Of Computer Science, University Of Massachusetts At Amherst) | Saayan Mitra (Adobe Research) | Minh Hoai (State University Of New York, Stony Brook)",2024-03-04 03:00:22+00:00,,,,,,
Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains,,,,"Bang-Dang Pham (VinAI Research) | Phong Tran (MBZUAI) | Anh Tran (VinAI Research) | Cuong Pham (Posts & Telecommunications Institute Of Technology And VinAI Research) | Rang Nguyen (VinAI Research) | Minh Hoai (State University Of New York, Stony Brook)",,,,,,,
AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation,"Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general transformer-based segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and model weights are available at: https://github.com/xmed-lab/AllSpark.",http://arxiv.org/abs/2403.01818v3,,Haonan Wang (The Hong Kong University Of Science And Technology) | Qixiang ZHANG (Hong Kong University Of Science And Technology) | Yi Li (Hong Kong University Of Science And Technology) | Xiaomeng Li (The Hong Kong University Of Science And Technology),2024-03-04 08:06:41+00:00,,,,,,
C2 RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction ,,,,Yiqun Lin (The Hong Kong University Of Science And Technology) | Jiewen Yang (Hong Kong University Of Science And Technology) | Hualiang Wang (HKUST) | Xinpeng Ding (The Hong Kong University Of Science And Technology) | Wei Zhao (Beijing University Of Aeronautics And Astronautics) | Xiaomeng Li (The Hong Kong University Of Science And Technology),,,,,,,
Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models,"The rise of multimodal large language models (MLLMs) has spurred interest in language-based driving tasks. However, existing research typically focuses on limited tasks and often omits key multi-view and temporal information which is crucial for robust autonomous driving. To bridge these gaps, we introduce NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17 subtasks, where each task demands holistic information (e.g., temporal, multi-view, and spatial), significantly elevating the challenge level. To obtain NuInstruct, we propose a novel SQL-based method to generate instruction-response pairs automatically, which is inspired by the driving logical progression of humans. We further present BEV-InMLLM, an end-to-end method for efficiently deriving instruction-aware Bird's-Eye-View (BEV) features, language-aligned for large language models. BEV-InMLLM integrates multi-view, spatial awareness, and temporal semantics to enhance MLLMs' capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g. around 9% improvement on various tasks. We plan to release our NuInstruct for future research development.",http://arxiv.org/abs/2401.00988v1,,Xinpeng Ding (The Hong Kong University Of Science And Technology) | Jianhua Han (Huawei Technologies Ltd.) | Hang Xu (Huawei Noah??S Ark Lab) | Xiaodan Liang (Sun Yat-Sen University) | Wei Zhang (Huawei Technologies Ltd.) | Xiaomeng Li (The Hong Kong University Of Science And Technology),2024-01-02 01:54:22+00:00,,,,,,
M3 -UDA: A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detection ,,,,Bin Pu (Hong Kong University Of Science And Technology) | Liwen Wang (Anhui University) | Jiewen Yang (Hong Kong University Of Science And Technology) | He Guannan (Sichuan University) | Xingbo Dong (Anhui University) | Shengli Li (Shenzhen Maternity And Child Healthcare Hospital) | Ying Tan (Shenzhen Maternity And Child Healthcare Hospital) | Ming Chen (Harbin Red Cross Central Hospital ) | Zhe Jin (Anhui University) | Kenli Li (Hunan University) | Xiaomeng Li (The Hong Kong University Of Science And Technology),,,,,,,
PeVL: Pose-Enhanced Vision-Language Model for Fine-Grained Human Action Recognition,,,,"Haosong Zhang (School Of Computer Science And Engineering, Nanyang Technological University) | Mei Leong (, A*STAR) | Liyuan Li (I2R, A*STAR) | Weisi Lin (Nanyang Technological University)",,,,,,,
R-Cyclic Diffuser: Reductive and Cyclic Latent Diffusion for 3D Clothed Human Digitalization,,,,"Kennard Chan (, A*STAR) | Fayao Liu (Institute For Infocomm Research, A*STAR) | Guosheng Lin (Nanyang Technological University) | Chuan-Sheng Foo (Centre For Frontier AI Research, A*STAR) | Weisi Lin (Nanyang Technological University)",,,,,,,
Boosting Image Quality Assessment through Efficient Transformer Adaptation with Local Feature Enhancement,,,,Kangmin Xu (Wuhan University) | Liang Liao (Nanyang Technological University) | Jing Xiao (Wuhan University) | Chaofeng Chen (Nanyang Technological University) | Haoning Wu (Nanyang Technological University) | Qiong Yan (SenseTime Research) | Weisi Lin (Nanyang Technological University),,,,,,,
Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models,"Multi-modality foundation models, as represented by GPT-4V, have brought a new paradigm for low-level visual perception and understanding tasks, that can respond to a broad range of natural human instructions in a model. While existing foundation models have shown exciting potentials on low-level visual tasks, their related abilities are still preliminary and need to be improved. In order to enhance these models, we conduct a large-scale subjective experiment collecting a vast number of real human feedbacks on low-level vision. Each feedback follows a pathway that starts with a detailed description on the low-level visual appearance (*e.g. clarity, color, brightness* of an image, and ends with an overall conclusion, with an average length of 45 words. The constructed **Q-Pathway** dataset includes 58K detailed human feedbacks on 18,973 images with diverse low-level appearance. Moreover, to enable foundation models to robustly respond to diverse types of questions, we design a GPT-participated conversion to process these feedbacks into diverse-format 200K instruction-response pairs. Experimental results indicate that the **Q-Instruct** consistently elevates low-level perception and understanding abilities across several foundational models. We anticipate that our datasets can pave the way for a future that general intelligence can perceive, understand low-level visual appearance and evaluate visual quality like a human. Our dataset, model zoo, and demo is published at: https://q-future.github.io/Q-Instruct.",http://arxiv.org/abs/2311.06783v1,,"Haoning Wu (Nanyang Technological University) | Zicheng Zhang (Shanghai Jiao Tong University) | Erli Zhang (Nanyang Technological University) | Chaofeng Chen (Nanyang Technological University) | Liang Liao (Nanyang Technological University) | Annan Wang (Nanyang Technological University) | Kaixin Xu (I2R, A*STAR) | Chunyi Li (None) | Jingwen Hou (Nanyang Technological University) | Guangtao Zhai (Shanghai Jiao Tong University) | Xue Geng (Institute For Infocomm Research, A*STAR) | Wenxiu Sun (SenseTime Research And Tetras.AI) | Qiong Yan (SenseTime Research) | Weisi Lin (Nanyang Technological University)",2023-11-12 09:10:51+00:00,,,,,,
Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior,"Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective ""Consistent3D"" method that explores the ODE deterministic sampling prior for text-to-3D generation. Specifically, at each training iteration, given a rendered image by a 3D model, we first estimate its desired 3D score function by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling. Next, we design a consistency distillation sampling loss which samples along the ODE trajectory to generate two adjacent samples and uses the less noisy sample to guide another more noisy one for distilling the deterministic prior into the 3D model. Experimental results show the efficacy of our Consistent3D in generating high-fidelity and diverse 3D objects and large-scale scenes, as shown in Fig. 1. The codes are available at https://github.com/sail-sg/Consistent3D.",http://arxiv.org/abs/2401.09050v1,,Zike Wu (Nanyang Technological University) | Pan Zhou (Sea Group) | YI Xuanyu (National Technological University) | Xiaoding Yuan (Johns Hopkins University) | Hanwang Zhang (Nanyang Technological University),2024-01-17 08:32:07+00:00,,,,,,
Diffusion Time-step Curriculum for One Image to 3D Generation,,,,"YI Xuanyu (National Technological University) | Zike Wu (Nanyang Technological University) | Qingshan Xu (Nanyang Technological University) | Pan Zhou (Sea Group) | Joo Lim (I2R, A*STAR) | Hanwang Zhang (Nanyang Technological University)",,,,,,,
Classes Are Not Equal: An Empirical Study on Image Recognition Fairness,"In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification. The Code is available at https://github.com/dvlab-research/Parametric-Contrastive-Learning.",http://arxiv.org/abs/2402.18133v2,,"Jiequan Cui (Nanyang Technological University) | Beier Zhu (Nanyang Technological University) | Xin Wen (The University Of Hong Kong) | Xiaojuan Qi (University Of Oxford) | Bei Yu (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Hanwang Zhang (Nanyang Technological University)",2024-02-28 07:54:50+00:00,,,,,,
Distributionally Generative Augmentation for Fair Facial Attribute Classification,"Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from a uniform distribution, while keeping target attribute unchanged. Then we train a fair FAC model by fostering model invariance to these augmentation. Extensive experiments on three common datasets demonstrate the effectiveness of our method in promoting fairness in FAC without compromising accuracy. Codes are in https://github.com/heqianpei/DiGA.",http://arxiv.org/abs/2403.06606v1,,Fengda Zhang (Zhejiang University) | Qianpei He (Zhejiang University) | Kun Kuang (Zhejiang University) | Jiashuo Liu (Tsinghua University) | Long Chen (HKUST) | Chao Wu (Zhejiang University) | Jun Xiao (Zhejiang University) | Hanwang Zhang (Nanyang Technological University),2024-03-11 10:50:53+00:00,,,,,,
Generative Image Dynamics,,,,Zhengqi Li (Google) | Richard Tucker (Google) | Noah Snavely (Google / Cornell) | Aleksander Holynski (UC Berkeley & Google Research),,,,,,,
Readout Guidance: Learning Control from Diffusion Features,"We present Readout Guidance, a method for controlling text-to-image diffusion models with learned signals. Readout Guidance uses readout heads, lightweight networks trained to extract signals from the features of a pre-trained, frozen diffusion model at every timestep. These readouts can encode single-image properties, such as pose, depth, and edges; or higher-order properties that relate multiple images, such as correspondence and appearance similarity. Furthermore, by comparing the readout estimates to a user-defined target, and back-propagating the gradient through the readout head, these estimates can be used to guide the sampling process. Compared to prior methods for conditional generation, Readout Guidance requires significantly fewer added parameters and training samples, and offers a convenient and simple recipe for reproducing different forms of conditional control under a single framework, with a single architecture and sampling procedure. We showcase these benefits in the applications of drag-based manipulation, identity-consistent generation, and spatially aligned control. Project page: https://readout-guidance.github.io.",http://arxiv.org/abs/2312.02150v1,,"Grace Luo (University Of California, Berkeley) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Oliver Wang (Adobe Research) | Dan Goldman (Google) | Aleksander Holynski (UC Berkeley & Google Research)",2023-12-04 18:59:32+00:00,,,,,,
Generative Powers of Ten,,,,"Xiaojuan Wang (Department Of Computer Science) | Janne Kontkanen (Research, Google) | Brian Curless (University Of Washington) | Steve Seitz (University Of Washington) | Ira Kemelmacher-Shlizerman (University Of Washington) | Ben Mildenhall (Google) | Pratul P. Srinivasan (Google Research) | Dor Verbin (None) | Aleksander Holynski (UC Berkeley & Google Research)",,,,,,,
ReconFusion: 3D Reconstruction with Diffusion Priors,"3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.",http://arxiv.org/abs/2312.02981v1,,Rundi Wu (Columbia University) | Ben Mildenhall (Google) | Philipp Henzler (Google) | Ruiqi Gao (Google) | Keunhong Park (Google) | Daniel Watson (Google DeepMind) | Pratul P. Srinivasan (Google Research) | Dor Verbin (None) | Jonathan T. Barron (Google) | Ben Poole (Google) | Aleksander Holynski (UC Berkeley & Google Research),2023-12-05 18:59:58+00:00,,,,,,
LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge,"Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and understand multi-modal signals. However, most of the existing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient extraction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empowers the MLLM by injecting visual knowledge in two levels. 1) Progressive incorporation of fine-grained spatial-aware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between image-level and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual evidence. We facilitate the MLLM with high-level semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imperfect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruction. Comprehensive experiments on several multi-modal benchmarks demonstrate the superiority of our model (e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).",http://arxiv.org/abs/2311.11860v2,,Gongwei Chen (Harbin Institute Of Technology) | Leyang Shen (Harbin Institute Of Technology) | Rui Shao (Harbin Institute Of Technology) | Xiang Deng (Harbin Institute Of Technology (Shenzhen)) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen)),2023-11-20 15:56:44+00:00,,,,,,
VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation,,,,Yang Chen (None) | Yingwei Pan (HiDream.Ai) | Haibo Yang (Fudan University) | Ting Yao (JD AI Research) | Tao Mei (JD Explore Academy),,,,,,,
GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians,"We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.",http://arxiv.org/abs/2312.02134v1,,"Liangxiao Hu (Harbin Institute Of Technology) | Hongwen Zhang (Beijing Normal University) | Yuxiang Zhang (Tsinghua University) | Boyao ZHOU (Tsinghua University) | Boning Liu (Department Of Automation, Tsinghua University) | Shengping Zhang (Harbin Institute Of Technology) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen))",2023-12-04 18:55:45+00:00,,,,,,
TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models,,,,Zhongwei Zhang (University Of Science And Technology Of China) | Fuchen Long (JD.Com) | Yingwei Pan (HiDream.Ai) | Zhaofan Qiu (University Of Science And Technology Of China) | Ting Yao (JD AI Research) | Yang Cao (University Of Science And Technology Of China) | Tao Mei (JD Explore Academy),,,,,,,
Boosting Diffusion Models with Moving Average Sampling in Frequency Domain,,,,Yurui Qian (University Of Science And Technology Of China) | Qi Cai (JD) | Yingwei Pan (HiDream.Ai) | Yehao Li (JD AI Research) | Ting Yao (JD AI Research) | Qibin Sun (University Of Science And Technology Of China) | Tao Mei (JD Explore Academy),,,,,,,
Fourier Priors-Guided Diffusion for Zero-Shot Joint Low-Light Enhancement and Deblurring,,,,Xiaoqian Lv (None) | Shengping Zhang (Harbin Institute Of Technology) | Chenyang Wang (Harbin Institute Of Technology) | Yichen Zheng (Huazhong University Of Science And Technology) | Bineng Zhong (Guangxi Normal University) | Chongyi Li (None) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen)),,,,,,,
DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation,,,,Chenyang Wang (Harbin Institute Of Technology) | Zerong Zheng (Tsinghua University) | Tao Yu (Tsinghua University) | Xiaoqian Lv (None) | Bineng Zhong (Guangxi Normal University) | Shengping Zhang (Harbin Institute Of Technology) | Liqiang Nie (Harbin Institute Of Technology (Shenzhen)),,,,,,,
Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution,,,,Zhikai Chen (None) | Fuchen Long (JD.Com) | Zhaofan Qiu (University Of Science And Technology Of China) | Ting Yao (JD AI Research) | Wengang Zhou (University Of Science And Technology Of China) | Jiebo Luo (University Of Rochester) | Tao Mei (JD Explore Academy),,,,,,,
PHYSCENE: Physically Interactable 3D Scene Synthesis for Embodied AI,,,,"Yandan Yang (Beihang University) | Baoxiong Jia (University Of California, Los Angeles) | Peiyuan Zhi (Beijing Institute For General Artificial Intelligence) | Siyuan Huang (Beijing Institute Of General Artificial Intelligence)",,,,,,,
AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents,,,,Jieming Cui (None) | Tengyu Liu (None) | Nian Liu (Beijing University Of Posts And Telecommunications) | Yaodong Yang (Peking University) | Yixin Zhu (Peking University) | Siyuan Huang (Beijing Institute Of General Artificial Intelligence),,,,,,,
Scaling Up Dynamic 3D Human-Scene Interaction Modelling,"Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.",http://arxiv.org/abs/2403.08629v1,,"Nan Jiang (Peking University) | Zhiyuan Zhang (Department Of Automation, Tsinghua University) | Hongjie Li (Peking University) | Xiaoxuan Ma (Peking University) | Zan Wang (None) | Yixin Chen (BIGAI) | Tengyu Liu (None) | Yixin Zhu (Peking University) | Siyuan Huang (Beijing Institute Of General Artificial Intelligence)",2024-03-13 15:45:04+00:00,,,,,,
"Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance",,,,"Zan Wang (None) | Yixin Chen (BIGAI) | Baoxiong Jia (University Of California, Los Angeles) | Puhao Li (Department Of Automation, Tsinghua University) | Jinlu Zhang (Peking University) | Jingze Zhang (Tsinghua University) | Tengyu Liu (None) | Yixin Zhu (Peking University) | Wei Liang (Beijing Institute Of Technology) | Siyuan Huang (Beijing Institute Of General Artificial Intelligence)",,,,,,,
DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free 2D-3D Matching,,,,Shuzhe Wang (Aalto University) | Juho Kannala (University Of Oulu) | Daniel Barath (ETHZ - ETH Zurich),,,,,,,
Scalable and Simplified Functional Map Learning,,,,"Robin Magnet (??cole Polytechnique) | Maks Ovsjanikov (Ecole Polytechnique, France)",,,,,,,
Functional Diffusion,"We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",http://arxiv.org/abs/2311.15435v1,,Biao Zhang (KAUST) | Peter Wonka (KAUST),2023-11-26 21:35:34+00:00,,,,,,
Towards Calibrated Multi-label Deep Neural Networks,,,,"Jiacheng Cheng (University Of California, San Diego) | Nuno Vasconcelos (University Of California San Diego)",,,,,,,
Self-Supervised Dual Contouring,,,,"Ramana Sundararaman (??cole Polytechnique) | Roman Klokov (??cole Polytechnique) | Maks Ovsjanikov (Ecole Polytechnique, France)",,,,,,,
ProTeCt: Prompt Tuning for Taxonomic Open Set Classification,,,,"Tz-Ying Wu (University Of California, San Diego) | Chih-Hui Ho (University Of California San Diego) | Nuno Vasconcelos (University Of California San Diego)",,,,,,,
Long-Tailed Anomaly Detection with Learnable Class Names,,,,Chih-Hui Ho (University Of California San Diego) | Kuan-Chuan Peng (Mitsubishi Electric Research Laboratories (MERL)) | Nuno Vasconcelos (University Of California San Diego),,,,,,,
WinSyn: A High Resolution Testbed for Synthetic Data,"We present WinSyn, a dataset consisting of high-resolution photographs and renderings of 3D models as a testbed for synthetic-to-real research. The dataset consists of 75,739 high-resolution photographs of building windows, including traditional and modern designs, captured globally. These include 89,318 cropped subimages of windows, of which 9,002 are semantically labeled. Further, we present our domain-matched photorealistic procedural model which enables experimentation over a variety of parameter distributions and engineering approaches. Our procedural model provides a second corresponding dataset of 21,290 synthetic images. This jointly developed dataset is designed to facilitate research in the field of synthetic-to-real learning and synthetic data generation. WinSyn allows experimentation into the factors that make it challenging for synthetic data to compete with real-world data. We perform ablations using our synthetic model to identify the salient rendering, materials, and geometric factors pertinent to accuracy within the labeling task. We chose windows as a benchmark because they exhibit a large variability of geometry and materials in their design, making them ideal to study synthetic data generation in a constrained setting. We argue that the dataset is a crucial step to enable future research in synthetic data generation for deep learning.",http://arxiv.org/abs/2310.08471v1,,Tom Kelly (King Abdullah University Of Science And Technology) | John Femiani (None) | Peter Wonka (KAUST),2023-10-09 20:18:10+00:00,,,,,,
Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features,"With the immense growth of dataset sizes and computing resources in recent years, so-called foundation models have become popular in NLP and vision tasks. In this work, we propose to explore foundation models for the task of keypoint detection on 3D shapes. A unique characteristic of keypoint detection is that it requires semantic and geometric awareness while demanding high localization accuracy. To address this problem, we propose, first, to back-project features from large pre-trained 2D vision models onto 3D shapes and employ them for this task. We show that we obtain robust 3D features that contain rich semantic information and analyze multiple candidate features stemming from different 2D foundation models. Second, we employ a keypoint candidate optimization module which aims to match the average observed distribution of keypoints on the shape and is guided by the back-projected features. The resulting approach achieves a new state of the art for few-shot keypoint detection on the KeyPointNet dataset, almost doubling the performance of the previous best methods.",http://arxiv.org/abs/2311.18113v1,,"Thomas Wimmer (Technical University Of Munich) | Peter Wonka (KAUST) | Maks Ovsjanikov (Ecole Polytechnique, France)",2023-11-29 21:58:41+00:00,,,,,,
PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation,"Single image depth estimation is a foundational task in computer vision and generative modeling. However, prevailing depth estimation models grapple with accommodating the increasing resolutions commonplace in today's consumer cameras and devices. Existing high-resolution strategies show promise, but they often face limitations, ranging from error propagation to the loss of high-frequency details. We present PatchFusion, a novel tile-based framework with three key components to improve the current state of the art: (1) A patch-wise fusion network that fuses a globally-consistent coarse prediction with finer, inconsistent tiled predictions via high-level feature guidance, (2) A Global-to-Local (G2L) module that adds vital context to the fusion network, discarding the need for patch selection heuristics, and (3) A Consistency-Aware Training (CAT) and Inference (CAI) approach, emphasizing patch overlap consistency and thereby eradicating the necessity for post-processing. Experiments on UnrealStereo4K, MVS-Synth, and Middleburry 2014 demonstrate that our framework can generate high-resolution depth maps with intricate details. PatchFusion is independent of the base model for depth estimation. Notably, our framework built on top of SOTA ZoeDepth brings improvements for a total of 17.3% and 29.4% in terms of the root mean squared error (RMSE) on UnrealStereo4K and MVS-Synth, respectively.",http://arxiv.org/abs/2312.02284v1,,Zhenyu Li (King Abdullah University Of Science And Technology) | Shariq Bhat (King Abdullah University Of Science And Technology (KAUST)) | Peter Wonka (KAUST),2023-12-04 19:03:12+00:00,,,,,,
Multiway Point Cloud Mosaicking with Diffusion and Global Optimization,,,,"Shengze Jin (Department Of Computer Science, ETHZ - ETH Zurich) | Iro Armeni (Stanford University) | Marc Pollefeys (ETH Zurich / Microsoft) | Daniel Barath (ETHZ - ETH Zurich)",,,,,,,
Absolute Pose from One or Two Scaled and Oriented Features,,,,Jonathan Ventura (None) | Zuzana Kukelova (Czech Technical University In Prague) | Torsten Sattler (Czech Technical University In Prague) | Daniel Barath (ETHZ - ETH Zurich),,,,,,,
Improved NeRF in Non-static Scenes Using Heuristics-Guided Segmentation,,,,"Jiahao Chen (SUN YAT-SEN UNIVERSITY) | Yipeng Qin (Cardiff University) | Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) | Jiangbo Lu (SmartMore Corporation) | Guanbin Li (Sun Yat-Sen University)",,,,,,,
AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning,,,,Duojun Huang (SUN YAT-SEN UNIVERSITY) | Xinyu Xiong (SUN YAT-SEN UNIVERSITY) | Jie Ma (SUN YAT-SEN UNIVERSITY) | Jichang Li (The University Of Hong Kong) | Zequn Jie (Meituan) | Lin Ma (Meituan) | Guanbin Li (Sun Yat-Sen University),,,,,,,
Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection,,,,Jiaming Li (Baidu) | Jiacheng Zhang (SUN YAT-SEN UNIVERSITY) | Jichang Li (The University Of Hong Kong) | Ge Li (Peking University Shenzhen Graduate School) | Si Liu (Beihang University) | Liang Lin (Sun Yat-Sen University) | Guanbin Li (Sun Yat-Sen University),,,,,,,
Decoupled Pseudo-labeling in Semi-Supervised Monocular 3D Object Detection,,,,Jiacheng Zhang (SUN YAT-SEN UNIVERSITY) | Jiaming Li (Baidu) | Xiangru Lin (Baidu) | Wei Zhang (Baidu) | Xiao Tan (Baidu) | Junyu Han (Baidu) | Errui Ding (Baidu) | Jingdong Wang (Baidu) | Guanbin Li (Sun Yat-Sen University),,,,,,,
DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video,"Recent advancements in dynamic neural radiance field methods have yielded remarkable outcomes. However, these approaches rely on the assumption of sharp input images. When faced with motion blur, existing dynamic NeRF methods often struggle to generate high-quality novel views. In this paper, we propose DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views from a monocular video affected by motion blur. To account for motion blur in input images, we simultaneously capture the camera trajectory and object Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we employ a global cross-time rendering approach to ensure consistent temporal coherence across the entire scene. We curate a dataset comprising diverse dynamic scenes that are specifically tailored for our task. Experimental results on our dataset demonstrate that our method outperforms existing approaches in generating sharp novel views from motion-blurred inputs while maintaining spatial-temporal consistency of the scene.",http://arxiv.org/abs/2403.10103v1,,"Huiqiang Sun (None) | Xingyi Li (Huazhong University Of Science And Technology) | Liao Shen (Huazhong University Of Science And Technology) | Xinyi Ye (School Of Artificial Intelligence And Automation, Huazhong University Of Science And Technology) | Ke Xian (Nanyang Technological University) | Zhiguo Cao (None)",2024-03-15 08:48:37+00:00,,,,,,
3D Multi-frame Fusion for Video Stabilization,,,,"Zhan Peng (None) | Xinyi Ye (School Of Artificial Intelligence And Automation, Huazhong University Of Science And Technology) | Weiyue Zhao (Shenzhen Dajiang Innovation Technology Co., Ltd) | TIANQI LIU (None) | Huiqiang Sun (None) | Baopu Li (Baidu) | Zhiguo Cao (None)",,,,,,,
Unifying Automatic and Interactive Matting with Pretrained ViTs,,,,Zixuan Ye (None) | Wenze Liu (Huazhong University Of Science And Technology) | He Guo (None) | Yujia Liang (Huazhong University Of Science And Technology) | Chaoyi Hong (Huazhong University Of Science And Technology) | Hao Lu (Huazhong University Of Science And Technology) | Zhiguo Cao (None),,,,,,,
Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields,,,,"TIANQI LIU (None) | Xinyi Ye (School Of Artificial Intelligence And Automation, Huazhong University Of Science And Technology) | Min Shi (None) | Zihao Huang (None) | Zhiyu Pan (None) | Zhan Peng (None) | Zhiguo Cao (None)",,,,,,,
Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction,"Integrating whole-slide images (WSIs) and bulk transcriptomics for predicting patient survival can improve our understanding of patient prognosis. However, this multimodal task is particularly challenging due to the different nature of these data: WSIs represent a very high-dimensional spatial description of a tumor, while bulk transcriptomics represent a global description of gene expression levels within that tumor. In this context, our work aims to address two key challenges: (1) how can we tokenize transcriptomics in a semantically meaningful and interpretable way?, and (2) how can we capture dense multimodal interactions between these two modalities? Specifically, we propose to learn biological pathway tokens from transcriptomics that can encode specific cellular functions. Together with histology patch tokens that encode the different morphological patterns in the WSI, we argue that they form appropriate reasoning units for downstream interpretability analyses. We propose fusing both modalities using a memory-efficient multimodal Transformer that can model interactions between pathway and histology patch tokens. Our proposed model, SURVPATH, achieves state-of-the-art performance when evaluated against both unimodal and multimodal baselines on five datasets from The Cancer Genome Atlas. Our interpretability framework identifies key multimodal prognostic factors, and, as such, can provide valuable insights into the interaction between genotype and phenotype, enabling a deeper understanding of the underlying biological mechanisms at play. We make our code public at: https://github.com/ajv012/SurvPath.",http://arxiv.org/abs/2304.06819v1,,"Guillaume Jaume (Harvard University) | Anurag Vaidya (Massachusetts Institute Of Technology) | Richard J. Chen (Harvard University) | Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) | Paul Pu Liang (Carnegie Mellon University) | Faisal Mahmood (Harvard University)",2023-04-13 21:02:32+00:00,,,,,,
Transcriptomics-guided Slide Representation Learning in Computational Pathology,,,,"Guillaume Jaume (Harvard University) | Lukas Oldenburg (Harvard University) | Anurag Vaidya (Massachusetts Institute Of Technology) | Richard J. Chen (Harvard University) | Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) | Thomas Peeters (Harvard University) | Andrew Song (Brigham And Women's Hospital) | Faisal Mahmood (Harvard University)",,,,,,,
Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology,,,,"Andrew Song (Brigham And Women's Hospital) | Richard J. Chen (Harvard University) | Tong Ding (Harvard University) | Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) | Guillaume Jaume (Harvard University) | Faisal Mahmood (Harvard University)",,,,,,,
LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis,,,,Zehan Zheng (Tongji University) | Fan Lu (Tongji University) | Weiyi Xue (Tongji University) | Guang Chen (Tongji University) | Changjun Jiang (Tongji University),,,,,,,
LEAD: Learning Decomposition for Source-free Universal Domain Adaptation,"Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at https://github.com/ispc-lab/LEAD.",http://arxiv.org/abs/2403.03421v1,,Sanqing Qu (Tongji University) | Tianpei Zou (Tongji University) | Lianghua He (Tongji University) | Florian R??hrbein (Chemnitz University Of Technology) | Alois Knoll (Technical University Munich) | Guang Chen (Tongji University) | Changjun Jiang (Tongji University),2024-03-06 03:08:20+00:00,,,,,,
MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection,"Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models. It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains. Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data. In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection. To achieve this, we propose a novel MAsk Pruning (MAP) framework. MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and pruning them is the key to IP protection. Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data. Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation. To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free. Extensive experiments indicate that MAP yields new state-of-the-art performance.",http://arxiv.org/abs/2403.04149v1,,Boyang Peng (Tongji University) | Sanqing Qu (Tongji University) | Yong Wu (Tongji University) | Tianpei Zou (Tongji University) | Lianghua He (Tongji University) | Alois Knoll (Technical University Munich) | Guang Chen (Tongji University) | Changjun Jiang (Tongji University),2024-03-07 02:10:59+00:00,,,,,,
POCE: Primal Policy Optimization with Conservative Estimation for Multi-constraint Offline Reinforcement Learning,,,,Jiayi Guan (Tongji University) | Li Shen (JD Explore Academy) | Ao Zhou (Tongji University) | Lusong Li (JDT) | Han Hu (Beijing Institute Of Technology) | Xiaodong He (JD AI Research) | Guang Chen (Tongji University) | Changjun Jiang (Tongji University),,,,,,,
Novel View Synthesis with View-Dependent Effects from a Single Image,"In this paper, we firstly consider view-dependent effects into single image-based novel view synthesis (NVS) problems. For this, we propose to exploit the camera motion priors in NVS to model view-dependent appearance or effects (VDE) as the negative disparity in the scene. By recognizing specularities ""follow"" the camera motion, we infuse VDEs into the input images by aggregating input pixel colors along the negative depth region of the epipolar lines. Also, we propose a `relaxed volumetric rendering' approximation that allows computing the densities in a single pass, improving efficiency for NVS from single images. Our method can learn single-image NVS from image sequences only, which is a completely self-supervised learning method, for the first time requiring neither depth nor camera pose annotations. We present extensive experiment results and show that our proposed method can learn NVS with VDEs, outperforming the SOTA single-view NVS methods on the RealEstate10k and MannequinChallenge datasets.",http://arxiv.org/abs/2312.08071v1,,Juan Luis Gonzalez Bello (KAIST) | Munchurl Kim (Korea Advanced Institute Of Science And Technology),2023-12-13 11:29:47+00:00,,,,,,
FMA-Net: Flow Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring,"We present a joint learning scheme of video super-resolution and deblurring, called VSRDB, to restore clean high-resolution (HR) videos from blurry low-resolution (LR) ones. This joint restoration problem has drawn much less attention compared to single restoration problems. In this paper, we propose a novel flow-guided dynamic filtering (FGDF) and iterative feature refinement with multi-attention (FRMA), which constitutes our VSRDB framework, denoted as FMA-Net. Specifically, our proposed FGDF enables precise estimation of both spatio-temporally-variant degradation and restoration kernels that are aware of motion trajectories through sophisticated motion representation learning. Compared to conventional dynamic filtering, the FGDF enables the FMA-Net to effectively handle large motions into the VSRDB. Additionally, the stacked FRMA blocks trained with our novel temporal anchor (TA) loss, which temporally anchors and sharpens features, refine features in a course-to-fine manner through iterative updates. Extensive experiments demonstrate the superiority of the proposed FMA-Net over state-of-the-art methods in terms of both quantitative and qualitative quality. Codes and pre-trained models are available at: https://kaist-viclab.github.io/fmanet-site",http://arxiv.org/abs/2401.03707v1,,Geunhyuk Youk (Korea Advanced Institute Of Science And Technology) | Jihyong Oh (Chung-Ang University) | Munchurl Kim (Korea Advanced Institute Of Science And Technology),2024-01-08 07:34:43+00:00,,,,,,
From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior,"Self-supervised monocular depth estimation (DE) is an approach to learning depth without costly depth ground truths. However, it often struggles with moving objects that violate the static scene assumption during training. To address this issue, we introduce a coarse-to-fine training strategy leveraging the ground contacting prior based on the observation that most moving objects in outdoor scenes contact the ground. In the coarse training stage, we exclude the objects in dynamic classes from the reprojection loss calculation to avoid inaccurate depth learning. To provide precise supervision on the depth of the objects, we present a novel Ground-contacting-prior Disparity Smoothness Loss (GDS-Loss) that encourages a DE network to align the depth of the objects with their ground-contacting points. Subsequently, in the fine training stage, we refine the DE network to learn the detailed depth of the objects from the reprojection loss, while ensuring accurate DE on the moving object regions by employing our regularization loss with a cost-volume-based weighting factor. Our overall coarse-to-fine training strategy can easily be integrated with existing DE methods without any modifications, significantly enhancing DE performance on challenging Cityscapes and KITTI datasets, especially in the moving object regions.",http://arxiv.org/abs/2312.10118v1,,Jaeho Moon (KAIST) | Juan Luis Gonzalez Bello (KAIST) | Byeongjun Kwon (KAIST) | Munchurl Kim (Korea Advanced Institute Of Science And Technology),2023-12-15 11:22:17+00:00,,,,,,
DETRs Beat YOLOs on Real-time Object Detection,"Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.",http://arxiv.org/abs/2304.08069v2,,Yian Zhao (Peking University) | Wenyu Lv (Baidu) | Shangliang Xu (Baidu) | Jinman Wei (Tianjin University) | Guanzhong Wang (Baidu) | Qingqing Dang (Baidu) | Yi Liu (None) | Jie Chen (Peking University),2023-04-17 08:30:02+00:00,,,,,,
GraCo: Granularity-Controllable Interactive Segmentation,,,,Yian Zhao (Peking University) | Kehan Li (Peking University) | Zesen Cheng (Peking University) | Pengchong Qiao (Peking University) | Xiawu Zheng (Xiamen University) | Rongrong Ji (Xiamen University) | Chang Liu (Tsinghua University) | Li Yuan (Peking University) | Jie Chen (Peking University),,,,,,,
Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation,"Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation. Typical works focus on learning the new subject's private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (https://github.com/modelscope/facechain).",http://arxiv.org/abs/2403.06775v1,,Pengchong Qiao (Peking University) | Lei Shang (Alibaba Group) | Chang Liu (Tsinghua University) | Baigui Sun (Alibaba Group) | Xiangyang Ji (Tsinghua University) | Jie Chen (Peking University),2024-03-11 14:43:40+00:00,,,,,,
Object Dynamics Modeling with Hierarchical Point Cloud-based Representations,,,,Chanho Kim (Oregon State University) | Li Fuxin (Oregon State University),,,,,,,
Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods,"In order to learn better about how different visual recognition backbones make decisions, we propose a methodology that systematically applies deep explanation algorithms on a dataset-wide basis, and compares the statistics generated from the amount and nature of the explanations to gain insights about the decision-making of different models. Specifically, we propose two methodologies called sub-explanation counting and cross-testing. These methodologies reveal the difference among networks in terms of two properties called compositionality and disjunctivism. Transformers and ConvNeXt are found to be more compositional, in the sense that they jointly consider multiple parts of the image in building their decisions, whereas traditional CNNs and distilled transformers are less compositional and more disjunctive, which means that they use multiple diverse but smaller set of parts to achieve a confident prediction. Through further experiments, we pinpointed the choice of normalization to be especially important in the compositionality of a model, in that batch normalization leads to less compositionality while group and layer normalization lead to more. Finally, we also analyze the features shared by different backbones and plot a landscape of different models based on their feature-use similarity.",http://arxiv.org/abs/2212.06872v3,,Mingqi Jiang (Oregon State University) | Saeed Khorram (Apple) | Li Fuxin (Oregon State University),2022-12-13 19:38:13+00:00,,,,,,
Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions,"Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on several long-tail benchmarks and GAN architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images. The code is available at https://github.com/khorrams/utlo.",http://arxiv.org/abs/2402.17065v1,,Saeed Khorram (Apple) | Mingqi Jiang (Oregon State University) | Mohamad Shahbazi (ETH Zurich) | Mohamad Hosein Danesh (McGill University) | Li Fuxin (Oregon State University),2024-02-26 23:03:00+00:00,,,,,,
AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing,"Editable 3D-aware generation, which supports user-interacted editing, has witnessed rapid development recently. However, existing editable 3D GANs either fail to achieve high-accuracy local editing or suffer from huge computational costs. We propose AttriHuman-3D, an editable 3D human generation model, which address the aforementioned problems with attribute decomposition and indexing. The core idea of the proposed model is to generate all attributes (e.g. human body, hair, clothes and so on) in an overall attribute space with six feature planes, which are then decomposed and manipulated with different attribute indexes. To precisely extract features of different attributes from the generated feature planes, we propose a novel attribute indexing method as well as an orthogonal projection regularization to enhance the disentanglement. We also introduce a hyper-latent training strategy and an attribute-specific sampling strategy to avoid style entanglement and misleading punishment from the discriminator. Our method allows users to interactively edit selected attributes in the generated 3D human avatars while keeping others fixed. Both qualitative and quantitative experiments demonstrate that our model provides a strong disentanglement between different attributes, allows fine-grained image editing and generates high-quality 3D human avatars.",http://arxiv.org/abs/2312.02209v3,,Fan Yang (None) | Tianyi Chen (Nanyang Technological University) | XIAOSHENG HE (Nanyang Technological University) | Zhongang Cai (Nanyang Technological University) | Lei Yang (The Chinese University Of Hong Kong) | Si Wu (South China University Of Technology) | Guosheng Lin (Nanyang Technological University),2023-12-03 03:20:10+00:00,,,,,,
S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes,"Current 3D stylization methods often assume static scenes, which violates the dynamic nature of our real world. To address this limitation, we present S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural radiance fields. However, stylizing dynamic 3D scenes is inherently challenging due to the limited availability of stylized reference images along the temporal axis. Our key insight lies in introducing additional temporal cues besides the provided reference. To this end, we generate temporal pseudo-references from the given stylized reference. These pseudo-references facilitate the propagation of style information from the reference to the entire dynamic 3D scene. For coarse style transfer, we enforce novel views and times to mimic the style details present in pseudo-references at the feature level. To preserve high-frequency details, we create a collection of stylized temporal pseudo-rays from temporal pseudo-references. These pseudo-rays serve as detailed and explicit stylization guidance for achieving fine style transfer. Experiments on both synthetic and real-world datasets demonstrate that our method yields plausible stylized results of space-time view synthesis on dynamic 3D scenes.",http://arxiv.org/abs/2403.06205v2,,Xingyi Li (Huazhong University Of Science And Technology) | Zhiguo Cao (None) | Yizheng Wu (Nanyang Technological University) | Kewei Wang (Huazhong University Of Science And Technology) | Ke Xian (Nanyang Technological University) | Zhe Wang (Sensetime Group Limited) | Guosheng Lin (Nanyang Technological University),2024-03-10 13:04:01+00:00,,,,,,
Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations,,,,Kewei Wang (Huazhong University Of Science And Technology) | Yizheng Wu (Nanyang Technological University) | Jun Cen (None) | Zhiyu Pan (None) | Xingyi Li (Huazhong University Of Science And Technology) | Zhe Wang (Sensetime Group Limited) | Zhiguo Cao (None) | Guosheng Lin (Nanyang Technological University),,,,,,,
GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting,"3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/",http://arxiv.org/abs/2311.14521v4,,Yiwen Chen (Nanyang Technological University) | Zilong Chen (Tsinghua University) | Chi Zhang (Tencent ) | Feng Wang (Tsinghua University) | Xiaofeng Yang (Nanyang Technological University) | Yikai Wang (Tsinghua University) | Zhongang Cai (Nanyang Technological University) | Lei Yang (The Chinese University Of Hong Kong) | Huaping Liu (Tsinghua University) | Guosheng Lin (Nanyang Technological University),2023-11-24 14:46:59+00:00,,,,,,
BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics,"The recently emerging text-to-motion advances have spired numerous attempts for convenient and interactive human motion generation. Yet, existing methods are largely limited to generating body motions only without considering the rich two-hand motions, let alone handling various conditions like body dynamics or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal dataset for two-hand motion generation. Our dataset includes accurate motion tracking for the human body and hands and provides pair-wised finger-level hand annotations and body descriptions. We further provide a strong baseline method, BOTH2Hands, for the novel task: generating vivid two-hand motions from both implicit body dynamics and explicit text prompts. We first warm up two parallel body-to-hand and text-to-hand diffusion models and then utilize the cross-attention transformer for motion blending. Extensive experiments and cross-validations demonstrate the effectiveness of our approach and dataset for generating convincing two-hand motions from the hybrid body-and-textual conditions. Our dataset and code will be disseminated to the community for future research.",http://arxiv.org/abs/2312.07937v3,,Wenqian Zhang (ShanghaiTech University) | Molin Huang (Shanghaitech University) | Yuxuan Zhou (None) | Juze Zhang (ShanghaiTech University) | Jingyi Yu (ShanghaiTech University) | Jingya Wang (ShanghaiTech University) | Lan Xu (None),2023-12-13 07:30:19+00:00,,,,,,
HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting,"We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.",http://arxiv.org/abs/2312.03461v2,,Yuheng Jiang (ShanghaiTech University) | Zhehao Shen (ShanghaiTech University) | Penghao Wang (None) | Zhuo Su (ByteDance) | Yu Hong (ShanghaiTech University) | Yingliang Zhang (DGene) | Jingyi Yu (ShanghaiTech University) | Lan Xu (None),2023-12-06 12:36:53+00:00,,,,,,
I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions,"We are living in a world surrounded by diverse and ""smart"" devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.",http://arxiv.org/abs/2312.08869v1,,Chengfeng Zhao (ShanghaiTech University) | Juze Zhang (ShanghaiTech University) | Jiashen Du (None) | Ziwei Shan (ShanghaiTech University) | Junye Wang (ShanghaiTech University) | Jingyi Yu (Shanghai Tech University) | Jingya Wang (ShanghaiTech University) | Lan Xu (None),2023-12-10 08:25:41+00:00,,,,,,
OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers,"We have recently seen tremendous progress in realistic text-to-motion generation. Yet, the existing methods often fail or produce implausible motions with unseen text inputs, which limits the applications. In this paper, we present OMG, a novel framework, which enables compelling motion generation from zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the pretrain-then-finetune paradigm into the text-to-motion generation. At the pre-training stage, our model improves the generation ability by learning the rich out-of-domain inherent motion traits. To this end, we scale up a large unconditional diffusion model up to 1B parameters, so as to utilize the massive unlabeled motion data up to over 20M motion instances. At the subsequent fine-tuning stage, we introduce motion ControlNet, which incorporates text prompts as conditioning information, through a trainable copy of the pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block. MoC block adaptively recognizes various ranges of the sub-motions with a cross-attention mechanism and processes them separately with the text-token-specific experts. Such a design effectively aligns the CLIP token embeddings of text prompts to various ranges of compact and expressive motion features. Extensive experiments demonstrate that our OMG achieves significant improvements over the state-of-the-art methods on zero-shot text-to-motion generation. Project page: https://tr3e.github.io/omg-page.",http://arxiv.org/abs/2312.08985v2,,"Han Liang (ShanghaiTech University) | Jiacheng Bao (Shanghai Tech University) | Ruichi Zhang (ShanghaiTech University) | Sihan Ren (ShanghaiTech University) | Yuecheng Xu (ShanghaiTech University) | Sibei Yang (None) | Xin Chen (University Of Chinese Academy Of Sciences, ShanghaiTech University) | Jingyi Yu (ShanghaiTech University) | Lan Xu (None)",2023-12-14 14:31:40+00:00,,,,,,
Unsupervised Gaze Representation Learning from Multi-view Face Images,,,,"Yiwei Bao (Beihang University) | Feng Lu (Beihang University, Tsinghua University)",,,,,,,
From Feature to Gaze: A Generalizable Replacement of Linear Layer for Gaze Estimation,,,,"Yiwei Bao (Beihang University) | Feng Lu (Beihang University, Tsinghua University)",,,,,,,
Attack To Defend: Exploiting Adversarial Attacks for Detecting Poisoned Models,,,,Samar Fares (None) | Karthik Nandakumar (Mohamed Bin Zayed University Of Artificial Intelligence),,,,,,,
Ink Dot-Oriented Differentiable Optimization for Neural Image Halftoning,,,,Hao Jiang (Peking University) | Bingfeng Zhou (Peking University) | Yadong Mu (Peking University),,,,,,,
G3DR: Generative 3D Reconstruction in ImageNet,"We introduce a novel 3D generative method, Generative 3D Reconstruction (G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects from single images, addressing the limitations of existing methods. At the heart of our framework is a novel depth regularization technique that enables the generation of scenes with high-geometric fidelity. G3DR also leverages a pretrained language-vision model, such as CLIP, to enable reconstruction in novel views and improve the visual realism of generations. Additionally, G3DR designs a simple but effective sampling procedure to further improve the quality of generations. G3DR offers diverse and efficient 3D asset generation based on class or text conditioning. Despite its simplicity, G3DR is able to beat state-of-theart methods, improving over them by up to 22% in perceptual metrics and 90% in geometry scores, while needing only half of the training time. Code is available at https://github.com/preddy5/G3DR",http://arxiv.org/abs/2403.00939v2,,Pradyumna Reddy (None) | Ismail Elezi (Huawei Noah's Ark) | Jiankang Deng (Imperial College London & Huawei UKRD),2024-03-01 19:36:11+00:00,,,,,,
Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation,,,,Zhipeng Du (University Of Edinburgh) | Miaojing Shi (King's College London) | Jiankang Deng (Imperial College London & Huawei UKRD),,,,,,,
Vk D :  Improving knowledge distillation using orthogonal projections ,"Knowledge distillation is an effective method for training small and efficient deep learning models. However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures. To address this limitation, we propose a novel constrained feature distillation method. This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components, our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method, we apply it to object detection and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available: https://github.com/roymiles/vkd",http://arxiv.org/abs/2403.06213v1,,Roy Miles (Imperial College London) | Ismail Elezi (Huawei Noah's Ark) | Jiankang Deng (Imperial College London & Huawei UKRD),2024-03-10 13:26:24+00:00,,,,,,
Open-Vocabulary 3D Semantic Segmentation with Foundation Models,,,,"Li Jiang (Max Planck Institute For Informatics) | Shaoshuai Shi (Saarland Informatics Campus, Max-Planck Institute) | Bernt Schiele (Max Planck Institute For Informatics)",,,,,,,
FedAS: Bridging Inconsistency in Personalized Federated Learning,,,,Xiyuan Yang (Wuhan University) | Wenke Huang (Wuhan University) | Mang Ye (Wuhan University),,,,,,,
Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification,,,,Bin Yang (Wuhan University) | Jun Chen (Wuhan University) | Mang Ye (Wuhan University),,,,,,,
OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning,,,,"Noor Ahmed (Saarland Informatics Campus, Max-Planck Institute) | Anna Kukleva (MPII) | Bernt Schiele (Max Planck Institute For Informatics)",,,,,,,
Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity,,,,Yuhang Chen (Wuhan University) | Wenke Huang (Wuhan University) | Mang Ye (Wuhan University),,,,,,,
Exploring Orthogonality in Open World Object Detection,,,,Zhicheng Sun (Peking University) | Jinghan Li (Peking University) | Yadong Mu (Peking University),,,,,,,
Countering Personalized Text-to-Image Generation with Influence Watermarks,,,,Hanwen Liu (Peking University) | Zhicheng Sun (Peking University) | Yadong Mu (Peking University),,,,,,,
Training Vision Transformers for Semi-Supervised Semantic Segmentation,,,,Xinting Hu (Nanyang Technological University) | Li Jiang (Max Planck Institute For Informatics) | Bernt Schiele (Max Planck Institute For Informatics),,,,,,,
DiffuseMix: An Effective Image Augmentation Approach Driven by Vision-Language Diffusion Models,,,,"Khawar Islam (FloppyDisk.AI) | Muhammad Zaigham Zaheer (Mohamed Bin Zayed University Of Artificial Intelligence) | Arif Mahmood (Information Technology University, Lahore) | Karthik Nandakumar (Mohamed Bin Zayed University Of Artificial Intelligence)",,,,,,,
Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline,,,,Anas Al-Lahham (Mohamed Bin Zayed University Of Artificial Intelligence) | Muhammad Zaigham Zaheer (Mohamed Bin Zayed University Of Artificial Intelligence) | Nurbek Tastan (Mohamed Bin Zayed University Of Artificial Intelligence) | Karthik Nandakumar (Mohamed Bin Zayed University Of Artificial Intelligence),,,,,,,
Epistemic Uncertainty Quantification For Pre-trained Neural Networks,,,,Hanjing Wang (Rensselaer Polytechnic Institute) | Qiang Ji (Rensselaer Polytechnic Institute),,,,,,,
NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models,"Generative models have been very popular in the recent years for their image generation capabilities. GAN-based models are highly regarded for their disentangled latent space, which is a key feature contributing to their success in controlled image editing. On the other hand, diffusion models have emerged as powerful tools for generating high-quality images. However, the latent space of diffusion models is not as thoroughly explored or understood. Existing methods that aim to explore the latent space of diffusion models usually relies on text prompts to pinpoint specific semantics. However, this approach may be restrictive in areas such as art, fashion, or specialized fields like medicine, where suitable text prompts might not be available or easy to conceive thus limiting the scope of existing work. In this paper, we propose an unsupervised method to discover latent semantics in text-to-image diffusion models without relying on text prompts. Our method takes a small set of unlabeled images from specific domains, such as faces or cats, and a pre-trained diffusion model, and discovers diverse semantics in unsupervised fashion using a contrastive learning objective. Moreover, the learned directions can be applied simultaneously, either within the same domain (such as various types of facial edits) or across different domains (such as applying cat and face edits within the same image) without interfering with each other. Our extensive experiments show that our method achieves highly disentangled edits, outperforming existing approaches in both diffusion-based and GAN-based latent space editing methods.",http://arxiv.org/abs/2312.05390v1,,Yusuf Dalva (Virginia Polytechnic Institute And State University) | Pinar Yanardag (Virginia Polytechnic Institute And State University),2023-12-08 22:04:53+00:00,,,,,,
PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos,,,,"Yufei Zhang (None) | Jeffrey Kephart (IBM, International Business Machines) | Zijun Cui (University Of Southern California) | Qiang Ji (Rensselaer Polytechnic Institute)",,,,,,,
CONFORM: Contrast is All You Need for High-Fidelity Text-to-Image Diffusion Models,"Images produced by text-to-image diffusion models might not always faithfully represent the semantic intent of the provided text prompt, where the model might overlook or entirely fail to produce certain objects. Existing solutions often require customly tailored functions for each of these problems, leading to sub-optimal results, especially for complex prompts. Our work introduces a novel perspective by tackling this challenge in a contrastive context. Our approach intuitively promotes the segregation of objects in attention maps while also maintaining that pairs of related attributes are kept close to each other. We conduct extensive experiments across a wide variety of scenarios, each involving unique combinations of objects, attributes, and scenes. These experiments effectively showcase the versatility, efficiency, and flexibility of our method in working with both latent and pixel-based diffusion models, including Stable Diffusion and Imagen. Moreover, we publicly share our source code to facilitate further research.",http://arxiv.org/abs/2312.06059v1,,"Tuna Han Salih Meral (Virginia Tech) | Enis Simsar (ETH Zurich) | Federico Tombari (Google, TUM) | Pinar Yanardag (Virginia Polytechnic Institute And State University)",2023-12-11 01:42:15+00:00,,,,,,
Uncertainty-aware Action Decoupling Transformer for Action Anticipation,,,,Hongji Guo (None) | Nakul Agarwal (None) | Shao-Yuan Lo (Johns Hopkins University) | Kwonjoon Lee (Honda Research Institute USA) | Qiang Ji (Rensselaer Polytechnic Institute),,,,,,,
RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models,"Recent advancements in diffusion-based models have demonstrated significant success in generating images from text. However, video editing models have not yet reached the same level of visual quality and user control. To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training. RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure. It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods. It is also efficient in terms of memory requirements, allowing it to handle longer videos. RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations. In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats. Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods. Our code, dataset and videos can be found in https://rave-video.github.io.",http://arxiv.org/abs/2312.04524v1,,Ozgur Kara (Georgia Institute Of Technology) | Bariscan Kurtkaya (Koc University) | Hidir Yesiltepe (Virginia Polytechnic Institute And State University) | James Rehg (None) | Pinar Yanardag (Virginia Polytechnic Institute And State University),2023-12-07 18:43:45+00:00,,,,,,
Multiscale Vision Transformers meet Bipartite Matching for efficient single-stage Action Localization,"Action Localization is a challenging problem that combines detection and recognition tasks, which are often addressed separately. State-of-the-art methods rely on off-the-shelf bounding box detections pre-computed at high resolution and propose transformer models that focus on the classification task alone. Such two-stage solutions are prohibitive for real-time deployment. On the other hand, single-stage methods target both tasks by devoting part of the network (generally the backbone) to sharing the majority of the workload, compromising performance for speed. These methods build on adding a DETR head with learnable queries that, after cross- and self-attention can be sent to corresponding MLPs for detecting a person's bounding box and action. However, DETR-like architectures are challenging to train and can incur in big complexity.   In this paper, we observe that a straight bipartite matching loss can be applied to the output tokens of a vision transformer. This results in a backbone + MLP architecture that can do both tasks without the need of an extra encoder-decoder head and learnable queries. We show that a single MViT-S architecture trained with bipartite matching to perform both tasks surpasses the same MViT-S when trained with RoI align on pre-computed bounding boxes. With a careful design of token pooling and the proposed training pipeline, our MViTv2-S model achieves +3 mAP on AVA2.2. w.r.t. the two-stage counterpart. Code and models will be released after paper revision.",http://arxiv.org/abs/2312.17686v1,,Ioanna Ntinou (Queen Mary University Of London) | Enrique Sanchez (Samsung AI Center Cambridge) | Georgios Tzimiropoulos (Queen Mary University London),2023-12-29 17:08:38+00:00,,,,,,
Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models,,,,Adrian Bulat (None) | Yassine Ouali (Samsung) | Georgios Tzimiropoulos (Queen Mary University London),,,,,,,
LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition,"In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios.",http://arxiv.org/abs/2403.08161v1,,Zhonglin Sun (Queen Mary University Of London) | Chen Feng (Queen Mary University Of London) | Ioannis Patras (Queen Mary University Of London) | Georgios Tzimiropoulos (Queen Mary University London),2024-03-13 01:07:55+00:00,,,,,,
"Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",,,,"Hongchi Xia (Shanghai Jiao Tong University) | Chih-Hao Lin (None) | Wei-Chiu Ma (Cornell University) | Shenlong Wang (University Of Illinois, Urbana Champaign)",,,,,,,
GaussianAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh,,,,"Jing Wen (University Of Illinois Urbana-Champaign) | Xiaoming Zhao (UIUC) | Jason Ren (Apple) | Alexander G. Schwing (UIUC) | Shenlong Wang (University Of Illinois, Urbana Champaign)",,,,,,,
Physical Property Understanding from Language-Embedded Feature Fields,,,,"Albert J. Zhai (University Of Illinois At Urbana-Champaign) | Yuan Shen (University Of Illinois At Urbana-Champaign) | Emily Y. Chen (University Of Illinois Urbana Champaign) | Gloria Wang (Department Of Computer Science) | Xinlei Wang (University Of Illinois Urbana-Champaign) | Sheng Wang (University Of Illinois Urbana-Champaign) | Kaiyu Guan (University Of Illinois, Urbana Champaign) | Shenlong Wang (University Of Illinois, Urbana Champaign)",,,,,,,
ShapeWalk: A Benchmark for Compositional Shape Editing through Language-Guided Chains,,,,Habib Slim (KAUST) | Mohamed Elhoseiny (KAUST),,,,,,,
Overcoming Generic Knowledge Loss with Selective Parameter Update,"Foundation models encompass an extensive knowledge base and offer remarkable transferability. However, this knowledge becomes outdated or insufficient over time. The challenge lies in continuously updating foundation models to accommodate novel information while retaining their original capabilities. Leveraging the fact that foundation models have initial knowledge on various tasks and domains, we propose a novel approach that, instead of updating all parameters equally, localizes the updates to a sparse set of parameters relevant to the task being learned. We strike a balance between efficiency and new task performance, while maintaining the transferability and generalizability of foundation models. We extensively evaluate our method on foundational vision-language models with a diverse spectrum of continual learning tasks. Our method achieves improvements on the accuracy of the newly learned tasks up to 7% while preserving the pretraining knowledge with a negligible decrease of 0.9% on a representative control set accuracy.",http://arxiv.org/abs/2308.12462v3,,Wenxuan Zhang (King Abdullah University Of Science And Technology) | Paul Janson (Concordia University/ MILA) | Rahaf Aljundi (Toyota Motor Europe) | Mohamed Elhoseiny (KAUST),2023-08-23 22:55:45+00:00,,,,,,
Adversarial Text to Continuous Image Generation,,,,"Kilichbek Haydarov (King Abdullah University Of Science And Technology) | Aashiq Muhamed (CMU, Carnegie Mellon University) | Xiaoqian Shen (King Abdullah University Of Science And Technology) | Jovana Lazarevic (University Of Novi Sad) | Ivan Skorokhodov (KAUST) | Chamuditha Galappaththige (Mohamed Bin Zayed University Of Artificial Intelligence) | Mohamed Elhoseiny (KAUST)",,,,,,,
"Show, Search, and Tell: Exploring Guided Visual Search as a Core Mechanism in Multimodal LLMs","When we look around and perform complex tasks, how we see and selectively process what we see is crucial. However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. This integration results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL). We further create V*Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. Our study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code is available https://github.com/penghao-wu/vstar.",http://arxiv.org/abs/2312.14135v2,,"Penghao Wu (University Of California, San Diego) | Saining Xie (Facebook)",2023-12-21 18:55:06+00:00,,,,,,
On the test-time generalization of zero-shot CLIP: A training-free approach,,,,Maxime Zanella (Universit?? Catholique De Louvain) | Ismail Ben Ayed (ETS Montreal),,,,,,,
Material Palette: Extraction of Materials from a Single Image,"In this paper, we propose a method to extract physically-based rendering (PBR) materials from a single real-world image. We do so in two steps: first, we map regions of the image to material concepts using a diffusion model, which allows the sampling of texture images resembling each material in the scene. Second, we benefit from a separate network to decompose the generated textures into Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to be used in rendering applications. Our approach builds on existing synthetic material libraries with SVBRDF ground truth, but also exploits a diffusion-generated RGB texture dataset to allow generalization to new samples using unsupervised domain adaptation (UDA). Our contributions are thoroughly evaluated on synthetic and real-world datasets. We further demonstrate the applicability of our method for editing 3D scenes with materials estimated from real photographs. The code and models will be made open-source. Project page: https://astra-vision.github.io/MaterialPalette/",http://arxiv.org/abs/2311.17060v1,,Ivan Lopes (INRIA) | Fabio Pizzati (University Of Oxford) | Raoul De Charette (Inria),2023-11-28 18:59:58+00:00,,,,,,
Visual Anagrams: Synthesizing Multi-View Optical Illusions with Diffusion Models,"We address the problem of synthesizing multi-view optical illusions: images that change appearance upon a transformation, such as a flip or rotation. We propose a simple, zero-shot method for obtaining these illusions from off-the-shelf text-to-image diffusion models. During the reverse diffusion process, we estimate the noise from different views of a noisy image. We then combine these noise estimates together and denoise the image. A theoretical analysis suggests that this method works precisely for views that can be written as orthogonal transformations, of which permutations are a subset. This leads to the idea of a visual anagram--an image that changes appearance under some rearrangement of pixels. This includes rotations and flips, but also more exotic pixel permutations such as a jigsaw rearrangement. Our approach also naturally extends to illusions with more than two views. We provide both qualitative and quantitative results demonstrating the effectiveness and flexibility of our method. Please see our project webpage for additional visualizations and results: https://dangeng.github.io/visual_anagrams/",http://arxiv.org/abs/2311.17919v1,,Daniel Geng (University Of Michigan) | Inbum Park (University Of Michigan) | Andrew Owens (University Of Michigan),2023-11-29 18:59:59+00:00,,,,,,
PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness,"We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene. Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions. Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and instance-wise uncertainties along PSC. This is achieved by building on a multi-input multi-output (MIMO) strategy, while improving performance and yielding better uncertainty for little additional compute. Additionally, we introduce a technique to aggregate permutation-invariant mask predictions. Our experiments demonstrate that our method surpasses all baselines in both Panoptic Scene Completion and uncertainty estimation on three large-scale autonomous driving datasets. Our code and data are available at https://astra-vision.github.io/PaSCo .",http://arxiv.org/abs/2312.02158v1,,Anh-Quan Cao (INRIA) | Angela Dai (None) | Raoul De Charette (Inria),2023-12-04 18:59:59+00:00,,,,,,
Masking Clusters in Vision-language Pretraining,,,,Zihao Wei (University Of Michigan - Ann Arbor) | Zixuan Pan (University Of Michigan - Ann Arbor) | Andrew Owens (University Of Michigan),,,,,,,
Image Sculpting: Precise Object Editing with 3D Geometry Control,"We present Image Sculpting, a new framework for editing 2D images by incorporating tools from 3D geometry and graphics. This approach differs markedly from existing methods, which are confined to 2D spaces and typically rely on textual instructions, leading to ambiguity and limited control. Image Sculpting converts 2D objects into 3D, enabling direct interaction with their 3D geometry. Post-editing, these objects are re-rendered into 2D, merging into the original image to produce high-fidelity results through a coarse-to-fine enhancement process. The framework supports precise, quantifiable, and physically-plausible editing options such as pose editing, rotation, translation, 3D composition, carving, and serial addition. It marks an initial step towards combining the creative freedom of generative models with the precision of graphics pipelines.",http://arxiv.org/abs/2401.01702v1,,Jiraphon Yenphraphai (New York University) | Xichen Pan (New York University) | Sainan Liu (Intel) | Daniele Panozzo (New York University) | Saining Xie (Facebook),2024-01-02 18:59:35+00:00,,,,,,
Tactile-Augmented Radiance Fields,,,,"Yiming Dou (University Of Michigan - Ann Arbor) | Fengyu Yang (Yale University) | Yi Liu (University Of Michigan - Ann Arbor) | Antonio Loquercio (University Of California, Berkeley) | Andrew Owens (University Of Michigan)",,,,,,,
Transductive Zero-Shot &  Few-Shot CLIP ,,,,"S??gol??ne Martin (CentraleSupelec) | Yunshi HUANG (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Fereshteh Shakeri (??cole De Technologie Sup??rieure) | Jean-Christophe Pesquet (CentraleSupelec) | Ismail Ben Ayed (ETS Montreal)",,,,,,,
A Simple Recipe for Language-guided Domain Generalized Segmentation,"Generalization to new domains not seen during training is one of the long-standing goals and challenges in deploying neural networks in real-world applications. Existing generalization techniques necessitate substantial data augmentation, potentially sourced from external datasets, and aim at learning invariant representations by imposing various alignment constraints. Large-scale pretraining has recently shown promising generalization capabilities, along with the potential of bridging different modalities. For instance, the recent advent of vision-language models like CLIP has opened the doorway for vision models to exploit the textual modality. In this paper, we introduce a simple framework for generalizing semantic segmentation networks by employing language as the source of randomization. Our recipe comprises three key ingredients: i) the preservation of the intrinsic CLIP robustness through minimal fine-tuning, ii) language-driven local style augmentation, and iii) randomization by locally mixing the source and augmented styles during training. Extensive experiments report state-of-the-art results on various generalization benchmarks. The code will be made available.",http://arxiv.org/abs/2311.17922v1,,Mohammad Fahes (INRIA) | TUAN-HUNG VU (None) | Andrei Bursuc (Valeo.Ai) | Patrick P??rez (None) | Raoul De Charette (Inria),2023-11-29 18:59:59+00:00,,,,,,
LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP,,,,"Yunshi HUANG (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Fereshteh Shakeri (??cole De Technologie Sup??rieure) | Jose Dolz (??cole De Technologie Sup??rieure) | Malik Boudiaf (??cole De Technologie Sup??rieure) | Houda Bahig (University Of Montreal) | Ismail Ben Ayed (ETS Montreal)",,,,,,,
Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",http://arxiv.org/abs/2401.06209v1,,"Shengbang Tong (New York University / Meta AI) | Zhuang Liu (FAIR, Meta AI) | Yuexiang Zhai (University Of California Berkeley) | Yi Ma (UC Berkeley) | Yann LeCun (Facebook) | Saining Xie (Facebook)",2024-01-11 18:58:36+00:00,,,,,,
Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Normal Sample Prompts,"This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.",http://arxiv.org/abs/2403.06495v3,,Jiawen Zhu (Singapore Management University) | Guansong Pang (Singapore Management University),2024-03-11 08:07:46+00:00,,,,,,
Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection,"Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly detection area - aims at utilizing a few samples of anomaly classes seen during training to detect unseen anomalies (i.e., samples from open-set anomaly classes), while effectively identifying the seen anomalies. Benefiting from the prior knowledge illustrated by the seen anomalies, current OSAD methods can often largely reduce false positive errors. However, these methods treat the anomaly examples as from a homogeneous distribution, rendering them less effective in generalizing to unseen anomalies that can be drawn from any distribution. In this paper, we propose to learn heterogeneous anomaly distributions using the limited anomaly examples to address this issue. To this end, we introduce a novel approach, namely Anomaly Heterogeneity Learning (AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomaly distributions and then utilizes them to learn a unified heterogeneous abnormality model. Further, AHL is a generic framework that existing OSAD models can plug and play for enhancing their abnormality modeling. Extensive experiments on nine real-world anomaly detection datasets show that AHL can 1) substantially enhance different state-of-the-art (SOTA) OSAD models in detecting both seen and unseen anomalies, achieving new SOTA performance on a large set of datasets, and 2) effectively generalize to unseen anomalies in new target domains.",http://arxiv.org/abs/2310.12790v2,,Jiawen Zhu (Singapore Management University) | Choubo Ding (University Of Adelaide) | Yu Tian (None) | Guansong Pang (Singapore Management University),2023-10-19 14:47:11+00:00,,,,,,
Projecting Trackable Thermal Patterns for Dynamic Computer Vision,,,,Mark Sheinin (Weizmann Institute Of Science) | Aswin C. Sankaranarayanan (Carnegie Mellon University) | Srinivasa G. Narasimhan (Carnegie Mellon University),,,,,,,
Contrastive Mean-Shift Learning for Generalized Category Discovery,,,,Sua Choi (POSTECH) | Dahyun Kang (POSTECH) | Minsu Cho (POSTECH),,,,,,,
Learning Correlation Structures for Vision Transformers,,,,Manjin Kim (POSTECH) | Paul Hongsuck Seo (Google) | Cordelia Schmid (Inria / Google) | Minsu Cho (POSTECH),,,,,,,
Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform,,,,Chunghyun Park (POSTECH) | Seungwook Kim (POSTECH) | Jaesik Park (Seoul National University) | Minsu Cho (POSTECH),,,,,,,
A Theory of Joint Light and Heat Transport for Lambertian Scenes,,,,Mani Ramanagopal (Carnegie Mellon University) | Sriram Narayanan (Carnegie Mellon University) | Aswin C. Sankaranarayanan (Carnegie Mellon University) | Srinivasa G. Narasimhan (Carnegie Mellon University),,,,,,,
Generating Realistic Training Data from Time-Lapse Imagery for Reconstructing Dynamic Objects under Occlusion,,,,Khiem Vuong (Carnegie Mellon University) | N. Dinesh Reddy (Carnegie Mellon University) | Robert Tamburo (Carnegie Mellon University) | Srinivasa G. Narasimhan (Carnegie Mellon University),,,,,,,
En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data,"We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars. Unlike previous works that rely on scarce 3D datasets or limited 2D collections with imbalanced viewing angles and imprecise pose priors, our approach aims to develop a zero-shot 3D generative scheme capable of producing visually realistic, geometrically accurate and content-wise diverse 3D humans without relying on pre-existing 3D or 2D assets. To address this challenge, we introduce a meticulously crafted workflow that implements accurate physical modeling to learn the enhanced 3D generative model from synthetic 2D data. During inference, we integrate optimization modules to bridge the gap between realistic appearances and coarse 3D shapes. Specifically, En3D comprises three modules: a 3D generator that accurately models generalizable 3D humans with realistic appearance from synthesized balanced, diverse, and structured human images; a geometry sculptor that enhances shape quality using multi-view normal constraints for intricate human anatomy; and a texturing module that disentangles explicit texture maps with fidelity and editability, leveraging semantical UV partitioning and a differentiable rasterizer. Experimental results show that our approach significantly outperforms prior works in terms of image quality, geometry accuracy and content diversity. We also showcase the applicability of our generated avatars for animation and editing, as well as the scalability of our approach for content-style free adaptation.",http://arxiv.org/abs/2401.01173v1,,Yifang Men (Alibaba Group) | Biwen Lei (Alibaba Group) | Yuan Yao (Alibaba Group) | Miaomiao Cui (Alibaba Group) | Zhouhui Lian (Peking University) | Xuansong Xie (Alibaba Group),2024-01-02 12:06:31+00:00,,,,,,
DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors,"Text-guided domain adaption and generation of 3D-aware portraits find many applications in various fields. However, due to the lack of training data and the challenges in handling the high variety of geometry and appearance, the existing methods for these tasks suffer from issues like inflexibility, instability, and low fidelity. In this paper, we propose a novel framework DiffusionGAN3D, which boosts text-guided 3D domain adaption and generation by combining 3D GANs and diffusion priors. Specifically, we integrate the pre-trained 3D generative models (e.g., EG3D) and text-to-image diffusion models. The former provides a strong foundation for stable and high-quality avatar generation from text. And the diffusion models in turn offer powerful priors and guide the 3D generator finetuning with informative direction to achieve flexible and efficient text-guided domain adaption. To enhance the diversity in domain adaption and the generation capability in text-to-avatar, we introduce the relative distance loss and case-specific learnable triplane respectively. Besides, we design a progressive texture refinement module to improve the texture quality for both tasks above. Extensive experiments demonstrate that the proposed framework achieves excellent results in both domain adaption and text-to-avatar tasks, outperforming existing methods in terms of generation quality and efficiency. The project homepage is at https://younglbw.github.io/DiffusionGAN3D-homepage/.",http://arxiv.org/abs/2312.16837v2,,Biwen Lei (Alibaba Group) | Kai Yu (None) | Mengyang Feng (Alibaba Group) | Miaomiao Cui (Alibaba Group) | Xuansong Xie (Alibaba Group),2023-12-28 05:46:26+00:00,,,,,,
Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception,,,,"Junwen He (Dalian University Of Technology) | Yifan Wang (Dalian University Of Technology) | Lijun Wang (Dalian University Of Technology) | Huchuan Lu (Dalian University Of Technology) | Bin Luo (Alibaba Group) | Jun-Yan He (DAMO Academy, Alibaba Group) | Jin-Peng Lan (Alibaba Group) | Xuansong Xie (Alibaba Group)",,,,,,,
Grounded Question-Answering in Long Egocentric Videos,,,,Shangzhe Di (Shanghai Jiao Tong University) | Weidi Xie (Shanghai Jiao Tong University),,,,,,,
Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models,"Generative models have recently exhibited exceptional capabilities in text-to-image generation, but still struggle to generate image sequences coherently. In this work, we focus on a novel, yet challenging task of generating a coherent image sequence based on a given storyline, denoted as open-ended visual storytelling. We make the following three contributions: (i) to fulfill the task of visual storytelling, we propose a learning-based auto-regressive image generation model, termed as StoryGen, with a novel vision-language context module, that enables to generate the current frame by conditioning on the corresponding text prompt and preceding image-caption pairs; (ii) to address the data shortage of visual storytelling, we collect paired image-text sequences by sourcing from online videos and open-source E-books, establishing processing pipeline for constructing a large-scale dataset with diverse characters, storylines, and artistic styles, named StorySalon; (iii) Quantitative experiments and human evaluations have validated the superiority of our StoryGen, where we show StoryGen can generalize to unseen characters without any optimization, and generate image sequences with coherent content and consistent character. Code, dataset, and models are available at https://haoningwu3639.github.io/StoryGen_Webpage/",http://arxiv.org/abs/2306.00973v3,,Chang Liu (Shanghai Jiao Tong University) | Haoning Wu (Shanghai Jiao Tong University) | Yujie Zhong (Meituan) | Xiaoyun Zhang (Shanghai Jiao Tong University) | Yanfeng Wang (Shanghai Jiao Tong University) | Weidi Xie (Shanghai Jiao Tong University),2023-06-01 17:58:50+00:00,,,,,,
Retrieval-Augmented Egocentric Video Captioning,"Understanding human actions from videos of first-person view poses significant challenges. Most prior approaches explore representation learning on egocentric videos only, while overlooking the potential benefit of exploiting existing large-scale third-person videos. In this paper, (1) we develop EgoInstructor, a retrieval-augmented multimodal captioning model that automatically retrieves semantically relevant third-person instructional videos to enhance the video captioning of egocentric videos. (2) For training the cross-view retrieval module, we devise an automatic pipeline to discover ego-exo video pairs from distinct large-scale egocentric and exocentric datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE loss that pulls egocentric and exocentric video features closer by aligning them to shared text features that describe similar actions. (4) Through extensive experiments, our cross-view retrieval module demonstrates superior performance across seven benchmarks. Regarding egocentric video captioning, EgoInstructor exhibits significant improvements by leveraging third-person videos as references.",http://arxiv.org/abs/2401.00789v2,,Jilan Xu (None) | Yifei Huang (The University Of Tokyo) | Junlin Hou (Hong Kong University Of Science And Technology) | Guo Chen (Nanjing University) | Yuejie Zhang (Fudan University) | Rui Feng (Fudan University) | Weidi Xie (Shanghai Jiao Tong University),2024-01-01 15:31:06+00:00,,,,,,
Bayesian Differentiable Physics for Cloth Digitalization,"We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization",http://arxiv.org/abs/2402.17664v4,,Deshan Gong (University Of Leeds) | Ningtao Mao (University Of Leeds) | He Wang (None),2024-02-27 16:35:07+00:00,,,,,,
FCS: Feature Calibration and Separation for Non-Exemplar Class Incremental Learning,,,,Qiwei Li (Peking University) | Yuxin Peng (Peking University) | Jiahaun Zhou (Peking University),,,,,,,
Incremental Nuclei Segmentation from Histopathological Images via Future-class Awareness and Compatibility-inspired Distillation,,,,Huyong Wang (Shenzhen University) | Huisi Wu (Shenzhen University) | Jing Qin (Hong Kong Polytechnic University),,,,,,,
Distribution-aware Knowledge Prototyping for Non-exemplar Lifelong Person Re-identification,,,,Kunlun Xu (Peking University) | Xu Zou (Huazhong University Of Science And Technology) | Yuxin Peng (Peking University) | Jiahaun Zhou (Peking University),,,,,,,
MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation,"Open-vocabulary 3D instance segmentation has emerged as a frontier topic due to its capability to segment 3D instances beyond a predefined set of categories. However, compared to significant progress in the 2D domain, methods for 3D open-vocabulary instance segmentation are hindered by the limited scale of high-quality annotated 3D data. To harness the capabilities of 2D models, recent efforts have focused on merging 2D masks based on metrics such as geometric and semantic similarity to form 3D instances. In contrast to these local metrics, we propose a novel metric called view consensus to better exploit multi-view observation. The key insight is that two 2D masks should be considered as belonging to the same instance if a considerable number of other 2D masks from other views contain both these two masks. Based on this metric, we build a global mask graph and iteratively cluster masks, prioritizing mask pairs with solid view consensus. The corresponding 3D points cluster of these 2D mask clusters can be regarded as 3D instances, along with the fused open-vocabulary features from clustered 2D masks. Through this multi-view verification and fusion mechanism, our method effectively leverages the prior instance knowledge from massive 2D masks predicted by visual foundation models, eliminating the need for training on 3D data. Experiments on publicly available datasets, including ScanNet200 and MatterPort3D, demonstrate that our method achieves state-of-the-art performance in both open-vocabulary instance segmentation and class-agnostic mask generation. Our project page is at https://pku-epic.github.io/MaskClustering.",http://arxiv.org/abs/2401.07745v1,,Mi Yan (Peking University) | Jiazhao Zhang (None) | Yan Zhu (Peking University) | He Wang (None),2024-01-15 14:56:15+00:00,,,,,,
MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation,,,,Xiaolong Deng (Shenzhen University) | Huisi Wu (Shenzhen University) | Runhao Zeng (Shenzhen University) | Jing Qin (Hong Kong Polytechnic University),,,,,,,
PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness,,,,Siyao Jiang (Shenzhen University) | Huisi Wu (Shenzhen University) | Junyang Chen (None) | Qin Zhang (Shenzhen University) | Jing Qin (Hong Kong Polytechnic University),,,,,,,
Human Motion Prediction under Unexpected Perturbation,,,,Jiangbei Yue (University Of Leeds) | Baiyi Li (University Of Leeds) | Julien Pettr?? (INRIA) | Armin Seyfried (Forschungszentrum J??lich) | He Wang (None),,,,,,,
SNIDA: Unlocking Few-Shot Object Detection with Non-linear Semantic Decoupling Augmentation,,,,Yanjie Wang (Huazhong University Of Science And Technology) | Xu Zou (Huazhong University Of Science And Technology) | Luxin Yan (Huazhong University Of Science And Technology) | Sheng Zhong (Huazhong University Of Science And Technology) | Jiahaun Zhou (Peking University),,,,,,,
Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving,"In autonomous driving, behavior prediction is fundamental for safe motion planning, hence the security and robustness of prediction models against adversarial attacks are of paramount importance. We propose a novel adversarial backdoor attack against trajectory prediction models as a means of studying their potential vulnerabilities. Our attack affects the victim at training time via naturalistic, hence stealthy, poisoned samples crafted using a novel two-step approach. First, the triggers are crafted by perturbing the trajectory of attacking vehicle and then disguised by transforming the scene using a bi-level optimization technique. The proposed attack does not depend on a particular model architecture and operates in a black-box manner, thus can be effective without any knowledge of the victim model. We conduct extensive empirical studies using state-of-the-art prediction models on two benchmark datasets using metrics customized for trajectory prediction. We show that the proposed attack is highly effective, as it can significantly hinder the performance of prediction models, unnoticeable by the victims, and efficient as it forces the victim to generate malicious behavior even under constrained conditions. Via ablative studies, we analyze the impact of different attack design choices followed by an evaluation of existing defence mechanisms against the proposed attack.",http://arxiv.org/abs/2306.15755v2,,MOZHGAN POURKESHAVARZ (Huawei Technologies Ltd.) | Mohammad Sabokrou (Okinawa Institute Of Science And Technology (OIST)) | Amir Rasouli (Huawei Technologies Canada),2023-06-27 19:15:06+00:00,,,,,,
CaDeT: a Causal Disentanglement Approach for Robust Trajectory Prediction in Autonomous Driving,,,,MOZHGAN POURKESHAVARZ (Huawei Technologies Ltd.) | Junrui Zhang (University Of Toronto) | Amir Rasouli (Huawei Technologies Canada),,,,,,,
A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation,,,,Qucheng Peng (University Of Central Florida) | Ce Zheng (University Of Central Florida) | Chen Chen (None),,,,,,,
MMM: Generative Masked Motion Model,"Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at \url{https://exitudio.github.io/MMM-page}.",http://arxiv.org/abs/2312.03596v1,,"Ekkasit Pinyoanuntapong (University Of North Carolina At Charlotte) | Pu Wang (University Of North Carolina At Charlotte) | Minwoo Lee (University Of North Carolina, Charlotte) | Chen Chen (None)",2023-12-06 16:35:59+00:00,,,,,,
OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition,"Due to the resource-intensive nature of training vision-language models on expansive video data, a majority of studies have centered on adapting pre-trained image-language models to the video domain. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the substantial discrepancy for web-scaled descriptive narratives and concise action category names, leading to less distinct semantic space and potential performance limitations. In this work, we prioritize the refinement of text knowledge to facilitate generalizable video recognition. To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors thus bridging the textual discrepancy and serving as a knowledge base for general recognition. Moreover, to assign the best descriptors with different video instances, we propose Optimal Descriptor Solver, forming the video recognition problem as solving the optimal matching flow across frame-level representations and descriptors. Comprehensive evaluations in zero-shot, few-shot, and fully supervised video recognition highlight the effectiveness of our approach. Our best model achieves a state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",http://arxiv.org/abs/2312.00096v1,,Tongjia Chen (Hunan University) | Hongshan Yu (Hunan University) | Zhengeng Yang (Hunan University) | Zechuan Li (Hunan University) | Wei Sun (Hunan University) | Chen Chen (None),2023-11-30 13:32:43+00:00,,,,,,
Inter-X: Towards Versatile Human-Human Interaction Analysis,"The analysis of the ubiquitous human-human interactions is pivotal for understanding humans as social beings. Existing human-human interaction datasets typically suffer from inaccurate body motions, lack of hand gestures and fine-grained textual descriptions. To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body movements and diverse interaction patterns, together with detailed hand gestures. The dataset includes ~11K interaction sequences and more than 8.1M frames. We also equip Inter-X with versatile annotations of more than 34K fine-grained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects. Based on the elaborate annotations, we propose a unified benchmark composed of 4 categories of downstream tasks from both the perceptual and generative directions. Extensive experiments and comprehensive analysis show that Inter-X serves as a testbed for promoting the development of versatile human-human interaction analysis. Our dataset and benchmark will be publicly available for research purposes.",http://arxiv.org/abs/2312.16051v1,,"Liang Xu (Shanghai Jiao Tong University) | Xintao Lv (Shanghai Jiao Tong University) | Yichao Yan (Shanghai Jiao Tong University) | Xin Jin (Eastern Institute For Advanced Study) | Wu Shuwen (Shanghai Jiao Tong University) | Congsheng Xu (Shanghai Jiao Tong University) | Yifan Liu (Shanghai Jiao Tong University) | Yizhou Zhou (WeChat AI) | Fengyun Rao (WeChat, Tencent) | Xingdong Sheng (Shanghai Jiao Tong University) | Yunhui LIU (Lenovo Research) | Wenjun Zeng (None) | Xiaokang Yang (Shanghai Jiao Tong University, China)",2023-12-26 13:36:05+00:00,,,,,,
Domain Prompt Learning with Quaternion Networks,"Prompt learning has emerged as an effective and data-efficient technique in large Vision-Language Models (VLMs). However, when adapting VLMs to specialized domains such as remote sensing and medical imaging, domain prompt learning remains underexplored. While large-scale domain-specific foundation models can help tackle this challenge, their concentration on a single vision level makes it challenging to prompt both vision and language modalities. To overcome this, we propose to leverage domain-specific knowledge from domain-specific foundation models to transfer the robust recognition ability of VLMs from generalized to specialized domains, using quaternion networks. Specifically, the proposed method involves using domain-specific vision features from domain-specific foundation models to guide the transformation of generalized contextual embeddings from the language branch into a specialized space within the quaternion networks. Moreover, we present a hierarchical approach that generates vision prompt features by analyzing intermodal relationships between hierarchical language prompt features and domain-specific vision features. In this way, quaternion networks can effectively mine the intermodal relationships in the specific domain, facilitating domain-specific vision-language contrastive learning. Extensive experiments on domain-specific datasets show that our proposed method achieves new state-of-the-art results in prompt learning.",http://arxiv.org/abs/2312.08878v1,,"Qinglong Cao (Shanghai Jiao Tong University) | Zhengqin Xu (Shanghai Jiao Tong University) | Yuntian Chen (Eastern Institute For Advanced Study) | Chao Ma (Shanghai Jiao Tong University) | Xiaokang Yang (Shanghai Jiao Tong University, China)",2023-12-12 08:49:39+00:00,,,,,,
Monocular Identity-Conditioned Facial Reflectance Reconstruction,,,,"Xingyu Ren (Shanghai Jiao Tong University) | Jiankang Deng (Imperial College London & Huawei UKRD) | Yuhao Cheng (Shanghai Jiao Tong University) | Jia Guo (InsightFace.AI) | Chao Ma (Shanghai Jiao Tong University) | Yichao Yan (Shanghai Jiao Tong University) | Wenhan Zhu (None) | Xiaokang Yang (Shanghai Jiao Tong University, China)",,,,,,,
Combining Frame and GOP Embeddings for Neural Video Representation,,,,"Jens Eirik Saethre (ETH Zurich & Disney Research|Studios) | Roberto Azevedo (Disney Research, Disney) | Christopher Schroers (Disney Research|Studios, Disney)",,,,,,,
Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners,,,,Keon Hee Park (Kyung Hee University) | Kyungwoo Song (Yonsei University) | Gyeong-Moon Park (Kyung Hee University),,,,,,,
DiVAS: Video and Audio Synchronization with Dynamic Frame Rates,,,,"Clara Maria Fernandez Labrador (Disney Research) | Mertcan Akcay (Disney Research) | Eitan Abecassis (Walt Disney Company) | Joan Massich (Disney Research) | Christopher Schroers (Disney Research|Studios, Disney)",,,,,,,
Generative Unlearning for Any Identity,,,,Juwon Seo (Kyung Hee University) | Sung-Hoon Lee (Kyung Hee University) | Tae-Young Lee (Kyung Hee University) | SeungJun Moon (KLleon) | Gyeong-Moon Park (Kyung Hee University),,,,,,,
QUADify: Extracting Meshes with Pixel-level Details and Materials from Images,,,,"Maximilian Fr??hauf (ETH Zurich & Disney Research | Studios) | Hayko Riemenschneider (Disney Research|Studios) | Markus Gross (Disney Research, Disney) | Christopher Schroers (Disney Research|Studios, Disney)",,,,,,,
Open Set Domain Adaptation for Semantic Segmentation,,,,Seun-An Choe (Kyung Hee University) | Ah-Hyung Shin (Kyung Hee University) | Keon Hee Park (Kyung Hee University) | Jinwoo Choi (Kyung Hee University) | Gyeong-Moon Park (Kyung Hee University),,,,,,,
Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer,"Despite the impressive generative capabilities of diffusion models, existing diffusion model-based style transfer methods require inference-stage optimization (e.g. fine-tuning or textual inversion of style) which is time-consuming, or fails to leverage the generative ability of large-scale diffusion models. To address these issues, we introduce a novel artistic style transfer method based on a pre-trained large-scale diffusion model without any optimization. Specifically, we manipulate the features of self-attention layers as the way the cross-attention mechanism works; in the generation process, substituting the key and value of content with those of style image. This approach provides several desirable characteristics for style transfer including 1) preservation of content by transferring similar styles into similar image patches and 2) transfer of style based on similarity of local texture (e.g. edge) between content and style images. Furthermore, we introduce query preservation and attention temperature scaling to mitigate the issue of disruption of original content, and initial latent Adaptive Instance Normalization (AdaIN) to deal with the disharmonious color (failure to transfer the colors of style). Our experimental results demonstrate that our proposed method surpasses state-of-the-art methods in both conventional and diffusion-based style transfer baselines.",http://arxiv.org/abs/2312.09008v1,,Jiwoo Chung (Sungkyunkwan University) | Sangeek Hyun (Sungkyunkwan University) | Jae-Pil Heo (Sungkyunkwan University),2023-12-11 09:53:12+00:00,,,,,,
Diversity-aware Channel Pruning for StyleGAN Compression,,,,Jiwoo Chung (Sungkyunkwan University) | Sangeek Hyun (Sungkyunkwan University) | Sang-Heon Shim (Sungkyunkwan University) | Jae-Pil Heo (Sungkyunkwan University),,,,,,,
"AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond","Large Language Models(LLMs) have shown remarkable emergent abilities in unifying almost all (if not every) NLP tasks. In the human motion-related realm, however, researchers still develop siloed models for each task. Inspired by InstuctGPT, and the generalist concept behind Gato, we introduce AvatarGPT, an All-in-One framework for motion understanding, planning, generations as well as other tasks such as motion in-between synthesis. AvatarGPT treats each task as one type of instruction fine-tuned on the shared LLM. All the tasks are seamlessly interconnected with language as the universal interface, constituting a closed-loop within the framework. To achieve this, human motion sequences are first encoded as discrete tokens, which serve as the extended vocabulary of LLM. Then, an unsupervised pipeline to generate natural language descriptions of human action sequences from in-the-wild videos is developed. Finally, all tasks are jointly trained. Extensive experiments show that AvatarGPT achieves SOTA on low-level tasks, and promising results on high-level tasks, demonstrating the effectiveness of our proposed All-in-One framework. Moreover, for the first time, AvatarGPT enables a principled approach by iterative traversal of the tasks within the closed-loop for unlimited long-motion synthesis.",http://arxiv.org/abs/2311.16468v1,,Zixiang Zhou (Xiaobing.Ai) | Yu Wan (None) | Baoyuan Wang (Xiaobing.Ai),2023-11-28 04:10:07+00:00,,,,,,
Design2Cloth: 3D Cloth Generation from 2D Masks,,,,Jiali Zheng (Imperial College London) | Rolandos Alexandros Potamias (Imperial College London) | Stefanos Zafeiriou (Imperial College London),,,,,,,
Improved Implicity Neural Representation with Fourier Bases Reparameterized Training,"Implicit Neural Representation (INR) as a mighty representation paradigm has achieved success in various computer vision tasks recently. Due to the low-frequency bias issue of vanilla multi-layer perceptron (MLP), existing methods have investigated advanced techniques, such as positional encoding and periodic activation function, to improve the accuracy of INR. In this paper, we connect the network training bias with the reparameterization technique and theoretically prove that weight reparameterization could provide us a chance to alleviate the spectral bias of MLP. Based on our theoretical analysis, we propose a Fourier reparameterization method which learns coefficient matrix of fixed Fourier bases to compose the weights of MLP. We evaluate the proposed Fourier reparameterization method on different INR tasks with various MLP architectures, including vanilla MLP, MLP with positional encoding and MLP with advanced activation function, etc. The superiority approximation results on different MLP architectures clearly validate the advantage of our proposed method. Armed with our Fourier reparameterization method, better INR with more textures and less artifacts can be learned from the training data.",http://arxiv.org/abs/2401.07402v3,,Kexuan Shi (None) | Xingyu Zhou (University Of Electronic Science And Technology Of China) | Shuhang Gu (University Of Electronic Science And Technology Of China),2024-01-15 00:40:41+00:00,,,,,,
SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation,"Zero-shot 6D object pose estimation involves the detection of novel objects with their 6D poses in cluttered scenes, presenting significant challenges for model generalizability. Fortunately, the recent Segment Anything Model (SAM) has showcased remarkable zero-shot transfer performance, which provides a promising solution to tackle this task. Motivated by this, we introduce SAM-6D, a novel framework designed to realize the task through two steps, including instance segmentation and pose estimation. Given the target objects, SAM-6D employs two dedicated sub-networks, namely Instance Segmentation Model (ISM) and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-D images. ISM takes SAM as an advanced starting point to generate all possible object proposals and selectively preserves valid ones through meticulously crafted object matching scores in terms of semantics, appearance and geometry. By treating pose estimation as a partial-to-partial point matching problem, PEM performs a two-stage point matching process featuring a novel design of background tokens to construct dense 3D-3D correspondence, ultimately yielding the pose estimates. Without bells and whistles, SAM-6D outperforms the existing methods on the seven core datasets of the BOP Benchmark for both instance segmentation and pose estimation of novel objects.",http://arxiv.org/abs/2311.15707v2,,Jiehong Lin (South China University Of Technology) | Lihua Liu (South China University Of Technology) | ?????? ??? (South China University Of Technology) | Kui Jia (South China University Of Technology),2023-11-27 10:50:47+00:00,,,,,,
Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation,"The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything(SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.",http://arxiv.org/abs/2312.03502v1,,Haojie Zhang (South China University Of Technology) | Yongyi Su (South China University Of Technology) | Xun Xu (A*STAR) | Kui Jia (South China University Of Technology),2023-12-06 13:59:22+00:00,,,,,,
Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data,"Existing one-shot 4D head synthesis methods usually learn from monocular videos with the aid of 3DMM reconstruction, yet the latter is evenly challenging which restricts them from reasonable 4D head synthesis. We present a method to learn one-shot 4D head synthesis via large-scale synthetic data. The key is to first learn a part-wise 4D generative model from monocular images via adversarial learning, to synthesize multi-view images of diverse identities and full motions as training data; then leverage a transformer-based animatable triplane reconstructor to learn 4D head reconstruction using the synthetic data. A novel learning strategy is enforced to enhance the generalizability to real images by disentangling the learning process of 3D reconstruction and reenactment. Experiments demonstrate our superiority over the prior art.",http://arxiv.org/abs/2311.18729v1,,Yu Deng (Xiaobing.Ai) | Duomin Wang (None) | Xiaohang Ren (Xiaobing) | Xingyu Chen (Xiaobing.AI) | Baoyuan Wang (Xiaobing.Ai),2023-11-30 17:26:33+00:00,,,,,,
Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary,"Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adaptive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.",http://arxiv.org/abs/2401.08209v2,,Leheng Zhang (University Of Electronic Science And Technology Of China) | Yawei Li (ETH Zurich) | Xingyu Zhou (University Of Electronic Science And Technology Of China) | Xiaorui Zhao (None) | Shuhang Gu (University Of Electronic Science And Technology Of China),2024-01-16 08:50:44+00:00,,,,,,
GS-IR: 3D Gaussian Splatting for Inverse Rendering,"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian Splatting (GS) that leverages forward mapping volume rendering to achieve photorealistic novel view synthesis and relighting results. Unlike previous works that use implicit neural representations and volume rendering (e.g. NeRF), which suffer from low expressive power and high computational complexity, we extend GS, a top-performance representation for novel view synthesis, to estimate scene geometry, surface material, and environment illumination from multi-view images captured under unknown lighting conditions. There are two main problems when introducing GS to inverse rendering: 1) GS does not support producing plausible normal natively; 2) forward mapping (e.g. rasterization and splatting) cannot trace the occlusion like backward mapping (e.g. ray tracing). To address these challenges, our GS-IR proposes an efficient optimization scheme that incorporates a depth-derivation-based regularization for normal estimation and a baking-based occlusion to model indirect lighting. The flexible and expressive GS representation allows us to achieve fast and compact geometry reconstruction, photorealistic novel view synthesis, and effective physically-based rendering. We demonstrate the superiority of our method over baseline methods through qualitative and quantitative evaluations on various challenging scenes.",http://arxiv.org/abs/2311.16473v2,,Zhihao Liang (None) | Qi Zhang (Tencent AI Lab) | Ying Feng (Tencent AI Lab) | Ying Shan (Tencent) | Kui Jia (South China University Of Technology),2023-11-26 02:35:09+00:00,,,,,,
Locally Adaptive Neural 3D Morphable Models,"We present the Locally Adaptive Morphable Model (LAMM), a highly flexible Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes. We train our architecture following a simple self-supervised training scheme in which input displacements over a set of sparse control vertices are used to overwrite the encoded geometry in order to transform one training sample into another. During inference, our model produces a dense output that adheres locally to the specified sparse geometry while maintaining the overall appearance of the encoded object. This approach results in state-of-the-art performance in both disentangling manipulated geometry and 3D mesh reconstruction. To the best of our knowledge LAMM is the first end-to-end framework that enables direct local control of 3D vertex geometry in a single forward pass. A very efficient computational graph allows our network to train with only a fraction of the memory required by previous methods and run faster during inference, generating 12k vertex meshes at $>$60fps on a single CPU thread. We further leverage local geometry control as a primitive for higher level editing operations and present a set of derivative capabilities such as swapping and sampling object parts. Code and pretrained models can be found at https://github.com/michaeltrs/LAMM.",http://arxiv.org/abs/2401.02937v1,,Michail Tarasiou (Imperial College London) | Rolandos Alexandros Potamias (Imperial College London) | Eimear O' Sullivan (Huawei Technologies Ltd.) | Stylianos Ploumpis (Imperial College London) | Stefanos Zafeiriou (Imperial College London),2024-01-05 18:28:51+00:00,,,,,,
Neural Sign Actors: A diffusion model for 3D sign language production from text,"Sign Languages (SL) serve as the predominant mode of communication for the Deaf and Hard of Hearing communities. The advent of deep learning has aided numerous methods in SL recognition and translation, achieving remarkable results. However, Sign Language Production (SLP) poses a challenge for the computer vision community as the motions generated must be realistic and have precise semantic meanings. Most SLP methods rely on 2D data, thus impeding their ability to attain a necessary level of realism. In this work, we propose a diffusion-based SLP model trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through a series of quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. We believe that this work presents an important and necessary step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities. The code, method and generated data will be made publicly available.",http://arxiv.org/abs/2312.02702v1,,Vasileios Baltatzis (None) | Rolandos Alexandros Potamias (Imperial College London) | Evangelos Ververas (Huawei Technologies Ltd.) | Guanxiong Sun (Huawei Technologies Ltd.) | Jiankang Deng (Imperial College London & Huawei UKRD) | Stefanos Zafeiriou (Imperial College London),2023-12-05 12:04:34+00:00,,,,,,
HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images,"As for human avatar reconstruction, contemporary techniques commonly necessitate the acquisition of costly data and struggle to achieve satisfactory results from a small number of casual images. In this paper, we investigate this task from a few-shot unconstrained photo album. The reconstruction of human avatars from such data sources is challenging because of limited data amount and dynamic articulated poses. For handling dynamic data, we integrate a skinning mechanism with deep marching tetrahedra (DMTet) to form a drivable tetrahedral representation, which drives arbitrary mesh topologies generated by the DMTet for the adaptation of unconstrained images. To effectively mine instructive information from few-shot data, we devise a two-phase optimization method with few-shot reference and few-shot guidance. The former focuses on aligning avatar identity with reference images, while the latter aims to generate plausible appearances for unseen regions. Overall, our framework, called HaveFun, can undertake avatar reconstruction, rendering, and animation. Extensive experiments on our developed benchmarks demonstrate that HaveFun exhibits substantially superior performance in reconstructing the human body and hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.",http://arxiv.org/abs/2311.15672v1,,"Xihe Yang (The Chinese University Of Hong Kong, Shenzhen) | Xingyu Chen (Xiaobing.AI) | Daiheng Gao (None) | Finn Wong (Xiaobing.AI) | Xiaoguang Han (The Chinese University Of Hong Kong, Shenzhen) | Baoyuan Wang (Xiaobing.Ai)",2023-11-27 10:01:31+00:00,,,,,,
Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention,,,,Xingyu Zhou (University Of Electronic Science And Technology Of China) | Leheng Zhang (University Of Electronic Science And Technology Of China) | Xiaorui Zhao (None) | Keze Wang (SUN YAT-SEN UNIVERSITY) | Leida Li (Xidian University) | Shuhang Gu (University Of Electronic Science And Technology Of China),,,,,,,
Perceptual Assessment and Optimization of HDR Image Rendering,"The increasing popularity of high dynamic range (HDR) imaging stems from its ability to faithfully capture luminance levels in natural scenes. However, HDR image quality assessment has been insufficiently addressed. Existing models are mostly designed for low dynamic range (LDR) images, which exhibit poorly correlated with human perception of HDR image quality. To fill this gap, we propose a family of HDR quality metrics by transferring the recent advancements in LDR domain. The key step in our approach is to employ a simple inverse display model to decompose an HDR image into a stack of LDR images with varying exposures. Subsequently, these LDR images are evaluated using state-of-the-art LDR quality metrics. Our family of HDR quality models offer three notable advantages. First, specific exposures (i.e., luminance ranges) can be weighted to emphasize their assessment when calculating the overall quality score. Second, our HDR quality metrics directly inherit the capabilities of their base LDR quality models in assessing LDR images. Third, our metrics do not rely on human perceptual data of HDR image quality for re-calibration. Experiments conducted on four human-rated HDR image quality datasets indicate that our HDR quality metrics consistently outperform existing methods, including the HDR-VDP family. Furthermore, we demonstrate the promise of our models in the perceptual optimization of HDR novel view synthesis.",http://arxiv.org/abs/2310.12877v3,,Peibei Cao (City University Of Hong Kong) | Rafal Mantiuk (University Of Cambridge) | Kede Ma (City University Of Hong Kong),2023-10-19 16:32:18+00:00,,,,,,
Learned Scanpaths Aid Blind Panoramic Video Quality Assessment,,,,"Kanglong FAN (City University Of Hong Kong) | Wen Wen (City University Of Hong Kong) | Mu Li (The Chinese University Of Hong Kong, Shenzhen) | YIFAN PENG (University Of Hong Kong) | Kede Ma (City University Of Hong Kong)",,,,,,,
Modular Blind Video Quality Assessment,"Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model, and a method of training it to improve its modularity. Specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods. Furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities. Last, our BVQA model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers.",http://arxiv.org/abs/2402.19276v3,,"Wen Wen (City University Of Hong Kong) | Mu Li (The Chinese University Of Hong Kong, Shenzhen) | Yabin ZHANG (Bytedance) | Yiting Liao (Bytedance) | Junlin Li (ByteDance) | Li Zhang (Bytedance) | Kede Ma (City University Of Hong Kong)",2024-02-29 15:44:00+00:00,,,,,,
Towards Generalizing to Unseen Domains with Few Labels,,,,Chamuditha Galappaththige (Mohamed Bin Zayed University Of Artificial Intelligence) | Sanoojan Baliah (Mohamed Bin Zayed University Of Artificial Intelligence) | Malitha Gunawardhana (University Of Auckland) | Muhammad Haris Khan (None),,,,,,,
Leveraging Stereo Prior for Generalizable Novel-View Synthesis,,,,Haechan Lee (Pohang University Of Science And Technology) | Wonjoon Jin (Pohang University Of Science And Technology) | Seung-Hwan Baek (POSTECH) | Sunghyun Cho (POSTECH),,,,,,,
CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment,,,,Hyeongmin Lee (Pohang University Of Science And Technology) | Kyoungkook Kang (Pohang University Of Science And Technology) | Jungseul Ok (POSTECH) | Sunghyun Cho (POSTECH),,,,,,,
Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery,,,,"Siddharth Tourani (MBZUAI) | Ahmed Alwheibi (Mohamed Bin Zayed University Of Artificial Intelligence) | Arif Mahmood (Information Technology University, Lahore) | Muhammad Haris Khan (None)",,,,,,,
Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos,"In this paper, we explore the capability of an agent to construct a logical sequence of action steps, thereby assembling a strategic procedural plan. This plan is crucial for navigating from an initial visual observation to a target visual outcome, as depicted in real-life instructional videos. Existing works have attained partial success by extensively leveraging various sources of information available in the datasets, such as heavy intermediate visual observations, procedural names, or natural language step-by-step instructions, for features or supervision signals. However, the task remains formidable due to the implicit causal constraints in the sequencing of steps and the variability inherent in multiple feasible plans. To tackle these intricacies that previous efforts have overlooked, we propose to enhance the capabilities of the agent by infusing it with procedural knowledge. This knowledge, sourced from training procedure plans and structured as a directed weighted graph, equips the agent to better navigate the complexities of step sequencing and its potential variations. We coin our approach KEPP, a novel Knowledge-Enhanced Procedure Planning system, which harnesses a probabilistic procedural knowledge graph extracted from training data, effectively acting as a comprehensive textbook for the training domain. Experimental evaluations across three widely-used datasets under settings of varying complexity reveal that KEPP attains superior, state-of-the-art results while requiring only minimal supervision.",http://arxiv.org/abs/2403.02782v1,,Kumaranage Ravindu Nagasinghe (Mohamed Bin Zayed University Of Artificial Intelligence) | Honglu Zhou (Rutgers University) | Malitha Gunawardhana (University Of Auckland) | Martin Renqiang Min (NEC Laboratories America) | Daniel Harari (Weizmann Institute Of Science) | Muhammad Haris Khan (None),2024-03-05 08:55:51+00:00,,,,,,
ParamISP: Learned Forward and Inverse ISPs using Camera Parameters,"RAW images are rarely shared mainly due to its excessive data size compared to their sRGB counterparts obtained by camera ISPs. Learning the forward and inverse processes of camera ISPs has been recently demonstrated, enabling physically-meaningful RAW-level image processing on input sRGB images. However, existing learning-based ISP methods fail to handle the large variations in the ISP processes with respect to camera parameters such as ISO and exposure time, and have limitations when used for various applications. In this paper, we propose ParamISP, a learning-based method for forward and inverse conversion between sRGB and RAW images, that adopts a novel neural-network module to utilize camera parameters, which is dubbed as ParamNet. Given the camera parameters provided in the EXIF data, ParamNet converts them into a feature vector to control the ISP networks. Extensive experiments demonstrate that ParamISP achieve superior RAW and sRGB reconstruction results compared to previous methods and it can be effectively used for a variety of applications such as deblurring dataset synthesis, raw deblurring, HDR reconstruction, and camera-to-camera transfer.",http://arxiv.org/abs/2312.13313v1,,Woohyeok Kim (POSTECH) | Geonu Kim (POSTECH) | Junyong Lee (None) | Seungyong Lee (POSTECH) | Seung-Hwan Baek (POSTECH) | Sunghyun Cho (POSTECH),2023-12-20 09:16:47+00:00,,,,,,
Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization,,,,Guopeng Li (Wuhan University) | Ming Qian (None) | Gui-Song Xia (Wuhan University),,,,,,,
FreePoint: Unsupervised Point Cloud Instance Segmentation,"Instance segmentation of point clouds is a crucial task in 3D field with numerous applications that involve localizing and segmenting objects in a scene. However, achieving satisfactory results requires a large number of manual annotations, which is a time-consuming and expensive process. To alleviate dependency on annotations, we propose a method, called FreePoint, for underexplored unsupervised class-agnostic instance segmentation on point clouds. In detail, we represent the point features by combining coordinates, colors, normals, and self-supervised deep features. Based on the point features, we perform a multicut algorithm to segment point clouds into coarse instance masks as pseudo labels, which are used to train a point cloud instance segmentation model. To alleviate the inaccuracy of coarse masks during training, we propose a weakly-supervised training strategy and corresponding loss. Our work can also serve as an unsupervised pre-training pretext for supervised semantic instance segmentation with limited annotations. For class-agnostic instance segmentation on point clouds, FreePoint largely fills the gap with its fully-supervised counterpart based on the state-of-the-art instance segmentation model Mask3D and even surpasses some previous fully-supervised methods. When serving as a pretext task and fine-tuning on S3DIS, FreePoint outperforms training from scratch by 5.8% AP with only 10% mask annotations.",http://arxiv.org/abs/2305.06973v1,,Zhikai Zhang (Wuhan University) | Jian Ding (None) | Li Jiang (Max Planck Institute For Informatics) | Dengxin Dai (None) | Gui-Song Xia (Wuhan University),2023-05-11 16:56:26+00:00,,,,,,
Anchor-based Robust Finetuning of Vision-Language Models,,,,Jinwei Han (Wuhan University) | Zhiwen Lin (Tencent) | Zhongyisun Sun (Tencent Youtu Lab) | Yingguo Gao (Tencent Youtu Lab) | Ke Yan (None) | Shouhong Ding (Tencent Youtu Lab) | Yuan Gao (Wuhan University) | Gui-Song Xia (Wuhan University),,,,,,,
SnAG: Scalable and Accurate Video Grounding,,,,"Fangzhou Mu (University Of Wisconsin-Madison) | SICHENG MO (University Of California, Los Angeles) | Yin Li (University Of Wisconsin, Madison)",,,,,,,
Towards 3D Vision with Low-Cost Single-Photon Cameras,,,,"Fangzhou Mu (University Of Wisconsin-Madison) | Carter Sifferman (University Of Wisconsin - Madison) | Sacha Jungerman (University Of Wisconsin - Madison) | Yiquan Li (University Of Wisconsin - Madison) | Zhiyue Han (None) | Michael Gleicher (Department Of Computer Sciences, University Of Wisconsin - Madison) | Mohit Gupta (Department Of Computer Sciences, University Of Wisconsin - Madison) | Yin Li (University Of Wisconsin, Madison)",,,,,,,
Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations,"Active recognition, which allows intelligent agents to explore observations for better recognition performance, serves as a prerequisite for various embodied AI tasks, such as grasping, navigation and room arrangements. Given the evolving environment and the multitude of object classes, it is impractical to include all possible classes during the training stage. In this paper, we aim at advancing active open-vocabulary recognition, empowering embodied agents to actively perceive and classify arbitrary objects. However, directly adopting recent open-vocabulary classification models, like Contrastive Language Image Pretraining (CLIP), poses its unique challenges. Specifically, we observe that CLIP's performance is heavily affected by the viewpoint and occlusions, compromising its reliability in unconstrained embodied perception scenarios. Further, the sequential nature of observations in agent-environment interactions necessitates an effective method for integrating features that maintains discriminative strength for open-vocabulary classification. To address these issues, we introduce a novel agent for active open-vocabulary recognition. The proposed method leverages inter-frame and inter-concept similarities to navigate agent movements and to fuse features, without relying on class-specific knowledge. Compared to baseline CLIP model with 29.6% accuracy on ShapeNet dataset, the proposed agent could achieve 53.3% accuracy for open-vocabulary recognition, without any fine-tuning to the equipped CLIP model. Additional experiments conducted with the Habitat simulator further affirm the efficacy of our method.",http://arxiv.org/abs/2311.17938v1,,Lei Fan (Northwestern University) | Jianxiong Zhou (Northwestern University) | Xiaoying Xing (Northwestern University) | Ying Wu (Northwestern University),2023-11-28 19:24:07+00:00,,,,,,
Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception,"Active recognition enables robots to intelligently explore novel observations, thereby acquiring more information while circumventing undesired viewing conditions. Recent approaches favor learning policies from simulated or collected data, wherein appropriate actions are more frequently selected when the recognition is accurate. However, most recognition modules are developed under the closed-world assumption, which makes them ill-equipped to handle unexpected inputs, such as the absence of the target object in the current observation. To address this issue, we propose treating active recognition as a sequential evidence-gathering process, providing by-step uncertainty quantification and reliable prediction under the evidence combination theory. Additionally, the reward function developed in this paper effectively characterizes the merit of actions when operating in open-world environments. To evaluate the performance, we collect a dataset from an indoor simulator, encompassing various recognition challenges such as distance, occlusion levels, and visibility. Through a series of experiments on recognition and robustness analysis, we demonstrate the necessity of introducing uncertainties to active recognition and the superior performance of the proposed method.",http://arxiv.org/abs/2311.13793v1,,Lei Fan (Northwestern University) | Mingfu Liang (Northwestern University) | Yunxuan Li (Northwestern University) | Gang Hua (Wormpex AI Research) | Ying Wu (Northwestern University),2023-11-23 03:51:46+00:00,,,,,,
Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification,,,,"Sravanti Addepalli (Indian Institute Of Science) | Ashish Asokan (Indian Institute Of Science, Indian Institute Of Science, Bangalore) | Lakshay Sharma (Indian Institute Of Science, Indian Institute Of Science, Bangalore) | R. Venkatesh Babu (Indian Institute Of Science)",,,,,,,
Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching,"Stereo matching methods based on iterative optimization, like RAFT-Stereo and IGEV-Stereo, have evolved into a cornerstone in the field of stereo matching. However, these methods struggle to simultaneously capture high-frequency information in edges and low-frequency information in smooth regions due to the fixed receptive field. As a result, they tend to lose details, blur edges, and produce false matches in textureless areas. In this paper, we propose Selective Recurrent Unit (SRU), a novel iterative update operator for stereo matching. The SRU module can adaptively fuse hidden disparity information at multiple frequencies for edge and smooth regions. To perform adaptive fusion, we introduce a new Contextual Spatial Attention (CSA) module to generate attention maps as fusion weights. The SRU empowers the network to aggregate hidden disparity information across multiple frequencies, mitigating the risk of vital hidden disparity information loss during iterative processes. To verify SRU's universality, we apply it to representative iterative stereo matching methods, collectively referred to as Selective-Stereo. Our Selective-Stereo ranks $1^{st}$ on KITTI 2012, KITTI 2015, ETH3D, and Middlebury leaderboards among all published methods. Code is available at https://github.com/Windsrain/Selective-Stereo.",http://arxiv.org/abs/2403.00486v1,,Xianqi Wang (Huazhong University Of Science And Technology) | Gangwei Xu (Huazhong University Of Science And Technology) | Hao Jia (Huazhong University Of Science And Technology) | Xin Yang (Huazhong University Of Science And Technology),2024-03-01 12:13:20+00:00,,,,,,
HDRFlow: Real-Time HDR Video Reconstruction with Large Motions,"Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion. Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting. However, they often struggle to handle large complex motions and are computationally expensive. To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow. HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme. The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK can effectively model large motions at a negligible cost. In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions. Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard benchmarks. To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms.",http://arxiv.org/abs/2403.03447v1,,Gangwei Xu (Huazhong University Of Science And Technology) | Yujin Wang (Shanghai Artificial Intelligence Laboratory) | Jinwei Gu (The Chinese University Of Hong Kong) | Tianfan Xue (The Chinese University Of Hong Kong) | Xin Yang (Huazhong University Of Science And Technology),2024-03-06 04:13:29+00:00,,,,,,
DeiT-LT: Distillation Strikes Back for Vision Transformer training on Long-Tailed Datasets,,,,"Harsh Rangwani (Indian Institute Of Science) | Pradipto Mondal (Indian Institute Of Technology, Kharagpur) | Mayank Mishra (CMU, Carnegie Mellon University) | Ashish Asokan (Indian Institute Of Science, Indian Institute Of Science, Bangalore) | R. Venkatesh Babu (Indian Institute Of Science)",,,,,,,
Balancing Act: Distribution-Guided Debiasing in Diffusion Models,"Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.",http://arxiv.org/abs/2402.18206v1,,"Rishubh Parihar (Indian Institute Of Science, Bangalore) | Abhijnya Bhat (Indian Institute Of Science, Indian Institute Of Science, Bangalore) | Abhipsa Basu (Indian Institute Of Science) | Saswat Mallick (Indian Institute Of Science, Indian Institute Of Science, Bangalore) | Jogendra Kundu Kundu (None) | R. Venkatesh Babu (Indian Institute Of Science)",2024-02-28 09:53:17+00:00,,,,,,
Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving,"Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations. Project website: https://github.com/Junda24/AFNet/.",http://arxiv.org/abs/2403.07535v1,,JunDa Cheng (Huazhong University Of Science And Technology) | Wei Yin ( Shenzhen DJI Sciences And Technologies Ltd.) | Kaixuan Wang (Hong Kong University Of Science And Technology) | Xiaozhi Chen (DJI Innovations) | Shijie Wang (Huazhong University Of Science And Technology) | Xin Yang (Huazhong University Of Science And Technology),2024-03-12 11:18:35+00:00,,,,,,
Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling,,,,Jiawei Shi (Northwest Polytechnical University Xi&Amp;#x27;An) | Hui Deng (Northwest Polytechnical University Xi'an) | Yuchao Dai (Northwestern Polytechnical University),,,,,,,
SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion,"A long-standing goal of 3D human reconstruction is to create lifelike and fully detailed 3D humans from single images. The main challenge lies in inferring unknown human shapes, clothing, and texture information in areas not visible in the images. To address this, we propose SiTH, a novel pipeline that uniquely integrates an image-conditioned diffusion model into a 3D mesh reconstruction workflow. At the core of our method lies the decomposition of the ill-posed single-view reconstruction problem into hallucination and reconstruction subproblems. For the former, we employ a powerful generative diffusion model to hallucinate back appearances from the input images. For the latter, we leverage skinned body meshes as guidance to recover full-body texture meshes from the input and back-view images. Our designs enable training of the pipeline with only about 500 3D human scans while maintaining its generality and robustness. Extensive experiments and user studies on two 3D reconstruction benchmarks demonstrated the efficacy of our method in generating realistic, fully textured 3D humans from a diverse range of unseen images.",http://arxiv.org/abs/2311.15855v1,,Hsuan-I Ho (ETHZ - ETH Zurich) | Jie Song (ETHZ - ETH Zurich) | Otmar Hilliges (None),2023-11-27 14:22:07+00:00,,,,,,
Improving Depth Completion via Depth Feature Upsampling,,,,Yufei Wang (Northwest Polytechnical University Xi&Amp;#x27;An) | Ge Zhang (Northwest Polytechnical University Xi'an) | Shaoqian Wang (Northwest Polytechnical University Xi'an) | Bo Li (None) | Qi Liu (Northwest Polytechnical University Xi'an) | Le Hui (Nanjing University Of Science And Technology) | Yuchao Dai (Northwestern Polytechnical University),,,,,,,
HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video,"Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold",http://arxiv.org/abs/2311.18448v1,,"Zicong Fan (ETH Zurich) | Maria Parelli (ETH Zurich) | Maria Kadoglou (ETHZ - ETH Zurich) | Xu Chen (Google) | Muhammed Kocabas (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Michael J. Black (University Of T??bingen) | Otmar Hilliges (None)",2023-11-30 10:50:35+00:00,,,,,,
3D Geomery-aware Deformable Gaussian Splatting for Dynamic View Synthesis,,,,Zhicheng Lu (Northwest Polytechnical University Xi&Amp;#x27;An) | Xiang Guo (Northwest Polytechnical University Xi'an) | Le Hui (Nanjing University Of Science And Technology) | Tianrui Chen (Northwest Polytechnical University Xi'an) | Min Yang (None) | Xiao Tang (None) | Feng Zhu (None) | Yuchao Dai (Northwestern Polytechnical University),,,,,,,
4D-DRESS: A 4D Dataset of Real-World Human Clothing With Semantic Annotations,,,,"Wenbo Wang (ETHZ - ETH Zurich) | Hsuan-I Ho (ETHZ - ETH Zurich) | Chen Guo (ETH Zurich) | Boxiang Rong (ETHZ - ETH Zurich) | Artur Grigorev (None) | Jie Song (ETHZ - ETH Zurich) | Juan Jose Zarate (Department Of Computer Science, ETHZ - ETH Zurich) | Otmar Hilliges (None)",,,,,,,
Adversarially Robust Few-shot Learning via Parameter Co-distillation of Similarity and Class Concept Learners,,,,Junhao Dong (Nanyang Technological University) | Piotr Koniusz (Australian National University) | Junxi Chen (SUN YAT-SEN UNIVERSITY) | Xiaohua Xie (SUN YAT-SEN UNIVERSITY) | Yew-Soon Ong (Nanyang Technological University),,,,,,,
Robust Distillation via Untargeted and Targeted Intermediate Adversarial Samples,,,,Junhao Dong (Nanyang Technological University) | Piotr Koniusz (Australian National University) | Junxi Chen (SUN YAT-SEN UNIVERSITY) | Z. Wang (University Of British Columbia) | Yew-Soon Ong (Nanyang Technological University),,,,,,,
Neural Fields as Distributions: Signal Processing Beyond Euclidean Space,,,,"Daniel Rebain (None) | Soroosh Yazdani (Google) | Kwang Moo Yi (University Of British Columbia) | Andrea Tagliasacchi (Simon Fraser University, Google Brain)",,,,,,,
Rethinking Visual Instruction Tuning,,,,"Haotian Liu (University Of Wisconsin-Madison) | Chunyuan Li (Microsoft Research, Redmond) | Yuheng Li (University Of Wisconsin - Madison) | Yong Jae Lee (Department Of Computer Sciences, University Of Wisconsin - Madison)",,,,,,,
Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields,"Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce BayesRays, a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and show its superior performance in key metrics and applications. Additional results available at: https://bayesrays.github.io.",http://arxiv.org/abs/2309.03185v1,,"Leili Goli (University Of Toronto) | Cody Reading (Simon Fraser University) | Silvia Sell??n (University Of Toronto) | Alec Jacobson (University Of Toronto And Adobe Systems) | Andrea Tagliasacchi (Simon Fraser University, Google Brain)",2023-09-06 17:44:34+00:00,,,,,,
Edit One for All: Interactive Batch Image Editing,"In recent years, image editing has advanced remarkably. With increased human control, it is now possible to edit an image in a plethora of ways; from specifying in text what we want to change, to straight up dragging the contents of the image in an interactive point-based manner. However, most of the focus has remained on editing single images at a time. Whether and how we can simultaneously edit large batches of images has remained understudied. With the goal of minimizing human supervision in the editing process, this paper presents a novel method for interactive batch image editing using StyleGAN as the medium. Given an edit specified by users in an example image (e.g., make the face frontal), our method can automatically transfer that edit to other test images, so that regardless of their initial state (pose), they all arrive at the same final state (e.g., all facing front). Extensive experiments demonstrate that edits performed using our method have similar visual quality to existing single-image-editing methods, while having more visual consistency and saving significant time and human effort.",http://arxiv.org/abs/2401.10219v1,,"Thao Nguyen (UW-Madison) | Utkarsh Ojha (University Of Wisconsin - Madison) | Yuheng Li (University Of Wisconsin - Madison) | Haotian Liu (University Of Wisconsin-Madison) | Yong Jae Lee (Department Of Computer Sciences, University Of Wisconsin - Madison)",2024-01-18 18:58:44+00:00,,,,,,
BANF: Band-limited Neural Fields for Levels of Detail Reconstruction,,,,"Ahan Shabanov (Simon Fraser University) | Shrisudhan Govindarajan (Simon Fraser University) | Cody Reading (Simon Fraser University) | Leili Goli (University Of Toronto) | Daniel Rebain (None) | Kwang Moo Yi (University Of British Columbia) | Andrea Tagliasacchi (Simon Fraser University, Google Brain)",,,,,,,
Making Large Multimodal Models Understand Arbitrary Visual Prompts,"While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a ""red bounding box"" or ""pointed arrow"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.",http://arxiv.org/abs/2312.00784v1,,"Mu Cai (Department Of Computer Science, University Of Wisconsin, Madison) | Haotian Liu (University Of Wisconsin-Madison) | Siva Mustikovela (Heidelberg University) | Gregory P. Meyer (Cruise) | Yuning Chai (Cruise) | Dennis Park (Toyota Research Institute) | Yong Jae Lee (Department Of Computer Sciences, University Of Wisconsin - Madison)",2023-12-01 18:59:56+00:00,,,,,,
Improving Spectral Snapshot Reconstruction with Spectral-Spatial Rectification,,,,Jiancheng Zhang (Northwest Polytechnical University Xi'an) | Haijin Zeng (IMEC & Universiteit Gent) | Yongyong Chen (Harbin Institute Of Technology (Shenzhen)) | Dengxiu Yu (Northwest Polytechnical University) | Yinping Zhao (Northwestern Polytechnical University),,,,,,,
Text-Enhanced Data-free Approach for Federated Class-Incremental Learning,,,,Minh-Tuan Tran (Monash University) | Trung Le (Monash University) | Xuan-May Le (University Of Melbourne) | Mehrtash Harandi (Monash University) | Dinh Phung (Monash University),,,,,,,
RobustSAM: Segment Anything Robustly on Degraded Images,,,,Wei-Ting Chen (National Taiwan University) | Yu Jiet Vong (National Taiwan University) | Sy-Yen Kuo (National Taiwan University) | Sizhuo Ma (Snap) | Jian Wang (Snap),,,,,,,
Dual Prior Unfolding for Snapshot Compressive Imaging,,,,Jiancheng Zhang (Northwest Polytechnical University Xi'an) | Haijin Zeng (IMEC & Universiteit Gent) | Jiezhang Cao (ETH Zurich) | Yongyong Chen (Harbin Institute Of Technology (Shenzhen)) | Dengxiu Yu (Northwest Polytechnical University) | Yinping Zhao (Northwestern Polytechnical University),,,,,,,
NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free Knowledge Distillation,"Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches.",http://arxiv.org/abs/2310.00258v1,,Minh-Tuan Tran (Monash University) | Trung Le (Monash University) | Xuan-May Le (University Of Melbourne) | Mehrtash Harandi (Monash University) | Quan Tran (Servicenow) | Dinh Phung (Monash University),2023-09-30 05:19:10+00:00,,,,,,
DSL-FIQA: Assessing Facial Image Quality via Dual-Set Degradation Learning and Landmark-Guided Transformer,,,,Wei-Ting Chen (National Taiwan University) | Gurunandan Krishnan (Snap) | Qiang Gao (Snap) | Sy-Yen Kuo (National Taiwan University) | Sizhuo Ma (Snap) | Jian Wang (Snap),,,,,,,
RankMatch: Exploring the Better Consistency Regularization for Semi-supervised Semantic Segmentation,,,,"Huayu Mai (University Of Science And Technology Of China) | Rui Sun (University Of Science And Technology Of China) | Tianzhu Zhang (University Of Science And Technology Of China, Tsinghua University) | Feng Wu (University Of Science And Technology Of China)",,,,,,,
OVMR: Open-Vocabulary Recognition with Multi-Modal References,,,,Zehong Ma (Peking University) | Shiliang Zhang (Peking University) | Longhui Wei (Huawei Cloud Technologies Ltd.) | Qi Tian (Huawei Technologies Ltd.),,,,,,,
Scene Adaptive Sparse Transformer for Event-based Object Detection,,,,Yansong Peng (None) | Li Hebei (University Of Science And Technology Of China) | Yueyi Zhang (University Of Science And Technology Of China) | Xiaoyan Sun (University Of Science And Technology Of China) | Feng Wu (University Of Science And Technology Of China),,,,,,,
GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions,"Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes -- 2 hours).",http://arxiv.org/abs/2311.16037v1,,Junjie Wang (None) | Jiemin Fang (Huawei Technologies Ltd.) | Xiaopeng Zhang (Huawei Technologies Ltd.) | Lingxi Xie (Huawei Technologies Ltd.) | Qi Tian (Huawei Technologies Ltd.),2023-11-27 17:58:21+00:00,,,,,,
Learning Large-Factor EM Image Super-Resolution with Generative Priors,,,,"Jiateng Shou (University Of Science And Technology Of China) | Zeyu Xiao (None) | Shiyu Deng (University Of Science And Technology Of China) | Wei Huang (University Of Science And Technology Of China) | ShiPeiyao (Suzhou Institute Of Biomedical Engineering And Technology, Chinese Academy Of Sciences) | Ruobing Zhang (Suzhou Institute Of Biomedical Engineering And Technology) | Zhiwei Xiong (None) | Feng Wu (University Of Science And Technology Of China)",,,,,,,
Enhance Image Classification Via Inter-Class Image Mixup With Diffusion Model,,,,Zhicai Wang (None) | Longhui Wei (Huawei Cloud Technologies Ltd.) | Tan Wang (Nanyang Technological University) | Heyu Chen (None) | Yanbin Hao (None) | Xiang Wang (University Of Science And Technology Of China) | Xiangnan He (University Of Science And Technology Of China) | Qi Tian (Huawei Technologies Ltd.),,,,,,,
Test-Time Adaptation for Depth Completion,,,,Hyoungseob Park (Yale University) | Anjali W Gupta (Yale) | Alex Wong (Yale University),,,,,,,
WorDepth: Variational Language Prior for Monocular Depth Estimation,,,,"Ziyao Zeng (Yale University) | Hyoungseob Park (Yale University) | Fengyu Yang (Yale University) | Daniel Wang (Yale University) | Stefano Soatto (University Of California, Los Angeles) | Dong Lao (University Of California, Los Angeles) | Alex Wong (Yale University)",,,,,,,
Binding Touch to Everything: Learning Unified Multimodal Tactile Representations,"The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/",http://arxiv.org/abs/2401.18084v1,,Fengyu Yang (Yale University) | Chao Feng (None) | Ziyang Chen (University Of Michigan) | Hyoungseob Park (Yale University) | Daniel Wang (Yale University) | Yiming Dou (University Of Michigan - Ann Arbor) | Ziyao Zeng (Yale University) | Xien Chen (Yale University) | Suchisrit Gangopadhyay (Yale University) | Andrew Owens (University Of Michigan) | Alex Wong (Yale University),2024-01-31 18:59:57+00:00,,,,,,
360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model,"360-degree panoramic videos recently attract more interest in both studies and applications, courtesy of the heightened immersive experiences they engender. Due to the expensive cost of capturing 360-degree panoramic videos, generating desirable panoramic videos by given prompts is urgently required. Recently, the emerging text-to-video (T2V) diffusion methods demonstrate notable effectiveness in standard video generation. However, due to the significant gap in content and motion patterns between panoramic and standard videos, these methods encounter challenges in yielding satisfactory 360-degree panoramic videos. In this paper, we propose a controllable panorama video generation pipeline named 360-Degree Video Diffusion model (360DVD) for generating panoramic videos based on the given prompts and motion conditions. Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted 360 Enhancement Techniques to transform pre-trained T2V models for 360-degree video generation. We further propose a new panorama dataset named WEB360 consisting of 360-degree video-text pairs for training 360DVD, addressing the absence of captioned panoramic video datasets. Extensive experiments demonstrate the superiority and effectiveness of 360DVD for panorama video generation. The code and dataset will be released soon.",http://arxiv.org/abs/2401.06578v1,,Qian Wang (Peking University) | Weiqi Li (Peking University) | Chong Mou (Peking University) | Xinhua Cheng (Peking University) | Jian Zhang (None),2024-01-12 13:52:29+00:00,,,,,,
Test-Time Zero-Shot Temporal Action Localization,,,,Benedetta Liberatori (University Of Trento) | Alessandro Conti (University Of Trento) | Paolo Rota (University Of Trento) | Yiming Wang (Fondazione Bruno Kessler) | Elisa Ricci (University Of Trento),,,,,,,
DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing,"Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.",http://arxiv.org/abs/2402.02583v1,,Chong Mou (Peking University) | Xintao Wang (Tencent) | Jiechong Song (None) | Ying Shan (Tencent) | Jian Zhang (None),2024-02-04 18:50:29+00:00,,,,,,
RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding,"We propose a lightweight and scalable Regional Point-Language Contrastive learning framework, namely \textbf{RegionPLC}, for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2\% and 9.1\% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code will be released.",http://arxiv.org/abs/2304.00962v3,,"Jihan Yang (The University Of Hong Kong) | Runyu Ding (Electrical And Electronic Engineering, University Of Hong Kong) | Weipeng DENG (University Of Hong Kong) | Zhe Wang (Sensetime Group Limited) | Xiaojuan Qi (University Of Oxford)",2023-04-03 13:30:04+00:00,,,,,,
Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts,"Federated learning facilitates the collaborative learning of a global model across multiple distributed medical institutions without centralizing data. Nevertheless, the expensive cost of annotation on local clients remains an obstacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leveraging local and global model predictions to select a relatively small amount of informative local data for annotation. However, existing methods mainly focus on all local data sampled from the same domain, making them unreliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first attempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specifically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribution over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncertainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data redundancy and maintain data diversity. Extensive experiments and analyses are conducted to show the superiority of FEAL over the state-of-the-art active learning methods and the efficiency of FEAL under the federated active learning framework.",http://arxiv.org/abs/2312.02567v1,,Jiayi Chen (Northwestern Polytechnical University) | Benteng Ma (Hong Kong University Of Science And Technology) | Hengfei Cui (Northwest Polytechnical University Xi'an) | Kwang-Ting Cheng (Hong Kong University Of Science And Technology) | Yong Xia (Northwestern Polytechnical University),2023-12-05 08:32:27+00:00,,,,,,
Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation,"Distribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pre-trained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pre-trained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pre-trained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pre-trained source models are available at https://github.com/Chen-Ziyang/VPTTA.",http://arxiv.org/abs/2311.18363v2,,Ziyang Chen (Northwestern Polytechnical University) | Yongsheng Pan (ShanghaiTech University) | Yiwen Ye (Northwestern Polytechnical University) | Mengkang Lu (Nwpu) | Yong Xia (Northwestern Polytechnical University),2023-11-30 09:03:47+00:00,,,,,,
Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction,,,,Xiaoyang Lyu (University Of Hong Kong) | Chirui Chang (None) | Peng Dai (None) | Yangtian Sun (None) | Xiaojuan Qi (University Of Oxford),,,,,,,
Harnessing Large Language Models for Training-free Video Anomaly Detection,,,,Luca Zanella (University Of Trento) | Willi Menapace (University Of Trento) | Massimiliano Mancini (University Of Trento) | Yiming Wang (Fondazione Bruno Kessler) | Elisa Ricci (University Of Trento),,,,,,,
SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes,"Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications. Project page: https://yihua7.github.io/SC-GS-web/",http://arxiv.org/abs/2312.14937v2,,Yihua Huang (None) | Yangtian Sun (None) | Ziyi Yang (None) | Xiaoyang Lyu (University Of Hong Kong) | Yan-Pei Cao (Tencent ARC Lab) | Xiaojuan Qi (University Of Oxford),2023-12-04 11:57:14+00:00,,,,,,
MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning,,,,Matteo Farina (University Of Trento) | Massimiliano Mancini (University Of Trento) | Elia Cunegatti (University Of Trento) | Gaowen Liu (None) | Giovanni Iacca (University Of Trento) | Elisa Ricci (University Of Trento),,,,,,,
EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection,"In the era where AI-generated content (AIGC) models can produce stunning and lifelike images, the lingering shadow of unauthorized reproductions and malicious tampering poses imminent threats to copyright integrity and information security. Current image watermarking methods, while widely accepted for safeguarding visual content, can only protect copyright and ensure traceability. They fall short in localizing increasingly realistic image tampering, potentially leading to trust crises, privacy violations, and legal disputes. To solve this challenge, we propose an innovative proactive forensics framework EditGuard, to unify copyright protection and tamper-agnostic localization, especially for AIGC-based editing methods. It can offer a meticulous embedding of imperceptible watermarks and precise decoding of tampered areas and copyright information. Leveraging our observed fragility and locality of image-into-image steganography, the realization of EditGuard can be converted into a united image-bit steganography issue, thus completely decoupling the training process from the tampering types. Extensive experiments demonstrate that our EditGuard balances the tamper localization accuracy, copyright recovery precision, and generalizability to various AIGC-based tampering methods, especially for image forgery that is difficult for the naked eye to detect. The project page is available at https://xuanyuzhang21.github.io/project/editguard/.",http://arxiv.org/abs/2312.08883v1,,Xuanyu Zhang (Peking University Shenzhen Graduate School) | Runyi Li (Peking University) | Jiwen Yu (Peking University) | Youmin Xu (Peking University) | Weiqi Li (Peking University) | Jian Zhang (None),2023-12-12 15:41:24+00:00,,,,,,
Continual Self-supervised Learning: Towards Universal Multi-modal Medical Data Representation Learning,"Self-supervised learning is an efficient pre-training method for medical image analysis. However, current research is mostly confined to specific-modality data pre-training, consuming considerable time and resources without achieving universality across different modalities. A straightforward solution is combining all modality data for joint self-supervised pre-training, which poses practical challenges. Firstly, our experiments reveal conflicts in representation learning as the number of modalities increases. Secondly, multi-modal data collected in advance cannot cover all real-world scenarios. In this paper, we reconsider versatile self-supervised learning from the perspective of continual learning and propose MedCoSS, a continuous self-supervised learning approach for multi-modal medical data. Unlike joint self-supervised learning, MedCoSS assigns different modality data to different training stages, forming a multi-stage pre-training process. To balance modal conflicts and prevent catastrophic forgetting, we propose a rehearsal-based continual learning method. We introduce the k-means sampling strategy to retain data from previous modalities and rehearse it when learning new modalities. Instead of executing the pretext task on buffer data, a feature distillation strategy and an intra-modal mixup strategy are applied to these data for knowledge retention. We conduct continuous self-supervised pre-training on a large-scale multi-modal unlabeled dataset, including clinical reports, X-rays, CT scans, MRI scans, and pathological images. Experimental results demonstrate MedCoSS's exceptional generalization ability across nine downstream datasets and its significant scalability in integrating new modality data. Code and pre-trained weight are available at https://github.com/yeerwen/MedCoSS.",http://arxiv.org/abs/2311.17597v2,,Yiwen Ye (Northwestern Polytechnical University) | Yutong Xie (University Of Adelaide) | Jianpeng Zhang (None) | Ziyang Chen (Northwestern Polytechnical University) | Qi Wu (University Of Adelaide) | Yong Xia (Northwestern Polytechnical University),2023-11-29 12:47:42+00:00,,,,,,
Prompting Vision Foundation Models for Pathology Image Analysis,,,,CHONG YIN (Hong Kong Baptist University) | Siqi Liu (Shenzhen Research Institute Of Big Data) | Kaiyang Zhou (Hong Kong Baptist University) | Vincent Wong (The Chinese University Of Hong Kong) | Pong C. Yuen (Hong Kong Baptist Unviersity),,,,,,,
XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images,,,,CHONG YIN (Hong Kong Baptist University) | Siqi Liu (Shenzhen Research Institute Of Big Data) | Fei Lyu (Hong Kong Baptist University) | Jiahao Lu (Copenhagen University) | Sune Darkner (Copenhagen University) | Vincent Wong (The Chinese University Of Hong Kong) | Pong C. Yuen (Hong Kong Baptist Unviersity),,,,,,,
GLID: Pre-training a Generalist Encoder-Decoder Vision Model,,,,Jihao Liu (The Chinese University Of Hong Kong) | Jinliang Zheng (SenseTime) | Yu Liu (The Chinese University Of Hong Kong) | Hongsheng Li (The Chinese University Of Hong Kong),,,,,,,
Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds,"Computer-Aided Design (CAD) model reconstruction from point clouds is an important problem at the intersection of computer vision, graphics, and machine learning; it saves the designer significant time when iterating on in-the-wild objects. Recent advancements in this direction achieve relatively reliable semantic segmentation but still struggle to produce an adequate topology of the CAD model. In this work, we analyze the current state of the art for that ill-posed task and identify shortcomings of existing methods. We propose a hybrid analytic-neural reconstruction scheme that bridges the gap between segmented point clouds and structured CAD models and can be readily combined with different segmentation backbones. Moreover, to power the surface fitting stage, we propose a novel implicit neural representation of freeform surfaces, driving up the performance of our overall CAD reconstruction scheme. We extensively evaluate our method on the popular ABC benchmark of CAD models and set a new state-of-the-art for that dataset. Project page: https://www.obukhov.ai/point2cad}{https://www.obukhov.ai/point2cad.",http://arxiv.org/abs/2312.04962v1,,Yujia Liu (Swiss Federal Institute Of Technology) | Anton Obukhov (None) | Jan D. Wegner (University Of Zurich) | Konrad Schindler (ETH Zurich),2023-12-07 08:23:44+00:00,,,,,,
Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation,"Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.",http://arxiv.org/abs/2312.02145v1,,Bingxin Ke (ETH Zurich) | Anton Obukhov (None) | Shengyu Huang (None) | Nando Metzger (ETH Zurich) | Rodrigo Caye Daudt (ETH Zurich) | Konrad Schindler (ETH Zurich),2023-12-04 18:59:13+00:00,,,,,,
DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation,"We present DiffInDScene, a novel framework for tackling the problem of high-quality 3D indoor scene generation, which is challenging due to the complexity and diversity of the indoor scene geometry. Although diffusion-based generative models have previously demonstrated impressive performance in image generation and object-level 3D generation, they have not yet been applied to room-level 3D generation due to their computationally intensive costs. In DiffInDScene, we propose a cascaded 3D diffusion pipeline that is efficient and possesses strong generative performance for Truncated Signed Distance Function (TSDF). The whole pipeline is designed to run on a sparse occupancy space in a coarse-to-fine fashion. Inspired by KinectFusion's incremental alignment and fusion of local TSDF volumes, we propose a diffusion-based SDF fusion approach that iteratively diffuses and fuses local TSDF volumes, facilitating the generation of an entire room environment. The generated results demonstrate that our work is capable to achieve high-quality room generation directly in three-dimensional space, starting from scratch. In addition to the scene generation, the final part of DiffInDScene can be used as a post-processing module to refine the 3D reconstruction results from multi-view stereo. According to the user study, the mesh quality generated by our DiffInDScene can even outperform the ground truth mesh provided by ScanNet. Please visit our project page for the latest progress and demonstrations: https://github.com/AkiraHero/diffindscene.",http://arxiv.org/abs/2306.00519v4,,Xiaoliang Ju (The Chinese University Of Hong Kong) | Zhaoyang Huang (The Chinese University Of Hong Kong) | Yijin Li (Zhejiang University) | Guofeng Zhang (Zhejiang University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Hongsheng Li (The Chinese University Of Hong Kong),2023-06-01 10:19:23+00:00,,,,,,
LMDrive: Closed-Loop End-to-End Driving with Large Language Models,"Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach ""Artificial General Intelligence"". On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans. To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes, models, and datasets can be found at https://github.com/opendilab/LMDrive",http://arxiv.org/abs/2312.07488v2,,Hao Shao (None) | Yuxuan Hu (The Chinese University Of Hong Kong) | Letian Wang (University Of Toronto) | Guanglu Song (Sensetime X-Lab) | Steven L. Waslander (University Of Toronto) | Yu Liu (The Chinese University Of Hong Kong) | Hongsheng Li (The Chinese University Of Hong Kong),2023-12-12 18:24:15+00:00,,,,,,
StegoGAN: Bootstrapping Non-bijective Image-to-Image Translation with CycleGAN Steganography,,,,Sidi Wu (ETH Zurich) | Yizi Chen (ETHZ - ETH Zurich) | Loic Landrieu (ENPC) | Nicolas Gonthier (IGN) | Samuel Mermet (Ecole Nationale Des Sciences G??ographiques) | Lorenz Hurni (ETHZ - ETH Zurich) | Konrad Schindler (ETH Zurich),,,,,,,
Rethinking FID: Towards a Better Evaluation Metric for Image Generation,"As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Frechet Inception Distance (FID). FID estimates the distance between a distribution of Inception-v3 features of real images, and those of images generated by the algorithm. We highlight important drawbacks of FID: Inception's poor representation of the rich and varied content generated by modern text-to-image models, incorrect normality assumptions, and poor sample complexity. We call for a reevaluation of FID's use as the primary quality metric for generated images. We empirically demonstrate that FID contradicts human raters, it does not reflect gradual improvement of iterative text-to-image models, it does not capture distortion levels, and that it produces inconsistent results when varying the sample size. We also propose an alternative new metric, CMMD, based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient. Through extensive experiments and analysis, we demonstrate that FID-based evaluations of text-to-image models may be unreliable, and that CMMD offers a more robust and reliable assessment of image quality.",http://arxiv.org/abs/2401.09603v2,,Sadeep Jayasumana (Google) | Srikumar Ramalingam (Google) | Andreas Veit (Google) | Daniel Glasner (Google) | Ayan Chakrabarti (Google) | Sanjiv Kumar (Google),2023-11-30 19:11:01+00:00,,,,,,
MarkovGen: Structured Prediction for Efficient Text-to-Image Generation,"Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running sampling multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. We demonstrate the effectiveness of this method on top of the latent token-based Muse text-to-image model. The MRF richly encodes the compatibility among image tokens at different spatial locations to improve quality and significantly reduce the required number of Muse sampling steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, MarkovGen, uses this proposed MRF model to both speed up Muse by 1.5X and produce higher quality images by decreasing undesirable image artifacts.",http://arxiv.org/abs/2308.10997v3,,Sadeep Jayasumana (Google) | Daniel Glasner (Google) | Srikumar Ramalingam (Google) | Andreas Veit (Google) | Ayan Chakrabarti (Google) | Sanjiv Kumar (Google),2023-08-14 14:07:17+00:00,,,,,,
MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections,,,,"Mude Hui (University Of California, Santa Cruz) | Zihao Wei (University Of Michigan - Ann Arbor) | Hongru Zhu (None) | Fei Xia (Ecole Normale Sup??rieure De Paris) | Yuyin Zhou (UC Santa Cruz)",,,,,,,
Cinematic Behavior Transfer via NeRF-based Differentiable Filming,"In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired. Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses. To address these issues, we first introduce a reverse filming behavior estimation technique. It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment. The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study.",http://arxiv.org/abs/2311.17754v1,,Xuekun Jiang (Shanghai Artificial Intelligence Laboratory) | Anyi Rao (None) | Jingbo Wang (Shanghai AI LAB) | Dahua Lin (The Chinese University Of Hong Kong) | Bo Dai (Shanghai AI Laboratory),2023-11-29 15:56:58+00:00,,,,,,
Generative Multi-modal Models are Good Class Incremental Learners,,,,"Xu Cao Cao (None) | Haori Lu (Nankai University) | Linlan Huang (Nankai University) | Xialei Liu (Nankai University) | Ming-Ming Cheng (Nankai University, Tsinghua University)",,,,,,,
Bridging the Gap Between End-to-End and Two-Step Text Spotting,,,,Mingxin Huang (None) | Hongliang Li (South China University Of Technology) | Yuliang Liu (Huazhong University Of Science And Technology) | Xiang Bai (Huazhong University Of Science And Technology) | Lianwen Jin (South China University Of Technology),,,,,,,
DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks,,,,Jiaxin Zhang (South China University Of Technology) | Dezhi Peng (South China University Of Technology) | Chongyu Liu (South China University Of Technology) | Peirong Zhang (None) | Lianwen Jin (South China University Of Technology),,,,,,,
Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental Learning,"Data-Free Class Incremental Learning (DFCIL) aims to sequentially learn tasks with access only to data from the current one. DFCIL is of interest because it mitigates concerns about privacy and long-term storage of data, while at the same time alleviating the problem of catastrophic forgetting in incremental learning. In this work, we introduce robust saliency guidance for DFCIL and propose a new framework, which we call RObust Saliency Supervision (ROSS), for mitigating the negative effect of saliency drift. Firstly, we use a teacher-student architecture leveraging low-level tasks to supervise the model with global saliency. We also apply boundary-guided saliency to protect it from drifting across object boundaries at intermediate layers. Finally, we introduce a module for injecting and recovering saliency noise to increase robustness of saliency preservation. Our experiments demonstrate that our method can retain better saliency maps across tasks and achieve state-of-the-art results on the CIFAR-100, Tiny-ImageNet and ImageNet-Subset DFCIL benchmarks. Code will be made publicly available.",http://arxiv.org/abs/2212.08251v1,,"Xialei Liu (Nankai University) | Jiang-Tian Zhai (Nankai University) | Andrew Bagdanov (Universit?? Degli Studi Di Firenze) | Ke Li (Tencent) | Ming-Ming Cheng (Nankai University, Tsinghua University)",2022-12-16 02:43:52+00:00,,,,,,
PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios,,,,Jingbo Wang (Shanghai AI LAB) | Zhengyi Luo (Carnegie Mellon University) | Ye Yuan (NVIDIA Research) | Yixuan LI (The Chinese University Of Hong Kong) | Bo Dai (Shanghai AI Laboratory),,,,,,,
Sculpting Holistic 3D Representation in Contrastive Language-Image-3D Pre-training,,,,"Yipeng Gao (SUN YAT-SEN UNIVERSITY) | Zeyu Wang (University Of California, Santa Cruz) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) | Cihang Xie (University Of California, Santa Cruz) | Yuyin Zhou (UC Santa Cruz)",,,,,,,
TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes,"Recent progress in the text-driven 3D stylization of a single object has been considerably promoted by CLIP-based methods. However, the stylization of multi-object 3D scenes is still impeded in that the image-text pairs used for pre-training CLIP mostly consist of an object. Meanwhile, the local details of multiple objects may be susceptible to omission due to the existing supervision manner primarily relying on coarse-grained contrast of image-text pairs. To overcome these challenges, we present a novel framework, dubbed TeMO, to parse multi-object 3D scenes and edit their styles under the contrast supervision at multiple levels. We first propose a Decoupled Graph Attention (DGA) module to distinguishably reinforce the features of 3D surface points. Particularly, a cross-modal graph is constructed to align the object points accurately and noun phrases decoupled from the 3D mesh and textual description. Then, we develop a Cross-Grained Contrast (CGC) supervision system, where a fine-grained loss between the words in the textual description and the randomly rendered images are constructed to complement the coarse-grained loss. Extensive experiments show that our method can synthesize high-quality stylized content and outperform the existing methods over a wide range of multi-object 3D meshes. Our code and results will be made publicly available",http://arxiv.org/abs/2312.04248v1,,"Xuying Zhang (Nankai University) | Bo-Wen Yin (Nankai University) | Yuming Chen (None) | Zheng Lin (Nankai University) | Yunheng Li (Nankai University) | Qibin Hou (Nankai University) | Ming-Ming Cheng (Nankai University, Tsinghua University)",2023-12-07 12:10:05+00:00,,,,,,
Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering,"Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved the state-of-the-art rendering quality and speed combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less area and lighting effects. We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrates an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing the rendering speed.",http://arxiv.org/abs/2312.00109v1,,Tao Lu (Nanjing University) | Mulin Yu (Shanghai AI Laboratory) | Linning Xu (The Chinese University Of Hong Kong) | Yuanbo Xiangli (None) | Limin Wang (Nanjing University) | Dahua Lin (The Chinese University Of Hong Kong) | Bo Dai (Shanghai AI Laboratory),2023-11-30 17:58:57+00:00,,,,,,
Towards Modern Image Manipulation Localization: A Large-Scale Dataset and Novel Methods,,,,"Chenfan Qu (South China University Of Technology) | Yiwu Zhong (University Of Wisconsin, Madison) | Chongyu Liu (South China University Of Technology) | Guitao Xu (South China University Of Technology) | Dezhi Peng (South China University Of Technology) | Fengjun Guo (Shanghai Jiao Tong University) | Lianwen Jin (South China University Of Technology)",,,,,,,
Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding,,,,Zhiheng Cheng (None) | Qingyue Wei (Stanford University) | Hongru Zhu (None) | Yan Wang (East China Normal University) | Liangqiong Qu (The University Of Hong Kong) | Wei Shao (None) | Yuyin Zhou (UC Santa Cruz),,,,,,,
Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning,"Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt representation towards the current visual content. We conduct extensive experiments on three popular benchmarks, where our method significantly outperforms existing methods in both closed-world and open-world settings.",http://arxiv.org/abs/2303.15230v1,,Siteng Huang (Zhejiang University & Westlake University) | Biao Gong (Alibaba Group) | Yutong Feng (Alibaba Group) | Zhang Min (Westlake University) | Yiliang Lv (Gientech AIL) | Donglin Wang (Westlake University),2023-03-27 14:10:26+00:00,,,,,,
General Point Model Pretraining with Autoencoding and Autoregressive,"The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves superior performance in point cloud understanding tasks. Furthermore, the integration of autoregressive and autoencoding within the same transformer underscores its versatility across different downstream tasks.",http://arxiv.org/abs/2310.16861v1,,"Zhe Li (?????????????????) | Zhangyang Gao (Westlake University, China) | Cheng Tan (None) | Bocheng Ren (None) | Laurence Yang (Hainan University) | Stan Z. Li (Westlake University)",2023-10-25 06:08:24+00:00,,,,,,
Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation,,,,"Siteng Huang (Zhejiang University & Westlake University) | Biao Gong (Alibaba Group) | Yutong Feng (Alibaba Group) | Xi Chen (The University Of Hong Kong, University Of Hong Kong) | Yuqian Fu (Fudan University) | Yu Liu (Alibaba Group) | Donglin Wang (Westlake University)",,,,,,,
MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning,"The scarcity of annotated data has sparked significant interest in unsupervised pre-training methods that leverage medical reports as auxiliary signals for medical visual representation learning. However, existing research overlooks the multi-granularity nature of medical visual representation and lacks suitable contrastive learning techniques to improve the models' generalizability across different granularities, leading to the underutilization of image-text information. To address this, we propose MLIP, a novel framework leveraging domain-specific medical knowledge as guiding signals to integrate language information into the visual domain through image-text contrastive learning. Our model includes global contrastive learning with our designed divergence encoder, local token-knowledge-patch alignment contrastive learning, and knowledge-guided category-level contrastive learning with expert knowledge. Experimental evaluations reveal the efficacy of our model in enhancing transfer performance for tasks such as image classification, object detection, and semantic segmentation. Notably, MLIP surpasses state-of-the-art methods even with limited annotated data, highlighting the potential of multimodal pre-training in advancing medical representation learning.",http://arxiv.org/abs/2402.02045v1,,"Zhe Li (?????????????????) | Laurence Yang (Hainan University) | Bocheng Ren (None) | Xin Nie (Huazhong University Of Science And Technology) | Zhangyang Gao (Westlake University, China) | Cheng Tan (None) | Stan Z. Li (Westlake University)",2024-02-03 05:48:50+00:00,,,,,,
Generating Content for HDR Deghosting from Frequency View,,,,"Tao Hu (Northwestern Polytechnical University, Xi??an University Of Architecture And Technology) | Qingsen Yan (Northwest Polytechnical University Xi'an) | Yuankai Qi (The University Of Adelaide) | Yanning Zhang (Northwestern Polytechnical University)",,,,,,,
Open-Vocabulary Video Anomaly Detection,,,,"Peng Wu (Northwest Polytechnical University Xi'an) | Xuerong Zhou (Northwest Polytechnical University Xi'an) | Guansong Pang (Singapore Management University) | Yujia Sun (Xi'an University Of Electronic Science And Technology) | Jing Liu (Guangzhou Institute Of Technology, Xidian University) | Peng Wang (Northwestern Polytechnical University) | Yanning Zhang (Northwestern Polytechnical University)",,,,,,,
GC-MVSNet: Geometrically Consistent Cost Aggregation for Multi-view Stereo,,,,"Jiang Wu (Northwest Polytechnical University Xi'an) | Rui Li (None) | Haofei Xu (Department Of Computer Science, ETHZ - ETH Zurich) | Wenxun Zhao (None) | Yu Zhu (Northwest Polytechnical University Xi'an) | Jinqiu Sun (Northwest Polytechnical University Xi'an) | Yanning Zhang (Northwestern Polytechnical University)",,,,,,,
Infrared Adversarial Car Stickers,,,,"Xiaopei Zhu (Tsinghua University) | Yuqiu Liu (Beijing Forestry University) | Zhanhao Hu (UC Berkeley) | Jianmin Li (Department Of Computer Science And Technology, Tsinghua University) | Xiaolin Hu (Tsinghua University)",,,,,,,
Language-Driven Anchors for Zero-Shot Adversarial Robustness,"Deep Neural Networks (DNNs) are known to be susceptible to adversarial attacks. Previous researches mainly focus on improving adversarial robustness in the fully supervised setting, leaving the challenging domain of zero-shot adversarial robustness an open question. In this work, we investigate this domain by leveraging the recent advances in large vision-language models, such as CLIP, to introduce zero-shot adversarial robustness to DNNs. We propose LAAT, a Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes the features of a text encoder for each category as fixed anchors (normalized feature embeddings) for each category, which are then employed for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT aims to enhance the adversarial robustness of the image model on novel categories. However, naively using text encoders leads to poor results. Through analysis, we identified the issue to be the high cosine similarity between text encoders. We then design an expansion algorithm and an alignment cross-entropy loss to alleviate the problem. Our experimental results demonstrated that LAAT significantly improves zero-shot adversarial robustness over state-of-the-art methods. LAAT has the potential to enhance adversarial robustness by large-scale multimodal models, especially when labeled data is unavailable during training.",http://arxiv.org/abs/2301.13096v3,,"Xiao Li (Tsinghua University) | Wei Zhang (Department Of Computer Science And Technology, Tsinghua University) | Yining Liu (Harbin Institute Of Technology At Weihai) | Zhanhao Hu (UC Berkeley) | Bo Zhang (Tsinghua University) | Xiaolin Hu (Tsinghua University)",2023-01-30 17:34:43+00:00,,,,,,
SparseDet: A Simple and Effective Network for Fully Sparse 3D Object Detection,,,,"Gang Zhang (Tsinghua University) | Chen Junnan (Huazhong University Of Science And Technology) | Guohuan Gao (Beijing Institute Of Technology) | Jianmin Li (Department Of Computer Science And Technology, Tsinghua University) | Si Liu (Beihang University) | Xiaolin Hu (Tsinghua University)",,,,,,,
Generative Proxemics: A Prior for 3D Social Interaction from Images,"Social interaction is a fundamental aspect of human behavior and communication. The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction. Reconstructing such interaction from images presents challenges because of mutual occlusion and the limited availability of large training datasets. To address this, we present a novel approach that learns a prior over the 3D proxemics two people in close social interaction and demonstrate its use for single-view 3D reconstruction. We start by creating 3D training data of interacting people using image datasets with contact annotations. We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution over the poses of two people in close social interaction. Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a perceptual study. We use BUDDI in reconstructing two people in close proximity from a single image without any contact annotation via an optimization approach that uses the diffusion model as a prior. Our approach recovers accurate and plausible 3D social interactions from noisy initial estimates, outperforming state-of-the-art methods. Our code, data, and model are availableat our project website at: muelea.github.io/buddi.",http://arxiv.org/abs/2306.09337v2,,"Lea M??ller (University Of California, Berkeley) | Vickie Ye (University Of California, Berkeley) | Georgios Pavlakos (University Of Texas At Austin) | Michael J. Black (University Of T??bingen) | Angjoo Kanazawa (UC Berkeley)",2023-06-15 17:59:20+00:00,,,,,,
Text-conditional Attribute Alignment across Latent Spaces for 3D Controllable Face Image Synthesis,,,,FeiFan Xu (None) | Rui Li (Shantou University) | Si Wu (South China University Of Technology) | Yong Xu (Peng Cheng Laboratory) | Hau San Wong (City University Of Hong Kong),,,,,,,
GARField: Group Anything with Radiance Fields,"Grouping is inherently ambiguous due to the multiple levels of granularity in which one can decompose a scene -- should the wheels of an excavator be considered separate or part of the whole? We present Group Anything with Radiance Fields (GARField), an approach for decomposing 3D scenes into a hierarchy of semantically meaningful groups from posed image inputs. To do this we embrace group ambiguity through physical scale: by optimizing a scale-conditioned 3D affinity feature field, a point in the world can belong to different groups of different sizes. We optimize this field from a set of 2D masks provided by Segment Anything (SAM) in a way that respects coarse-to-fine hierarchy, using scale to consistently fuse conflicting masks from different viewpoints. From this field we can derive a hierarchy of possible groupings via automatic tree construction or user interaction. We evaluate GARField on a variety of in-the-wild scenes and find it effectively extracts groups at many levels: clusters of objects, objects, and various subparts. GARField inherently represents multi-view consistent groupings and produces higher fidelity groups than the input SAM masks. GARField's hierarchical grouping could have exciting downstream applications such as 3D asset extraction or dynamic scene understanding. See the project website at https://www.garfield.studio/",http://arxiv.org/abs/2401.09419v1,,"Chung Min Kim (University Of California, Berkeley) | Mingxuan Wu (None) | Justin Kerr (University Of California Berkeley) | Ken Goldberg (University Of California Berkeley) | Matthew Tancik (Luma AI) | Angjoo Kanazawa (UC Berkeley)",2024-01-17 18:57:53+00:00,,,,,,
VRetouchEr: Learning Cross-frame Feature Interdependence with Imperfection Flow for Face Retouching in Videos,,,,Wen Xue (South China University Of Technology) | Le Jiang (South China University Of Technology) | Lianxin Xie (South China University Of Technology) | Si Wu (South China University Of Technology) | Yong Xu (Peng Cheng Laboratory) | Hau San Wong (City University Of Hong Kong),,,,,,,
Learning Degradation-unaware Representation with Prior-based Latent Transformations for Blind Face Restoration,,,,Lianxin Xie (South China University Of Technology) | Csbingbing Zheng (South China University Of Technology) | Wen Xue (South China University Of Technology) | Le Jiang (South China University Of Technology) | Cheng Liu (Shantou University) | Si Wu (South China University Of Technology) | Hau San Wong (City University Of Hong Kong),,,,,,,
NeRFiller: Completing Scenes via Generative 3D Inpainting,"We propose NeRFiller, an approach that completes missing portions of a 3D capture via generative 3D inpainting using off-the-shelf 2D visual generative models. Often parts of a captured 3D scene or object are missing due to mesh reconstruction failures or a lack of observations (e.g., contact regions, such as the bottom of objects, or hard-to-reach areas). We approach this challenging 3D inpainting problem by leveraging a 2D inpainting diffusion model. We identify a surprising behavior of these models, where they generate more 3D consistent inpaints when images form a 2$\times$2 grid, and show how to generalize this behavior to more than four images. We then present an iterative framework to distill these inpainted regions into a single consistent 3D scene. In contrast to related works, we focus on completing scenes rather than deleting foreground objects, and our approach does not require tight 2D object masks or text. We compare our approach to relevant baselines adapted to our setting on a variety of scenes, where NeRFiller creates the most 3D consistent and plausible scene completions. Our project page is at https://ethanweber.me/nerfiller.",http://arxiv.org/abs/2312.04560v1,,Ethan Weber (University Of California Berkeley) | Aleksander Holynski (UC Berkeley & Google Research) | Varun Jampani (Google Research) | Saurabh Saxena (None) | Noah Snavely (Google / Cornell) | Abhishek Kar (Google) | Angjoo Kanazawa (UC Berkeley),2023-12-07 18:59:41+00:00,,,,,,
ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion,"Given a single image of a 3D object, this paper proposes a novel method (named ConsistNet) that is able to generate multiple images of the same object, as if seen they are captured from different viewpoints, while the 3D (multi-view) consistencies among those multiple generated images are effectively exploited. Central to our method is a multi-view consistency block which enables information exchange across multiple single-view diffusion processes based on the underlying multi-view geometry principles. ConsistNet is an extension to the standard latent diffusion model, and consists of two sub-modules: (a) a view aggregation module that unprojects multi-view features into global 3D volumes and infer consistency, and (b) a ray aggregation module that samples and aggregate 3D consistent features back to each view to enforce consistency. Our approach departs from previous methods in multi-view image generation, in that it can be easily dropped-in pre-trained LDMs without requiring explicit pixel correspondences or depth prediction. Experiments show that our method effectively learns 3D consistency over a frozen Zero123 backbone and can generate 16 surrounding views of the object within 40 seconds on a single A100 GPU. Our code will be made available on https://github.com/JiayuYANG/ConsistNet",http://arxiv.org/abs/2310.10343v1,,Jiayu Yang (Australian National University) | Ziang Cheng (Australian National University) | Yunfei Duan (Tencent Game) | Pan Ji (Tencent XR Vision Labs) | Hongdong Li (Australian National University),2023-10-16 12:29:29+00:00,,,,,,
Weakly Supervised Monocular 3D Detection with a Single-View Image,"Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.",http://arxiv.org/abs/2402.19144v1,,Xueying Jiang (Nanyang Technological University) | Sheng Jin (Nanyang Technological University) | Lewei Lu (SenseTime) | Xiaoqin Zhang (Wenzhou University) | Shijian Lu (Nanyang Technological University),2024-02-29 13:26:47+00:00,,,,,,
StraightPCF: Straight Point Cloud Filtering,,,,"Dasith De Silva Edirimuni (Deakin University) | Xuequan Lu (La Trobe University) | Gang Li (Deakin University) | Lei Wei (Deakin University) | Antonio Robles-Kelly (Defence Science And Technology Group (DST), Deakin University) | Hongdong Li (Australian National University)",,,,,,,
Masked AutoDecoder is Effective Multi-Task Vision Generalist,"Inspired by the success of general-purpose models in NLP, recent studies attempt to unify different vision tasks in the same sequence format and employ autoregressive Transformers for sequence prediction. They apply uni-directional attention to capture sequential dependencies and generate task sequences recursively. However, such autoregressive Transformers may not fit vision tasks well, as vision task sequences usually lack the sequential dependencies typically observed in natural languages. In this work, we design Masked AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of two core designs. First, we develop a parallel decoding framework that introduces bi-directional attention to capture contextual dependencies comprehensively and decode vision task sequences in parallel. Second, we design a masked sequence modeling approach that learns rich task contexts by masking and reconstructing task sequences. In this way, MAD handles all the tasks by a single network branch and a simple cross-entropy loss with minimal task-specific designs. Extensive experiments demonstrate the great potential of MAD as a new paradigm for unifying various vision tasks. MAD achieves superior performance and inference efficiency compared to autoregressive counterparts while obtaining competitive accuracy with task-specific models. Code will be released.",http://arxiv.org/abs/2403.07692v2,,Han Qiu (Nanyang Technological University) | Jiaxing Huang (Nanyang Technological University) | Peng Gao (The Chinese University Of Hong Kong) | Lewei Lu (SenseTime) | Xiaoqin Zhang (Wenzhou University) | Shijian Lu (Nanyang Technological University),2024-03-12 14:36:52+00:00,,,,,,
Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining,"Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting novel categories from a distinct domain using only limited exemplars. In this paper, we undertake a comprehensive study of CD-FSS and uncover two crucial insights: (i) the necessity of a fine-tuning stage to effectively transfer the learned meta-knowledge across domains, and (ii) the overfitting risk during the na\""ive fine-tuning due to the scarcity of novel category examples. With these insights, we propose a novel cross-domain fine-tuning strategy that addresses the challenging CD-FSS tasks. We first design Bi-directional Few-shot Prediction (BFP), which establishes support-query correspondence in a bi-directional manner, crafting augmented supervision to reduce the overfitting risk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which is a recursive framework to capture the support-query correspondence iteratively, targeting maximal exploitation of supervisory signals from the sparse novel category samples. Extensive empirical evaluations show that our method significantly outperforms the state-of-the-arts (+7.8\%), which verifies that IFA tackles the cross-domain challenges and mitigates the overfitting simultaneously. The code is available at: https://github.com/niejiahao1998/IFA.",http://arxiv.org/abs/2401.08407v2,,Jiahao Nie (Nanyang Technological University) | Yun Xing (Nanyang Technological University) | Gongjie Zhang (Black Sesame Tech.) | Pei Yan (Huazhong University Of Science And Technology) | Aoran Xiao (Nanyang Technological University) | Yap-Peng Tan (Nanyang Technological University) | Alex C. Kot (Nanyang Technological University) | Shijian Lu (Nanyang Technological University),2024-01-16 14:45:41+00:00,,,,,,
View From Above: Orthogonal viewpoint aware Cross-view Localization,,,,"Shan Wang (ANU;CSIRO) | Chuong Nguyen (None) | Jiawei Liu (None) | Yanhao Zhang (University Of Technology Sydney) | Sundaram Muthu (, CSIRO) | Fahira Afzal Maken (CSIRO) | Kaihao Zhang (Australian National University) | Hongdong Li (Australian National University)",,,,,,,
Arbitrary Motion Style Transfer with Multi-condition Motion Latent Diffusion Model,,,,Wenfeng Song (Beijing Information Science And Technology University) | Xingliang Jin (Beijing Information Science And Information University ) | Shuai Li (Beijing University Of Aeronautics And Astronautics) | Chenglizhao Chen (China University Of Petroleum) | Aimin Hao (None) | Xia HOU (Beijing Information Science & Technology University) | Ning Li (Beijing Information Science And Technology University) | Hong Qin (Stony Brook University (SUNY At Stony Brook)),,,,,,,
HOIAnimator: Text-Prompt Human-Object Animations Generation with Perceptive Diffusion Models,,,,Wenfeng Song (Beijing Information Science And Technology University) | Xinyu Zhang (Beijing Information Science And Technology University) | Shuai Li (Beijing University Of Aeronautics And Astronautics) | Yang Gao (Beijing University Of Aeronautics And Astronautics) | Aimin Hao (None) | Xia HOU (Beijing Information Science & Technology University) | Chenglizhao Chen (China University Of Petroleum) | Ning Li (Beijing Information Science And Technology University) | Hong Qin (Stony Brook University (SUNY At Stony Brook)),,,,,,,
PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation,"This paper proposes a cross-modal distillation framework, PartDistill, which transfers 2D knowledge from vision-language models (VLMs) to facilitate 3D shape part segmentation. PartDistill addresses three major challenges in this task: the lack of 3D segmentation in invisible or undetected regions in the 2D projections, inaccurate and inconsistent 2D predictions by VLMs, and the lack of knowledge accumulation across different 3D shapes. PartDistill consists of a teacher network that uses a VLM to make 2D predictions and a student network that learns from the 2D predictions while extracting geometrical features from multiple 3D shapes to carry out 3D part segmentation. A bi-directional distillation, including forward and backward distillations, is carried out within the framework, where the former forward distills the 2D predictions to the student network, and the latter improves the quality of the 2D predictions, which subsequently enhances the final 3D part segmentation. Moreover, PartDistill can exploit generative models that facilitate effortless 3D shape creation for generating knowledge sources to be distilled. Through extensive experiments, PartDistill boosts the existing methods with substantial margins on widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher mIoU scores, respectively.",http://arxiv.org/abs/2312.04016v1,,Ardian Umam (National Yang Ming Chiao Tung University) | Cheng-Kun Yang (MediaTek) | Min-Hung Chen (NVIDIA) | Jen-Hui Chuang (None) | Yen-Yu Lin (National Yang Ming Chiao Tung University),2023-12-07 03:10:03+00:00,,,,,,
Mask Grounding for Referring Image Segmentation,"Referring Image Segmentation (RIS) is a challenging task that requires an algorithm to segment objects referred by free-form language expressions. Despite significant progress in recent years, most state-of-the-art (SOTA) methods still suffer from considerable language-image modality gap at the pixel and word level. These methods generally 1) rely on sentence-level language features for language-image alignment and 2) lack explicit training supervision for fine-grained visual grounding. Consequently, they exhibit weak object-level correspondence between visual and language features. Without well-grounded features, prior methods struggle to understand complex expressions that require strong reasoning over relationships among multiple objects, especially when dealing with rarely used or ambiguous clauses. To tackle this challenge, we introduce a novel Mask Grounding auxiliary task that significantly improves visual grounding within language features, by explicitly teaching the model to learn fine-grained correspondence between masked textual tokens and their matching visual objects. Mask Grounding can be directly used on prior RIS methods and consistently bring improvements. Furthermore, to holistically address the modality gap, we also design a cross-modal alignment loss and an accompanying alignment module. These additions work synergistically with Mask Grounding. With all these techniques, our comprehensive approach culminates in MagNet Mask-grounded Network), an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating our method's effectiveness in addressing current limitations of RIS algorithms. Our code and pre-trained weights will be released.",http://arxiv.org/abs/2312.12198v1,,Yong Xien Chng (None) | Henry Zheng (Tsinghua University) | Yizeng Han (Tsinghua University) | Xuchong QIU (Bosch) | Gao Huang (Tsinghua University),2023-12-19 14:34:36+00:00,,,,,,
Condition-Aware Neural Network for Controlled Image Generation,,,,Han Cai (Massachusetts Institute Of Technology) | Muyang Li (None) | Qinsheng Zhang (Georgia Institute Of Technology) | Ming-Yu Liu (NVIDIA) | Song Han (Massachusetts Institute Of Technology),,,,,,,
ID-Blau: Image Deblurring by Implicit Diffusion-based reBLurring AUgmentation,"Image deblurring aims to remove undesired blurs from an image captured in a dynamic scene. Much research has been dedicated to improving deblurring performance through model architectural designs. However, there is little work on data augmentation for image deblurring. Since continuous motion causes blurred artifacts during image exposure, we aspire to develop a groundbreaking blur augmentation method to generate diverse blurred images by simulating motion trajectories in a continuous space. This paper proposes Implicit Diffusion-based reBLurring AUgmentation (ID-Blau), utilizing a sharp image paired with a controllable blur condition map to produce a corresponding blurred image. We parameterize the blur patterns of a blurred image with their orientations and magnitudes as a pixel-wise blur condition map to simulate motion trajectories and implicitly represent them in a continuous space. By sampling diverse blur conditions, ID-Blau can generate various blurred images unseen in the training set. Experimental results demonstrate that ID-Blau can produce realistic blurred images for training and thus significantly improve performance for state-of-the-art deblurring models.",http://arxiv.org/abs/2312.10998v1,,"Jia-Hao Wu (National Yang Ming Chiao Tung University) | Fu-Jen Tsai (National Tsing Hua University) | Yan-Tsung Peng (National Chengchi University) | Charles Tsai (Qualcomm Inc, QualComm) | Chia-Wen Lin (National Tsing Hua University) | Yen-Yu Lin (National Yang Ming Chiao Tung University)",2023-12-18 07:47:43+00:00,,,,,,
GSVA: Generalized Segmentation via Multimodal Large Language Models,"Generalized Referring Expression Segmentation (GRES) extends the scope of classic RES to referring to multiple objects in one expression or identifying the empty targets absent in the image. GRES poses challenges in modeling the complex spatial relationships of the instances in the image and identifying non-existing referents. Recently, Multimodal Large Language Models (MLLMs) have shown tremendous progress in these complicated vision-language tasks. Connecting Large Language Models (LLMs) and vision models, MLLMs are proficient in understanding contexts with visual inputs. Among them, LISA, as a representative, adopts a special [SEG] token to prompt a segmentation mask decoder, e.g., SAM, to enable MLLMs in the RES task. However, existing solutions to of GRES remain unsatisfactory since current segmentation MLLMs cannot properly handle the cases where users might reference multiple subjects in a singular prompt or provide descriptions incongruent with any image target. In this paper, we propose Generalized Segmentation Vision Assistant (GSVA) to address this gap. Specifically, GSVA reuses the [SEG] token to prompt the segmentation model towards supporting multiple mask references simultaneously and innovatively learns to generate a [REJ] token to reject the null targets explicitly. Experiments validate GSVA's efficacy in resolving the GRES issue, marking a notable enhancement and setting a new record on the GRES benchmark gRefCOCO dataset. GSVA also proves effective across various classic referring expression segmentation and comprehension tasks.",http://arxiv.org/abs/2312.10103v1,,Zhuofan Xia (Tsinghua University) | Dongchen Han (Tsinghua University) | Yizeng Han (Tsinghua University) | Xuran Pan (Tsinghua University) | Shiji Song (Tsinghua University) | Gao Huang (Tsinghua University),2023-12-15 02:54:31+00:00,,,,,,
VILA: On Pre-training for Visual Language Models,"Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge.",http://arxiv.org/abs/2312.07533v3,,Ji Lin (Massachusetts Institute Of Technology) | Danny Yin (NVIDIA) | Wei Ping (NVIDIA) | Pavlo Molchanov (NVIDIA) | Mohammad Shoeybi (NVIDIA) | Song Han (Massachusetts Institute Of Technology),2023-12-12 18:58:18+00:00,,,,,,
Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation,,,,Ji-Jia Wu (National Taiwan University) | Andy Chang (National Yang Ming Chiao Tung University) | Chieh-Yu Chuang (National Yang Ming Chiao Tung University) | Chun-Pei Chen (National Yang Ming Chiao Tung University) | Yu-Lun Liu (National Yang Ming Chiao Tung University) | Min-Hung Chen (NVIDIA) | Hou-Ning Hu (MediaTek) | Yung-Yu Chuang (National Taiwan University) | Yen-Yu Lin (National Yang Ming Chiao Tung University),,,,,,,
Patch Diffusion: Parallel Inference for High-Resolution Diffusion Models,"Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, naively implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at https://github.com/mit-han-lab/distrifuser.",http://arxiv.org/abs/2402.19481v2,,Muyang Li (None) | Tianle Cai (Princeton University) | Jiaxin Cao (Lepton AI) | Qinsheng Zhang (Georgia Institute Of Technology) | Han Cai (Massachusetts Institute Of Technology) | Junjie Bai (Lepton AI) | Yangqing Jia (Lepton AI) | Kai Li (Princeton University) | Song Han (Massachusetts Institute Of Technology),2024-02-29 18:59:58+00:00,,,,,,
Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis,,,,"Zanlin Ni (Tsinghua University) | Yulin Wang (Tsinghua University) | Renping Zhou (, Tsinghua University) | Jiayi Guo (Tsinghua University) | Jinyi Hu (Tsinghua University) | Zhiyuan Liu (Tsinghua University) | Shiji Song (Tsinghua University) | Yuan Yao (Tsinghua University) | Gao Huang (Tsinghua University)",,,,,,,
WateRF: Robust Watermarks in Radiance Fields for Protection of Copyrights,,,,Youngdong Jang (Korea University) | Dong In Lee (Korea University) | MinHyuk Jang (Korea University) | Jong Wook Kim (Korea University) | Feng Yang (Google Research) | Sangpil Kim (Korea University),,,,,,,
Higher-order Relational Reasoning for Pedestrian Trajectory Prediction,,,,Sungjune Kim (Korea University) | Hyung-Gun Chi (Purdue University) | Hyerin Lim (Hyundai Motor Company) | Karthik Ramani (Purdue University) | Jinkyu Kim (Korea University) | Sangpil Kim (Korea University),,,,,,,
Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion,,,,Zixian Gao (University Of Electronic Science And Technology Of China) | Xun Jiang (University Of Electronic Science And Technology Of China) | Xing Xu (University Of Electronic Science And Technology Of China) | Fumin Shen (UESTC) | Yujie Li (Yangzhou University) | Heng Tao Shen (University Of Electronic Science And Technology Of China),,,,,,,
Ensemble Diversity Facilitates Adversarial Transferability,,,,Bowen Tang (University Of Electronic Science And Technology Of China) | Zheng Wang (University Of Electronic Science And Technology Of China) | Yi Bin (National University Of Singapore) | Qi Dou (The Chinese University Of Hong Kong) | Yang Yang (University Of Electronic Science And Technology Of China) | Heng Tao Shen (University Of Electronic Science And Technology Of China),,,,,,,
Edge-Aware 3D Instance Segmentation Network with Intelligent Semantic Prior,,,,Wonseok Roh (Korea University) | Hwanhee Jung (Korea University) | Giljoo Nam (Meta) | Jinseop Yeom (Korea University) | Hyunje Park (Korea University) | Sang Ho Yoon (KAIST) | Sangpil Kim (Korea University),,,,,,,
ProG: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval,,,,"Fang Kaipeng (None) | Jingkuan Song (University Of Electronic Science And Technology Of China,) | Lianli Gao (University Of Electronic Science And Technology Of China, Tsinghua University) | Pengpeng Zeng (University Of Electronic Science And Technology Of China) | Zhi-Qi Cheng (Carnegie Mellon University) | Xiyao LI (Kuaishou Technology) | Heng Tao Shen (University Of Electronic Science And Technology Of China)",,,,,,,
Efficient Meshflow and Optical Flow Estimation from Event Cameras,,,,Xinglong Luo (None) | Ao Luo (Megvii Technology) | Zhengning Wang (University Of Electronic Science And Technology Of China) | Chunyu Lin (Beijing Jiao Tong University) | Bing Zeng (None) | Shuaicheng Liu (None),,,,,,,
DMR: Decomposed Multi-Modality Representations for Frames and Events Fusion in Visual Reinforcement Learning,,,,Haoran Xu (None) | Peixi Peng (Peking University) | Guang Tan (Sun Yat-Sen University) | Yuan Li (Academy Of Military Sciences) | Xinhai Xu (Academy Of Military Sciences) | Yonghong Tian (Peking University),,,,,,,
FlowDiffuser: Advancing Optical Flow Estimation with Diffusion Models,,,,Ao Luo (Megvii Technology) | XIN LI (G42) | Fan Yang (AIQ) | Jiangyu Liu (Megvii Technology) | Haoqiang Fan (Megvii Technology) | Shuaicheng Liu (None),,,,,,,
Event-based Visible and Infrared Fusion via Multi-task Collaboration,,,,Mengyue Geng (Peking University) | Lin Zhu (Beijing Institute Of Technology) | Lizhi Wang (None) | Wei Zhang (Peng Cheng Laboratory) | Ruiqin Xiong (Peking University) | Yonghong Tian (Peking University),,,,,,,
Solving the Catastrophic Forgetting Problem in Generalized Category Discovery,,,,Xinzi Cao (Sun Yat-Sen University) | Xiawu Zheng (Xiamen University) | Guanhong Wang (Zhejiang University) | Weijiang Yu (SUN YAT-SEN UNIVERSITY) | Yunhang Shen (Tencent) | Ke Li (Tencent) | Yutong Lu (SUN YAT-SEN UNIVERSITY) | Yonghong Tian (Peking University),,,,,,,
Rectangling for Image Stitching with Diffusion Models,,,,"Tianhao Zhou (University Of Electronic Science And Technology Of China) | Li Haipeng (None) | Ziyi Wang (University Of Electronic Science And Technology Of China) | Ao Luo (Megvii Technology) | Chenlin Zhang (Moonshot AI, Ltd) | Jiajun Li (4Paradigm Technology) | Bing Zeng (None) | Shuaicheng Liu (None)",,,,,,,
Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?,"Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication.",http://arxiv.org/abs/2403.06092v1,,Hanxin Zhu (University Of Science And Technology Of China) | Tianyu He (None) | Xin Li (None) | Bingchen Li (University Of Science And Technology Of China) | Zhibo Chen (University Of Science And Technology Of China),2024-03-10 04:27:06+00:00,,,,,,
SeD: Semantic-Aware Discriminator for Image Super-Resolution,"Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.",http://arxiv.org/abs/2402.19387v1,,Bingchen Li (University Of Science And Technology Of China) | Xin Li (None) | Hanxin Zhu (University Of Science And Technology Of China) | YEYING JIN (National University Of Singapore) | Ruoyu Feng (University Of Science And Technology Of China) | Zhizheng Zhang (Microsoft Research) | Zhibo Chen (University Of Science And Technology Of China),2024-02-29 17:38:54+00:00,,,,,,
KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos,"Short-form UGC video platforms, like Kwai and TikTok, have been an emerging and irreplaceable mainstream media form, thriving on user-friendly engagement, and kaleidoscope creation, etc. However, the advancing content-generation modes, e.g., special effects, and sophisticated processing workflows, e.g., de-artifacts, have introduced significant challenges to recent UGC video quality assessment: (i) the ambiguous contents hinder the identification of quality-determined regions. (ii) the diverse and complicated hybrid distortions are hard to distinguish. To tackle the above challenges and assist in the development of short-form videos, we establish the first large-scale Kaleidoscope short Video database for Quality assessment, termed KVQ, which comprises 600 user-uploaded short videos and 3600 processed videos through the diverse practical processing workflows, including pre-processing, transcoding, and enhancement. Among them, the absolute quality score of each video and partial ranking score among indistinguishable samples are provided by a team of professional researchers specializing in image processing. Based on this database, we propose the first short-form video quality evaluator, i.e., KSVQE, which enables the quality evaluator to identify the quality-determined semantics with the content understanding of large vision language models (i.e., CLIP) and distinguish the distortions with the distortion understanding module. Experimental results have shown the effectiveness of KSVQE on our KVQ database and popular VQA databases.",http://arxiv.org/abs/2402.07220v2,,Yiting Lu (University Of Science And Technology Of China) | Xin Li (None) | Yajing Pei (None) | Kun Yuan (Kuaishou Technology) | Qizhi Xie (None) | Yunpeng Qu (None) | Ming Sun (Kuaishou Tech) | Chao Zhou (Peking University) | Zhibo Chen (University Of Science And Technology Of China),2024-02-11 14:37:54+00:00,,,,,,
A Specialized Dataset for Traffic Scene Perception,,,,"Peng-Tao Jiang (Vivo Mobile Communication (Hangzhou) Co., Ltd.) | Yuqi Yang (Nankai University) | Yang Cao (Hong Kong University Of Science And Technology) | Qibin Hou (Nankai University) | Ming-Ming Cheng (Nankai University, Tsinghua University) | Chunhua Shen (Zhejiang University)",,,,,,,
DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data,,,,"Chengxiang Fan (Zhejiang University) | Muzhi Zhu (Zhejiang University) | Hao Chen (Zhejiang University) | Yang Liu (Zhejiang University) | Weijia Wu (None) | Huaqi Zhang (Hangzhou VIVO Information Technology Co., Ltd) | Chunhua Shen (Zhejiang University)",,,,,,,
FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition,,,,Ganggui Ding (Zhejiang University) | Canyu Zhao (Zhejiang University) | Wen Wang (Zhejiang University) | Zhen Yang (Zhejiang University) | Zide Liu (Zhejiang University) | Hao Chen (Zhejiang University) | Chunhua Shen (Zhejiang University),,,,,,,
Building a Strong Pre-Training Baseline for Universal 3D Large-Scale Perception,,,,Haoming Chen (East China Normal Univeristy) | Zhizhong Zhang (East China Normal University) | Yanyun Qu (Xiamen University) | Ruixin Zhang (Tencent Youtu Lab) | Xin Tan (East China Normal University) | Yuan Xie (East China Normal University),,,,,,,
COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction,"The autonomous driving community has shown significant interest in 3D occupancy prediction, driven by its exceptional geometric perception and general object recognition capabilities. To achieve this, current works try to construct a Tri-Perspective View (TPV) or Occupancy (OCC) representation extending from the Bird-Eye-View perception. However, compressed views like TPV representation lose 3D geometry information while raw and sparse OCC representation requires heavy but reducant computational costs. To address the above limitations, we propose Compact Occupancy TRansformer (COTR), with a geometry-aware occupancy encoder and a semantic-aware group decoder to reconstruct a compact 3D OCC representation. The occupancy encoder first generates a compact geometrical OCC feature through efficient explicit-implicit view transformation. Then, the occupancy decoder further enhances the semantic discriminability of the compact OCC representation by a coarse-to-fine semantic grouping strategy. Empirical experiments show that there are evident performance gains across multiple baselines, e.g., COTR outperforms baselines with a relative improvement of 8%-15%, demonstrating the superiority of our method.",http://arxiv.org/abs/2312.01919v1,,"Qihang Ma (East China Normal Universitry) | Xin Tan (East China Normal University) | Yanyun Qu (Xiamen University) | Lizhuang Ma (Dept. Of Computer Sci. & Eng., Shanghai Jiao Tong University) | Zhizhong Zhang (East China Normal University) | Yuan Xie (East China Normal University)",2023-12-04 14:23:18+00:00,,,,,,
Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer,,,,Zhen Zhao (East China Normal University) | Jingqun Tang (Bytedance) | Chunhui Lin (Bytedance) | Binghong Wu (Bytedance) | Can Huang (Bytedance) | Hao Liu (Bytedance) | Xin Tan (East China Normal University) | Zhizhong Zhang (East China Normal University) | Yuan Xie (East China Normal University),,,,,,,
SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving,,,,"Yiming Xie (Shenzhen International Graduate School, Tsinghua University) | Henglu Wei (Tsinghua University) | Zhenyi Liu (Stanford University) | Xiaoyu Wang (Department Of Automation, Tsinghua University) | Xiangyang Ji (Tsinghua University)",,,,,,,
KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation,"In this paper, we present KP-RED, a unified KeyPoint-driven REtrieval and Deformation framework that takes object scans as input and jointly retrieves and deforms the most geometrically similar CAD models from a pre-processed database to tightly match the target. Unlike existing dense matching based methods that typically struggle with noisy partial scans, we propose to leverage category-consistent sparse keypoints to naturally handle both full and partial object scans. Specifically, we first employ a lightweight retrieval module to establish a keypoint-based embedding space, measuring the similarity among objects by dynamically aggregating deformation-aware local-global features around extracted keypoints. Objects that are close in the embedding space are considered similar in geometry. Then we introduce the neural cage-based deformation module that estimates the influence vector of each keypoint upon cage vertices inside its local support region to control the deformation of the retrieved shape. Extensive experiments on the synthetic dataset PartNet and the real-world dataset Scan2CAD demonstrate that KP-RED surpasses existing state-of-the-art approaches by a large margin. Codes and trained models will be released in https://github.com/lolrudy/KP-RED.",http://arxiv.org/abs/2403.10099v1,,"Ruida Zhang (Department Of Automation, Tsinghua University) | Chenyangguang Zhang (Tsinghua University) | Yan Di (Technische Universit??t M??nchen) | Fabian Manhardt (Google) | Xingyu Liu (Tsinghua University) | Federico Tombari (Google, TUM) | Xiangyang Ji (Tsinghua University)",2024-03-15 08:44:56+00:00,,,,,,
MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision,"Previous works concerning single-view hand-held object reconstruction typically rely on supervision from 3D ground-truth models, which are hard to collect in real world. In contrast, readily accessible hand-object videos offer a promising training data source, but they only give heavily occluded object observations. In this paper, we present a novel synthetic-to-real framework to exploit Multi-view Occlusion-aware supervision from hand-object videos for Hand-held Object reconstruction (MOHO) from a single image, tackling two predominant challenges in such setting: hand-induced occlusion and object's self-occlusion. First, in the synthetic pre-training stage, we render a large-scaled synthetic dataset SOMVideo with hand-object images and multi-view occlusion-free supervisions, adopted to address hand-induced occlusion in both 2D and 3D spaces. Second, in the real-world finetuning stage, MOHO leverages the amodal-mask-weighted geometric supervision to mitigate the unfaithful guidance caused by the hand-occluded supervising views in real world. Moreover, domain-consistent occlusion-aware features are amalgamated in MOHO to resist object's self-occlusion for inferring the complete object shape. Extensive experiments on HO3D and DexYCB datasets demonstrate 2D-supervised MOHO gains superior results against 3D-supervised methods by a large margin.",http://arxiv.org/abs/2310.11696v2,,"Chenyangguang Zhang (Tsinghua University) | Guanlong Jiao (Tsinghua University) | Yan Di (Technische Universit??t M??nchen) | Gu Wang (Tsinghua University) | Ziqin Huang (Tsinghua University) | Ruida Zhang (Department Of Automation, Tsinghua University) | Fabian Manhardt (Google) | Bowen Fu (Technische Universit??t M??nchen) | Federico Tombari (Google, TUM) | Xiangyang Ji (Tsinghua University)",2023-10-18 03:57:06+00:00,,,,,,
One-Shot Open Affordance Learning with Foundation Models,"We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances.",http://arxiv.org/abs/2311.17776v1,,Gen Li (University Of Edinburgh) | Deqing Sun (Google) | Laura Sevilla-Lara (University Of Edinburgh) | Varun Jampani (Google Research),2023-11-29 16:23:06+00:00,,,,,,
Probing the 3D Awareness of Visual Foundation Models,,,,Mohamed El Banani (University Of Michigan) | Amit Raj (Google ) | Kevis-Kokitsi Maninis (Google) | Abhishek Kar (Google) | Yuanzhen Li (Massachusetts Institute Of Technology) | Michael Rubinstein (Google) | Deqing Sun (Google) | Leonidas Guibas (Stanford University) | Justin Johnson (University Of Michigan) | Varun Jampani (Google Research),,,,,,,
SHINOBI: SHape and Illumination using Neural Object decomposition via BRDF optimization and Inverse rendering from unconstrained Image collections,,,,Andreas Engelhardt (University Of T??bingen) | Amit Raj (Google ) | Mark Boss (Stability AI) | Yunzhi Zhang (Stanford University) | Abhishek Kar (Google) | Yuanzhen Li (Massachusetts Institute Of Technology) | Ricardo Martin-Brualla (Google) | Jonathan T. Barron (Google) | Deqing Sun (Google) | Hendrik Lensch (University Of T??bingen) | Varun Jampani (Google Research),,,,,,,
MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World,"Human beings possess the capability to multiply a melange of multisensory cues while actively exploring and interacting with the 3D world. Current multi-modal large language models, however, passively absorb sensory data as inputs, lacking the capacity to actively interact with the objects in the 3D environment and dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition.",http://arxiv.org/abs/2401.08577v1,,"Yining Hong (None) | Zishuo Zheng (None) | Peihao Chen (South China University Of Technology) | Yian Wang (Department Of Computer Science, University Of Massachusetts At Amherst) | Junyan Li (Zhejiang University) | Chuang Gan (MIT-IBM Watson AI Lab)",2024-01-16 18:59:45+00:00,,,,,,
MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction,,,,"Xiaolu Liu (Zhejiang University) | Song Wang (Zhejiang University) | Wentong Li (College Of Computer Science And Technology, Zhejiang University) | Ruizi Yang (Zhejiang University) | Junbo Chen (UDEER AI PTE.LTD) | Jianke Zhu (Zhejiang University)",,,,,,,
Multi Attributes Interactions Matters for 3D Visual Grounding,,,,"Can Xu (Nanjing University Of Science And Technology) | Yuehui Han (Nanjing University Of Science And Technology) | Rui Xu (Nanjing University Of Science And Technology) | Le Hui (Nanjing University Of Science And Technology) | Jin Xie (Department Of Computer Science, Nanjing University Of Science And Technology) | Jian Yang (Nanjing University Of Science And Technology)",,,,,,,
Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance,,,,Junkai Fan (Nanjing University Of Science And Technology) | Jiangwei Weng (Nanjing University Of Science And Technology) | Kun Wang (Nanjing University Of Science And Technology) | Yijun Yang (None) | Jianjun Qian (Nanjing University Of Science And Techonology) | Jun Li (Nanjing University Of Science And Technology) | Jian Yang (Nanjing University Of Science And Technology),,,,,,,
RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation,,,,"Zeyuan Yang (, Tsinghua University) | LIU JIAGENG (None) | Peihao Chen (South China University Of Technology) | Anoop Cherian (None) | Tim Marks (None) | Jonathan Le Roux (Mitsubishi Electric Research Labs) | Chuang Gan (MIT-IBM Watson AI Lab)",,,,,,,
Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation,,,,"Song Wang (Zhejiang University) | Jiawei Yu (Zhejiang University) | Wentong Li (College Of Computer Science And Technology, Zhejiang University) | Wenyu Liu (Zhejiang University) | Xiaolu Liu (Zhejiang University) | Junbo Chen (UDEER AI PTE.LTD) | Jianke Zhu (Zhejiang University)",,,,,,,
Tri-Perspective View Decomposition for Geometry-Aware Depth Completion,,,,Zhiqiang Yan (Nanjing University Of Science And Technology) | Yuankai Lin (Huazhong University Of Science And Technology) | Kun Wang (Nanjing University Of Science And Technology) | Yupeng Zheng (Institute Of Automation???Chinese Academy Of Sciences) | Yufei Wang (Northwest Polytechnical University Xi&Amp;#x27;An) | Zhenyu Zhang (None) | Jun Li (Nanjing University Of Science And Technology) | Jian Yang (Nanjing University Of Science And Technology),,,,,,,
Osprey: Pixel Understanding with Visual Instruction Tuning,"Multimodal large language models (MLLMs) have recently achieved impressive general-purpose vision-language capabilities through visual instruction tuning. However, current MLLMs primarily focus on image-level or box-level understanding, falling short in achieving fine-grained vision-language alignment at pixel level. Besides, the lack of mask-based instruction data limits their advancements. In this paper, we propose Osprey, a mask-text instruction tuning approach, to extend MLLMs by incorporating fine-grained mask regions into language instruction, aiming at achieving pixel-wise visual understanding. To achieve this goal, we first meticulously curate a mask-based region-text dataset with 724K samples, and then design a vision-language model by injecting pixel-level representation into LLM. Specifically, Osprey adopts a convolutional CLIP backbone as the vision encoder and employs a mask-aware visual extractor to extract precise visual mask features from high resolution input. Experimental results demonstrate Osprey's superiority in various region understanding tasks, showcasing its new capability for pixel-level instruction tuning. In particular, Osprey can be integrated with Segment Anything Model (SAM) seamlessly to obtain multi-granularity semantics. The source code, dataset and demo can be found at https://github.com/CircleRadon/Osprey.",http://arxiv.org/abs/2312.10032v3,,"Yuqian Yuan (Zhejiang University) | Wentong Li (College Of Computer Science And Technology, Zhejiang University) | Jian Liu (AntGroup) | Dongqi Tang (Ant Group) | Xinjie Luo (Zhejiang University) | Chi Qin (Microsoft) | Lei Zhang (The Hong Kong Polytechnic University) | Jianke Zhu (Zhejiang University)",2023-12-15 18:58:11+00:00,,,,,,
SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge,,,,"Andong Wang (University Of Hong Kong) | Bo Wu (MIT-IBM Watson AI Lab) | Sunli Chen (Tsinghua University) | Zhenfang Chen (MIT-IBM Watson AI Lab) | Haotian Guan (The University Of Hong Kong) | Wei-Ning Lee (University Of Hong Kong) | Li Erran Li (AWS AI, Amazon) | Chuang Gan (MIT-IBM Watson AI Lab)",,,,,,,
CaKDP: Category-aware Knowledge Distillation and Pruning Framework for Lightweight 3D Object Detection,,,,Haonan Zhang (Xi'an Jiao Tong University) | Longjun Liu (Xi'an Jiao Tong University) | Yuqi Huang (Xi'an Jiao Tong University) | YangZhao (Xi'an Jiao Tong University) | Xinyu Lei (Xi'an Jiao Tong University) | Bihan Wen (Nanyang Technological University),,,,,,,
Progressive Divide-and-Conquer via Subsampling Decomposition for Accelerated MRI,"Deep unfolding networks (DUN) have emerged as a popular iterative framework for accelerated magnetic resonance imaging (MRI) reconstruction. However, conventional DUN aims to reconstruct all the missing information within the entire null space in each iteration. Thus it could be challenging when dealing with highly ill-posed degradation, usually leading to unsatisfactory reconstruction. In this work, we propose a Progressive Divide-And-Conquer (PDAC) strategy, aiming to break down the subsampling process in the actual severe degradation and thus perform reconstruction sequentially. Starting from decomposing the original maximum-a-posteriori problem of accelerated MRI, we present a rigorous derivation of the proposed PDAC framework, which could be further unfolded into an end-to-end trainable network. Specifically, each iterative stage in PDAC focuses on recovering a distinct moderate degradation according to the decomposition. Furthermore, as part of the PDAC iteration, such decomposition is adaptively learned as an auxiliary task through a degradation predictor which provides an estimation of the decomposed sampling mask. Following this prediction, the sampling mask is further integrated via a severity conditioning module to ensure awareness of the degradation severity at each stage. Extensive experiments demonstrate that our proposed method achieves superior performance on the publicly available fastMRI and Stanford2D FSE datasets in both multi-coil and single-coil settings.",http://arxiv.org/abs/2403.10064v1,,"Chong Wang (Nanyang Technological University) | Lanqing Guo (Nanyang Technological University) | Yufei Wang (Nanyang Technological University) | Hao Cheng (Nanyang Technological University) | Yi Yu (Nanyang Technological University, Singapore) | Bihan Wen (Nanyang Technological University)",2024-03-15 07:14:01+00:00,,,,,,
SinSR: Diffusion-Based Image Super-Resolution in a Single Step,"While super-resolution (SR) methods based on diffusion models exhibit promising results, their practical application is hindered by the substantial number of required inference steps. Recent methods utilize degraded images in the initial state, thereby shortening the Markov chain. Nevertheless, these solutions either rely on a precise formulation of the degradation process or still necessitate a relatively lengthy generation path (e.g., 15 iterations). To enhance inference speed, we propose a simple yet effective method for achieving single-step SR generation, named SinSR. Specifically, we first derive a deterministic sampling process from the most recent state-of-the-art (SOTA) method for accelerating diffusion-based SR. This allows the mapping between the input random noise and the generated high-resolution image to be obtained in a reduced and acceptable number of inference steps during training. We show that this deterministic mapping can be distilled into a student model that performs SR within only one inference step. Additionally, we propose a novel consistency-preserving loss to simultaneously leverage the ground-truth image during the distillation process, ensuring that the performance of the student model is not solely bound by the feature manifold of the teacher model, resulting in further performance improvement. Extensive experiments conducted on synthetic and real-world datasets demonstrate that the proposed method can achieve comparable or even superior performance compared to both previous SOTA methods and the teacher model, in just one sampling step, resulting in a remarkable up to x10 speedup for inference. Our code will be released at https://github.com/wyf0912/SinSR",http://arxiv.org/abs/2311.14760v1,,Yufei Wang (Nanyang Technological University) | Wenhan Yang (Peng Cheng Lab) | Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) | Yaohui Wang (Shanghai AI Laboratory) | Lanqing Guo (Nanyang Technological University) | Lap-Pui Chau (The Hong Kong Polytechnic University) | Ziwei Liu (Nanyang Technological University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Alex C. Kot (Nanyang Technological University) | Bihan Wen (Nanyang Technological University),2023-11-23 16:21:29+00:00,,,,,,
Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation,,,,Ivan Skorokhodov (KAUST) | Willi Menapace (University Of Trento) | Aliaksandr Siarohin (Snap) | Sergey Tulyakov (Snap),,,,,,,
Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers,"The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.",http://arxiv.org/abs/2402.19479v1,,"Tsai-Shien Chen (University Of California, Merced) | Aliaksandr Siarohin (Snap) | Willi Menapace (University Of Trento) | Ekaterina Deyneka (Snap) | Hsiang-Wei Chao (Snap) | Byung Jeon (Snap) | Yuwei Fang (Snap) | Hsin-Ying Lee (Snap) | Jian Ren (Snap) | Ming-Hsuan Yang (University Of California At Merced) | Sergey Tulyakov (Snap)",2024-02-29 18:59:50+00:00,,,,,,
Mind the Time: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis,,,,"Willi Menapace (University Of Trento) | Aliaksandr Siarohin (Snap) | Ivan Skorokhodov (KAUST) | Ekaterina Deyneka (Snap) | Tsai-Shien Chen (University Of California, Merced) | Anil Kag (Snap) | Yuwei Fang (Snap) | Aleksei Stoliar (None) | Elisa Ricci (University Of Trento) | Jian Ren (Snap) | Sergey Tulyakov (Snap)",,,,,,,
Revisiting Spatial-Frequency Information Integration from a Hierarchical Perspective for Panchromatic and Multi-Spectral Image Fusion,,,,"Jiangtong Tan (None) | Jie Huang (University Of Science And Technology Of China) | Kaiwen Zheng (University Of Science And Technology Of China) | Man Zhou (University Of Science And Technology Of China) | Keyu Yan (University Of Science And Technology Of China) | Danfeng Hong (Chinese Academy Of Sciences, Aerospace Information Research Institute) | Feng Zhao (University Of Science And Technology Of China)",,,,,,,
Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion,,,,Kaiwen Zheng (University Of Science And Technology Of China) | Man Zhou (University Of Science And Technology Of China) | Jie Huang (University Of Science And Technology Of China) | Junming Hou (Southeast University) | Haoying Li (Zhejiang University) | Yuan Xu (Nanyang Technological University) | Feng Zhao (University Of Science And Technology Of China),,,,,,,
Empowering Resampling Operation for Ultra-High-Definition Image Enhancement with Model-Aware Guidance,,,,Yu (None) | Jie Huang (University Of Science And Technology Of China) | Li (None) | Naishan Zheng (University Of Science And Technology Of China) | Qi Zhu (University Of Science And Technology Of China) | Man Zhou (University Of Science And Technology Of China) | Feng Zhao (University Of Science And Technology Of China),,,,,,,
Batch Normalization Alleviates the Spectral Bias in Coordinate Networks,,,,Zhicheng Cai (Nanjing University) | Hao Zhu (Nanjing University) | Qiu Shen (Nanjing University) | Xinran Wang (Nanjing University) | Xun Cao (Nanjing University),,,,,,,
FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions,"Implicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes, is causing a revolution in the field of signal processing. However, current INR techniques suffer from a restricted capability to tune their supported frequency set, resulting in imperfect performance when representing complex signals with multiple frequencies. We have identified that this frequency-related problem can be greatly alleviated by introducing variable-periodic activation functions, for which we propose FINER. By initializing the bias of the neural network within different ranges, sub-functions with various frequencies in the variable-periodic function are selected for activation. Consequently, the supported frequency set of FINER can be flexibly tuned, leading to improved performance in signal representation. We demonstrate the capabilities of FINER in the contexts of 2D image fitting, 3D signed distance field representation, and 5D neural radiance fields optimization, and we show that it outperforms existing INRs.",http://arxiv.org/abs/2312.02434v1,,Zhen Liu (Nanjing University) | Hao Zhu (Nanjing University) | Qi Zhang (Tencent AI Lab) | Jingde Fu (Nanjing University) | Weibing Deng (Nanjing University) | Zhan Ma (Nanjing University) | Yanwen Guo (Nanjing University) | Xun Cao (Nanjing University),2023-12-05 02:23:41+00:00,,,,,,
MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors,,,,He Zhang (None) | Shenghao Ren (Nanjing University) | Haolei Yuan (None) | Jianhui Zhao (Beijing University Of Aeronautics And Astronautics) | Fan Li (Beijing University Of Aeronautics And Astronautics) | Shuangpeng Sun (Tsinghua University) | Zhenghao Liang (Tsinghua University) | Tao Yu (Tsinghua University) | Qiu Shen (Nanjing University) | Xun Cao (Nanjing University),,,,,,,
Riemannian Multiclass Logistics Regression for SPD Neural Networks,"Deep neural networks for learning symmetric positive definite (SPD) matrices are gaining increasing attention in machine learning. Despite the significant progress, most existing SPD networks use traditional Euclidean classifiers on approximated spaces rather than intrinsic classifiers that accurately capture the geometry of SPD manifolds. Inspired by the success of hyperbolic neural networks (HNNs), we propose Riemannian multiclass logistics regression (RMLR) for SPD networks. We introduce a general unified framework for a family of Riemannian metrics on SPD manifolds and showcase the specific $\orth{n}$-invariant Log-Euclidean Metrics for SPD networks. Moreover, we encompass the most popular classifier in existing SPD networks as a special case of our framework. Extensive experiments on popular SPD learning benchmarks demonstrate the superiority of our classifiers.",http://arxiv.org/abs/2305.11288v1,,Ziheng Chen (University Of Trento) | Yue Song (University Of Trento) | Gaowen Liu (None) | Ramana Kompella (Cisco) | Xiaojun Wu (Jiangnan University) | Nicu Sebe (University Of Trento),2023-05-18 20:12:22+00:00,,,,,,
Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation,"Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and estimation accuracy compared to the original VPT models. For instance, applying to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop, respectively. Our source code will be open-sourced.",http://arxiv.org/abs/2311.12028v1,,Wenhao Li (Peking University) | Mengyuan Liu (SUN YAT-SEN UNIVERSITY) | Hong Liu (Peking University) | Pichao Wang (Amazon) | Jialun Cai (Peking University) | Nicu Sebe (University Of Trento),2023-11-20 18:59:51+00:00,,,,,,
OpenBias: Open-set Bias Detection in Generative Models,,,,"Moreno D'Inc?? (University Of Trento) | Elia Peruzzo (University Of Trento) | Massimiliano Mancini (University Of Trento) | Dejia Xu (University Of Texas At Austin) | Vidit Goel (Snap) | Xingqian Xu (University Of Illinois, Urbana Champaign) | Zhangyang Wang (University Of Texas At Austin) | Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR) | Nicu Sebe (University Of Trento)",,,,,,,
ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models,"Generating novel views of an object from a single image is a challenging task. It requires an understanding of the underlying 3D structure of the object from an image and rendering high-quality, spatially consistent new views. While recent methods for view synthesis based on diffusion have shown great progress, achieving consistency among various view estimates and at the same time abiding by the desired camera pose remains a critical problem yet to be solved. In this work, we demonstrate a strikingly simple method, where we utilize a pre-trained video diffusion model to solve this problem. Our key idea is that synthesizing a novel view could be reformulated as synthesizing a video of a camera going around the object of interest -- a scanning video -- which then allows us to leverage the powerful priors that a video diffusion model would have learned. Thus, to perform novel-view synthesis, we create a smooth camera trajectory to the target view that we wish to render, and denoise using both a view-conditioned diffusion model and a video diffusion model. By doing so, we obtain a highly consistent novel view synthesis, outperforming the state of the art.",http://arxiv.org/abs/2312.01305v1,,Jeong-Gi Kwak (Korea University) | Erqun Dong (University Of British Columbia) | Yuhe Jin (University Of British Columbia) | Hanseok Ko (Korea University) | Shweta Mahajan (University Of British Columbia) | Kwang Moo Yi (University Of British Columbia),2023-12-03 06:50:15+00:00,,,,,,
Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal,"Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.",http://arxiv.org/abs/2403.07684v1,,Yijun Yang (None) | Hongtao Wu (None) | Angelica I. Aviles-Rivero (University Of Cambridge) | Yulun Zhang (ETH Zurich) | Jing Qin (Hong Kong Polytechnic University) | Lei Zhu (Hong Kong University Of Science And Technology (Guangzhou) & HKUST),2024-03-12 14:21:30+00:00,,,,,,
Accelerating Neural Field Training via Soft Mining,"We present an approach to accelerate Neural Field training by efficiently selecting sampling locations. While Neural Fields have recently become popular, it is often trained by uniformly sampling the training domain, or through handcrafted heuristics. We show that improved convergence and final training quality can be achieved by a soft mining technique based on importance sampling: rather than either considering or ignoring a pixel completely, we weigh the corresponding loss by a scalar. To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed. The code and related resources for this study are publicly available at https://ubc-vision.github.io/nf-soft-mining/.",http://arxiv.org/abs/2312.00075v1,,"Shakiba Kheradmand (None) | Daniel Rebain (None) | Gopal Sharma (None) | Hossam Isack (Google) | Abhishek Kar (Google) | Andrea Tagliasacchi (Simon Fraser University, Google Brain) | Kwang Moo Yi (University Of British Columbia)",2023-11-29 23:48:46+00:00,,,,,,
Learning Diffusion Texture Priors for Image Restoration,,,,"Tian Ye (Hong Kong University Of Science And Technology, Guangzhou Campus) | Sixiang Chen (Hong Kong University Of Science And Technology (GZ)) | Wenhao Chai (University Of Washington) | Zhaohu Xing (Hong Kong University Of Science And Technology) | Jing Qin (Hong Kong Polytechnic University) | Ge Lin (Hong Kong University Of Science And Technology (Guangzhou)) | Lei Zhu (Hong Kong University Of Science And Technology (Guangzhou) & HKUST)",,,,,,,
Unsupervised Keypoints from Pretrained Diffusion Models,"Unsupervised learning of keypoints and landmarks has seen significant progress with the help of modern neural network architectures, but performance is yet to match the supervised counterpart, making their practicability questionable. We leverage the emergent knowledge within text-to-image diffusion models, towards more robust unsupervised keypoints. Our core idea is to find text embeddings that would cause the generative model to consistently attend to compact regions in images (i.e. keypoints). To do so, we simply optimize the text embedding such that the cross-attention maps within the denoising network are localized as Gaussians with small standard deviations. We validate our performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD, DeepFashion, and Human3.6m datasets. We achieve significantly improved accuracy, sometimes even outperforming supervised ones, particularly for data that is non-aligned and less curated. Our code is publicly available and can be found through our project page: https://ubc-vision.github.io/StableKeypoints/",http://arxiv.org/abs/2312.00065v2,,"Eric Hedlin (University Of British Columbia) | Gopal Sharma (None) | Shweta Mahajan (University Of British Columbia) | Xingzhe He (None) | Hossam Isack (Google) | Abhishek Kar (Google) | Helge Rhodin (UBC) | Andrea Tagliasacchi (Simon Fraser University, Google Brain) | Kwang Moo Yi (University Of British Columbia)",2023-11-29 19:43:38+00:00,,,,,,
Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning,,,,Haoyu Chen (Hong Kong University Of Science And Technology (Guangzhou)) | Wenbo Li (Huawei Technologies Ltd.) | Jinjin Gu (University Of Sydney) | Jingjing Ren (The Hong Kong University Of Science And Technology (Guangzhou)) | Haoze Sun (Tsinghua University) | Xueyi Zou (Huawei Technologies Ltd.) | Youliang Yan (Huawei Technologies Ltd.) | Zhensong Zhang (Huawei Noah's Ark Lab) | Lei Zhu (Hong Kong University Of Science And Technology (Guangzhou) & HKUST),,,,,,,
MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints,,,,Pengfei Xie (Shanghai Jiao Tong University) | Wenqiang Xu (Shanghai Jiao Tong University) | Tutian Tang (Shanghai Jiao Tong University) | Zhenjun Yu (Shanghai JiaoTong University) | Cewu Lu (Shanghai Jiao Tong University),,,,,,,
OakInk2: A Dataset of Embodied Hands-Object Manipulation in Long-Horizon Complex Task Completion,,,,Xinyu Zhan (Shanghai Jiao Tong University) | Lixin Yang (Shanghai Jiao Tong University) | Yifei Zhao (Shanghai Jiao Tong University) | Kangrui Mao (Shanghai Jiao Tong University) | Hanlin Xu (Shanghai Jiao Tong University) | Zenan Lin (South China University Of Technology) | Kailin Li (Shanghai Jiao Tong University) | Cewu Lu (Shanghai Jiao Tong University),,,,,,,
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding,"As a vital step toward the intelligent agent, Action understanding matters for intelligent agents and has attracted long-term attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like ""Isolated Islands"" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a structured action semantic space in view of verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging ``isolated islands'' into a ""Pangea"". Accordingly, we propose a novel model mapping from the physical space to semantic space to fully use Pangea. In extensive experiments, our new system shows significant superiority, especially in transfer learning. Code and data will be made publicly available.",http://arxiv.org/abs/2304.00553v3,,Yonglu Li (Shanghai Jiao Tong University) | Xiaoqian Wu (None) | Xinpeng Liu (Shanghai Jiao Tong University) | Zehao Wang (Shanghai Jiao Tong University) | Yiming Dou (University Of Michigan - Ann Arbor) | Yikun Ji (Shanghai Jiao Tong University) | Junyi Zhang (Shanghai Jiao Tong University) | Yixing Li (Shanghai Jiao Tong University) | Xudong LU (The Chinese University Of Hong Kong) | Jingru Tan (Central South University) | Cewu Lu (Shanghai Jiao Tong University),2023-04-02 15:04:43+00:00,,,,,,
RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation,"Real-time multi-person pose estimation presents significant challenges in balancing speed and precision. While two-stage top-down methods slow down as the number of people in the image increases, existing one-stage methods often fail to simultaneously deliver high accuracy and real-time performance. This paper introduces RTMO, a one-stage pose estimation framework that seamlessly integrates coordinate classification by representing keypoints using dual 1-D heatmaps within the YOLO architecture, achieving accuracy comparable to top-down methods while maintaining high speed. We propose a dynamic coordinate classifier and a tailored loss function for heatmap learning, specifically designed to address the incompatibilities between coordinate classification and dense prediction models. RTMO outperforms state-of-the-art one-stage pose estimators, achieving 1.1% higher AP on COCO while operating about 9 times faster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP on COCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and accuracy. The code and models are available at https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo.",http://arxiv.org/abs/2312.07526v1,,"Peng Lu (SIGS, Tsinghua University) | Tao Jiang (Shanghai AI Laboratory) | Yining Li (Shanghai AI Laboratory) | Xiangtai Li (Nanyang Technological University) | Kai Chen (Shanghai AI Laboratory) | Wenming Yang (Tsinghua University,)",2023-12-12 18:55:29+00:00,,,,,,
Bilateral Event Mining and Complementary for Event Stream Super-Resolution,,,,"Zhilin Huang (Tsinghua University) | Quanmin Liang (Sun Yat-Sen University) | Yijie Yu (Tsinghua University) | Chujun Qin (China Southern Power Grid ) | Xiawu Zheng (Xiamen University) | Kai Huang (SUN YAT-SEN UNIVERSITY,) | Zikun Zhou (Peng Cheng Laboratory) | Wenming Yang (Tsinghua University,)",,,,,,,
VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction,"Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.",http://arxiv.org/abs/2402.17427v1,,"Jiaqi Lin (Tsinghua University) | Zhihao Li (Huawei Noah's Ark Lab) | Xiao Tang (Huawei Technologies Ltd.) | Jianzhuang Liu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Shiyong Liu (Huawei Noah's Ark Lab) | Jiayue Liu (Tsinghua University) | Yangdi Lu (Huawei Technologies Ltd.) | Xiaofei Wu (Huawei Technologies Ltd.) | Songcen Xu (Huawei Noah's Ark Lab) | Youliang Yan (Huawei Technologies Ltd.) | Wenming Yang (Tsinghua University,)",2024-02-27 11:40:50+00:00,,,,,,
Real-Time Exposure Correction via Collaborative Transformations and Adaptive Sampling,,,,Ziwen Li (Huazhong University Of Science And Technology) | Feng Zhang (Huazhong University Of Science And Technology) | Meng Cao (Mohamed Bin Zayed University Of Artificial Intelligence) | Jinpu Zhang (Huazhong University Of Science And Technology) | Yuanjie Shao (Huazhong University Of Science And Technology) | Yuehuan Wang (Huazhong University Of Science And Technology) | Nong Sang (Huazhong University Of Science And Technology),,,,,,,
Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos,,,,Chen Liu (University Of Technology Sydney) | Peike Li (Futureverse AI) | Qingtao Yu (Australian National University) | Hongwei Sheng (University Of Queensland) | Dadong Wang (CSIRO) | Lincheng Li (None) | Xin Yu (University Of Queensland),,,,,,,
MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer,"Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.",http://arxiv.org/abs/2403.02991v1,,Jianjian Cao (Fudan University) | Peng Ye (Fudan University) | Shengze Li (Fudan University) | Chong Yu (Fudan University NVIDIA Corporation) | Yansong Tang (Tsinghua University) | Jiwen Lu (Tsinghua University) | Tao Chen (Fudan University),2024-03-05 14:13:50+00:00,,,,,,
Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression,,,,Hancheng Ye (Fudan University) | Chong Yu (Fudan University NVIDIA Corporation) | Peng Ye (Fudan University) | Renqiu Xia (Shanghai Jiao Tong University) | Bo Zhang (Shanghai AI Laboratory) | Yansong Tang (Tsinghua University) | Jiwen Lu (Tsinghua University) | Tao Chen (Fudan University),,,,,,,
Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation,,,,"Zhiwu Qing (Huazhong University Of Science And Technology, Tsinghua University) | Shiwei Zhang (Alibaba Group) | Jiayu Wang (None) | Xiang Wang (Huazhong University Of Science And Technology) | Yujie Wei (Fudan University) | Yingya Zhang (Alibaba Group) | Changxin Gao (Huazhong University Of Science And Technology) | Nong Sang (Huazhong University Of Science And Technology)",,,,,,,
Text-Guided 3D Face Synthesis - From Generation to Editing,,,,"Yunjie Wu (NetEase,) | Yapeng Meng (Tsinghua University) | Zhipeng Hu (Leihuo Game, NetEase) | Lincheng Li (None) | Haoqian Wu (NetEase Fuxi AI Lab) | Kun Zhou (Zhejiang University) | Weiwei Xu (Zhejiang University) | Xin Yu (University Of Queensland)",,,,,,,
EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Priors,"While the image diffusion model has made significant strides in text-driven 3D content creation, it often falls short in accurately capturing the intended meaning of the text prompt, particularly with respect to direction information. This shortcoming gives rise to the Janus problem, where multi-faced 3D models are produced with the guidance of such diffusion models. In this paper, we present a robust pipeline for generating high-fidelity 3D content with orthogonal-view image guidance. Specifically, we introduce a novel 2D diffusion model that generates an image consisting of four orthogonal-view sub-images for the given text prompt. The 3D content is then created with this diffusion model, which enhances 3D consistency and provides strong structured semantic priors. This addresses the infamous Janus problem and significantly promotes generation efficiency. Additionally, we employ a progressive 3D synthesis strategy that results in substantial improvement in the quality of the created 3D contents. Both quantitative and qualitative evaluations show that our method demonstrates a significant improvement over previous text-to-3D techniques.",http://arxiv.org/abs/2308.13223v1,,"Zhipeng Hu (Leihuo Game, NetEase) | Minda Zhao (NetEase Fuxi AI Lab) | Chaoyi Zhao (Fuxi AI Lab, NetEase) | Xinyue Liang (Nanjing University) | Lincheng Li (None) | Zeng Zhao (Fuxi AI Lab,NetEase,) | Changjie Fan (Netease, Fuxi AI Lab) | Xiaowei Zhou (None) | Xin Yu (University Of Queensland)",2023-08-25 07:39:26+00:00,,,,,,
"LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning","Recent advances in Large Multimodal Models (LMM) have made it possible for various applications in human-machine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud 3D representations of the 3D scene. Existing works seek help from multi-view images, and project 2D features to 3D space as 3D scene representations. This, however, leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and respond to both textual-instructions and visual-prompts. This help LMMs better comprehend human interactions and further help to remove the ambiguities in cluttered 3D scenes. Experiments show that LL3DA achieves remarkable results, and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.",http://arxiv.org/abs/2311.18651v1,,"Sijin Chen (Fudan University) | Xin Chen (University Of Chinese Academy Of Sciences, ShanghaiTech University) | Chi Zhang (Tencent ) | Mingsheng Li (Fudan University) | Gang Yu (Tencent) | Hao Fei (National University Of Singapore) | Hongyuan Zhu (Institute For Infocomm Research) | Jiayuan Fan (Fudan University) | Tao Chen (Fudan University)",2023-11-30 16:00:23+00:00,,,,,,
A Recipe for Scaling up Text-to-Video Generation with Text-free Videos,"Diffusion-based text-to-video generation has witnessed impressive progress in the past year yet still falls behind text-to-image generation. One of the key reasons is the limited scale of publicly available data (e.g., 10M video-text pairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high cost of video captioning. Instead, it could be far easier to collect unlabeled clips from video platforms like YouTube. Motivated by this, we come up with a novel text-to-video generation framework, termed TF-T2V, which can directly learn with text-free videos. The rationale behind is to separate the process of text decoding from that of temporal modeling. To this end, we employ a content branch and a motion branch, which are jointly optimized with weights shared. Following such a pipeline, we study the effect of doubling the scale of training set (i.e., video-only WebVid10M) with some randomly collected text-free videos and are encouraged to observe the performance improvement (FID from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of our approach. We also find that our model could enjoy sustainable performance gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some text labels for training. Finally, we validate the effectiveness and generalizability of our ideology on both native text-to-video generation and compositional video synthesis paradigms. Code and models will be publicly available at https://tf-t2v.github.io/.",http://arxiv.org/abs/2312.15770v1,,"Xiang Wang (Huazhong University Of Science And Technology) | Shiwei Zhang (Alibaba Group) | Hangjie Yuan (Nanyang Technological University) | Zhiwu Qing (Huazhong University Of Science And Technology, Tsinghua University) | Biao Gong (Alibaba Group) | Yingya Zhang (Alibaba Group) | Yujun Shen (The Chinese University Of Hong Kong) | Changxin Gao (Huazhong University Of Science And Technology) | Nong Sang (Huazhong University Of Science And Technology)",2023-12-25 16:37:39+00:00,,,,,,
Towards Robust Emotion Recognition in Context Debiasing,"Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes, resulting in bias mitigation and robust prediction. As a model-agnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.",http://arxiv.org/abs/2403.05963v1,,Dingkang Yang (Fudan University) | Kun Yang (Fudan University) | Mingcheng Li (Fudan University) | Shunli Wang (Fudan University) | Shuaibing Wang (Fudan University) | Lihua Zhang (Fudan University),2024-03-09 17:05:43+00:00,,,,,,
Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models,,,,Zijin Yang (None) | Kai Zeng (University Of Science And Technology Of China) | Kejiang Chen (University Of Science And Technology Of China) | Han Fang (National University Of Singapore) | Weiming Zhang (University Of Science And Technology Of China) | Nenghai Yu (University Of Science And Technology Of China),,,,,,,
OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation,"Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial over-trust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is available at: https://github.com/shikiw/OPERA.",http://arxiv.org/abs/2311.17911v3,,Qidong Huang (University Of Science And Technology Of China) | Xiaoyi Dong (Microsoft) | Pan Zhang (Shanghai Artificial Intelligence Laboratory) | Bin Wang (Shanghai AI Laboratory) | Conghui He (None) | Jiaqi Wang (Shanghai AI Laboratory) | Dahua Lin (The Chinese University Of Hong Kong) | Weiming Zhang (University Of Science And Technology Of China) | Nenghai Yu (University Of Science And Technology Of China),2023-11-29 18:57:07+00:00,,,,,,
CPR-Coach: Recognizing Composite Error Actions based on Single-class Training,"The fine-grained medical action analysis task has received considerable attention from pattern recognition communities recently, but it faces the problems of data and algorithm shortage. Cardiopulmonary Resuscitation (CPR) is an essential skill in emergency treatment. Currently, the assessment of CPR skills mainly depends on dummies and trainers, leading to high training costs and low efficiency. For the first time, this paper constructs a vision-based system to complete error action recognition and skill assessment in CPR. Specifically, we define 13 types of single-error actions and 74 types of composite error actions during external cardiac compression and then develop a video dataset named CPR-Coach. By taking the CPR-Coach as a benchmark, this paper thoroughly investigates and compares the performance of existing action recognition models based on different data modalities. To solve the unavoidable Single-class Training & Multi-class Testing problem, we propose a humancognition-inspired framework named ImagineNet to improve the model's multierror recognition performance under restricted supervision. Extensive experiments verify the effectiveness of the framework. We hope this work could advance research toward fine-grained medical action analysis and skill assessment. The CPR-Coach dataset and the code of ImagineNet are publicly available on Github.",http://arxiv.org/abs/2309.11718v1,,Shunli Wang (Fudan University) | Shuaibing Wang (Fudan University) | Dingkang Yang (Fudan University) | Mingcheng Li (Fudan University) | Haopeng Kuang (Fudan University) | Xiao Zhao (None) | Liuzhen Su (Fudan University) | Peng Zhai (Fudan University) | Lihua Zhang (Fudan University),2023-09-21 01:39:13+00:00,,,,,,
Towards More Unified In-context Visual Understanding,"The rapid advancement of large language models (LLMs) has accelerated the emergence of in-context learning (ICL) as a cutting-edge approach in the natural language processing domain. Recently, ICL has been employed in visual understanding tasks, such as semantic segmentation and image captioning, yielding promising results. However, existing visual ICL framework can not enable producing content across multiple modalities, which limits their potential usage scenarios. To address this issue, we present a new ICL framework for visual understanding with multi-modal output enabled. First, we quantize and embed both text and visual prompt into a unified representational space, structured as interleaved in-context sequences. Then a decoder-only sparse transformer architecture is employed to perform generative modeling on them, facilitating in-context learning. Thanks to this design, the model is capable of handling in-context vision understanding tasks with multimodal output in a unified pipeline.Experimental results demonstrate that our model achieves competitive performance compared with specialized models and previous ICL baselines. Overall, our research takes a further step toward unified multimodal in-context learning.",http://arxiv.org/abs/2312.02520v2,,Dianmo Sheng (University Of Science And Technology Of China) | Dongdong Chen (Microsoft Research) | Zhentao Tan (Alibaba DAMO Academy; University Of Science And Technology Of China) | Qiankun Liu (Beijing Institute Of Technology) | Qi Chu (University Of Science And Technology Of China) | Jianmin Bao (Microsoft) | Tao Gong (University Of Science And Technology Of China) | Bin Liu (None) | Shengwei Xu (Beijing Electronic Science And Technology Institute) | Nenghai Yu (University Of Science And Technology Of China),2023-12-05 06:02:21+00:00,,,,,,
Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities,,,,"Mingcheng Li (Fudan University) | Dingkang Yang (Fudan University) | Xiao Zhao (None) | Shuaibing Wang (Fudan University) | Yan Wang (Fudan University) | Kun Yang (Fudan University) | Mingyang Sun (Fudan University) | Dongliang Kou (Academy For Engineering And Technology, Fudan University, Shanghai, China.) | Qian (Fudan University) | Lihua Zhang (Fudan University)",,,,,,,
Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection,"Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted significant attention due to the growing demand for video analysis. Recent approaches treat MR and HD as similar video grounding problems and address them together with transformer-based architecture. However, we observe that the emphasis of MR and HD differs, with one necessitating the perception of local relationships and the other prioritizing the understanding of global contexts. Consequently, the lack of task-specific design will inevitably lead to limitations in associating the intrinsic specialty of two tasks. To tackle the issue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the gap and jointly solve MR and HD effectively. By performing progressive integration on intra and inter-modality across multi-granularity, UVCOM achieves the comprehensive understanding in processing a video. Moreover, we present multi-aspect contrastive learning to consolidate the local relation modeling and global knowledge accumulation via well aligned multi-modal space. Extensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights and TVSum datasets demonstrate the effectiveness and rationality of UVCOM which outperforms the state-of-the-art methods by a remarkable margin.",http://arxiv.org/abs/2311.16464v1,,Yicheng Xiao (Tsinghua University) | Zhuoyan Luo (Tsinghua University) | Yong Liu (None) | Yue Ma (Tsinghua University) | Hengwei Bian (Carnegie Mellon University) | Yatai Ji (None) | Yujiu Yang (Tsinghua University) | Xiu Li (Tsinghua University),2023-11-28 03:55:23+00:00,,,,,,
Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives,"We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.",http://arxiv.org/abs/2403.10518v1,,Ronghui Li (Tsinghua University) | Yuxiang Zhang (Tsinghua University) | Yachao Zhang (Tsinghua University) | Hongwen Zhang (Beijing Normal University) | Jie Guo (Peng Cheng Laboratory) | Yan Zhang (ETH Zurich) | Yebin Liu (Tsinghua University) | Xiu Li (Tsinghua University),2024-03-15 17:59:33+00:00,,,,,,
Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model,"Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available in https://github.com/yk7333/D3PO/tree/main.",http://arxiv.org/abs/2311.13231v2,,"Kai Yang (Tsinghua University) | Jian Tao (Tsinghua University) | Jiafei Lyu (Tsinghua University) | Chunjiang Ge (Control Science And Technology, Tsinghua University) | Jiaxin Chen (Parametrix.Ai) | Weihan Shen (Parametrix) | Xiaolong Zhu (Parametrix) | Xiu Li (Tsinghua University)",2023-11-22 08:42:46+00:00,,,,,,
SNI-SLAM: Semantic Neural Implicit SLAM,"We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit representation, that simultaneously performs accurate semantic mapping, high-quality surface reconstruction, and robust camera tracking. In this system, we introduce hierarchical semantic representation to allow multi-level semantic comprehension for top-down structured semantic mapping of the scene. In addition, to fully utilize the correlation between multiple attributes of the environment, we integrate appearance, geometry and semantic features through cross-attention for feature collaboration. This strategy enables a more multifaceted understanding of the environment, thereby allowing SNI-SLAM to remain robust even when single attribute is defective. Then, we design an internal fusion-based decoder to obtain semantic, RGB, Truncated Signed Distance Field (TSDF) values from multi-level features for accurate decoding. Furthermore, we propose a feature loss to update the scene representation at the feature level. Compared with low-level losses such as RGB loss and depth loss, our feature loss is capable of guiding the network optimization on a higher-level. Our SNI-SLAM method demonstrates superior performance over all recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in accurate semantic segmentation and real-time semantic mapping.",http://arxiv.org/abs/2311.11016v2,,"Siting Zhu (Shanghai Jiao Tong University) | Guangming Wang (University Of Cambridge) | Hermann Blum (Computer Vision And Geometry Lab, ETH Zurich) | Jiuming Liu (Shanghai Jiao Tong University) | LiangSong (China University Of Mining Technology - Xuzhou) | Marc Pollefeys (ETH Zurich / Microsoft) | Hesheng Wang (Shanghai Jiao Tong University)",2023-11-18 09:08:46+00:00,,,,,,
Symphonize 3D Semantic Scene Completion with Contextual Instance Queries,"`3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal undertaking in autonomous driving, aiming to predict voxel occupancy within volumetric scenes. However, prevailing methodologies primarily focus on voxel-wise feature aggregation, while neglecting instance semantics and scene context. In this paper, we present a novel paradigm termed Symphonies (Scene-from-Insts), that delves into the integration of instance queries to orchestrate 2D-to-3D reconstruction and 3D scene modeling. Leveraging our proposed Serial Instance-Propagated Attentions, Symphonies dynamically encodes instance-centric semantics, facilitating intricate interactions between image-based and volumetric domains. Simultaneously, Symphonies enables holistic scene comprehension by capturing context through the efficient fusion of instance queries, alleviating geometric ambiguity such as occlusion and perspective errors through contextual scene reasoning. Experimental results demonstrate that Symphonies achieves state-of-the-art performance on challenging benchmarks SemanticKITTI and SSCBench-KITTI-360, yielding remarkable mIoU scores of 15.04 and 18.58, respectively. These results showcase the paradigm's promising advancements. The code is available at https://github.com/hustvl/Symphonies.",http://arxiv.org/abs/2306.15670v2,,Haoyi Jiang (Huazhong University Of Science And Technology) | Tianheng Cheng (Huazhong University Of Science And Technology) | Naiyu Gao (HorizonRobotics) | Haoyang Zhang (Horizon Robotics) | Tianwei Lin (Horizon Robotics) | Wenyu Liu (Huazhong University Of Science And Technology) | Xinggang Wang (Huazhong University Of Science And Technology),2023-06-27 17:59:46+00:00,,,,,,
DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement,"Scene flow estimation, which aims to predict per-point 3D displacements of dynamic scenes, is a fundamental task in the computer vision field. However, previous works commonly suffer from unreliable correlation caused by locally constrained searching ranges, and struggle with accumulated inaccuracy arising from the coarse-to-fine structure. To alleviate these problems, we propose a novel uncertainty-aware scene flow estimation network (DifFlow3D) with the diffusion probabilistic model. Iterative diffusion-based refinement is designed to enhance the correlation robustness and resilience to challenging cases, e.g., dynamics, noisy inputs, repetitive patterns, etc. To restrain the generation diversity, three key flow-related features are leveraged as conditions in our diffusion model. Furthermore, we also develop an uncertainty estimation module within diffusion to evaluate the reliability of estimated scene flow. Our DifFlow3D achieves state-of-the-art performance, with 6.7\% and 19.1\% EPE3D reduction respectively on FlyingThings3D and KITTI 2015 datasets. Notably, our method achieves an unprecedented millimeter-level accuracy (0.0089m in EPE3D) on the KITTI dataset. Additionally, our diffusion-based refinement paradigm can be readily integrated as a plug-and-play module into existing scene flow networks, significantly increasing their estimation accuracy. Codes will be released on https://github.com/IRMVLab/DifFlow3D.",http://arxiv.org/abs/2311.17456v2,,Jiuming Liu (Shanghai Jiao Tong University) | Guangming Wang (University Of Cambridge) | Weicai Ye (Zhejiang University) | Chaokang Jiang (None) | Jinru Han (Shanghai Jiao Tong University) | Zhe Liu (Shanghai Jiao Tong University) | Guofeng Zhang (Zhejiang University) | Dalong Du (PhiGent Robotics) | Hesheng Wang (Shanghai Jiao Tong University),2023-11-29 08:56:24+00:00,,,,,,
Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications,"Understanding how the surrounding environment changes is crucial for performing downstream tasks safely and reliably in autonomous driving applications. Recent occupancy estimation techniques using only camera images as input can provide dense occupancy representations of large-scale scenes based on the current observation. However, they are mostly limited to representing the current 3D space and do not consider the future state of surrounding objects along the time axis. To extend camera-only occupancy estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark for camera-only 4D occupancy forecasting, evaluating the surrounding scene changes in a near future. We build our benchmark based on multiple publicly available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5, which provides sequential occupancy states of general movable and static objects, as well as their 3D backward centripetal flow. To establish this benchmark for future research with comprehensive comparisons, we introduce four baseline types from diverse camera-based perception and prediction implementations, including a static-world occupancy model, voxelization of point cloud prediction, 2D-3D instance-based prediction, and our proposed novel end-to-end 4D occupancy forecasting network. Furthermore, the standardized evaluation protocol for preset multiple tasks is also provided to compare the performance of all the proposed baselines on present and future occupancy estimation with respect to objects of interest in autonomous driving scenarios. The dataset and our implementation of all four baselines in the proposed Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.",http://arxiv.org/abs/2311.17663v2,,"Junyi Ma (Shanghai Jiao Tong University) | Xieyuanli Chen (National University Of Defense Technology) | Jiawei Huang (HAOMO Technology Co., Ltd) | Jingyi Xu (Beijing Institute Of Technology) | Zhen Luo (Beijing Institute Of Technology) | Jintao Xu (Xi'an Jiao Tong University) | Weihao Gu (Tsinghua University) | Rui Ai (HAOMO.AI Technology Co.,Ltd. ) | Hesheng Wang (Shanghai Jiao Tong University)",2023-11-29 14:25:46+00:00,,,,,,
GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models,"In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can help generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D object generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D avatar within 15 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.",http://arxiv.org/abs/2310.08529v2,,Taoran Yi (Huazhong University Of Science And Technology) | Jiemin Fang (Huawei Technologies Ltd.) | Junjie Wang (None) | Guanjun Wu (None) | Lingxi Xie (Huawei Technologies Ltd.) | Xiaopeng Zhang (Huawei Technologies Ltd.) | Wenyu Liu (Huazhong University Of Science And Technology) | Qi Tian (Huawei Technologies Ltd.) | Xinggang Wang (Huazhong University Of Science And Technology),2023-10-12 17:22:24+00:00,,,,,,
4D Gaussian Splatting for Real-Time Dynamic Scene Rendering,"Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.",http://arxiv.org/abs/2310.08528v2,,Guanjun Wu (None) | Taoran Yi (Huazhong University Of Science And Technology) | Jiemin Fang (Huawei Technologies Ltd.) | Lingxi Xie (Huawei Technologies Ltd.) | Xiaopeng Zhang (Huawei Technologies Ltd.) | Wei Wei (Huazhong University Of Science And Technology) | Wenyu Liu (Huazhong University Of Science And Technology) | Qi Tian (Huawei Technologies Ltd.) | Xinggang Wang (Huazhong University Of Science And Technology),2023-10-12 17:21:41+00:00,,,,,,
MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images,"Recent learning methods for object pose estimation require resource-intensive training for each individual object instance or category, hampering their scalability in real applications when confronted with previously unseen objects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for 6D pose estimation from RGB-D images. MatchU is a generic approach that fuses 2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We rely on learning geometric 3D descriptors that are rotation-invariant by design. By encoding pose-agnostic geometry, the learned descriptors naturally generalize to unseen objects and capture symmetries. To tackle ambiguous associations using 3D geometry only, we fuse additional RGB information into our descriptor. This is achieved through a novel attention-based mechanism that fuses cross-modal information, together with a matching loss that leverages the latent space learned from RGB data to guide the descriptor learning process. Extensive experiments reveal the generalizability of both the RGB-D fusion strategy as well as the descriptor efficacy. Benefiting from the novel designs, MatchU surpasses all existing methods by a significant margin in terms of both accuracy and speed, even without the requirement of expensive re-training or rendering.",http://arxiv.org/abs/2403.01517v1,,Junwen Huang (Technische Universit??t M??nchen) | Hao Yu (Technical University Munich) | Kuan-Ting Yu (XYZ Robotics) | Nassir Navab (TU Munich) | Slobodan Ilic (Technical University Munich) | Benjamin Busam (Technical University Of Munich),2024-03-03 14:01:03+00:00,,,,,,
SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation,,,,"Yamei Chen (Technische Universit??t M??nchen) | Yan Di (Technische Universit??t M??nchen) | Guangyao Zhai (Technical University Of Munich) | Fabian Manhardt (Google) | Chenyangguang Zhang (Tsinghua University) | Ruida Zhang (Department Of Automation, Tsinghua University) | Federico Tombari (Google, TUM) | Nassir Navab (TU Munich) | Benjamin Busam (Technical University Of Munich)",,,,,,,
HouseCat6D - A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios,,,,HyunJun Jung (Technische Universit??t M??nchen) | Shun-Cheng Wu (Technical University Munich) | Patrick Ruhkamp (Technical University Munich) | Guangyao Zhai (Technical University Of Munich) | Hannah Schieber (Technische Universit??t M??nchen/Friedrich-Alexander Universit??t Erlangen-N??rnberg) | Giulia Rizzoli (University Of Padua) | Pengyuan Wang (Technische Universit??t M??nchen) | Hongcheng Zhao (Technische Universit??t M??nchen) | Lorenzo Garattoni (Toyota Motor Europe) | Sven Meier (Toyota Motor Europe NV/SA) | Daniel Roth (Technische Universit??t M??nchen) | Nassir Navab (TU Munich) | Benjamin Busam (Technical University Of Munich),,,,,,,
GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting,"In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.",http://arxiv.org/abs/2311.11700v3,,"Chi Yan (Shanghai AI Laboratory) | Delin Qu (Fudan University) | Dong Wang (Shanghai AI Laboratory) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology) | Zhigang Wang (Shanghai AI Lab) | Bin Zhao (Northwest Polytechnical University Xi'an) | Xuelong Li (Northwestern Polytechnical University)",2023-11-20 12:08:23+00:00,,,,,,
Implicit Event-RGBD Neural SLAM,,,,"Delin Qu (Fudan University) | Chi Yan (Shanghai AI Laboratory) | Dong Wang (Shanghai AI Laboratory) | Jie Yin (Shanghai Jiao Tong University) | Qizhi Chen (None) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology) | Yiting Zhang (Zhejiang University) | Bin Zhao (Northwest Polytechnical University Xi'an) | Xuelong Li (Northwestern Polytechnical University)",,,,,,,
HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation,,,,Linglin Jing (Loughborough University) | Yiming Ding (Fudan University) | Yunpeng Gao (Northwest Polytechnical University Xi'an) | Zhigang Wang (Shanghai AI Lab) | Xu Yan (None) | Dong Wang (Shanghai AI Laboratory) | Gerald Schaefer (Loughborough University) | Hui Fang (Loughborough University) | Bin Zhao (Northwest Polytechnical University Xi'an) | Xuelong Li (Northwestern Polytechnical University),,,,,,,
Map-Relative Pose Regression for Visual Re-Localization,,,,Shuai Chen (University Of Oxford) | Tommaso Cavallari (Niantic) | Victor Adrian Prisacariu (None) | Eric Brachmann (None),,,,,,,
FlowTrack: Revisiting Optical Flow for Long-Range Dense Tracking,,,,Seokju Cho (Korea University) | Gabriel Huang (None) | Seungryong Kim (Korea University) | Joon-Young Lee (Adobe Research),,,,,,,
Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM,,,,Linyu Tang (Chongqing University) | Lei Zhang (Chongqing University),,,,,,,
CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing,,,,Zhen Guo (Northwestern Polytechnical University) | Hongping Gan (Northwest Polytechnical University Xi'an),,,,,,,
UFC-Net: Unrolling Fixed-point Continuous Network for Deep Compressive Sensing,,,,Xiaoyang Wang (Northwest Polytechnical University Xi'an) | Hongping Gan (Northwest Polytechnical University Xi'an),,,,,,,
Optimal Transport Aggregation for Visual Place Recognition,"The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a 'dustbin' cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad.",http://arxiv.org/abs/2311.15937v1,,Sergio Izquierdo (None) | Javier Civera (None),2023-11-27 15:46:19+00:00,,,,,,
Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification,,,,Kaijie Ren (Chongqing University) | Lei Zhang (Chongqing University),,,,,,,
From Correspondences to Pose: Non-minimal Certifiable Relative Pose without Disambiguation,"Estimating the relative camera pose from $n \geq 5$ correspondences between two calibrated views is a fundamental task in computer vision. This process typically involves two stages: 1) estimating the essential matrix between the views, and 2) disambiguating among the four candidate relative poses that satisfy the epipolar geometry. In this paper, we demonstrate a novel approach that, for the first time, bypasses the second stage. Specifically, we show that it is possible to directly estimate the correct relative camera pose from correspondences without needing a post-processing step to enforce the cheirality constraint on the correspondences. Building on recent advances in certifiable non-minimal optimization, we frame the relative pose estimation as a Quadratically Constrained Quadratic Program (QCQP). By applying the appropriate constraints, we ensure the estimation of a camera pose that corresponds to a valid 3D geometry and that is globally optimal when certified. We validate our method through exhaustive synthetic and real-world experiments, confirming the efficacy, efficiency and accuracy of the proposed approach. Code is available at https://github.com/javrtg/C2P.",http://arxiv.org/abs/2312.05995v1,,Javier Tirado-Gar??n (Universidad De Zaragoza) | Javier Civera (None),2023-12-10 20:57:31+00:00,,,,,,
Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences,,,,Axel Barroso-Laguna (None) | Sowmya Munukutla (None) | Victor Adrian Prisacariu (None) | Eric Brachmann (None),,,,,,,
Masked Guided Instance Matting for Human Images and Videos,,,,"Chuong Huynh (University Of Maryland, College Park) | Seoung Wug Oh (Adobe Systems) | Abhinav Shrivastava (University Of Maryland) | Joon-Young Lee (Adobe Research)",,,,,,,
MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning,"We present MM-Narrator, a novel system leveraging GPT-4 with multimodal in-context learning for the generation of audio descriptions (AD). Unlike previous methods that primarily focused on downstream fine-tuning with short video clips, MM-Narrator excels in generating precise audio descriptions for videos of extensive lengths, even beyond hours, in an autoregressive manner. This capability is made possible by the proposed memory-augmented generation process, which effectively utilizes both the short-term textual context and long-term visual memory through an efficient register-and-recall mechanism. These contextual memories compile pertinent past information, including storylines and character identities, ensuring an accurate tracking and depicting of story-coherent and character-centric audio descriptions. Maintaining the training-free design of MM-Narrator, we further propose a complexity-based demonstration selection strategy to largely enhance its multi-step reasoning capability via few-shot multimodal in-context learning (MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator consistently outperforms both the existing fine-tuning-based approaches and LLM-based approaches in most scenarios, as measured by standard evaluation metrics. Additionally, we introduce the first segment-based evaluator for recurrent text generation. Empowered by GPT-4, this evaluator comprehensively reasons and marks AD generation performance in various extendable dimensions.",http://arxiv.org/abs/2311.17435v1,,"Chaoyi Zhang (The University Of Sydney, University Of Sydney) | Kevin Lin (Microsoft) | Zhengyuan Yang (Microsoft) | Jianfeng Wang (Microsoft) | Linjie Li (Microsoft) | Chung-Ching Lin (Microsoft) | Zicheng Liu (Microsoft) | Lijuan Wang (Microsoft)",2023-11-29 08:27:00+00:00,,,,,,
DisCo: Disentangled Control for Realistic Human Dance Generation,"Generative AI has made significant strides in computer vision, particularly in text-driven image/video synthesis (T2I/T2V). Despite the notable advancements, it remains challenging in human-centric content synthesis such as realistic dance generation. Current methodologies, primarily tailored for human motion transfer, encounter difficulties when confronted with real-world dance scenarios (e.g., social media dance) which require to generalize across a wide spectrum of poses and intricate human details. In this paper, we depart from the traditional paradigm of human motion transfer and emphasize two additional critical attributes for the synthesis of human dance content in social media contexts: (i) Generalizability: the model should be able to generalize beyond generic human viewpoints as well as unseen human subjects, backgrounds, and poses; (ii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources seamlessly. To address these challenges, we introduce DisCo, which includes a novel model architecture with disentangled control to improve the compositionality of dance synthesis, and an effective human attribute pre-training for better generalizability to unseen humans. Extensive qualitative and quantitative results demonstrate that DisCo can generate high-quality human dance images and videos with diverse appearances and flexible motions. Code, demo, video and visualization are available at: https://disco-dance.github.io/.",http://arxiv.org/abs/2307.00040v2,,Tan Wang (Nanyang Technological University) | Linjie Li (Microsoft) | Kevin Lin (Microsoft) | Yuanhao Zhai (State University Of New York At Buffalo) | Chung-Ching Lin (Microsoft) | Zhengyuan Yang (Microsoft) | Hanwang Zhang (Nanyang Technological University) | Zicheng Liu (Microsoft) | Lijuan Wang (Microsoft),2023-06-30 17:37:48+00:00,,,,,,
MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos,"Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient maintenance, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the \textbf{MMSum} dataset. Our new dataset features (1) Human-validated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess various tasks and methods, including \textit{video summarization}, \textit{text summarization}, and \textit{multimodal summarization}. To champion accessibility and collaboration, we will release the \textbf{MMSum} dataset and the data collection tool as fully open-source resources, fostering transparency and accelerating future developments. Our project website can be found at~\url{https://mmsum-dataset.github.io/}",http://arxiv.org/abs/2306.04216v2,,"Jielin Qiu (Carnegie Mellon University) | Jiacheng Zhu (Massachusetts Institute Of Technology) | William Han (Carnegie Mellon University) | Aditesh Kumar (Carnegie Mellon University) | Karthik Mittal (School Of Computer Science, Carnegie Mellon University) | Claire Jin (School Of Computer Science, Carnegie Mellon University) | Zhengyuan Yang (Microsoft) | Linjie Li (Microsoft) | Jianfeng Wang (Microsoft) | DING ZHAO (Carnegie Mellon University) | Bo Li (UIUC) | Lijuan Wang (Microsoft)",2023-06-07 07:43:11+00:00,,,,,,
Generate Like Experts: Multi-Stage Font Generation by Incorporating Font Transfer Process into Diffusion Models,,,,"Bin Fu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Fanghua Yu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Anran Liu (None) | Zixuan Wang (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Jie Wen (Harbin Institute Of Technology, Shenzhen) | Junjun He (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)",,,,,,,
EgoBridge: A Dataset for Bridging Asynchronous First- and Third-Person View of Activities in Real World,,,,"Yifei Huang (The University Of Tokyo) | Guo Chen (Nanjing University) | Jilan Xu (None) | Mingfang Zhang (None) | Lijin Yang (The University Of Tokyo) | Baoqi Pei (Zhejiang University) | Hongjie Zhang (Shanghai Artificial Intelligence Laboratory) | Lu Dong (University Of Science And Technology Of China) | Yali Wang (SIAT, Chinese Academy Of Sciences) | Limin Wang (Nanjing University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)",,,,,,,
MVBench: A Comprehensive Multi-modal Video Understanding Benchmark,"With the rapid development of Multi-modal Large Language Models (MLLMs), a number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models. However, most benchmarks predominantly assess spatial understanding in the static image tasks, while overlooking temporal understanding in the dynamic video tasks. To alleviate this issue, we introduce a comprehensive Multi-modal Video understanding Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot be effectively solved with a single frame. Specifically, we first introduce a novel static-to-dynamic method to define these temporal-related tasks. By transforming various static tasks into dynamic ones, we enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition. Then, guided by the task definition, we automatically convert public video annotations into multiple-choice QA to evaluate each task. On one hand, such a distinct paradigm allows us to build MVBench efficiently, without much manual intervention. On the other hand, it guarantees evaluation fairness with ground-truth video annotations, avoiding the biased scoring of LLMs. Moreover, we further develop a robust video MLLM baseline, i.e., VideoChat2, by progressive multi-modal training with diverse instruction-tuning data. The extensive results on our MVBench reveal that, the existing MLLMs are far from satisfactory in temporal understanding, while our VideoChat2 largely surpasses these leading models by over 15% on MVBench. All models and data are available at https://github.com/OpenGVLab/Ask-Anything.",http://arxiv.org/abs/2311.17005v3,,"Kunchang Li (SIAT, UCAS) | Yali Wang (SIAT, Chinese Academy Of Sciences) | Yinan He (Shanghai AI Laboratory) | Yizhuo Li (The University Of Hong Kong) | Yi Wang (Shanghai AI Laboratory) | Yi Liu (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Zun Wang (Australian National University) | Jilan Xu (None) | Guo Chen (Nanjing University) | Ping Luo (The University Of Hong Kong) | Limin Wang (Nanjing University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)",2023-11-28 17:59:04+00:00,,,,,,
Look-Up Table Compression for Efficient Image Restoration,,,,Yinglong Li (None) | Jiacheng Li (None) | Zhiwei Xiong (None),,,,,,,
Taming Self-Training for Open-Vocabulary Object Detection,,,,"Shiyu Zhao (Rutgers University, New Brunswick) | Samuel Schulter (None) | Long Zhao (Google Research) | Zhixing Zhang (Rutgers University) | Vijay Kumar BG (NEC Laboratories America) | Yumin Suh (NEC Labs America) | Manmohan Chandraker (UC San Diego) | Dimitris N. Metaxas (Rutgers)",,,,,,,
Score-Guided Diffusion for 3D Human Recovery,,,,"Anastasis Stathopoulos (Rutgers University, Newark) | Ligong Han (Rutgers University) | Dimitris N. Metaxas (Rutgers)",,,,,,,
Cross-dimension Affinity Distillation for 3D EM Neuron Segmentation,,,,"Xiaoyu Liu (University Of Science And Technology Of China) | Miaomiao Cai (University Of Science And Technology Of China) | Yinda Chen (University Of Science And Technology Of China) | Yueyi Zhang (University Of Science And Technology Of China) | Te Shi (Institute Of Artificial Intelligence, Hefei Comprehensive National Science Center) | Ruobing Zhang (Suzhou Institute Of Biomedical Engineering And Technology) | Xuejin Chen (University Of Science And Technology Of China) | Zhiwei Xiong (None)",,,,,,,
Self-Supervised Multi-Object Tracking with Path Consistency,,,,Zijia Lu (Northeastern University) | Bing Shuai (Amazon Web Service) | Yanbei Chen (Amazon) | Zhenlin Xu (Amazon) | Davide Modolo (Amazon),,,,,,,
LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network,,,,Hao Yang (Beijing Institute Of Technology) | Liyuan Pan (Beijing Institute Of Technology) | Yan Yang (ANU) | Richard Hartley (Google) | Miaomiao Liu (Australian National University),,,,,,,
Mining Supervision for Dynamic Regions in Self-Supervised Monocular Depth Estimation,,,,Hoang Chuong Nguyen (Australian National University) | Tianyu Wang (Australian National University) | Jose M. Alvarez (NVIDIA) | Miaomiao Liu (Australian National University),,,,,,,
Hyperbolic Learning with Synthetic Captions for Open-World Detection,,,,Fanjie Kong (Duke University) | Yanbei Chen (Amazon) | Jiarui Cai (Amazon) | Davide Modolo (Amazon),,,,,,,
3DToonify: Creating Your High-Fidelity 3D Stylized Avatar Easily from 2D Portrait Images,,,,Yifang Men (Alibaba Group) | Hanxi Liu (Tianjin University) | Yuan Yao (Alibaba Group) | Miaomiao Cui (Alibaba Group) | Xuansong Xie (Alibaba Group) | Zhouhui Lian (Peking University),,,,,,,
TextNeRF: A Novel Scene-Text Image Synthesis Method based on Neural Radiance Fields,,,,Jialei Cui (Peking University) | Jianwei Du (Southeast University) | Wenzhuo Liu (China University Of Mining Technology - Beijing) | Zhouhui Lian (Peking University),,,,,,,
CCEdit: Creative and Controllable Video Editing via Diffusion Models,"In this paper, we present CCEdit, a versatile generative video editing framework based on diffusion models. Our approach employs a novel trident network structure that separates structure and appearance control, ensuring precise and creative editing capabilities. Utilizing the foundational ControlNet architecture, we maintain the structural integrity of the video during editing. The incorporation of an additional appearance branch enables users to exert fine-grained control over the edited key frame. These two side branches seamlessly integrate into the main branch, which is constructed upon existing text-to-image (T2I) generation models, through learnable temporal layers. The versatility of our framework is demonstrated through a diverse range of choices in both structure representations and personalized T2I models, as well as the option to provide the edited key frame. To facilitate comprehensive evaluation, we introduce the BalanceCC benchmark dataset, comprising 100 videos and 4 target prompts for each video. Our extensive user studies compare CCEdit with eight state-of-the-art video editing methods. The outcomes demonstrate CCEdit's substantial superiority over all other methods.",http://arxiv.org/abs/2309.16496v2,,Ruoyu Feng (University Of Science And Technology Of China) | Wenming Weng (None) | Yanhui Wang (None) | Yuhui Yuan (Microsoft Research Asia) | Jianmin Bao (Microsoft) | Chong Luo (Microsoft Research Asia) | Zhibo Chen (University Of Science And Technology Of China) | Baining Guo (Microsoft Research),2023-09-28 15:03:44+00:00,,,,,,
InstructDiffusion: A Generalist Modeling Interface for Vision Tasks,"We present InstructDiffusion, a unifying and generic framework for aligning computer vision tasks with human instructions. Unlike existing approaches that integrate prior knowledge and pre-define the output space (e.g., categories and coordinates) for each vision task, we cast diverse vision tasks into a human-intuitive image-manipulating process whose output space is a flexible and interactive pixel space. Concretely, the model is built upon the diffusion process and is trained to predict pixels according to user instructions, such as encircling the man's left shoulder in red or applying a blue mask to the left car. InstructDiffusion could handle a variety of vision tasks, including understanding tasks (such as segmentation and keypoint detection) and generative tasks (such as editing and enhancement). It even exhibits the ability to handle unseen tasks and outperforms prior methods on novel datasets. This represents a significant step towards a generalist modeling interface for vision tasks, advancing artificial general intelligence in the field of computer vision.",http://arxiv.org/abs/2309.03895v1,,"Zigang Geng (University Of Science And Technology Of China) | Binxin Yang (University Of Science And Technology Of China) | Tiankai Hang (Southeast University) | Chen Li (Xi'an Jiao Tong University) | Shuyang Gu (Research, Microsoft) | Ting Zhang (Beijing Normal University) | Jianmin Bao (Microsoft) | Zheng Zhang (Microsoft) | Houqiang Li (University Of Science And Technology Of China) | Han Hu (Microsft Research Asia) | Dong Chen (Microsoft) | Baining Guo (Microsoft Research)",2023-09-07 17:56:57+00:00,,,,,,
MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation,"We present MicroCinema, a straightforward yet effective framework for high-quality and coherent text-to-video generation. Unlike existing approaches that align text prompts with video directly, MicroCinema introduces a Divide-and-Conquer strategy which divides the text-to-video into a two-stage process: text-to-image generation and image\&text-to-video generation. This strategy offers two significant advantages. a) It allows us to take full advantage of the recent advances in text-to-image models, such as Stable Diffusion, Midjourney, and DALLE, to generate photorealistic and highly detailed images. b) Leveraging the generated image, the model can allocate less focus to fine-grained appearance details, prioritizing the efficient learning of motion dynamics. To implement this strategy effectively, we introduce two core designs. First, we propose the Appearance Injection Network, enhancing the preservation of the appearance of the given image. Second, we introduce the Appearance Noise Prior, a novel mechanism aimed at maintaining the capabilities of pre-trained 2D diffusion models. These design elements empower MicroCinema to generate high-quality videos with precise motion, guided by the provided text prompts. Extensive experiments demonstrate the superiority of the proposed framework. Concretely, MicroCinema achieves SOTA zero-shot FVD of 342.86 on UCF-101 and 377.40 on MSR-VTT. See https://wangyanhui666.github.io/MicroCinema.github.io/ for video samples.",http://arxiv.org/abs/2311.18829v2,,"Yanhui Wang (None) | Jianmin Bao (Microsoft) | Wenming Weng (None) | Ruoyu Feng (University Of Science And Technology Of China) | Dacheng Yin (University Of Science And Technology Of China) | Tao Yang (Xi'an JiaoTong University) | Jingxu Zhang (Research, Microsoft) | Qi Dai (Microsoft Research Asia) | Zhiyuan Zhao (Microsoft) | Chunyu Wang (Microsoft) | Kai Qiu (Microsoft) | Yuhui Yuan (Microsoft Research Asia) | Xiaoyan Sun (University Of Science And Technology Of China) | Chong Luo (Microsoft Research Asia) | Baining Guo (Microsoft Research)",2023-11-30 18:59:30+00:00,,,,,,
PanoContext-Former: Panoramic Total Scene Understanding with a Transformer,"Panoramic image enables deeper understanding and more holistic perception of $360^\circ$ surrounding environment, which can naturally encode enriched scene context information compared to standard perspective image. Previous work has made lots of effort to solve the scene understanding task in a bottom-up form, thus each sub-task is processed separately and few correlations are explored in this procedure. In this paper, we propose a novel method using depth prior for holistic indoor scene understanding which recovers the objects' shapes, oriented bounding boxes and the 3D room layout simultaneously from a single panorama. In order to fully utilize the rich context information, we design a transformer-based context module to predict the representation and relationship among each component of the scene. In addition, we introduce a real-world dataset for scene understanding, including photo-realistic panoramas, high-fidelity depth images, accurately annotated room layouts, and oriented object bounding boxes and shapes. Experiments on the synthetic and real-world datasets demonstrate that our method outperforms previous panoramic scene understanding methods in terms of both layout estimation and 3D object detection.",http://arxiv.org/abs/2305.12497v2,,Yuan Dong (Alibaba Group) | Chuan Fang (Hong Kong University Of Science And Technology) | Liefeng Bo (None) | Zilong Dong (Alibaba Group) | Ping Tan (Hong Kong University Of Science And Technology),2023-05-21 16:20:57+00:00,,,,,,
ZeroShape: Regression-based Zero-shot Shape Reconstruction,"We study the problem of single-image zero-shot 3D shape reconstruction. Recent works learn zero-shot shape reconstruction through generative modeling of 3D assets, but these models are computationally expensive at train and inference time. In contrast, the traditional approach to this problem is regression-based, where deterministic models are trained to directly regress the object shape. Such regression methods possess much higher computational efficiency than generative methods. This raises a natural question: is generative modeling necessary for high performance, or conversely, are regression-based approaches still competitive? To answer this, we design a strong regression-based model, called ZeroShape, based on the converging findings in this field and a novel insight. We also curate a large real-world evaluation benchmark, with objects from three different real-world 3D datasets. This evaluation benchmark is more diverse and an order of magnitude larger than what prior works use to quantitatively evaluate their models, aiming at reducing the evaluation variance in our field. We show that ZeroShape not only achieves superior performance over state-of-the-art methods, but also demonstrates significantly higher computational and data efficiency.",http://arxiv.org/abs/2312.14198v2,,Zixuan Huang (University Of Illinois Urbana-Champaign) | Stefan Stojanov (Georgia Institute Of Technology) | Anh Thai (Georgia Institute Of Technology) | Varun Jampani (Google Research) | James Rehg (None),2023-12-21 01:56:34+00:00,,,,,,
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations,"Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates concurrently capturing verbal and non-verbal cues pertinent to social reasoning. Experiments demonstrate the effectiveness of the proposed approach with densely aligned multimodal representations in modeling social interactions. We will release our benchmarks and source code to facilitate further research.",http://arxiv.org/abs/2403.02090v1,,"Sangmin Lee (University Of Illinois Urbana-Champaign) | Bolin Lai (Georgia Institute Of Technology) | Fiona Ryan (Georgia Institute Of Technology) | Bikram Boote (University Of Illinois, Urbana Champaign) | James Rehg (None)",2024-03-04 14:46:58+00:00,,,,,,
Bilateral Propagation Network for Depth Completion,,,,Jie Tang (National University Of Defense Technology) | Fei-Peng Tian (Light Illusions) | Boshi An (Peking University) | Jian Li (National University Of Defense Technology) | Ping Tan (Hong Kong University Of Science And Technology),,,,,,,
Neural Refinement for Absolute Pose Regression with Feature Synthesis,"Absolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during training and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a progressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",http://arxiv.org/abs/2303.10087v2,,"Shuai Chen (University Of Oxford) | Yash Bhalgat (Visual Geometry Group, University Of Oxford) | Xinghui Li (University Of Oxford) | Jia-Wang Bian (University Of Oxford) | Kejie Li (University Of Oxford) | Zirui Wang (University Of Oxford) | Victor Adrian Prisacariu (None)",2023-03-17 16:10:50+00:00,,,,,,
SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching,"In this paper, we address the challenge of matching semantically similar keypoints across image pairs. Existing research indicates that the intermediate output of the UNet within the Stable Diffusion (SD) can serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets new benchmarks in accuracy across all these datasets. Particularly, SD4Match outperforms the previous state-of-the-art by a margin of 12 percentage points on the challenging SPair-71k dataset.",http://arxiv.org/abs/2310.17569v1,,Xinghui Li (University Of Oxford) | Jingyi Lu (University Of Hong Kong) | Kai Han (The University Of Hong Kong) | Victor Adrian Prisacariu (None),2023-10-26 16:58:01+00:00,,,,,,
Aligning Logits Generatively for Principled Black-Box Knowledge Distillation,,,,Jing Ma (None) | Xiang Xiang (Huazhong University Of Science And Technology) | Ke Wang (Alibaba Group) | Yuchuan Wu (Alibaba Group) | Yongbin Li (Alibaba Group),,,,,,,
Differentiable Micro-Mesh Construction,,,,Yishun Dou (Huawei) | Zhong Zheng (Huawei.Com) | Qiaoqiao Jin (Shanghai Jiao Tong University) | Rui Shi (Shanghai Jiao Tong University) | Yuhan Li (None) | Bingbing Ni (Shanghai Jiao Tong University),,,,,,,
Vector Graphics Generation via Mutually Impulsed Dual-domain Diffusion,,,,Zhongyin Zhao (Shanghai Jiao Tong University) | Ye Chen (Shanghai Jiao Tong University) | Zhangli Hu (Shanghai Jiao Tong University) | Xuanhong Chen (Shanghai Jiao Tong University) | Bingbing Ni (Shanghai Jiao Tong University),,,,,,,
Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer,,,,Yuwen Tan (Huazhong University Of Science And Technology) | Qinhao Zhou (Huazhong University Of Science And Technology) | Xiang Xiang (Huazhong University Of Science And Technology) | Ke Wang (Alibaba Group) | Yuchuan Wu (Alibaba Group) | Yongbin Li (Alibaba Group),,,,,,,
Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft,"Traditional reinforcement-learning-based agents rely on sparse rewards that often only use binary values to indicate task completion or failure. The challenge in exploration efficiency makes it difficult to effectively learn complex tasks in Minecraft. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors. Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories. In the next round, Reward Designer will take further refine and iterate the dense reward function based on feedback. Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse on the plains biome.",http://arxiv.org/abs/2312.09238v1,,Hao Li (The Chinese University Of Hong Kong) | Xue Yang (Shanghai AI Laboratory) | Zhaokai Wang (Shanghai Jiao Tong University) | Xizhou Zhu (Shanghai AI Laboratory) | Jie Zhou (None) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Xiaogang Wang (The Chinese University Of Hong Kong) | Hongsheng Li (The Chinese University Of Hong Kong) | Lewei Lu (SenseTime) | Jifeng Dai (Tsinghua University),2023-12-14 18:58:12+00:00,,,,,,
Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications,"We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.",http://arxiv.org/abs/2401.06197v1,,"Yuwen Xiong (University Of Toronto) | Zhiqi Li (Nanjing University) | Yuntao Chen (CAIR, HKISI, CAS) | Feng Wang (Tsinghua University) | Xizhou Zhu (Shanghai AI Laboratory) | Jiapeng Luo (SenseTime Research) | Wenhai Wang (Shanghai AI Laboratory) | Tong Lu (Nanjing University) | Hongsheng Li (The Chinese University Of Hong Kong) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Lewei Lu (SenseTime) | Jie Zhou (None) | Jifeng Dai (Tsinghua University)",2024-01-11 14:53:24+00:00,,,,,,
InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks,"The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at https://github.com/OpenGVLab/InternVL.",http://arxiv.org/abs/2312.14238v3,,Zhe Chen (Nanjing University) | Jiannan Wu (University Of Hong Kong) | Wenhai Wang (Shanghai AI Laboratory) | Weijie Su (University Of Science And Technology Of China) | Guo Chen (Nanjing University) | Sen Xing (Tsinghua University) | Zhong Muyan (Tsinghua University) | Qing-Long Zhang (Shanghai Artificial Intelligence Laboratory) | Xizhou Zhu (Shanghai AI Laboratory) | Lewei Lu (SenseTime) | Bin Li (University Of Science And Technology Of China) | Ping Luo (The University Of Hong Kong) | Tong Lu (Nanjing University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Jifeng Dai (Tsinghua University),2023-12-21 18:59:31+00:00,,,,,,
Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation,"We contribute the Habitat Synthetic Scene Dataset, a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from prior work, we find that scale helps in generalization, but the benefits quickly saturate, making visual fidelity and correlation to real-world scenes more important. Our experiments show that agents trained on our smaller-scale dataset can match or outperform agents trained on much larger datasets. Surprisingly, we observe that agents trained on just 122 scenes from our dataset outperform agents trained on 10,000 scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in real-world scanned environments.",http://arxiv.org/abs/2306.11290v3,,"Mukul Khanna (Georgia Institute Of Technology) | Yongsen Mao (Simon Fraser University) | Hanxiao Jiang (University Of Illinois Urbana-Champaign) | Sanjay Haresh (Qualcomm Inc, QualComm) | Brennan Shacklett (Stanford University) | Dhruv Batra (FAIR (Meta) And Georgia Tech) | Alexander William Clegg (Meta AI) | Eric Undersander (Meta) | Angel Xuan Chang (Simon Fraser University) | Manolis Savva (Simon Fraser University)",2023-06-20 05:07:23+00:00,,,,,,
CAGE: Controllable Articulation GEneration,"We address the challenge of generating 3D articulated objects in a controllable fashion. Currently, modeling articulated 3D objects is either achieved through laborious manual authoring, or using methods from prior work that are hard to scale and control directly. We leverage the interplay between part shape, connectivity, and motion using a denoising diffusion-based method with attention modules designed to extract correlations between part attributes. Our method takes an object category label and a part connectivity graph as input and generates an object's geometry and motion parameters. The generated objects conform to user-specified constraints on the object category, part shape, and part articulation. Our experiments show that our method outperforms the state-of-the-art in articulated object generation, producing more realistic objects while conforming better to user constraints.   Video Summary at: http://youtu.be/cH_rbKbyTpE",http://arxiv.org/abs/2312.09570v1,,Jiayi Liu (None) | Hou In Ivan Tam (Simon Fraser University) | Ali Mahdavi Amiri (Simon Fraser University) | Manolis Savva (Simon Fraser University),2023-12-15 07:04:27+00:00,,,,,,
CAT-Seg: Cost Aggregation for Open-vocabulary Semantic Segmentation,"Existing works on open-vocabulary semantic segmentation have utilized large-scale vision-language models, such as CLIP, to leverage their exceptional open-vocabulary recognition capabilities. However, the problem of transferring these capabilities learned from image-level supervision to the pixel-level task of segmentation and addressing arbitrary unseen categories at inference makes this task challenging. To address these issues, we aim to attentively relate objects within an image to given categories by leveraging relational information among class categories and visual semantics through aggregation, while also adapting the CLIP representations to the pixel-level task. However, we observe that direct optimization of the CLIP embeddings can harm its open-vocabulary capabilities. In this regard, we propose an alternative approach to optimize the image-text similarity map, i.e. the cost map, using a novel cost aggregation-based method. Our framework, namely CAT-Seg, achieves state-of-the-art performance across all benchmarks. We provide extensive ablation studies to validate our choices. Project page: https://ku-cvlab.github.io/CAT-Seg/.",http://arxiv.org/abs/2303.11797v1,,Seokju Cho (Korea University) | Heeseong Shin (Korea University) | Sunghwan Hong (Korea University) | Anurag Arnab (Google) | Paul Hongsuck Seo (Google) | Seungryong Kim (Korea University),2023-03-21 12:28:21+00:00,,,,,,
"Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs","This work delves into the task of pose-free novel view synthesis from stereo pairs, a challenging and pioneering task in 3D vision. Our innovative framework, unlike any before, seamlessly integrates 2D correspondence matching, camera pose estimation, and NeRF rendering, fostering a synergistic enhancement of these tasks. We achieve this through designing an architecture that utilizes a shared representation, which serves as a foundation for enhanced 3D geometry understanding. Capitalizing on the inherent interplay between the tasks, our unified framework is trained end-to-end with the proposed training strategy to improve overall model accuracy. Through extensive evaluations across diverse indoor and outdoor scenes from two real-world datasets, we demonstrate that our approach achieves substantial improvement over previous methodologies, especially in scenarios characterized by extreme viewpoint changes and the absence of accurate camera poses.",http://arxiv.org/abs/2312.07246v1,,Sunghwan Hong (Korea University) | Jaewoo Jung (Korea University) | Heeseong Shin (Korea University) | Jiaolong Yang (Microsoft Research) | Chong Luo (Microsoft Research Asia) | Seungryong Kim (Korea University),2023-12-12 13:22:44+00:00,,,,,,
Mitigating Motion Blur in Neural Radiance Fields with Events and Frames,,,,"Marco Cannici (Robotics And Perception Group, Department Of Informatics, University Of Zurich) | Davide Scaramuzza (University Of Zurich)",,,,,,,
Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance,"Reflectance bounds the frequency spectrum of illumination in the object appearance. In this paper, we introduce the first stochastic inverse rendering method, which recovers the full frequency spectrum of an illumination jointly with the object reflectance from a single image. Our key idea is to solve this blind inverse problem in the reflectance map, an appearance representation invariant to the underlying geometry, by learning to reverse the image formation with a novel diffusion model which we refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed reflectance map converted and completed from the single input image, DRMNet generates a reflectance map corresponding to a perfect mirror sphere while jointly estimating the reflectance. The forward process can be understood as gradually filtering a natural illumination with lower and lower frequency reflectance and additive Gaussian noise. DRMNet learns to invert this process with two subnetworks, IllNet and RefNet, which work in concert towards this joint estimation. The network is trained on an extensive synthetic dataset and is demonstrated to generalize to real images, showing state-of-the-art accuracy on established datasets.",http://arxiv.org/abs/2312.04529v1,,Yuto Enyo (Kyoto University) | Ko Nishino (Kyoto University),2023-12-07 18:50:00+00:00,,,,,,
Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation,,,,Shuting He (Nanyang Technological University) | Henghui Ding (Fudan University),,,,,,,
NViST: In the Wild New View Synthesis from a Single Image with Transformers,"We propose NViST, a transformer-based model for novel-view synthesis from a single image, trained on a large-scale dataset of in-the-wild images with complex backgrounds. NViST transforms image inputs directly into a radiance field, adopting a scalable transformer-based architecture. In practice, NViST exploits the self-supervised features learnt by a masked autoencoder (MAE), and learns a novel decoder that translates features to 3D tokens via cross-attention and adaptive layer normalization. Our model is efficient at inference since only a single forward-pass is needed to predict a 3D representation, unlike methods that require test-time optimization or sampling such as 3D-aware diffusion models. We tackle further limitations of current new-view synthesis models. First, unlike most generative models that are trained in a category-specific manner, often on synthetic datasets or on masked inputs, our model is trained on MVImgNet, a large-scale dataset of real-world, casually-captured videos containing hundreds of object categories with diverse backgrounds. Secondly, our model does not require canonicalization of the training data - i.e. aligning all objects with a frontal view - only needing relative pose at training time which removes a substantial barrier to it being used on casually captured datasets. We show results on unseen objects and categories on MVImgNet and even casual phone captures. We conduct qualitative and quantitative evaluations on MVImgNet and ShapeNet to show that our model represents a step forward towards enabling true in-the-wild novel-view synthesis from a single image.",http://arxiv.org/abs/2312.08568v1,,Wonbong Jang (University College London) | Lourdes Agapito (University College London),2023-12-13 23:41:17+00:00,,,,,,
Referring Image Editing: Object-level Image Editing via Referring Expressions,,,,Chang Liu (None) | Xiangtai Li (Nanyang Technological University) | Henghui Ding (Fudan University),,,,,,,
MorpheuS: Neural Dynamic 360\degree  Surface Reconstruction from Monocular RGB-D Video ,"Neural rendering has demonstrated remarkable success in dynamic scene reconstruction. Thanks to the expressiveness of neural representations, prior works can accurately capture the motion and achieve high-fidelity reconstruction of the target object. Despite this, real-world video scenarios often feature large unobserved regions where neural representations struggle to achieve realistic completion. To tackle this challenge, we introduce MorpheuS, a framework for dynamic 360{\deg} surface reconstruction from a casually captured RGB-D video. Our approach models the target scene as a canonical field that encodes its geometry and appearance, in conjunction with a deformation field that warps points from the current frame to the canonical space. We leverage a view-dependent diffusion prior and distill knowledge from it to achieve realistic completion of unobserved regions. Experimental results on various real-world and synthetic datasets show that our method can achieve high-fidelity 360{\deg} surface reconstruction of a deformable object from a monocular RGB-D video.",http://arxiv.org/abs/2312.00778v1,,Hengyi Wang (University College London) | Jingwen Wang (University College London) | Lourdes Agapito (University College London),2023-12-01 18:55:53+00:00,,,,,,
SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing,"Can we capture shape and reflectance in stealth? Such capability would be valuable for many application domains in vision, xR, robotics, and HCI. We introduce Structured Polarization, the first depth and reflectance sensing method using patterns of polarized light (SPIDeRS). The key idea is to modulate the angle of linear polarization (AoLP) of projected light at each pixel. The use of polarization makes it invisible and lets us recover not only depth but also directly surface normals and even reflectance. We implement SPIDeRS with a liquid crystal spatial light modulator (SLM) and a polarimetric camera. We derive a novel method for robustly extracting the projected structured polarization pattern from the polarimetric object appearance. We evaluate the effectiveness of SPIDeRS by applying it to a number of real-world objects. The results show that our method successfully reconstructs object shapes of various materials and is robust to diffuse reflection and ambient light. We also demonstrate relighting using recovered surface normals and reflectance. We believe SPIDeRS opens a new avenue of polarization use in visual sensing.",http://arxiv.org/abs/2312.04553v1,,Tomoki Ichikawa (Kyoto University) | Shohei Nobuhara (Kyoto Institute Of Technology) | Ko Nishino (Kyoto University),2023-12-07 18:59:21+00:00,,,,,,
State Space Models for Event Cameras,"Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minimal performance degradation when tested at higher frequencies than the training input. Traditional RNN and Transformer models exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31 mAP, highlighting the effectiveness of SSMs in event-based vision tasks.",http://arxiv.org/abs/2402.15584v1,,"Nikola Zubic (Robotics And Perception Group, University Of Zurich And ETH Zurich) | Mathias Gehrig (University Of Zurich) | Davide Scaramuzza (University Of Zurich)",2024-02-23 19:51:55+00:00,,,,,,
Visual Point Cloud Forecasting enables Scalable Autonomous Driving,"In contrast to extensive studies on general vision, pre-training for scalable visual autonomous driving remains seldom explored. Visual autonomous driving applications require features encompassing semantics, 3D geometry, and temporal information simultaneously for joint perception, prediction, and planning, posing dramatic challenges for pre-training. To resolve this, we bring up a new pre-training task termed as visual point cloud forecasting - predicting future point clouds from historical visual input. The key merit of this task captures the synergic learning of semantics, 3D structures, and temporal dynamics. Hence it shows superiority in various downstream tasks. To cope with this new problem, we present ViDAR, a general model to pre-train downstream visual encoders. It first extracts historical embeddings by the encoder. These representations are then transformed to 3D geometric space via a novel Latent Rendering operator for future point cloud prediction. Experiments show significant gain in downstream tasks, e.g., 3.1% NDS on 3D detection, ~10% error reduction on motion forecasting, and ~15% less collision rate on planning.",http://arxiv.org/abs/2312.17655v1,,Zetong Yang (The Chinese University Of Hong Kong) | Li Chen (Shanghai AI Laboratory) | Yanan Sun (The Hong Kong University Of Science And Technology) | Hongyang Li (Shanghai AI Lab),2023-12-29 15:44:13+00:00,,,,,,
Generalized Predictive Model for Autonomous Driving,,,,"Jiazhi Yang (Shanghai AI Laboratory) | Shenyuan Gao (HKUST) | Yihang Qiu (Shanghai Jiao Tong University) | Li Chen (Shanghai AI Laboratory) | Tianyu Li (Fudan University) | Bo Dai (Shanghai AI Laboratory) | Kashyap Chitta (None) | Penghao Wu (University Of California, San Diego) | Jia Zeng (Shanghai Jiao Tong University) | Ping Luo (The University Of Hong Kong) | Jun Zhang (The Hong Kong University Of Science And Technology) | Andreas Geiger (University Of T??bingen) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Hongyang Li (Shanghai AI Lab)",,,,,,,
Improving Distant 3D Object Detection Using 2D Box Supervision,"Improving the detection of distant 3d objects is an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant objects, which hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant objects since they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant objects. LR3D adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close objects. This mapping allows the depth estimation of distant objects conditioned on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant objects (over 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.",http://arxiv.org/abs/2403.09230v1,,"Zetong Yang (The Chinese University Of Hong Kong) | Zhiding Yu (NVIDIA) | Christopher Choy (Stanford University) | Renhao Wang (University Of California, Berkeley) | Anima Anandkumar (California Institute Of Technology) | Jose M. Alvarez (NVIDIA)",2024-03-14 09:54:31+00:00,,,,,,
Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?,"End-to-end autonomous driving recently emerged as a promising research direction to target autonomy from a full-stack perspective. Along this line, many of the latest works follow an open-loop evaluation setting on nuScenes to study the planning behavior. In this paper, we delve deeper into the problem by conducting thorough analyses and demystifying more devils in the details. We initially observed that the nuScenes dataset, characterized by relatively simple driving scenarios, leads to an under-utilization of perception information in end-to-end models incorporating ego status, such as the ego vehicle's velocity. These models tend to rely predominantly on the ego vehicle's status for future path planning. Beyond the limitations of the dataset, we also note that current metrics do not comprehensively assess the planning quality, leading to potentially biased conclusions drawn from existing benchmarks. To address this issue, we introduce a new metric to evaluate whether the predicted trajectories adhere to the road. We further propose a simple baseline able to achieve competitive results without relying on perception annotations. Given the current limitations on the benchmark and metrics, we suggest the community reassess relevant prevailing research and be cautious whether the continued pursuit of state-of-the-art would yield convincing and universal conclusions. Code and models are available at \url{https://github.com/NVlabs/BEV-Planner}",http://arxiv.org/abs/2312.03031v1,,Zhiqi Li (Nanjing University) | Zhiding Yu (NVIDIA) | Shiyi Lan (NVIDIA CORPORATION) | Jiahan Li (Nanjing University) | Jan Kautz (NVIDIA) | Tong Lu (Nanjing University) | Jose M. Alvarez (NVIDIA),2023-12-05 11:32:31+00:00,,,,,,
SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation,"Despite their ability to generate high-resolution and diverse images from text prompts, text-to-image diffusion models often suffer from slow iterative sampling processes. Model distillation is one of the most effective directions to accelerate these models. However, previous distillation methods fail to retain the generation quality while requiring a significant amount of images for training, either from real data or synthetically generated by the teacher model. In response to this limitation, we present a novel image-free distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from text-to-3D synthesis, in which a 3D neural radiance field that aligns with the input prompt can be obtained from a 2D text-to-image diffusion prior via a specialized loss without the use of any 3D data ground-truth, our approach re-purposes that same loss for distilling a pretrained multi-step text-to-image model to a student network that can generate high-fidelity images with just a single inference step. In spite of its simplicity, our model stands as one of the first one-step text-to-image generators that can produce images of comparable quality to Stable Diffusion without reliance on any training image data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive results or even substantially surpassing existing state-of-the-art distillation techniques.",http://arxiv.org/abs/2312.05239v1,,Thuan Nguyen (VinAI Research) | Anh Tran (VinAI Research),2023-12-08 18:44:09+00:00,,,,,,
AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search,,,,Junghyup Lee (Yonsei University) | Bumsub Ham (Yonsei University),,,,,,,
Motion Diversification Networks,,,,Hee Jae Kim (Boston University) | Eshed Ohn-Bar (Boston University),,,,,,,
Face2Diffusion for More Editable Face Personalization,"Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods.",http://arxiv.org/abs/2403.05094v1,,Kaede Shiohara (None) | Toshihiko Yamasaki (None),2024-03-08 06:46:01+00:00,,,,,,
SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation,"Human-centric video frame interpolation has great potential for improving people's entertainment experiences and finding commercial applications in the sports analysis industry, e.g., synthesizing slow-motion videos. Although there are multiple benchmark datasets available in the community, none of them is dedicated for human-centric scenarios. To bridge this gap, we introduce SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video frames of high-resolution ($\geq$720p) slow-motion sports videos crawled from YouTube. We re-train several state-of-the-art methods on our benchmark, and the results show a decrease in their accuracy compared to other datasets. It highlights the difficulty of our benchmark and suggests that it poses significant challenges even for the best-performing methods, as human bodies are highly deformable and occlusions are frequent in sports videos. To improve the accuracy, we introduce two loss terms considering the human-aware priors, where we add auxiliary supervision to panoptic segmentation and human keypoints detection, respectively. The loss terms are model agnostic and can be easily plugged into any video frame interpolation approaches. Experimental results validate the effectiveness of our proposed loss terms, leading to consistent performance improvement over 5 existing models, which establish strong baseline models on our benchmark. The dataset and code can be found at: https://neu-vi.github.io/SportsSlomo/.",http://arxiv.org/abs/2308.16876v2,,"Jiaben Chen (University Of California, San Diego) | Huaizu Jiang (Northeastern University)",2023-08-31 17:23:50+00:00,,,,,,
L0 -Sampler: An  L 0  Model Guided Volume Sampling for NeRF ,,,,Liangchen Li (University Of Science And Technology Of China) | Juyong Zhang (University Of Science And Technology Of China),,,,,,,
Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations,,,,Daan De Geus (Eindhoven University Of Technology) | Gijs Dubbelman (Eindhoven University Of Technology),,,,,,,
NightCC: Nighttime Color Constancy via Adaptive Channel Masking,,,,"Shuwei Li (National University Of Singaore, National University Of Singapore) | Robby T. Tan (National University Of Singapore)",,,,,,,
Instance-Aware Group Quantization for Vision Transformers,,,,Jaehyeon Moon (Yonsei University) | Dohyung Kim (Yonsei University) | Jun Yong Cheon (Yonsei University) | Bumsub Ham (Yonsei University),,,,,,,
Improving Plasticity in Online Continual Learning via Collaborative Learning,"Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a novel collaborative learning scheme to boost the training of the models. We adapted CCL-DC to existing representative online CL works. Extensive experiments demonstrate that even if the learners are well-trained with state-of-the-art online CL methods, our strategy can still improve model plasticity dramatically, and thereby improve the overall performance by a large margin.",http://arxiv.org/abs/2312.00600v1,,Maorong Wang (The University Of Tokyo) | Nicolas Michel (None) | Ling Xiao (None) | Toshihiko Yamasaki (None),2023-12-01 14:06:28+00:00,,,,,,
EFHQ: Multi-purpose ExtremePose-Face-HQ dataset,"The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment. Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments. Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.",http://arxiv.org/abs/2312.17205v3,,Trung Dao (VinAI) | Duc H Vu (VinAI Research) | Cuong Pham (Posts & Telecommunications Institute Of Technology And VinAI Research) | Anh Tran (VinAI Research),2023-12-28 18:40:31+00:00,,,,,,
Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions,"Zero-shot referring expression comprehension aims at localizing bounding boxes in an image corresponding to the provided textual prompts, which requires: (i) a fine-grained disentanglement of complex visual scene and textual context, and (ii) a capacity to understand relationships among disentangled entities. Unfortunately, existing large vision-language alignment (VLA) models, e.g., CLIP, struggle with both aspects so cannot be directly used for this task. To mitigate this gap, we leverage large foundation models to disentangle both images and texts into triplets in the format of (subject, predicate, object). After that, grounding is accomplished by calculating the structural similarity matrix between visual and textual triplets with a VLA model, and subsequently propagate it to an instance-level similarity matrix. Furthermore, to equip VLA models with the ability of relationship understanding, we design a triplet-matching objective to fine-tune the VLA models on a collection of curated dataset containing abundant entity relationships. Experiments demonstrate that our visual grounding performance increase of up to 19.5% over the SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo dataset, our zero-shot approach achieves comparable accuracy to the fully supervised model.",http://arxiv.org/abs/2311.17048v1,,Zeyu Han (Sichuan University) | Fangrui Zhu (Northeastern University) | Qianru Lao (Harvard University) | Huaizu Jiang (Northeastern University),2023-11-28 18:55:37+00:00,,,,,,
ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers,,,,Narges Norouzi (None) | Svetlana Orlova (Eindhoven University Of Technology) | Daan De Geus (Eindhoven University Of Technology) | Gijs Dubbelman (Eindhoven University Of Technology),,,,,,,
CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection,,,,Mikhail Kennerley (National University Of Singapore) | Jian-Gang Wang (A*STAR) | Bharadwaj Veeravalli (NUS) | Robby T. Tan (National University Of Singapore),,,,,,,
Feedback-Guided Autonomous Driving,,,,Jimuyang Zhang (None) | Zanming Huang (Boston University) | Arijit Ray (Boston University) | Eshed Ohn-Bar (Boston University),,,,,,,
FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS,"We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed. Project page: https://ustc3dv.github.io/FlashAvatar/",http://arxiv.org/abs/2312.02214v1,,Jun Xiang (University Of Science And Technology Of China) | Xuan Gao (University Of Science And Technology Of China) | Yudong Guo (Image Derivative Inc) | Juyong Zhang (University Of Science And Technology Of China),2023-12-03 07:23:53+00:00,,,,,,
Scaling Up Video Summarization Pretraining with Large Language Models,,,,Dawit Argaw Argaw (None) | Seunghyun Yoon (Adobe Research) | Fabian Caba Heilbron (Adobe Research) | Hanieh Deilamsalehy (None) | Trung Bui (Adobe Research) | Zhaowen Wang (Adobe Research) | Franck Dernoncourt (Adobe Systems) | Joon Chung (KAIST),,,,,,,
Faces that Speak: Jointly Synthesising Talking Face and Speech from Text,,,,Youngjoon Jang (Korea Advanced Institute Of Science & Technology) | Kim (None) | Junseok Ahn (Korea Advanced Institute Of Science And Technology) | Doyeop Kwak (Korea Advanced Institute Of Science & Technology) | Hongsun Yang (42dot) | Yooncheol Ju (42dot) | ILHWAN KIM (None) | Byeong-Yeol Kim (42dot) | Joon Chung (KAIST),,,,,,,
Text-Guided Variational Image Generation \for Industrial Anomaly Detection and Segmentation,"We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.",http://arxiv.org/abs/2403.06247v1,,"Mingyu Lee (Chung-Ang University, LGCNS) | Jongwon Choi (Chung-Ang University)",2024-03-10 16:11:17+00:00,,,,,,
Boosting Adversarial Training via Fisher-Rao Norm-based Regularization,,,,Xiangyu Yin (University Of Liverpool) | Wenjie Ruan (University Of Exeter),,,,,,,
MemFlow: Optical Flow Estimation and Prediction with Memory,,,,Qiaole Dong (Fudan University) | Yanwei Fu (Fudan University),,,,,,,
Exploiting Style Latent Flows for Generalizing Video Deepfake Detection,"This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.",http://arxiv.org/abs/2403.06592v1,,Jongwook Choi (Chung-Ang University) | Taehoon Kim (Chung-Ang University) | Yonghyun Jeong (NAVER) | Seungryul Baek (UNIST) | Jongwon Choi (Chung-Ang University),2024-03-11 10:35:58+00:00,,,,,,
Towards Fairness-Aware Adversarial Learning,,,,Yanghao Zhang (University Of Liverpool) | Tianle Zhang (University Of Liverpool) | Ronghui Mu (Lancaster University) | Xiaowei Huang (University Of Liverpool) | Wenjie Ruan (University Of Exeter),,,,,,,
LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model,"This paper introduces LeftRefill, an innovative approach to efficiently harness large Text-to-Image (T2I) diffusion models for reference-guided image synthesis. As the name implies, LeftRefill horizontally stitches reference and target views together as a whole input. The reference image occupies the left side, while the target canvas is positioned on the right. Then, LeftRefill paints the right-side target canvas based on the left-side reference and specific task instructions. Such a task formulation shares some similarities with contextual inpainting, akin to the actions of a human painter. This novel formulation efficiently learns both structural and textured correspondence between reference and target without other image encoders or adapters. We inject task and view information through cross-attention modules in T2I models, and further exhibit multi-view reference ability via the re-arranged self-attention modules. These enable LeftRefill to perform consistent generation as a generalized model without requiring test-time fine-tuning or model modifications. Thus, LeftRefill can be seen as a simple yet unified framework to address reference-guided synthesis. As an exemplar, we leverage LeftRefill to address two different challenges: reference-guided inpainting and novel view synthesis, based on the pre-trained StableDiffusion. Codes and models are released at https://github.com/ewrfcas/LeftRefill.",http://arxiv.org/abs/2305.11577v3,,Chenjie Cao (None) | Yunuo Cai (Fudan University) | Qiaole Dong (Fudan University) | Yikai Wang (None) | Yanwei Fu (Fudan University),2023-05-19 10:29:42+00:00,,,,,,
ContextSeg: Sketch Semantic Segmentation by Querying the Context with Attention,"Sketch semantic segmentation is a well-explored and pivotal problem in computer vision involving the assignment of pre-defined part labels to individual strokes. This paper presents ContextSeg - a simple yet highly effective approach to tackling this problem with two stages. In the first stage, to better encode the shape and positional information of strokes, we propose to predict an extra dense distance field in an autoencoder network to reinforce structural information learning. In the second stage, we treat an entire stroke as a single entity and label a group of strokes within the same semantic part using an auto-regressive Transformer with the default attention mechanism. By group-based labeling, our method can fully leverage the context information when making decisions for the remaining groups of strokes. Our method achieves the best segmentation accuracy compared with state-of-the-art approaches on two representative datasets and has been extensively evaluated demonstrating its superior performance. Additionally, we offer insights into solving part imbalance in training data and the preliminary experiment on cross-category training, which can inspire future research in this field.",http://arxiv.org/abs/2311.16682v1,,Jiawei Wang (Shandong University) | Changjian Li (University Of Edinburgh),2023-11-28 10:53:55+00:00,,,,,,
Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring,,,,Huicong Zhang (Harbin Institute Of Technology) | Haozhe Xie (Nanyang Technological University) | Hongxun Yao (Harbin Institute Of Technology),,,,,,,
Efficient Detection of Long Consistent Cycles and its Application to Distributed Synchronization,,,,"Shaohan Li (University Of Minnesota, Minneapolis) | Yunpeng Shi (University Of California, Davis) | Gilad Lerman (University Of Minnesota, Minneapolis)",,,,,,,
Amodal Completion via Progressive Mixed Context Diffusion,"Our brain can effortlessly recognize objects even when partially hidden from view. Seeing the visible of the hidden is called amodal completion; however, this task remains a challenge for generative AI despite rapid progress. We propose to sidestep many of the difficulties of existing approaches, which typically involve a two-step process of predicting amodal masks and then generating pixels. Our method involves thinking outside the box, literally! We go outside the object bounding box to use its context to guide a pre-trained diffusion inpainting model, and then progressively grow the occluded object and trim the extra background. We overcome two technical challenges: 1) how to be free of unwanted co-occurrence bias, which tends to regenerate similar occluders, and 2) how to judge if an amodal completion has succeeded. Our amodal completion method exhibits improved photorealistic completion results compared to existing approaches in numerous successful completion cases. And the best part? It doesn't require any special training or fine-tuning of models.",http://arxiv.org/abs/2312.15540v1,,"Katherine Xu (University Of Pennsylvania) | Lingzhi Zhang (School Of Engineering And Applied Science, University Of Pennsylvania) | Jianbo Shi (None)",2023-12-24 19:05:02+00:00,,,,,,
Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships,,,,"Rangel Daroya (University Of Massachusetts Amherst) | Aaron Sun (University Of Massachusetts Amherst) | Subhransu Maji (University Of Massachusetts, Amherst)",,,,,,,
Looking 3D: Anomaly Detection with 2D-3D Alignment,,,,Ankan Kumar Bhunia (The University Of Edinburgh) | Changjian Li (University Of Edinburgh) | Hakan Bilen (University Of Edinburgh),,,,,,,
Learning Group Activity Features Through Person Attribute Prediction,"This paper proposes Group Activity Feature (GAF) learning in which features of multi-person activity are learned as a compact latent vector. Unlike prior work in which the manual annotation of group activities is required for supervised learning, our method learns the GAF through person attribute prediction without group activity annotations. By learning the whole network in an end-to-end manner so that the GAF is required for predicting the person attributes of people in a group, the GAF is trained as the features of multi-person activity. As a person attribute, we propose to use a person's action class and appearance features because the former is easy to annotate due to its simpleness, and the latter requires no manual annotation. In addition, we introduce a location-guided attribute prediction to disentangle the complex GAF for extracting the features of each target person properly. Various experimental results validate that our method outperforms SOTA methods quantitatively and qualitatively on two public datasets. Visualization of our GAF also demonstrates that our method learns the GAF representing fined-grained group activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.",http://arxiv.org/abs/2403.02753v2,,Chihiro Nakatani (TTI-J) | Hiroaki Kawashima (University Of Hyogo) | Norimichi Ukita (None),2024-03-05 08:19:44+00:00,,,,,,
Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps,"Recent progress in self-supervised representation learning has resulted in models that are capable of extracting image features that are not only effective at encoding image level, but also pixel-level, semantics. These features have been shown to be effective for dense visual semantic correspondence estimation, even outperforming fully-supervised methods. Nevertheless, current self-supervised approaches still fail in the presence of challenging image characteristics such as symmetries and repeated parts. To address these limitations, we propose a new approach for semantic correspondence estimation that supplements discriminative self-supervised features with 3D understanding via a weak geometric spherical prior. Compared to more involved 3D pipelines, our model only requires weak viewpoint information, and the simplicity of our spherical representation enables us to inject informative geometric priors into the model during training. We propose a new evaluation metric that better accounts for repeated part and symmetry-induced mistakes. We present results on the challenging SPair-71k dataset, where we show that our approach demonstrates is capable of distinguishing between symmetric views and repeated parts across many object categories, and also demonstrate that we can generalize to unseen classes on the AwA dataset.",http://arxiv.org/abs/2312.13216v1,,Octave Mariotti (University Of Edinburgh) | Oisin Mac Aodha (University Of Edinburgh) | Hakan Bilen (University Of Edinburgh),2023-12-20 17:35:24+00:00,,,,,,
Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions,"The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale, aligned image and text datasets in specific domains. In this work, we leverage two complementary sources of information -- descriptions of categories generated by large language models (LLMs) and abundant, fine-grained image classification datasets -- to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side, we develop methods to train VLMs with this ""bag-level"" image-text supervision. We find that simply using these attributes at test-time does not improve performance, but our training strategy, for example, on the iNaturalist dataset, leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds and flowers. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways, we generate descriptions that capture visual appearance, habitat, and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We plan to release the benchmark, consisting of 7 datasets, which will contribute to future research in zero-shot recognition.",http://arxiv.org/abs/2401.02460v1,,"Oindrila Saha (University Of Massachusetts At Amherst) | Grant Horn (University Of Massachusetts At Amherst) | Subhransu Maji (University Of Massachusetts, Amherst)",2024-01-04 08:39:13+00:00,,,,,,
A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion,,,,"Feng Yu (University Of Minnesota - Twin Cities) | Teng Zhang (University Of Central Florida) | Gilad Lerman (University Of Minnesota, Minneapolis)",,,,,,,
3D-LFM: Lifting Foundation Model,"The lifting of 3D structure and camera from 2D landmarks is at the cornerstone of the entire discipline of computer vision. Traditional methods have been confined to specific rigid objects, such as those in Perspective-n-Point (PnP) problems, but deep learning has expanded our capability to reconstruct a wide range of object classes (e.g. C3PDO and PAUL) with resilience to noise, occlusions, and perspective distortions. All these techniques, however, have been limited by the fundamental need to establish correspondences across the 3D training data -- significantly limiting their utility to applications where one has an abundance of ""in-correspondence"" 3D data. Our approach harnesses the inherent permutation equivariance of transformers to manage varying number of points per 3D data instance, withstands occlusions, and generalizes to unseen categories. We demonstrate state of the art performance across 2D-3D lifting task benchmarks. Since our approach can be trained across such a broad class of structures we refer to it simply as a 3D Lifting Foundation Model (3D-LFM) -- the first of its kind.",http://arxiv.org/abs/2312.11894v1,,Mosam Dabhi (None) | L??szl?? A. Jeni (Carnegie Mellon University) | Simon Lucey (University Of Adelaide),2023-12-19 06:38:18+00:00,,,,,,
Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification,,,,Tingting Zheng (Harbin Institute Of Technology) | Kui Jiang (Harbin Institute Of Technology) | Hongxun Yao (Harbin Institute Of Technology),,,,,,,
From Activation to Initialization: Scaling Insights for Optimizing Neural Fields,,,,Hemanth Saratchandran (University Of Adelaide/Australian Institute Of Machine Learning) | Sameera Ramasinghe (Amazon) | Simon Lucey (University Of Adelaide),,,,,,,
Brain Decodes Deep Nets,"We developed a tool for visualizing and analyzing large pre-trained vision models by mapping them onto the brain, thus exposing their hidden inside. Our innovation arises from a surprising usage of brain encoding: predicting brain fMRI measurements in response to images. We report two findings. First, explicit mapping between the brain and deep-network features across dimensions of space, layers, scales, and channels is crucial. This mapping method, FactorTopy, is plug-and-play for any deep-network; with it, one can paint a picture of the network onto the brain (literally!). Second, our visualization shows how different training methods matter: they lead to remarkable differences in hierarchical organization and scaling behavior, growing with more data or network capacity. It also provides insight into finetuning: how pre-trained models change when adapting to small datasets. Our method is practical: only 3K images are enough to learn a network-to-brain mapping.",http://arxiv.org/abs/2312.01280v1,,Huzheng Yang (University Of Pennsylvania) | James Gee (University Of Pennsylvania) | Jianbo Shi (None),2023-12-03 04:36:04+00:00,,,,,,
READ: Retrieval-Enhanced Asymmetric Diffusion for Motion Planning,,,,Takeru Oba (None) | Matthew Walter (Toyota Technological Institute At Chicago) | Norimichi Ukita (None),,,,,,,
CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs,"CAD programs are a popular way to compactly encode shapes as a sequence of operations that are easy to parametrically modify. However, without sufficient semantic comments and structure, such programs can be challenging to understand, let alone modify. We introduce the problem of semantic commenting CAD programs, wherein the goal is to segment the input program into code blocks corresponding to semantically meaningful shape parts and assign a semantic label to each block. We solve the problem by combining program parsing with visual-semantic analysis afforded by recent advances in foundational language and vision models. Specifically, by executing the input programs, we create shapes, which we use to generate conditional photorealistic images to make use of semantic annotators for such images. We then distill the information across the images and link back to the original programs to semantically comment on them. Additionally, we collected and annotated a benchmark dataset, CADTalk, consisting of 5,280 machine-made programs and 45 human-made programs with ground truth semantic comments to foster future research. We extensively evaluated our approach, compared to a GPT-based baseline approach, and an open-set shape segmentation baseline, i.e., PartSLIP, and reported an 83.24% accuracy on the new CADTalk dataset. Project page: https://enigma-li.github.io/CADTalk/.",http://arxiv.org/abs/2311.16703v2,,Haocheng Yuan (University Of Edinburgh) | Jing Xu (University Of Edinburgh) | Hao Pan (Microsoft Research) | Adrien Bousseau (INRIA) | Niloy J. Mitra (University College London) | Changjian Li (University Of Edinburgh),2023-11-28 11:27:48+00:00,,,,,,
InstanceDiffusion: Instance-level Control for Image Generation,"Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs.",http://arxiv.org/abs/2402.03290v1,,"Xudong Wang (Electrical Engineering & Computer Science Department, University Of California Berkeley) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Sai Saketh Rambhatla (Meta) | Rohit Girdhar (Meta) | Ishan Misra (Facebook)",2024-02-05 18:49:17+00:00,,,,,,
Multi-Session SLAM using Wide-Baseline Optical Flow,,,,Lahav Lipson (Princeton University) | Jia Deng (Princeton University),,,,,,,
Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation,"We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visit https://infinigen.org for videos, code and pre-generated data.",http://arxiv.org/abs/2306.09310v2,,"Alexander Raistrick (Princeton University) | Lingjie Mei (Princeton University) | Karhan Kayan (Princeton University) | David Yan (Princeton University) | Yiming Zuo (Princeton University) | Beining Han (Department Of Computer Science, Princeton University) | Hongyu Wen (Princeton University) | Meenal Parakh (Princeton University) | Stamatis Alexandropoulos (Princeton University) | Lahav Lipson (Princeton University) | Zeyu Ma (Princeton University) | Jia Deng (Princeton University)",2023-06-15 17:46:16+00:00,,,,,,
SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks,,,,Yaxu Xie (German Research Center For Artificial Intelligence) | Alain Pagani (German Research Center For Artificial Intelligence (DFKI)) | Didier Stricker (Universit??t Kaiserslautern),,,,,,,
Learning to Count without Annotations,,,,Lukas Knobel (TNO) | Tengda Han (University Of Oxford) | Yuki Asano (University Of Amsterdam),,,,,,,
Active Domain Adaptation with False Negative Prediction for Object Detection,,,,Yuzuru Nakamura (Panasonic Holdings Corporation) | Yasunori Ishii (Panasonic Holdings Corporation) | Takayoshi Yamashita (Chubu University),,,,,,,
Posterior Distillation Sampling,,,,Juil Koo (None) | Chanho Park (KAIST) | Minhyuk Sung (KAIST),,,,,,,
Neural Markov Random Field for Stereo Matching,,,,Tongfan Guan (The Chinese University Of Hong Kong) | Chen Wang (University At Buffalo) | Yun-Hui Liu (The Chinese University Of Hong Kong),,,,,,,
Revisiting Sampson Approximations for Geometric Estimation Problems,"Many problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation ``agrees"" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error).   In this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks.",http://arxiv.org/abs/2401.07114v1,,Felix Rydell (KTH Royal Institute Of Technology) | Angelica Torres (Max Planck Institute For Mathematics In The Sciences) | Viktor Larsson (Lund University),2024-01-13 16:36:39+00:00,,,,,,
PELA: Learning Parameter-Efficient Models with Low-Rank Approximation,"Applying a pre-trained large model to downstream tasks is prohibitive under resource-constrained conditions. Recent dominant approaches for addressing efficiency issues involve adding a few learnable parameters to the fixed backbone model. This strategy, however, leads to more challenges in loading large models for downstream fine-tuning with limited resources. In this paper, we propose a novel method for increasing the parameter efficiency of pre-trained models by introducing an intermediate pre-training stage. To this end, we first employ low-rank approximation to compress the original large model and then devise a feature distillation module and a weight perturbation regularization module. These modules are specifically designed to enhance the low-rank model. In particular, we update only the low-rank model while freezing the backbone parameters during pre-training. This allows for direct and efficient utilization of the low-rank model for downstream fine-tuning tasks. The proposed method achieves both efficiencies in terms of required parameters and computation time while maintaining comparable results with minimal modifications to the backbone architecture. Specifically, when applied to three vision-only and one vision-language Transformer models, our approach often demonstrates a merely $\sim$0.6 point decrease in performance while reducing the original parameter size by 1/3 to 2/3.",http://arxiv.org/abs/2310.10700v2,,Yangyang Guo (National University Of Singapore) | Guangzhi Wang (National University Of Singapore) | Mohan Kankanhalli (National University Of Singapore),2023-10-16 07:17:33+00:00,,,,,,
EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image Segmentation,"In recent years, medical image segmentation has become an important application in the field of computer-aided diagnosis. In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks.",http://arxiv.org/abs/2310.16175v1,,"Md Mostafijur Rahman (University Of Texas At Austin) | Mustafa Munir (The University Of Texas At Austin) | Radu Marculescu (University Of Texas, Austin)",2023-10-24 20:41:04+00:00,,,,,,
"""Previously on ..."" From Recaps to Story Summarization",,,,"Aditya Kumar Singh (International Institute Of Information Technology, Hyderabad) | Dhruv Srivastava (International Institute Of Information Technology (IIIT-H), Hyderabad) | Makarand Tapaswi (Wadhwani AI, IIIT Hyderabad)",,,,,,,
One-Shot Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models,"Text-to-video diffusion models have advanced video generation significantly. However, customizing these models to generate videos with tailored motions presents a substantial challenge. In specific, they encounter hurdles in (a) accurately reproducing motion from a target video, and (b) creating diverse visual variations. For example, straightforward extensions of static image customization methods to video often lead to intricate entanglements of appearance and motion data. To tackle this, here we present the Video Motion Customization (VMC) framework, a novel one-shot tuning approach crafted to adapt temporal attention layers within video diffusion models. Our approach introduces a novel motion distillation objective using residual vectors between consecutive frames as a motion reference. The diffusion process then preserves low-frequency motion trajectories while mitigating high-frequency motion-unrelated noise in image space. We validate our method against state-of-the-art video generative models across diverse real-world motions and contexts. Our codes, data and the project demo can be found at https://video-motion-customization.github.io",http://arxiv.org/abs/2312.00845v1,,Hyeonho Jeong (Korea Advanced Institute Of Science & Technology) | Geon Yeong Park (Korea Advanced Institute Of Science And Technology) | Jong Chul Ye (Korea Advanced Institute Of Science And Technology),2023-12-01 06:50:11+00:00,,,,,,
PanoRecon: Real-Time Panoptic 3D Reconstruction from Monocular Video,,,,Dong Wu (None) | Zike Yan (Tsinghua University) | Hongbin Zha (Peking University),,,,,,,
G3 -LQ: Marrying Hyperbolic Alignment with Explicit Semantic-Geometric Modeling for 3D Visual Grounding ,,,,Yuan Wang (None) | Yali Li (Tsinghua University) | Shengjin Wang (Tsinghua University),,,,,,,
MICap: A Unified Model for Identity-aware Movie Descriptions,,,,"Haran Raajesh (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Naveen Reddy Desanur (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Zeeshan Khan (INRIA) | Makarand Tapaswi (Wadhwani AI, IIIT Hyderabad)",,,,,,,
MiKASA: Multi-Key-Anchor Scene-Aware Transformer for 3D Visual Grounding,"3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.   The source code and additional resources for this project are available on GitHub: https://github.com/birdy666/MiKASA-3DVG",http://arxiv.org/abs/2403.03077v2,,Chun-Peng Chang (DFKI) | Shaoxiang Wang (German Research Center For AI) | Alain Pagani (German Research Center For Artificial Intelligence (DFKI)) | Didier Stricker (Universit??t Kaiserslautern),2024-03-05 16:01:55+00:00,,,,,,
Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under Manhattan World Assumption,"In orthogonal world coordinates, a Manhattan world lying along cuboid buildings is widely useful for various computer vision tasks. However, the Manhattan world has much room for improvement because the origin of pan angles from an image is arbitrary, that is, four-fold rotational symmetric ambiguity of pan angles. To address this problem, we propose a definition for the pan-angle origin based on the directions of the roads with respect to a camera and the direction of travel. We propose a learning-based calibration method that uses heatmap regression to remove the ambiguity by each direction of labeled image coordinates, similar to pose estimation keypoints. Simultaneously, our two-branched network recovers the rotation and removes fisheye distortion from a general scene image. To alleviate the lack of vanishing points in images, we introduce auxiliary diagonal points that have the optimal 3D arrangement of spatial uniformity. Extensive experiments demonstrated that our method outperforms conventional methods on large-scale datasets and with off-the-shelf cameras.",http://arxiv.org/abs/2303.17166v1,,Nobuhiko Wakai (Panasonic Holdings Corporation) | Satoshi Sato (Panasonic Holdings Corporation) | Yasunori Ishii (Panasonic Holdings Corporation) | Takayoshi Yamashita (Chubu University),2023-03-30 05:57:59+00:00,,,,,,
Scalable 3D Registration via Truncated Entry-wise Absolute Residuals,,,,Tianyu Huang (None) | Liangzu Peng (Johns Hopkins University) | Rene Vidal (Johns Hopkins University) | Yun-Hui Liu (The Chinese University Of Hong Kong),,,,,,,
Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing,"With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. Inspired by the similarity and importance differences between DDS and the contrastive learning for unpaired image-to-image translation (CUT), here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Specifically, to enforce structural correspondence between the input and output while maintaining the controllability of contents, we introduce a straightforward approach to regulate structural consistency using CUT loss within the DDS framework. To calculate this loss, instead of employing auxiliary networks, we utilize the intermediate features of LDM, in particular, those from the self-attention layers, which possesses rich spatial information. Our approach enables zero-shot image-to-image translation and neural radiance field (NeRF) editing, achieving a well-balanced interplay between maintaining the structural details and transforming content. Qualitative results and comparisons demonstrates the effectiveness of our proposed method. Project page with code is available at https://hyelinnam.github.io/CDS/.",http://arxiv.org/abs/2311.18608v1,,Hyelin Nam (Korea Advanced Institute Of Science & Technology) | Gihyun Kwon (Korea Advanced Institute Of Science & Technology) | Geon Yeong Park (Korea Advanced Institute Of Science And Technology) | Jong Chul Ye (Korea Advanced Institute Of Science And Technology),2023-11-30 15:06:10+00:00,,,,,,
Noisy One Point Homographies are Surprisingly Good,,,,Yaqing Ding (None) | Jonathan Astermark (Lund University) | Magnus Oskarsson (Lund University) | Viktor Larsson (Lund University),,,,,,,
GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs,,,,"Mustafa Munir (The University Of Texas At Austin) | William Avery (None) | Md Mostafijur Rahman (University Of Texas At Austin) | Radu Marculescu (University Of Texas, Austin)",,,,,,,
Adaptive VIO: Deep Visual-Inertial Odometry with Online Continual Learning,,,,Youqi Pan (Peking University) | Wugen Zhou (Peking University) | Yingdian Cao (Peking University) | Hongbin Zha (Peking University),,,,,,,
Exploring Pose-Aware Human-Object Interaction via Hybrid Learning,,,,EASTMAN Z Y WU (Tsinghua University) | Yali Li (Tsinghua University) | Yuan Wang (None) | Shengjin Wang (Tsinghua University),,,,,,,
Bilateral Adaptation for Human-Object Interaction Detection with Occlusion-Robustness,,,,Guangzhi Wang (National University Of Singapore) | Yangyang Guo (National University Of Singapore) | Ziwei Xu (National University Of Singapore) | Mohan Kankanhalli (National University Of Singapore),,,,,,,
PIN: Positional Insert unlocks object localisation abilities in VLMs,,,,Michael Dorkenwald (University Of Amsterdam) | Nimrod Barazani (University Of Amsterdam) | Cees G. M. Snoek (University Of Amsterdam) | Yuki Asano (University Of Amsterdam),,,,,,,
As-Plausible-As-Possible: Semantic-Aware Shape Deformation using 2D Diffusion Priors,"We present As-Plausible-as-Possible (APAP) mesh deformation technique that leverages 2D diffusion priors to preserve the plausibility of a mesh under user-controlled deformation. Our framework uses per-face Jacobians to represent mesh deformations, where mesh vertex coordinates are computed via a differentiable Poisson Solve. The deformed mesh is rendered, and the resulting 2D image is used in the Score Distillation Sampling (SDS) process, which enables extracting meaningful plausibility priors from a pretrained 2D diffusion model. To better preserve the identity of the edited mesh, we fine-tune our 2D diffusion model with LoRA. Gradients extracted by SDS and a user-prescribed handle displacement are then backpropagated to the per-face Jacobians, and we use iterative gradient descent to compute the final deformation that balances between the user edit and the output plausibility. We evaluate our method with 2D and 3D meshes and demonstrate qualitative and quantitative improvements when using plausibility priors over geometry-preservation or distortion-minimization priors used by previous techniques.",http://arxiv.org/abs/2311.16739v1,,Seungwoo Yoo (Korea Advanced Institute Of Science And Technology (KAIST)) | Kunho Kim (Korea Advanced Institute Of Science & Technology) | Vladimir G. Kim (Adobe Systems) | Minhyuk Sung (KAIST),2023-11-28 12:35:13+00:00,,,,,,
Referring Expression Counting,,,,Siyang Dai (Singapore University Of Technology And Design) | Jun Liu (None) | Ngai-Man Cheung (Singapore University Of Technology And Design),,,,,,,
Low-Resource Vision Challenges for Foundation Models,,,,Yunhua Zhang (University Of Amsterdam) | Hazel Doughty (Leiden University) | Cees G. M. Snoek (University Of Amsterdam),,,,,,,
Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes,,,,"Takashi Otonari (The University Of Tokyo) | Satoshi Ikehata (NII, Tokyo Institute Of Technology) | Kiyoharu Aizawa (The University Of Tokyo)",,,,,,,
OmniLocalRF: Omnidirectional Local Radiance Fields from Dynamic Videos,,,,Dongyoung Choi (Korea Advanced Institute Of Science And Technology) | Hyeonjoong Jang (None) | Min H. Kim (KAIST),,,,,,,
Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair,,,,Jeonghoon Park (Korea Advanced Institute Of Science And Technology) | Chaeyeon Chung (Korea Advanced Institute Of Science And Technology) | Jaegul Choo (Korea Advanced Institute Of Science And Technology),,,,,,,
Kernel Adaptive Convolution for Scene Text Detection via Distance Map Prediction,,,,Jinzhi Zheng (University Of Chinese Academy Of Sciences) | Heng Fan (University Of North Texas) | Libo Zhang (Institute Of Software Chinese Academy Of Sciences),,,,,,,
"WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concept","Recent advancements in neural networks have showcased their remarkable capabilities across various domains. Despite these successes, the ""black box"" problem still remains. Addressing this, we propose a novel framework, WWW, that offers the 'what', 'where', and 'why' of the neural network decisions in human-understandable terms. Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain 'what'. To address the 'where' and 'why', we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs. Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate 'how' reliable the prediction is. Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability. WWW provides a unified solution for explaining 'what', 'where', and 'why', introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures.",http://arxiv.org/abs/2402.18956v1,,Yong Hyun Ahn (Kyung Hee University) | Hyeon Kim (Kyunghee University) | Seong Tae Kim (Kyung Hee University),2024-02-29 08:51:51+00:00,,,,,,
ElasticDiffusion: Training-free Arbitrary Size Image Generation,"Diffusion models have revolutionized image generation in recent years, yet they are still limited to a few sizes and aspect ratios. We propose ElasticDiffusion, a novel training-free decoding method that enables pretrained text-to-image diffusion models to generate images with various sizes. ElasticDiffusion attempts to decouple the generation trajectory of a pretrained model into local and global signals. The local signal controls low-level pixel information and can be estimated on local patches, while the global signal is used to maintain overall structural consistency and is estimated with a reference image. We test our method on CelebA-HQ (faces) and LAION-COCO (objects/indoor/outdoor scenes). Our experiments and qualitative results show superior image coherence quality across aspect ratios compared to MultiDiffusion and the standard decoding strategy of Stable Diffusion. Code: https://github.com/MoayedHajiAli/ElasticDiffusion-official.git",http://arxiv.org/abs/2311.18822v1,,Moayed Haji Ali (Rice University) | Guha Balakrishnan (Rice University) | Vicente Ordonez (Rice University),2023-11-30 18:58:17+00:00,,,,,,
Using ViT Embeddings is Better than Pseudo Multimodal Approach: A New State-of-the-art for Single Image Depth Estimation,,,,"Suraj Patni (Indian Institute Of Technology, Delhi) | Aradhye Agarwal (Indian Institute Of Technology Delhi) | Chetan Arora (Indian Institute Of Technology Delhi)",,,,,,,
Unified Entropy Optimization for Open-Set Test-Time Adaptation,,,,"Zhengqing Gao (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xu-Yao Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Cheng-Lin Liu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",,,,,,,
AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of Image Deblurring,,,,Xintian Mao (East China Normal University) | Xiwen Gao (East China Normal University) | Yan Wang (East China Normal University),,,,,,,
MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior,,,,Honghua Chen (National Technological University) | Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) | Xingang Pan (None),,,,,,,
TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model,"Prompt tuning represents a valuable technique for adapting pre-trained visual-language models (VLM) to various downstream tasks. Recent advancements in CoOp-based methods propose a set of learnable domain-shared or image-conditional textual tokens to facilitate the generation of task-specific textual classifiers. However, those textual tokens have a limited generalization ability regarding unseen domains, as they cannot dynamically adjust to the distribution of testing classes. To tackle this issue, we present a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly incorporates prior knowledge about classes to enhance their discriminability. The critical concept of TCP involves leveraging Textual Knowledge Embedding (TKE) to map the high generalizability of class-level textual knowledge into class-aware textual tokens. By seamlessly integrating these class-aware prompts into the Text Encoder, a dynamic class-aware classifier is generated to enhance discriminability for unseen domains. During inference, TKE dynamically generates class-aware prompts related to the unseen classes. Comprehensive evaluations demonstrate that TKE serves as a plug-and-play module effortlessly combinable with existing methods. Furthermore, TCP consistently achieves superior performance while demanding less training time. Code:https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/",http://arxiv.org/abs/2311.18231v2,,Hantao Yao (None) | Rui Zhang (None) | Changsheng Xu (None),2023-11-30 03:59:23+00:00,,,,,,
GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds,,,,Shengjun Zhang (Tsinghua University) | Xin Fei (Tsinghua University) | Yueqi Duan (None),,,,,,,
A Bayesian Approach to OOD Robustness in Image Classification,"An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset).",http://arxiv.org/abs/2403.07277v1,,Prakhar Kaushik (Johns Hopkins University) | Adam Kortylewski (University Of Freiburg & MPI-INF) | Alan L. Yuille (Johns Hopkins University),2024-03-12 03:15:08+00:00,,,,,,
Modality-Collaborative Test-Time Adaptation for Action Recognition,,,,"Baochen Xiong (Institute Of Automation, Chinese Academy Of Sciences; Peng Cheng Lab) | Xiaoshan Yang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yaguang Song (Peng Cheng Laboratory) | Yaowei Wang (Pengcheng Laboratory) | Changsheng Xu (None)",,,,,,,
Any-Shift Prompting for Generalization over Distributions,"Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.",http://arxiv.org/abs/2402.10099v1,,Zehao Xiao (University Of Amsterdam) | Jiayi Shen (University Of Amsterdam) | Mohammad Mahdi Derakhshani (University Of Amsterdam) | Shengcai Liao (Inception Institute Of Artificial Intelligence) | Cees G. M. Snoek (University Of Amsterdam),2024-02-15 16:53:42+00:00,,,,,,
StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On,"Given a clothing image and a person image, an image-based virtual try-on aims to generate a customized image that appears natural and accurately reflects the characteristics of the clothing image. In this work, we aim to expand the applicability of the pre-trained diffusion model so that it can be utilized independently for the virtual try-on task.The main challenge is to preserve the clothing details while effectively utilizing the robust generative capability of the pre-trained model. In order to tackle these issues, we propose StableVITON, learning the semantic correspondence between the clothing and the human body within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more precise representation of clothing details. StableVITON outperforms the baselines in qualitative and quantitative evaluation, showing promising quality in arbitrary person images. Our code is available at https://github.com/rlawjdghek/StableVITON.",http://arxiv.org/abs/2312.01725v1,,Jeongho Kim (KAIST) | Gyojung Gu (Korea Advanced Institute Of Science And Technology) | Minho Park (KAIST) | Sunghyun Park (KAIST) | Jaegul Choo (Korea Advanced Institute Of Science And Technology),2023-12-04 08:27:59+00:00,,,,,,
Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval,,,,Minkuk Kim (Kyung Hee University) | Hyeon Kim (Kyunghee University) | Jinyoung Moon (ETRI) | Jinwoo Choi (Kyung Hee University) | Seong Tae Kim (Kyung Hee University),,,,,,,
Improved Visual Grounding through Self-Consistent Explanations,"Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --""grounding""-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average).",http://arxiv.org/abs/2312.04554v1,,Ruozhen He (Rice University) | Paola Cascante-Bonilla (Rice University) | Ziyan Yang (Rice University) | Alex Berg (None) | Vicente Ordonez (Rice University),2023-12-07 18:59:22+00:00,,,,,,
DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data,,,,Qihao Liu (Johns Hopkins University) | Yi Zhang (Sony Corporation Of America) | Song Bai (ByteDance) | Adam Kortylewski (University Of Freiburg & MPI-INF) | Alan L. Yuille (Johns Hopkins University),,,,,,,
Retrieval-Augmentated Layout Transformer for Content-Aware Layout Generation,,,,Daichi Horita (None) | Naoto Inoue (CyberAgent) | Kotaro Kikuchi (None) | Kota Yamaguchi (CyberAgent) | Kiyoharu Aizawa (The University Of Tokyo),,,,,,,
FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders,"In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (US) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcomings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information regions, fostering a more refined representation of malignancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem, against an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer, and 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CT-based Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines. The source code and pretrained models are available at: https://github.com/sbasu276/FocusMAE.",http://arxiv.org/abs/2403.08848v1,,"Soumen Basu (Indian Institute Of Technology Delhi) | Mayuna Gupta (Indian Institute Of Technology, Delhi) | Chetan Madan (Indian Institute Of Technology, Delhi) | Pankaj Gupta (PGIMER Chandigarh) | Chetan Arora (Indian Institute Of Technology Delhi)",2024-03-13 16:57:04+00:00,,,,,,
Active Generalized Category Discovery,"Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes. Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime. Hence, some annotations of new classes are deemed necessary. However, labeling new classes is extremely costly. To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD). The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle. To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty. However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training. To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages. Our method achieves state-of-the-art performance on both generic and fine-grained datasets. Our code is available at https://github.com/mashijie1028/ActiveGCD",http://arxiv.org/abs/2403.04272v1,,"Shijie Ma (Institute Of Automation, Chinese Academy Of Sciences) | Fei Zhu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhun Zhong (University Of Nottingham) | Xu-Yao Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Cheng-Lin Liu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2024-03-07 07:12:24+00:00,,,,,,
Model Inversion Robustness: Can Transfer Learning Help?,,,,Sy-Tuyen Ho (Singapore University Of Technology And Design) | Koh Jun Hao (Singapore University Of Technology And Design) | Keshigeyan Chandrasegaran (Stanford University) | Ngoc-Bao Nguyen (Singapore University Of Technology And Design) | Ngai-Man Cheung (Singapore University Of Technology And Design),,,,,,,
Context-Guided Spatio-Temporal Video Grounding,,,,"Xin Gu (None) | Heng Fan (University Of North Texas) | Yan Huang (, University Of North Texas) | Tiejian Luo (University Of The Chinese Academy Of Sciences) | Libo Zhang (Institute Of Software Chinese Academy Of Sciences)",,,,,,,
Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models,"While Multi-modal Language Models (MLMs) demonstrate impressive multimodal ability, they still struggle on providing factual and precise responses for tasks like visual question answering (VQA). In this paper, we address this challenge from the perspective of contextual information. We propose Causal Context Generation, Causal-CoG, which is a prompting strategy that engages contextual information to enhance precise VQA during inference. Specifically, we prompt MLMs to generate contexts, i.e, text description of an image, and engage the generated contexts for question answering. Moreover, we investigate the advantage of contexts on VQA from a causality perspective, introducing causality filtering to select samples for which contextual information is helpful. To show the effectiveness of Causal-CoG, we run extensive experiments on 10 multimodal benchmarks and show consistent improvements, e.g., +6.30% on POPE, +13.69% on Vizwiz and +6.43% on VQAv2 compared to direct decoding, surpassing existing methods. We hope Casual-CoG inspires explorations of context knowledge in multimodal models, and serves as a plug-and-play strategy for MLM decoding.",http://arxiv.org/abs/2312.06685v1,,Shitian Zhao (East China Normal University) | Zhuowan Li (Johns Hopkins University) | YadongLu (ECNU) | Alan L. Yuille (Johns Hopkins University) | Yan Wang (East China Normal University),2023-12-09 08:44:41+00:00,,,,,,
OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees,,,,Hakyeong Kim (Korea Advanced Institute Of Science And Technology) | Andreas Meuleman (Korea Advanced Institute Of Science And Technology) | Hyeonjoong Jang (None) | James Tompkin (Brown University) | Min H. Kim (KAIST),,,,,,,
DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing,"Diffusion models have achieved remarkable image generation quality surpassing previous generative models. However, a notable limitation of diffusion models, in comparison to GANs, is their difficulty in smoothly interpolating between two image samples, due to their highly unstructured latent space. Such a smooth interpolation is intriguing as it naturally serves as a solution for the image morphing task with many applications. In this work, we present DiffMorpher, the first approach enabling smooth and natural image interpolation using diffusion models. Our key idea is to capture the semantics of the two images by fitting two LoRAs to them respectively, and interpolate between both the LoRA parameters and the latent noises to ensure a smooth semantic transition, where correspondence automatically emerges without the need for annotation. In addition, we propose an attention interpolation and injection technique and a new sampling schedule to further enhance the smoothness between consecutive images. Extensive experiments demonstrate that DiffMorpher achieves starkly better image morphing effects than previous methods across a variety of object categories, bridging a critical functional gap that distinguished diffusion models from GANs.",http://arxiv.org/abs/2312.07409v1,,Kaiwen Zhang (Tsinghua University) | Yifan Zhou (Nanyang Technological University) | Xudong XU (Shanghai AI Laboratory) | Bo Dai (Shanghai AI Laboratory) | Xingang Pan (None),2023-12-12 16:28:08+00:00,,,,,,
Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior,"Recently, 3D content creation from text prompts has demonstrated remarkable progress by utilizing 2D and 3D diffusion models. While 3D diffusion models ensure great multi-view consistency, their ability to generate high-quality and diverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion models find a distillation approach that achieves excellent generalization and rich details without any 3D data. However, 2D lifting methods suffer from inherent view-agnostic ambiguity thereby leading to serious multi-face Janus issues, where text prompts fail to provide sufficient guidance to learn coherent 3D results. Instead of retraining a costly viewpoint-aware model, we study how to fully exploit easily accessible coarse 3D knowledge to enhance the prompts and guide 2D lifting optimization for refinement. In this paper, we propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Specifically, we design a pair of guiding strategies derived from the coarse 3D prior generated by the 3D diffusion model: a structural guidance for geometric fidelity and a semantic guidance for 3D coherence. Employing the two types of guidance, the 2D diffusion model enriches the 3D content with diversified and high-quality results. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.",http://arxiv.org/abs/2312.06655v1,,Fangfu Liu (Tsinghua University) | Diankun Wu (Tsinghua University) | Yi Wei (None) | Yongming Rao (Tsinghua University) | Yueqi Duan (None),2023-12-11 18:59:18+00:00,,,,,,
Improved Self-Training for Test-Time Adaptation,,,,Jing Ma (None),,,,,,,
HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding,"Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. To achieve this goal, we first present a new dataset containing Appearance-Situation-Position-Interaction-Relation predicates, named ASPIRe, offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named Hierarchical Interlacement Graph (HIG), which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.",http://arxiv.org/abs/2312.03050v3,,Trong-Thuan Nguyen (University Of Arkansas) | Pha Nguyen (University Of Arkansas) | Khoa Luu (University Of Arkansas),2023-12-05 18:47:19+00:00,,,,,,
3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces,"This paper introduces 3DFIRES, a novel system for scene-level 3D reconstruction from posed images. Designed to work with as few as one view, 3DFIRES reconstructs the complete geometry of unseen scenes, including hidden surfaces. With multiple view inputs, our method produces full reconstruction within all camera frustums. A key feature of our approach is the fusion of multi-view information at the feature level, enabling the production of coherent and comprehensive 3D reconstruction. We train our system on non-watertight scans from large-scale real scene dataset. We show it matches the efficacy of single-view reconstruction methods with only one input and surpasses existing techniques in both quantitative and qualitative measures for sparse-view 3D reconstruction.",http://arxiv.org/abs/2403.08768v1,,Linyi Jin (None) | Nilesh Kulkarni (None) | David Fouhey (New York University),2024-03-13 17:59:50+00:00,,,,,,
CoDe: An Explicit Content Decoupling Framework for Image Restoration,,,,Enxuan Gu (Dalian University Of Technology) | Hongwei Ge (Dalian University Of Technology) | Yong Guo (Max-Planck Institute For Informatics),,,,,,,
LoCoNet: Long-Short Context Network for Active Speaker Detection,"Active Speaker Detection (ASD) aims to identify who is speaking in each frame of a video. ASD reasons from audio and visual information from two contexts: long-term intra-speaker context and short-term inter-speaker context. Long-term intra-speaker context models the temporal dependencies of the same speaker, while short-term inter-speaker context models the interactions of speakers in the same scene. These two contexts are complementary to each other and can help infer the active speaker. Motivated by these observations, we propose LoCoNet, a simple yet effective Long-Short Context Network that models the long-term intra-speaker context and short-term inter-speaker context. We use self-attention to model long-term intra-speaker context due to its effectiveness in modeling long-range dependencies, and convolutional blocks that capture local patterns to model short-term inter-speaker context. Extensive experiments show that LoCoNet achieves state-of-the-art performance on multiple datasets, achieving an mAP of 95.2%(+1.1%) on AVA-ActiveSpeaker, 68.1%(+22%) on Columbia dataset, 97.2%(+2.8%) on Talkies dataset and 59.7%(+8.0%) on Ego4D dataset. Moreover, in challenging cases where multiple speakers are present, or face of active speaker is much smaller than other faces in the same scene, LoCoNet outperforms previous state-of-the-art methods by 3.4% on the AVA-ActiveSpeaker dataset. The code will be released at https://github.com/SJTUwxz/LoCoNet_ASD.",http://arxiv.org/abs/2301.08237v1,,"Xizi Wang (Indiana University, Bloomington) | Feng Cheng (University Of North Carolina At Chapel Hill) | Gedas Bertasius (UNC Chapel Hill)",2023-01-19 18:54:43+00:00,,,,,,
MVCPS-NeuS: Multi-view Constrained Photometric Stereo for Neural Surface Reconstruction,,,,Hiroaki Santo (Osaka University) | Fumio Okura (Osaka University) | Yasuyuki Matsushita (Osaka University),,,,,,,
HIPTrack: Visual Tracking with Historical Prompts,,,,Wenrui Cai (Beihang University) | Qingjie Liu (None) | Yunhong Wang (Beihang University),,,,,,,
Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features,"We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds). Our method distills diffusion features from image foundational models onto input shapes. Specifically, we use the input shapes to produce depth and normal maps as guidance for conditional image synthesis, and in the process produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface. Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, the associated image features are robust and can be directly aggregated across views. This produces semantic features on the input shapes, without requiring additional data or training. We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometeric and non-isometrically related shape families.",http://arxiv.org/abs/2311.17024v1,,"Niladri Shekhar Dutt (Ready Player Me) | Sanjeev Muralikrishnan (University College London, University Of London) | Niloy J. Mitra (University College London)",2023-11-28 18:27:15+00:00,,,,,,
Selective View Pipelining: an Efficient Approach for Multi-view Understanding,,,,Yunzhong Hou (Australian National University) | Stephen Gould (Australian National University) | Liang Zheng (Australian National University),,,,,,,
Garment Recovery with Shape and Deformation Priors,"While modeling people wearing tight-fitting clothing has made great strides in recent years, loose-fitting clothing remains a challenge. We propose a method that delivers realistic garment models from real-world images, regardless of garment shape or deformation. To this end, we introduce a fitting approach that utilizes shape and deformation priors learned from synthetic data to accurately capture garment shapes and deformations, including large ones. Not only does our approach recover the garment geometry accurately, it also yields models that can be directly used by downstream applications such as animation and simulation.",http://arxiv.org/abs/2311.10356v2,,Ren Li (EPFL) | Corentin Dumery (EPFL) | Beno??t Guillard (Swiss Federal Institute Of Technology Lausanne) | Pascal Fua (Swiss Federal Institute Of Technology Lausanne),2023-11-17 07:06:21+00:00,,,,,,
CLOAF: CoLlisiOn-Aware Human Flow,"Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to fine-tune pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion field induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.",http://arxiv.org/abs/2403.09050v1,,Andrey Davydov (EPFL) | Martin Engilberge (EPFL - EPF Lausanne) | Mathieu Salzmann (EPFL) | Pascal Fua (Swiss Federal Institute Of Technology Lausanne),2024-03-14 02:38:09+00:00,,,,,,
Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis,,,,Atefeh Khoshkhahtinat (None) | Ali Zafari (West Virginia University) | Piyush Mehta (West Virginia University) | Nasser Nasrabadi (West Virginia University),,,,,,,
"Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models",,,,Luo Jiayun (Nanyang Technological University) | Siddhesh Khandelwal (None) | Leonid Sigal (University Of British Columbia) | Boyang Li (Nanyang Technological University),,,,,,,
CDFormer: When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution,,,,"Qingguo Liu (Nanjing University Of Aeronautics And Astronautics) | Chenyi Zhuang (Nanjing University Of Aeronautics And Astronautics) | Pan Gao (Nanjing University Of Aeronautics And Astronautics, Tsinghua University) | Jie Qin (Nanjing University Of Aeronautics And Astronautics)",,,,,,,
Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models,"Most diffusion models assume that the reverse process adheres to a Gaussian distribution. However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1. Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness. We primarily focus on tackling singularities from both theoretical and practical perspectives. Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps. Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property. Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of diffusion models without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores. Code and models are released at https://github.com/PangzeCheung/SingDiffusion.",http://arxiv.org/abs/2403.08381v1,,Pengze Zhang (Sun Yat-Sen University) | Hubery Yin (Tencent) | Chen Li (Tencent) | Xiaohua Xie (SUN YAT-SEN UNIVERSITY),2024-03-13 09:47:04+00:00,,,,,,
Improving Generalized Zero-Shot Learning by Exploring the Diverse Semantics from External Class Names,,,,Yapeng Li (Wuhan University) | Yong Luo (Wuhan University) | Zengmao Wang (Wuhan University) | Bo Du (Wuhan University),,,,,,,
Revisiting Global Translation Estimation with Feature Tracks,,,,"Peilin Tao (None) | Hainan Cui (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Mengqi Rong (, Institute Of Automation, Chinese Academy Of Science) | Shuhan Shen (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction,"The task of multimodal relation extraction has attracted significant research attention, but progress is constrained by the scarcity of available training data. One natural thought is to extend existing datasets with cross-modal generative models. In this paper, we consider a novel problem setting, where only unimodal data, either text or image, are available during training. We aim to train a multimodal classifier from synthetic data that perform well on real multimodal test data. However, training with synthetic data suffers from two obstacles: lack of data diversity and label information loss. To alleviate the issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to promote diversity in the generated data and exploits a teacher network to select valuable training samples with high mutual information with the ground-truth labels. Comparing our method to direct training on synthetic data, we observed a significant improvement of 24.06% F1 with synthetic text and 26.42% F1 with synthetic images. Notably, our best model trained on completely synthetic images outperforms prior state-of-the-art models trained on real multimodal data by a margin of 3.76% in F1. Our codebase will be made available upon acceptance.",http://arxiv.org/abs/2312.03025v1,,Zilin Du (Nanyang Technological University) | Haoxin Li (Nanyang Technological University) | Xu Guo (Nanyang Technological University) | Boyang Li (Nanyang Technological University),2023-12-05 08:11:34+00:00,,,,,,
PanoPose: Self-supervised Relative Pose Estimation for Panoramic Images,,,,"Diantao Tu (None) | Hainan Cui (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xianwei Zheng (Wuhan University) | Shuhan Shen (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
MMA: Multi-Modal Adapter for Vision-Language Models,,,,Lingxiao Yang (SUN YAT-SEN UNIVERSITY) | Ru-Yuan Zhang (None) | Yanchen Wang (Stanford University) | Xiaohua Xie (SUN YAT-SEN UNIVERSITY),,,,,,,
Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network,,,,"Sizhe Zheng (None) | Pan Gao (Nanjing University Of Aeronautics And Astronautics, Tsinghua University) | Peng Zhou (China Mobile (Suzhou) Software Technology Co., Ltd, China) | Jie Qin (Nanjing University Of Aeronautics And Astronautics)",,,,,,,
All in One Framework for Multimodal Re-identification in the Wild,,,,He Li (Wuhan University) | Mang Ye (Wuhan University) | Ming Zhang (Guangzhou Urban Planning & Design Survey Research Institute) | Bo Du (Wuhan University),,,,,,,
Steerers: A framework for rotation equivariant keypoint descriptors,"Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. We instead learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in all of these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish code and model weights at github.com/georg-bn/rotation-steerers.",http://arxiv.org/abs/2312.02152v1,,"Georg B??kman (Chalmers University Of Technology) | Johan Edstedt (Computer Vision Laboratory, Link??ping University) | Michael Felsberg (Link??ping University) | Fredrik Kahl (Chalmers University)",2023-12-04 18:59:44+00:00,,,,,,
Hyperspherical Classification with Dynamic Label-to-Prototype Assignment,,,,Mohammad Saadabadi Saadabadi (None) | Ali Dabouei (None) | Sahar Rahimi Malakshan (West Virginia University) | Nasser Nasrabadi (West Virginia University),,,,,,,
High Fidelity Person-centric Subject-to-Image Synthesis,,,,Yibin Wang (None) | Weizhong Zhang (Fudan University) | Jianwei Zheng (Zhejiang University Of Technology) | Cheng Jin (Fudan University),,,,,,,
Learning Structure-from-Motion with Graph Attention Networks,,,,Lucas Brynte (None) | Jos?? Pedro Iglesias (None) | Carl Olsson (Lund University) | Fredrik Kahl (Chalmers University),,,,,,,
PoseIRM: Enhance 3D Human Pose Estimation on Unseen Camera Settings via Invariant Risk Minimization,,,,Yanlu Cai (Fudan University) | Weizhong Zhang (Fudan University) | Yuan Wu (Fudan University) | Cheng Jin (Fudan University),,,,,,,
G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis,,,,Yufei Ye (Carnegie Mellon University) | Abhinav Gupta (Carnegie Mellon University) | Kris Kitani (Carnegie Mellon University) | Shubham Tulsiani (Carnegie Mellon University),,,,,,,
MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation,,,,Hanzhe Hu (Carnegie Mellon University) | Zhizhuo Zhou (Stanford University) | Varun Jampani (Google Research) | Shubham Tulsiani (Carnegie Mellon University),,,,,,,
Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions,,,,Runhao Zeng (Shenzhen University) | Xiaoyong Chen (Shenzhen University) | Jiaming Liang (Shenzhen University) | Huisi Wu (Shenzhen University) | Guang-Zhong Cao (Shenzhen University) | Yong Guo (Max-Planck Institute For Informatics),,,,,,,
"FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation","Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.",http://arxiv.org/abs/2403.03221v1,,Chris Rockwell (University Of Michigan) | Nilesh Kulkarni (None) | Linyi Jin (None) | Jeong Joon Park (Stanford University) | Justin Johnson (University Of Michigan) | David Fouhey (New York University),2024-03-05 18:59:51+00:00,,,,,,
Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding,"In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel ""Insect-1M"" dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identification labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efficiently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.",http://arxiv.org/abs/2311.15206v2,,Hoang-Quan Nguyen (University Of Arkansas - Fayetteville) | Thanh-Dat Truong (University Of Arkansas) | Xuan-Bac Nguyen (None) | Ashley Dowling (University Of Arkansas - Fayetteville) | Xin Li (State University Of New York At Albany) | Khoa Luu (University Of Arkansas),2023-11-26 06:17:29+00:00,,,,,,
SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control,"The field of generative image inpainting and object insertion has made significant progress with the recent advent of latent diffusion models. Utilizing a precise object mask can greatly enhance these applications. However, due to the challenges users encounter in creating high-fidelity masks, there is a tendency for these methods to rely on more coarse masks (e.g., bounding box) for these applications. This results in limited control and compromised background content preservation. To overcome these limitations, we introduce SmartMask, which allows any novice user to create detailed masks for precise object insertion. Combined with a ControlNet-Inpaint model, our experiments demonstrate that SmartMask achieves superior object insertion quality, preserving the background content more effectively than previous methods. Notably, unlike prior works the proposed approach can also be used even without user-mask guidance, which allows it to perform mask-free object insertion at diverse positions and scales. Furthermore, we find that when used iteratively with a novel instruction-tuning based planning model, SmartMask can be used to design detailed layouts from scratch. As compared with user-scribble based layout design, we observe that SmartMask allows for better quality outputs with layout-to-image generation methods. Project page is available at https://smartmask-gen.github.io",http://arxiv.org/abs/2312.05039v1,,Jaskirat Singh (Australian National University) | Jianming Zhang (Adobe Systems) | Qing Liu (Adobe Systems) | Cameron Smith (Adobe Systems) | Zhe Lin (Adobe Research) | Liang Zheng (Australian National University),2023-12-08 13:38:22+00:00,,,,,,
Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D,"Diffusion Handles is a novel approach to enabling 3D object edits on diffusion images. We accomplish these edits using existing pre-trained diffusion models, and 2D image depth estimation, without any fine-tuning or 3D object retrieval. The edited results remain plausible, photo-real, and preserve object identity. Diffusion Handles address a critically missing facet of generative image based creative design, and significantly advance the state-of-the-art in generative image editing. Our key insight is to lift diffusion activations for an object to 3D using a proxy depth, 3D-transform the depth and associated activations, and project them back to image space. The diffusion process applied to the manipulated activations with identity control, produces plausible edited images showing complex 3D occlusion and lighting effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic data benchmark; and qualitatively by a user study, showing our output to be more plausible, and better than prior art at both, 3D editing and identity control. Project Webpage: https://diffusionhandles.github.io/",http://arxiv.org/abs/2312.02190v2,,Karran Pandey (University Of Toronto) | Paul Guerrero (Adobe Systems) | Matheus Gadelha (Adobe Systems) | Yannick Hold-Geoffroy (Adobe Research) | Karan Singh (Department Of Computer Science) | Niloy J. Mitra (University College London),2023-12-02 03:29:39+00:00,,,,,,
DiLiGenRT: A Photometric Stereo Dataset with Quantified Roughness and Translucency,,,,Heng Guo (Beijing University Of Posts And Telecommunications) | Jieji Ren (Shanghai Jiao Tong University) | Feishi Wang (Peking University) | Boxin Shi (Peking University) | Mingjun Ren (Shanghai Jiao Tong University) | Yasuyuki Matsushita (Osaka University),,,,,,,
ActiveDC: Distribution Calibration for Active Finetuning,"The pretraining-finetuning paradigm has gained popularity in various computer vision tasks. In this paradigm, the emergence of active finetuning arises due to the abundance of large-scale data and costly annotation requirements. Active finetuning involves selecting a subset of data from an unlabeled pool for annotation, facilitating subsequent finetuning. However, the use of a limited number of training samples can lead to a biased distribution, potentially resulting in model overfitting. In this paper, we propose a new method called ActiveDC for the active finetuning tasks. Firstly, we select samples for annotation by optimizing the distribution similarity between the subset to be selected and the entire unlabeled pool in continuous space. Secondly, we calibrate the distribution of the selected samples by exploiting implicit category information in the unlabeled pool. The feature visualization provides an intuitive sense of the effectiveness of our approach to distribution calibration. We conducted extensive experiments on three image classification datasets with different sampling ratios. The results indicate that ActiveDC consistently outperforms the baseline performance in all image classification tasks. The improvement is particularly significant when the sampling ratio is low, with performance gains of up to 10%. Our code will be released.",http://arxiv.org/abs/2311.07634v3,,"Wenshuai Xu (None) | Zhenghui Hu (Hangzhou Innovation Institute, Beihang University) | Yu Lu (Beijing University Of Aeronautics And Astronautics) | Jinzhou Meng (Beijing University Of Aeronautics And Astronautics) | Qingjie Liu (None) | Yunhong Wang (Beihang University)",2023-11-13 14:35:18+00:00,,,,,,
Video ReCap: Recursive Captioning of Hour-Long Videos,"Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap",http://arxiv.org/abs/2402.13250v3,,Md Mohaiminul Islam (UNC Chapel Hill) | Vu Bao Ngan Ho (University Of North Carolina At Chapel Hill) | Xitong Yang (Meta) | Tushar Nagarajan (Meta) | Lorenzo Torresani (Facebook) | Gedas Bertasius (UNC Chapel Hill),2024-02-20 18:58:54+00:00,,,,,,
SIGNeRF: Scene Integrated Generation for Neural Radiance Fields,"Advances in image diffusion models have recently led to notable improvements in the generation of high-quality images. In combination with Neural Radiance Fields (NeRFs), they enabled new opportunities in 3D generation. However, most generative 3D approaches are object-centric and applying them to editing existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel approach for fast and controllable NeRF scene editing and scene-integrated object generation. A new generative update strategy ensures 3D consistency across the edited images, without requiring iterative optimization. We find that depth-conditioned diffusion models inherently possess the capability to generate 3D consistent views by requesting a grid of images instead of single views. Based on these insights, we introduce a multi-view reference sheet of modified images. Our method updates an image collection consistently based on the reference sheet and refines the original NeRF with the newly generated image set in one go. By exploiting the depth conditioning mechanism of the image diffusion model, we gain fine control over the spatial location of the edit and enforce shape guidance by a selected region or an external mesh.",http://arxiv.org/abs/2401.01647v1,,Jan-Niklas Dihlmann (Eberhard-Karls-Universit??t T??bingen) | Andreas Engelhardt (University Of T??bingen) | Hendrik Lensch (University Of T??bingen),2024-01-03 09:46:43+00:00,,,,,,
DiffusionTrack: Point Set Diffusion Model for Visual Object Tracking,,,,Fei Xie (None) | Zhongdao Wang (Huawei Technologies Ltd.) | Chao Ma (Shanghai Jiao Tong University),,,,,,,
DualAD: Disentangling the Dynamic and Static World for End-to-End Driving,,,,Simon Doll (Eberhard-Karls-Universit??t T??bingen) | Niklas Hanselmann (Mercedes Benz Research & Development) | Lukas Schneider (Mercedes Benz Research & Development) | Richard Schulz (Mercedes Benz AG) | Marius Cordts (Mercedes-Benz) | Markus Enzweiler (Esslingen University Of Applied Sciences) | Hendrik Lensch (University Of T??bingen),,,,,,,
SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction,,,,"Pin Tang (Shanghai Jiao Tong University) | Zhongdao Wang (Huawei Technologies Ltd.) | Guoqing Wang (Shanghai Jiao Tong University) | Jilai Zheng (Shanghai Jiao Tong University) | Xiangxuan Ren (Shanghai Jiao Tong University) | Bailan Feng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Chao Ma (Shanghai Jiao Tong University)",,,,,,,
FaceLift: Semi-supervised 3D Facial Landmark Localization,,,,David Ferman (Flawless AI) | Pablo Garrido (Flawless AI) | Gaurav Bharaj (Flawless AI),,,,,,,
AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection,,,,"Trevine Oorloff (University Of Maryland, College Park) | Surya Koppisetti (Reality Defender Inc) | Nicolo Bonettini (Reality Defender) | Divyaraj Solanki (Reality Defender) | Ben Colman (Reality Defender) | Yaser Yacoob (University Of Maryland, College Park) | Ali Shahriyari (Reality Defender) | Gaurav Bharaj (Flawless AI)",,,,,,,
SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation,,,,Aysim Toker (Technical University Munich) | Marvin Eisenberger (Technical University Munich) | Daniel Cremers (Technical University Munich) | Laura Leal-Taixe (NVIDIA),,,,,,,
Adaptive Bidirectional Displacement for Semi-Supervised Medical Image Segmentation,,,,Hanyang Chi (None) | Jian Pang (China University Of Petroleum (East China)) | Bingfeng Zhang (China University Of Petroleum (East China)) | Weifeng Liu (China University Of Petroleum (East China)),,,,,,,
Geometrically-informed aggregation for zero-shot point cloud understanding,"Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language Models (VLMs). Existing strategies directly map Vision-Language Models from 2D pixels of rendered or captured views to 3D points, overlooking the inherent and expressible point cloud geometric structure. Geometrically similar or close regions can be exploited for bolstering point cloud understanding as they are likely to share semantic information. To this end, we introduce the first training-free aggregation technique that leverages the point cloud's 3D geometric structure to improve the quality of the transferred Vision-Language Models. Our approach operates iteratively, performing local-to-global aggregation based on geometric and semantic point-level reasoning. We benchmark our approach on three downstream tasks, including classification, part segmentation, and semantic segmentation, with a variety of datasets representing both synthetic/real-world, and indoor/outdoor scenarios. Our approach achieves new state-of-the-art results in all benchmarks. We will release the source code publicly.",http://arxiv.org/abs/2312.02244v1,,Guofeng Mei (Fondazione Bruno Kessler) | Luigi Riz (Fondazione Bruno Kessler) | Yiming Wang (Fondazione Bruno Kessler) | Fabio Poiesi (Fondazione Bruno Kessler),2023-12-04 12:30:07+00:00,,,,,,
Unsupervised Semantic Segmentation Through Depth-Guided Feature Correlation and Sampling,"Traditionally, training neural networks to perform semantic segmentation required expensive human-made annotations. But more recently, advances in the field of unsupervised learning have made significant progress on this issue and towards closing the gap to supervised algorithms. To achieve this, semantic knowledge is distilled by learning to correlate randomly sampled features from images across an entire dataset. In this work, we build upon these advances by incorporating information about the structure of the scene into the training process through the use of depth information. We achieve this by (1) learning depth-feature correlation by spatially correlate the feature maps with the depth maps to induce knowledge about the structure of the scene and (2) implementing farthest-point sampling to more effectively select relevant features by utilizing 3D sampling techniques on depth information of the scene. Finally, we demonstrate the effectiveness of our technical contributions through extensive experimentation and present significant improvements in performance across multiple benchmark datasets.",http://arxiv.org/abs/2309.12378v1,,Leon Sick (Ulm University) | Dominik Engel (Ulm University) | Pedro Hermosilla (Technische Universit??t Wien) | Timo Ropinski (Ulm University),2023-09-21 11:47:01+00:00,,,,,,
Task-Customized Mixture of Adapters for General Image Fusion,,,,Pengfei Zhu (Tianjin University) | Yang Sun (Tianjin University) | Bing Cao (Tianjin University) | Qinghua Hu (Tianjin University),,,,,,,
Discover and Mitigate Multiple Biased Subgroups in Image Classifiers,,,,Zeliang Zhang (University Of Rochester) | Mingqian Feng (University Of Rochester) | Zhiheng Li (Amazon AGI) | Chenliang Xu (University Of Rochester),,,,,,,
SEAS: ShapE-Aligned Supervision for Person Re-Identification,,,,Haidong Zhu (University Of Southern California) | Pranav Budhwant (University Of Southern California) | Zhaoheng Zheng (University Of Southern California) | Ram Nevatia (None),,,,,,,
Data-Free Quantization via Pseudo-label Filtering,,,,Chunxiao Fan (Hefei University Of Technology) | Ziqi Wang (Hefei University Of Technology) | Dan Guo (Hefei University Of Technology) | Meng Wang (Hefei University Of Technology),,,,,,,
PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis,"Recent advancements in large-scale pre-trained text-to-image models have led to remarkable progress in semantic image synthesis. Nevertheless, synthesizing high-quality images with consistent semantics and layout remains a challenge. In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE) that harnesses pre-trained models to alleviate the aforementioned issues. Specifically, we first employ the layout control map to faithfully represent layouts in the feature space. Subsequently, we combine the layout and semantic features in a timestep-adaptive manner to synthesize images with realistic details. During fine-tuning, we propose the Semantic Alignment (SA) loss to further enhance layout alignment. Additionally, we introduce the Layout-Free Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the priors of pre-trained models, thereby improving the visual quality and semantic consistency of synthesized images. Extensive experiments demonstrate that our approach performs favorably in terms of visual quality, semantic consistency, and layout alignment. The source code and model are available at https://github.com/cszy98/PLACE/tree/main.",http://arxiv.org/abs/2403.01852v1,,"Zhengyao Lv (University Of Hong Kong) | Yuxiang Wei (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Wangmeng Zuo (Harbin Institute Of Technology) | Kwan-Yee K. Wong (The University Of Hong Kong)",2024-03-04 09:03:16+00:00,,,,,,
Video-conditioned Text Representations for Activity Recognition,"Vision-Language models have shown strong performance in the image-domain -- even in zero-shot settings, thanks to the availability of large amount of pretraining data (i.e., paired image-text examples). However for videos, such paired data is not as abundant. Thus, video-text models are usually designed by adapting pretrained image-text models to video-domain, instead of training from scratch. All such recipes rely on augmenting visual embeddings with temporal information (i.e., image -> video), often keeping text embeddings unchanged or even being discarded. In this paper, we argue that such adapted video-text models can benefit more by augmenting text rather than visual information. We propose VicTR, which jointly-optimizes text and video tokens, generating 'Video-conditioned Text' embeddings. Our method can further make use of freely-available semantic information, in the form of visually-grounded auxiliary text (e.g., object or scene information). We conduct experiments on multiple benchmarks including supervised (Kinetics-400, Charades), zero-shot and few-shot (HMDB-51, UCF-101) settings, showing competitive performance on activity recognition based on video-text models.",http://arxiv.org/abs/2304.02560v1,,Kumara Kahatapitiya (Stony Brook University) | Anurag Arnab (Google) | Arsha Nagrani (Google ) | Michael Ryoo (Stony Brook University),2023-04-05 16:30:36+00:00,,,,,,
Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction,,,,Junuk Cha (UNIST) | Jihyeon Kim (Ulsan National Institute Of Science And Technology) | Jae Shin Yoon (Adobe Systems) | Seungryul Baek (UNIST),,,,,,,
Denoising Point Cloud in Latent Space via Graph Convolution and Invertible Neural Network,,,,Aihua Mao (South China University Of Technology) | Biao Yan (None) | Zijing Ma (South China University Of Technology) | Ying He (Nanyang Technological University),,,,,,,
Dynamic Cues-Assisted Transformer for Robust Point Cloud Registration,,,,Hong Chen (Huazhong University Of Science And Technology) | Pei Yan (Huazhong University Of Science And Technology) | Sihe Xiang (None) | Yihua Tan (Huazhong University Of Science And Technology),,,,,,,
BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection,"Recently, the rise of query-based Transformer decoders is reshaping camera-based 3D object detection. These query-based decoders are surpassing the traditional dense BEV (Bird's Eye View)-based methods. However, we argue that dense BEV frameworks remain important due to their outstanding abilities in depth estimation and object localization, depicting 3D scenes accurately and comprehensively. This paper aims to address the drawbacks of the existing dense BEV-based 3D object detectors by introducing our proposed enhanced components, including a CRF-modulated depth estimation module enforcing object-level consistencies, a long-term temporal aggregation module with extended receptive fields, and a two-stage object decoder combining perspective techniques with CRF-modulated depth embedding. These enhancements lead to a ""modernized"" dense BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms both BEV-based and query-based frameworks under various settings, achieving a state-of-the-art result of 64.2 NDS on the nuScenes test set.",http://arxiv.org/abs/2312.01696v1,,Zhenxin Li (Fudan University) | Shiyi Lan (NVIDIA CORPORATION) | Jose M. Alvarez (NVIDIA) | Zuxuan Wu (Fudan University),2023-12-04 07:35:02+00:00,,,,,,
Task-Driven Wavelets using Constrained Empirical Risk Minimization,,,,Eric Marcus (Netherlands Cancer Institute) | Ray Sheombarsing (None) | Jan-Jakob Sonke (Netherlands Cancer Institute) | Jonas Teuwen (Netherlands Cancer Institute),,,,,,,
AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation,"This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust performance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in advance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive experiments in a many-to-many language translation setting. The demo page is available on https://choijeongsoo.github.io/av2av.",http://arxiv.org/abs/2312.02512v1,,Jeongsoo Choi (Korea Advanced Institute Of Science And Technology) | Se Jin Park (KAIST) | Minsu Kim (None) | Yong Man Ro (Korea Advanced Institute Of Science And Technology),2023-12-05 05:36:44+00:00,,,,,,
Joint-Task Regularization for Partially Labeled Multi-Task Learning,,,,Kento Nishi (Harvard University) | Junsik Kim (None) | Wanhua Li (Harvard University) | Hanspeter Pfister (Harvard University),,,,,,,
GLOW: Global Layout Aware Attacks on Object Detection,"Adversarial attacks aim to perturb images such that a predictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a promising yet practical defense against conventional adversarial attacks. More desired attacks, to this end, should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversarial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection task and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. Our proposed GLOW, on the one hand, is capable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consistency that both semantic category and global scene layout are considered. In experiment, we design multiple types of attack requests and validate our ideas on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30$\%$ average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, our method outperforms SOTAs significantly on more generic attack requests by about 20$\%$ in average; Finally, our method produces superior performance under challenging zero-query black-box setting, or 20$\%$ better than SOTAs. Our code, model and attack requests would be made available.",http://arxiv.org/abs/2302.14166v2,,Jun Bao (Hangzhou Dianzi University) | Buyu Liu (NEC-Labs) | Kui Ren (Zhejiang University) | Jun Yu (Hangzhou Dianzi University),2023-02-27 22:01:34+00:00,,,,,,
Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding,,,,Chaolei Tan (SUN YAT-SEN UNIVERSITY) | Jianhuang Lai (SUN YAT-SEN UNIVERSITY) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) | Jian-Fang Hu (SUN YAT-SEN UNIVERSITY),,,,,,,
A Novel Two-stage UDF Learning Method for Robust Non-watertight Model Reconstruction from Multi-view Images,,,,"Junkai Deng (Institute Of Software, Chinese Academy Of Sciences) | Fei Hou (Institute Of Software, Chinese Academy Of Sciences) | Xuhui Chen (Institute Of Software, Chinese Academy Of Sciences) | Wencheng Wang (Institute Of Software, Chinese Academy Of Sciences) | Ying He (Nanyang Technological University)",,,,,,,
Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels,,,,Tianming Liang (Sun Yat-Sen University) | Chaolei Tan (SUN YAT-SEN UNIVERSITY) | Beihao Xia (Huazhong University Of Science And Technology) | Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) | Jian-Fang Hu (SUN YAT-SEN UNIVERSITY),,,,,,,
Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms,"Image segmentation algorithms can be understood as a collection of pixel classifiers, for which the outcomes of nearby pixels are correlated. Classifier models can be calibrated using Inductive Conformal Prediction, but this requires holding back a sufficiently large calibration dataset for computing the distribution of non-conformity scores of the model's predictions. If one only requires only marginal calibration on the image level, this calibration set consists of all individual pixels in the images available for calibration. However, if the goal is to attain proper calibration for each individual pixel classifier, the calibration set consists of individual images. In a scenario where data are scarce (such as the medical domain), it may not always be possible to set aside sufficiently many images for this pixel-level calibration. The method we propose, dubbed ``Kandinsky calibration'', makes use of the spatial structure present in the distribution of natural images to simultaneously calibrate the classifiers of ``similar'' pixels. This can be seen as an intermediate approach between marginal (imagewise) and conditional (pixelwise) calibration, where non-conformity scores are aggregated over similar image regions, thereby making more efficient use of the images available for calibration. We run experiments on segmentation algorithms trained and calibrated on subsets of the public MS-COCO and Medical Decathlon datasets, demonstrating that Kandinsky calibration method can significantly improve the coverage. When compared to both pixelwise and imagewise calibration on little data, the Kandinsky method achieves much lower coverage errors, indicating the data efficiency of the Kandinsky calibration.",http://arxiv.org/abs/2311.11837v1,,Joren Brunekreef (Netherlands Cancer Institute) | Eric Marcus (Netherlands Cancer Institute) | Ray Sheombarsing (None) | Jan-Jakob Sonke (Netherlands Cancer Institute) | Jonas Teuwen (Netherlands Cancer Institute),2023-11-20 15:11:31+00:00,,,,,,
Large Language Models are Good Prompt Learners for Low-Shot Image Classification,"Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets.",http://arxiv.org/abs/2312.04076v1,,Zhaoheng Zheng (University Of Southern California) | Jingmin Wei (University Of Southern California) | Xuefeng Hu (University Of Southern California) | Haidong Zhu (University Of Southern California) | Ram Nevatia (None),2023-12-07 06:43:34+00:00,,,,,,
Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection,"RGBT multispectral pedestrian detection has emerged as a promising solution for safety-critical applications that require day/night operations. However, the modality bias problem remains unsolved as multispectral pedestrian detectors learn the statistical bias in datasets. Specifically, datasets in multispectral pedestrian detection mainly distribute between ROTO (day) and RXTO (night) data; the majority of the pedestrian labels statistically co-occur with their thermal features. As a result, multispectral pedestrian detectors show poor generalization ability on examples beyond this statistical correlation, such as ROTX data. To address this problem, we propose a novel Causal Mode Multiplexer (CMM) framework that effectively learns the causalities between multispectral inputs and predictions. Moreover, we construct a new dataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian detection. ROTX-MP mainly includes ROTX examples not presented in previous datasets. Extensive experiments demonstrate that our proposed CMM framework generalizes well on existing datasets (KAIST, CVC-14, FLIR) and the new ROTX-MP. We will release our new dataset to the public for future research.",http://arxiv.org/abs/2403.01300v1,,Taeheon Kim (Korea Advanced Institute Of Science & Technology) | Sebin Shin (KAIST) | Youngjoon Yu (Korea Advanced Institute Of Science And Technology (KAIST)) | Hak Gu Kim (Chung-Ang University) | Yong Man Ro (Korea Advanced Institute Of Science And Technology),2024-03-02 19:54:53+00:00,,,,,,
Open-vocabulary object 6D pose estimation,,,,Jaime Corsetti (Fondazione Bruno Kessler & University Of Trento) | Davide Boscaini (Fondazione Bruno Kessler) | Changjae Oh (Queen Mary University London) | Andrea Cavallaro (EPFL - EPF Lausanne) | Fabio Poiesi (Fondazione Bruno Kessler),,,,,,,
Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture,"Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$ and boosts inference speed by 1.68$\times$ than the latest method. Our code is available at https://github.com/Jiafei127/FD4MM.",http://arxiv.org/abs/2403.07347v1,,Fei Wang (Hefei University Of Technology) | Dan Guo (Hefei University Of Technology) | Kun Li (Hefei University Of Technology) | Zhun Zhong (University Of Nottingham) | Meng Wang (Hefei University Of Technology),2024-03-12 06:07:29+00:00,,,,,,
What Moves Together Belongs Together,"We tackle semi-supervised object detection based on motion cues. Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision. We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns. Revisiting correlation clustering in the context of message passing networks, we learn to group those motion patterns to cluster points to object instances. By estimating the full extent of the objects, we obtain per-scan 3D bounding boxes that we use to supervise a Lidar object detection network. Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train object detectors across datasets.",http://arxiv.org/abs/2402.19463v1,,"Jenny Seidenschwarz (Department Of Informatics, Technische Universit??t M??nchen) | Aljo??a O??ep (Carnegie Mellon University) | Francesco Ferroni (None) | Simon Lucey (University Of Adelaide) | Laura Leal-Taixe (NVIDIA)",2024-02-29 18:54:53+00:00,,,,,,
Augmented Identity Distraction for Face Anonymization,,,,Zhenzhong Kuang (Hangzhou Dianzi University) | Xiaochen Yang (Hangzhou Dianzi University) | Yingjie Shen (Hangzhou Dianzi University) | Chao Hu (Hangzhou Dianzi University) | Jun Yu (Hangzhou Dianzi University),,,,,,,
MAGICK: A Large-scale Captioned Dataset from Matting Generated Images using Chroma Keying,,,,Ryan Burgert (Stony Brook University) | Brian Price (Adobe Research) | Jason Kuen (Adobe Research) | Yijun Li (Adobe Research) | Michael Ryoo (Stony Brook University),,,,,,,
Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation,,,,Jin Wang (China University Of Petroleum) | Bingfeng Zhang (China University Of Petroleum (East China)) | Jian Pang (China University Of Petroleum (East China)) | Honglong Chen (China University Of Petroleum) | Weifeng Liu (China University Of Petroleum (East China)),,,,,,,
Learning to Transform Dynamically for Better Adversarial Transferability,,,,Rongyi Zhu (None) | Zeliang Zhang (University Of Rochester) | Susan Liang (University Of Rochester) | Zhuo Liu (University Of Rochester) | Chenliang Xu (University Of Rochester),,,,,,,
AMU-Tuning: Learning Effective Bias for CLIP-based Few-shot Classification,,,,Yuwei Tang (Tianjin University) | ZhenYi Lin (TianJin University) | Qilong Wang (University Of Tianjin Of China) | Pengfei Zhu (Tianjin University) | Qinghua Hu (Tianjin University),,,,,,,
"Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding","Vision language models (VLM) have demonstrated remarkable performance across various downstream tasks. However, understanding fine-grained visual-linguistic concepts, such as attributes and inter-object relationships, remains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary focus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspective. We introduce a progressive pipeline to synthesize images that vary in a specific attribute while ensuring consistency in all other aspects. Utilizing this data engine, we carefully design a benchmark, SPEC, to diagnose the comprehension of object size, position, existence, and count. Subsequently, we conduct a thorough evaluation of four leading VLMs on SPEC. Surprisingly, their performance is close to random guess, revealing significant limitations. With this in mind, we propose a simply yet effective approach to optimize VLMs in fine-grained understanding, achieving significant improvements on SPEC without compromising the zero-shot performance. Results on two additional fine-grained benchmarks also show consistent improvements, further validating the transferability of our approach.",http://arxiv.org/abs/2312.00081v1,,Wujian Peng (Fudan University) | Sicheng Xie (Fudan University) | Zuyao You (Fudan University) | Shiyi Lan (NVIDIA CORPORATION) | Zuxuan Wu (Fudan University),2023-11-30 03:20:37+00:00,,,,,,
LangSplat: 3D Language Gaussian Splatting,"Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io",http://arxiv.org/abs/2312.16084v1,,Minghan Qin (Tsinghua University) | Wanhua Li (Harvard University) | Jiawei ZHOU (Tsinghua University) | Haoqian Wang (Tsinghua University) | Hanspeter Pfister (Harvard University),2023-12-26 15:14:37+00:00,,,,,,
MonoCD: Monocular 3D Object Detection with Complementary Depths,,,,Longfei Yan (None) | Pei Yan (Huazhong University Of Science And Technology) | Shengzhou Xiong (Huazhong University Of Science And Technology) | Xuanyu Xiang (Huazhong University Of Science And Technology) | Yihua Tan (Huazhong University Of Science And Technology),,,,,,,
SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection,"In the field of class incremental learning (CIL), generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the continuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the complexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distillation technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.",http://arxiv.org/abs/2402.17323v1,,JUNSU KIM (Ulsan National Institute Of Science And Technology) | Hoseong Cho (None) | Jihyeon Kim (Ulsan National Institute Of Science And Technology) | Yihalem Tiruneh (Ulsan National Institute Of Science And Technology) | Seungryul Baek (UNIST),2024-02-27 09:01:03+00:00,,,,,,
DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models,"We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body's shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face ''Janus'' problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",http://arxiv.org/abs/2304.00916v3,,Yukang Cao (The University Of Hong Kong) | Yan-Pei Cao (Tencent ARC Lab) | Kai Han (The University Of Hong Kong) | Ying Shan (Tencent) | Kwan-Yee K. Wong (The University Of Hong Kong),2023-04-03 12:11:51+00:00,,,,,,
Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships,"Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of powerful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.",http://arxiv.org/abs/2402.12259v1,,"Sebastian Koch (Robert Bosch GmbH / Ulm University) | Narunas Vaskevicius (Robert Bosch GmbH, Bosch) | Mirco Colosi (Robert Bosch GmbH) | Pedro Hermosilla (Technische Universit??t Wien) | Timo Ropinski (Ulm University)",2024-02-19 16:15:03+00:00,,,,,,
NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation,,,,Ziyi Chen (Zhejiang University) | Xiaolong Wu (Georgia Institute Of Technology) | Yu Zhang (Zhejiang University),,,,,,,
ModaVerse: Efficiently Transforming Modalities with LLMs,"Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM's output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration.",http://arxiv.org/abs/2401.06395v1,,Xinyu Wang (University Of Adelaide) | Bohan Zhuang (Monash University) | Qi Wu (University Of Adelaide),2024-01-12 06:28:54+00:00,,,,,,
HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation,"In this work, we present a novel dense-correspondence method for 6DoF object pose estimation from a single RGB-D image. While many existing data-driven methods achieve impressive performance, they tend to be time-consuming due to their reliance on rendering-based refinement approaches. To circumvent this limitation, we present HiPose, which establishes 3D-3D correspondences in a coarse-to-fine manner with a hierarchical binary surface encoding. Unlike previous dense-correspondence methods, we estimate the correspondence surface by employing point-to-surface matching and iteratively constricting the surface until it becomes a correspondence point while gradually removing outliers. Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate that our method surpasses all refinement-free methods and is even on par with expensive refinement-based approaches. Crucially, our approach is computationally efficient and enables real-time critical applications with high accuracy requirements. Code and models will be released.",http://arxiv.org/abs/2311.12588v1,,Yongliang Lin (Zhejiang University) | Yongzhi Su (German Research Center For AI (DFKI)) | Praveen Nathan (German Research Center For AI) | Sandeep Inuganti (German Research Center For AI) | Yan Di (Technische Universit??t M??nchen) | Martin Sundermeyer (None) | Fabian Manhardt (Google) | Didier Stricker (Universit??t Kaiserslautern) | Jason Rambach (None) | Yu Zhang (Zhejiang University),2023-11-21 13:21:22+00:00,,,,,,
PairAug: What Can Augmented Image-Text Pairs Do for Radiology?,,,,"Yutong Xie (University Of Adelaide) | Qi Chen (The University Of Adelaide) | Sinuo Wang (University Of Adelaide) | Minh-Son To (Flinders University Of South Australia) | Iris Lee (South Australia Medical Imaging) | Ee Win Khoo (The Queen Elizabeth Hospital) | Kerolos Hendy (Flinders University Of South Australia) | Daniel Koh (Monash University, Malaysia Campus) | Yong Xia (Northwestern Polytechnical University) | Qi Wu (University Of Adelaide)",,,,,,,
T-VSL: Text-Guided Visual Sound Source Localization in Mixtures,,,,Tanvir Mahmud (University Of Texas At Austin) | Yapeng Tian (University Of Texas At Dallas) | Diana Marculescu (The University Of Texas At Austin),,,,,,,
FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Generation,"Diffusion models have transformed the image-to-image (I2I) synthesis and are now permeating into videos. However, the advancement of video-to-video (V2V) synthesis has been hampered by the challenge of maintaining temporal consistency across video frames. This paper proposes a consistent V2V synthesis framework by jointly leveraging spatial conditions and temporal optical flow clues within the source video. Contrary to prior methods that strictly adhere to optical flow, our approach harnesses its benefits while handling the imperfection in flow estimation. We encode the optical flow via warping from the first frame and serve it as a supplementary reference in the diffusion model. This enables our model for video synthesis by editing the first frame with any prevalent I2I models and then propagating edits to successive frames. Our V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility: FlowVid works seamlessly with existing I2I models, facilitating various modifications, including stylization, object swaps, and local edits. (2) Efficiency: Generation of a 4-second video with 30 FPS and 512x512 resolution takes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF, Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, our FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).",http://arxiv.org/abs/2312.17681v1,,"Feng Liang (The University Of Texas At Austin) | Bichen Wu (Facebook) | Jialiang Wang (Facebook) | Licheng Yu (None) | Kunpeng Li (Meta) | Yinan Zhao (Facebook) | Ishan Misra (Facebook) | Jia-Bin Huang (University Of Maryland, College Park) | Peizhao Zhang (Facebook) | Peter Vajda (Facebook) | Diana Marculescu (The University Of Texas At Austin)",2023-12-29 16:57:12+00:00,,,,,,
Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge,,,,Haoxiang Ma (Beihang University) | Modi Shi (Beijing University Of Aeronautics And Astronautics) | Boyang GAO (Geometry Robotics Ltd. & Harbin Institute Of Technology) | Di Huang (Beihang University),,,,,,,
Meta-Point Learning and Refining for Category-Agnostic Pose Estimation,,,,Junjie Chen (Jiangxi University Of Finance And Economics) | Jiebin Yan (Jiangxi University Of Finance And Economics) | Yuming Fang (Jiangxi University Of Finance And Economics) | Li Niu (None),,,,,,,
ES3 : Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations ,,,,"Yuanhang Zhang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Shuang Yang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Shiguang Shan (Institute Of Computing Technology, Chinese Academy Of Sciences) | Xilin Chen (None)",,,,,,,
StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN,,,,"Jongwoo Choi (Visual Media Lab, KAIST) | Kwanggyoon Seo (KAIST) | Amirsaman Ashtari (MD Anderson Cancer Center) | Junyong Noh (Korea Advanced Institute Of Science And Technology)",,,,,,,
Task-conditioned adaptation of visual features in multi-task policy learning,"Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.",http://arxiv.org/abs/2402.07739v1,,"Pierre Marza (Institut National Des Sciences Appliqu??es De Lyon) | Laetitia Matignon (LIRIS, CNRS) | Olivier Simonin (INSA De Lyon) | Christian Wolf (Naver Labs Europe)",2024-02-12 15:57:31+00:00,,,,,,
DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions,,,,Yunxiao Shi (Qualcomm AI Research Qualcomm) | Manish Singh (Qualcomm AI Research) | Hong Cai (Qualcomm AI Research) | Fatih Porikli (QualComm),,,,,,,
FastMAC: Stochastic Spectral Sampling of Correspondence Graph,"3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC.",http://arxiv.org/abs/2403.08770v1,,Yifei Zhang (University Of Chinese Academy Of Sciences) | Hao Zhao (Tsinghua University) | Hongyang Li (Shanghai AI Lab) | Siheng Chen (Shanghai Jiao Tong University),2024-03-13 17:59:56+00:00,,,,,,
ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe,"We present ARTrackV2, which integrates two pivotal aspects of tracking: determining where to look (localization) and how to describe (appearance analysis) the target object across video frames. Building on the foundation of its predecessor, ARTrackV2 extends the concept by introducing a unified generative framework to ""read out"" object's trajectory and ""retell"" its appearance in an autoregressive manner. This approach fosters a time-continuous methodology that models the joint evolution of motion and visual features, guided by previous estimates. Furthermore, ARTrackV2 stands out for its efficiency and simplicity, obviating the less efficient intra-frame autoregression and hand-tuned parameters for appearance updates. Despite its simplicity, ARTrackV2 achieves state-of-the-art performance on prevailing benchmark datasets while demonstrating remarkable efficiency improvement. In particular, ARTrackV2 achieves AO score of 79.5\% on GOT-10k, and AUC of 86.1\% on TrackingNet while being $3.6 \times$ faster than ARTrack. The code will be released.",http://arxiv.org/abs/2312.17133v3,,Yifan Bai (Xi??an Jiao Tong University) | Zeyang Zhao (Xi'an Jiao Tong University) | Yihong Gong (Xi'an Jiao Tong University) | Xing Wei (None),2023-12-28 17:08:11+00:00,,,,,,
HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation,"3D shape generation from text is a fundamental task in 3D representation learning. The text-shape pairs exhibit a hierarchical structure, where a general text like ""chair"" covers all 3D shapes of the chair, while more detailed prompts refer to more specific shapes. Furthermore, both text and 3D shapes are inherently hierarchical structures. However, existing Text2Shape methods, such as SDFusion, do not exploit that. In this work, we propose HyperSDFusion, a dual-branch diffusion model that generates 3D shapes from a given text. Since hyperbolic space is suitable for handling hierarchical data, we propose to learn the hierarchical representations of text and 3D shapes in hyperbolic space. First, we introduce a hyperbolic text-image encoder to learn the sequential and multi-modal hierarchical features of text in hyperbolic space. In addition, we design a hyperbolic text-graph convolution module to learn the hierarchical features of text in hyperbolic space. In order to fully utilize these text features, we introduce a dual-branch structure to embed text features in 3D feature space. At last, to endow the generated 3D shapes with a hierarchical structure, we devise a hyperbolic hierarchical loss. Our method is the first to explore the hyperbolic hierarchical representation for text-to-shape generation. Experimental results on the existing text-to-shape paired dataset, Text2Shape, achieved state-of-the-art results.",http://arxiv.org/abs/2403.00372v1,,"Zhiying Leng (Beihang University) | Tolga Birdal (Imperial College London) | Xiaohui Liang (Zhongguancun Laboratory) | Federico Tombari (Google, TUM)",2024-03-01 08:57:28+00:00,,,,,,
DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses,,,,Chen Zhao (EPFL) | Tong Zhang (EPFL) | Zheng Dang (None) | Mathieu Salzmann (EPFL),,,,,,,
Endow SAM with Keen Eyes: Temporal-spatial Prompt Learning for Video Camouflaged Object Detection,,,,Wenjun Hui (None) | Zhenfeng Zhu (Beijing Jiao Tong University) | Shuai Zheng (Beijing Jiao Tong University) | Yao Zhao (Beijing Jiao Tong University),,,,,,,
Exploring Regional Clues in CLIP for Zero-Shot Semantic Segmentation,,,,Yi Zhang (Beihang University) | Meng-Hao Guo (Tsinghua University) | Miao Wang (Beihang University) | Shi-Min Hu (Tsinghua University),,,,,,,
Disentangled Pre-training for Human-Object Interaction Detection,,,,Zhuolong Li (South China University Of Technology) | Xingao Li (South China University Of Technology) | Changxing Ding (South China University Of Technology) | Xiangmin Xu (South China University Of Technology),,,,,,,
Global and Local Prompts Cooperation via Optimal Transport for Federated Learning,"Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific category characteristics. Unbalanced Optimal Transport is then employed to align local visual features with these prompts, striking a balance between global consensus and local personalization. Extensive experiments on datasets with various types of heterogeneities have demonstrated that our FedOTP outperforms the state-of-the-art methods.",http://arxiv.org/abs/2403.00041v1,,Hongxia Li (ShanghaiTech University) | Wei Huang (RIKEN AIP) | Jingya Wang (ShanghaiTech University) | Ye Shi (ShanghaiTech University),2024-02-29 11:43:04+00:00,,,,,,
ColorPCR: Color Point Cloud Registration with Multi-Stage Geometric-Color Fusion,,,,"Juncheng Mu (, Tsinghua University) | Lin Bie (Tsinghua University) | Shaoyi Du (Xi'an Jiao Tong University) | Yue Gao (Tsinghua University)",,,,,,,
Automatic Controllable Colorization by Imagination,,,,Xiaoyan Cong (Zhejiang University) | Yue Wu (Huawei Technologies Ltd.) | Qifeng Chen (Hong Kong University Of Science And Technology) | Chenyang Lei (The Hong Kong University Of Science And Technology),,,,,,,
LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation,"Referring video object segmentation (RVOS) aims to segment the target instance referred by a given text expression in a video clip. The text expression normally contains sophisticated description of the instance's appearance, action, and relation with others. It is therefore rather difficult for a RVOS model to capture all these attributes correspondingly in the video; in fact, the model often favours more on the action- and relation-related visual attributes of the instance. This can end up with partial or even incorrect mask prediction of the target instance. We tackle this problem by taking a subject-centric short text expression from the original long text expression. The short one retains only the appearance-related information of the target instance so that we can use it to focus the model's attention on the instance's appearance. We let the model make joint predictions using both long and short text expressions; and insert a long-short cross-attention module to interact the joint features and a long-short predictions intersection loss to regulate the joint predictions. Besides the improvement on the linguistic part, we also introduce a forward-backward visual consistency loss, which utilizes optical flows to warp visual features between the annotated frames and their temporal neighbors for consistency. We build our method on top of two state of the art pipelines. Extensive experiments on A2D-Sentences, Refer-YouTube-VOS, JHMDB-Sentences and Refer-DAVIS17 show impressive improvements of our method.",http://arxiv.org/abs/2306.08736v2,,Linfeng Yuan (Nanjing University Of Science And Technology) | Miaojing Shi (King's College London) | Zijie Yue (Tongji University) | Qijun Chen (Tongji University),2023-06-14 20:40:28+00:00,,,,,,
InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization,,,,Xiefan Guo (Beihang University) | Jinlin Liu (Alibaba Group) | Miaomiao Cui (Alibaba Group) | Jiankai Li (Beihang University) | Hongyu Yang (Beihang University) | Di Huang (Beihang University),,,,,,,
OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation,,,,"Jisoo Jeong (Qualcomm Inc, QualComm) | Hong Cai (Qualcomm AI Research) | Risheek Garrepalli (Qualcomm Inc, QualComm) | Jamie Lin (Qualcomm) | Munawar Hayat (Monash University) | Fatih Porikli (QualComm)",,,,,,,
Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning,,,,"Rui Li (None) | Tobias Fischer (ETH Zurich) | Mattia Segu (ETH Zurich - Swiss Federal Institute Of Technology) | Marc Pollefeys (ETH Zurich / Microsoft) | Luc Van Gool (ETH Zurich) | Federico Tombari (Google, TUM)",,,,,,,
LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example,,,,Soyeon Yoon (Korea Advanced Institute Of Science & Technology) | Kwan Yun (Korea Advanced Institute Of Science & Technology) | Kwanggyoon Seo (KAIST) | Sihun Cha (Korea Advanced Institute Of Science And Technology) | Jung Eun Yoo (Korea Advanced Institute Of Science & Technology) | Junyong Noh (Korea Advanced Institute Of Science And Technology),,,,,,,
3D Feature Tracking at 250 FPS via Event Camera,,,,Siqi Li (Tsinghua University) | Zhou Zhikuan (None) | Zhou Xue (Li Auto) | Yipeng Li (Tsinghua University) | Shaoyi Du (Xi'an Jiao Tong University) | Yue Gao (Tsinghua University),,,,,,,
Theoretically Achieving Continuous Representation of Oriented Bounding Boxes,"Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based object representation. For fairness and transparency of experiments, we have developed a modularized benchmark based on the open-source deep learning framework Jittor's detection toolbox JDet for OOD evaluation. On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.",http://arxiv.org/abs/2402.18975v1,,Zikai Xiao (None) | Guo-Ye Yang (None) | Xue Yang (Shanghai AI Laboratory) | Tai-Jiang Mu (Tsinghua University) | Junchi Yan (Shanghai Jiao Tong University) | Shi-Min Hu (Tsinghua University),2024-02-29 09:27:40+00:00,,,,,,
Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection,"In this paper, we study the problem of generalizable synthetic image detection, aiming to detect forgery images from diverse generative methods, e.g., GANs and diffusion models. Cutting-edge solutions start to explore the benefits of pre-trained models, and mainly follow the fixed paradigm of solely training an attached classifier, e.g., combining frozen CLIP-ViT with a learnable linear layer in UniFD. However, our analysis shows that such a fixed paradigm is prone to yield detectors with insufficient learning regarding forgery representations. We attribute the key challenge to the lack of forgery adaptation, and present a novel forgery-aware adaptive transformer approach, namely FatFormer. Based on the pre-trained vision-language spaces of CLIP, FatFormer introduces two core designs for the adaption to build generalized forgery representations. First, motivated by the fact that both image and frequency analysis are essential for synthetic image detection, we develop a forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains. Second, we find that considering the contrastive objectives between adapted image features and text prompt embeddings, a previously overlooked aspect, results in a nontrivial generalization improvement. Accordingly, we introduce language-guided alignment to supervise the forgery adaptation with image and text prompts in FatFormer. Experiments show that, by coupling these two designs, our approach tuned on 4-class ProGAN data attains a remarkable detection performance, achieving an average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen diffusion models with 95% accuracy.",http://arxiv.org/abs/2312.16649v1,,Huan Liu (Beijing Jiao Tong University) | Zichang Tan (Baidu) | Chuangchuang Tan (Beijing Jiao Tong University) | Yunchao Wei (Beijing Jiao Tong University) | Jingdong Wang (Baidu) | Yao Zhao (Beijing Jiao Tong University),2023-12-27 17:36:32+00:00,,,,,,
Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On,,,,Xu Yang (South China University Of Technology) | Changxing Ding (South China University Of Technology) | Zhibin Hong (HeyGen) | Junhao Huang (HeyGen) | Jin Tao (South China University Of Technology) | Xiangmin Xu (South China University Of Technology),,,,,,,
Shadow Generation for Composite Image Using Diffusion Model,,,,Qingyang Liu (Shanghai Jiao Tong University) | Junqi You (Shanghai Jiao Tong University) | Jian-Ting Wang (Shanghai JiaoTong University) | Xinhao Tao (Shanghai Jiao Tong University) | Bo Zhang (Shanghai Jiao Tong University) | Li Niu (None),,,,,,,
Vision-and-Language Navigation via Causal Learning,,,,Liuyi Wang (Tongji University) | Zongtao He (Tongji University) | Ronghao Dang (Tongji University) | Mengjiao Shen (Tongji University) | Chengju Liu (Tongji University) | Qijun Chen (Tongji University),,,,,,,
Robust Depth Enhancement via Polarization Prompt Fusion Tuning,,,,"Kei IKEMURA (KTH Royal Institute Of Technology) | Yiming Huang (HKUST) | Felix Heide (Department Of Computer Science, Princeton University) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Qifeng Chen (Hong Kong University Of Science And Technology) | Chenyang Lei (The Hong Kong University Of Science And Technology)",,,,,,,
S2 Fusion: A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals ,,,,Jiangnan Tang (ShanghaiTech University) | Jingya Wang (ShanghaiTech University) | Kaiyang Ji (None) | Lan Xu (None) | Jingyi Yu (Shanghai Tech University) | Ye Shi (ShanghaiTech University),,,,,,,
Communication-Efficient Collaborative Perception via Information Filling with Codebook,,,,Yue Hu (Shanghai Jiao Tong University) | Juntong Peng (Shanghai Jiao Tong University) | Sifei Liu (Shanghai Jiao Tong University) | Junhao Ge (Shanghai Jiao Tong University) | Si Liu (Beihang University) | Siheng Chen (Shanghai Jiao Tong University),,,,,,,
Person-in-WiFi 3D: End-to-End Multi-Person 3D Pose Estimation with Wi-Fi,,,,Kangwei Yan (Xi'an Jiao Tong University) | Fei Wang (Xi'an Jiao Tong University) | Bo Qian (None) | Han Ding (Xi'an Jiao Tong University) | Jinsong Han (Zhejiang University) | Xing Wei (None),,,,,,,
Learning to navigate efficiently and precisely in real environments,,,,Guillaume Bono (Naver Labs Europe) | Herv?? Poirier (Naver Labs Europe) | Leonid Antsfeld (Naver Labs Europe) | Gianluca Monaci (Naver Labs Europe) | Boris Chidlovskii (None) | Christian Wolf (Naver Labs Europe),,,,,,,
Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange,,,,Yanhao Wu (Xi'an Jiao Tong University) | Tong Zhang (EPFL) | Wei Ke (Xi'an Jiao Tong University) | Congpei Qiu (Xi'an Jiao Tong University) | Sabine S??sstrunk (None) | Mathieu Salzmann (EPFL),,,,,,,
HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention,,,,"Xiaolong Tang (Institute Of Computing Technoloy, Chinese Academy Of Sciences) | Meina Kan (Institute Of Computing Technoloy, Chinese Academy Of Sciences) | Shiguang Shan (Institute Of Computing Technology, Chinese Academy Of Sciences) | Zhilong Ji (Tomorrow Advancing Life) | Jinfeng Bai (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xilin Chen (None)",,,,,,,
Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion,"Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services. However, challenges arise from significant view changes and scene scale. Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views. Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery. To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques. Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner. The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency. Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.",http://arxiv.org/abs/2401.10786v1,,Zuoyue Li (ETH Zurich) | Zhenqiang Li (The University Of Tokyo) | Zhaopeng Cui (None) | Marc Pollefeys (ETH Zurich / Microsoft) | Martin R. Oswald (University Of Amsterdam),2024-01-19 16:15:37+00:00,,,,,,
Open-World Semantic Segmentation Including Class Similarity,"Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles. Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations. This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training. We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data. Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping. Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes.",http://arxiv.org/abs/2403.07532v1,,"Matteo Sodano (Institute Of Photogrammetry And Robotics, University Of Bonn (Germany)) | Federico Magistri (Rheinische Friedrich-Wilhelms Universit??t Bonn) | Lucas Nunes (University Of Bonn) | Jens Behley (University Of Bonn) | Cyrill Stachniss (University Of Bonn)",2024-03-12 11:11:19+00:00,,,,,,
Diffusion-driven GAN Inversion for Multi-Modal Facial Image Generation,,,,"Jihyun Kim (Yonsei University, LG Electronics) | Changjae Oh (Queen Mary University London) | Hoseok Do (LG Electronics) | Soohyun Kim (Korea University) | Kwanghoon Sohn (Yonsei University)",,,,,,,
MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models,,,,"Sanjoy Chowdhury (None) | Sayan Nag (University Of Toronto) | Joseph J (Adobe Systems) | Balaji Vasan Srinivasan (Adobe Research) | Dinesh Manocha (University Of Maryland, College Park)",,,,,,,
AV-RIR: Audio-Visual Room Impulse Response Estimation,"Accurate estimation of Room Impulse Response (RIR), which captures an environment's acoustic properties, is important for speech processing and AR/VR applications. We propose AV-RIR, a novel multi-modal multi-task learning approach to accurately estimate the RIR from a given reverberant speech signal and the visual cues of its corresponding environment. AV-RIR builds on a novel neural codec-based architecture that effectively captures environment geometry and materials properties and solves speech dereverberation as an auxiliary task by using multi-task learning. We also propose Geo-Mat features that augment material information into visual cues and CRIP that improves late reverberation components in the estimated RIR via image-to-RIR retrieval by 86%. Empirical results show that AV-RIR quantitatively outperforms previous audio-only and visual-only approaches by achieving 36% - 63% improvement across various acoustic metrics in RIR estimation. Additionally, it also achieves higher preference scores in human evaluation. As an auxiliary benefit, dereverbed speech from AV-RIR shows competitive performance with the state-of-the-art in various spoken language processing tasks and outperforms reverberation time error score in the real-world AVSpeech dataset. Qualitative examples of both synthesized reverberant speech and enhanced speech can be found at https://www.youtube.com/watch?v=tTsKhviukAE.",http://arxiv.org/abs/2312.00834v1,,"Anton Ratnarajah (University Of Maryland, College Park) | Sreyan Ghosh (University Of Maryland, College Park) | Sonal Kumar (University Of Maryland, College Park) | Purva Chiniya (University Of Maryland, College Park) | Dinesh Manocha (University Of Maryland, College Park)",2023-11-30 22:58:30+00:00,,,,,,
Designing Scalable Vision Models in the Vision-Language Era,,,,Jieneng Chen (Johns Hopkins University) | Qihang Yu (Johns Hopkins University) | Xiaohui Shen (ByteDance) | Alan L. Yuille (Johns Hopkins University) | Liang-Chieh Chen (None),,,,,,,
FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization,"3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.",http://arxiv.org/abs/2403.06908v1,,Jiahui Zhang (Nanyang Technological University) | Fangneng Zhan (None) | MUYU XU (Nanyang Technological University) | Shijian Lu (Nanyang Technological University) | Eric P. Xing (Mohamed Bin Zayed Univeristy Of AI),2024-03-11 17:00:27+00:00,,,,,,
From Variance to Veracity: Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers,,,,"Swaminathan Gurumurthy (School Of Computer Science, Carnegie Mellon University) | Karnik Ram (Technische Universit??t M??nchen) | Bingqing Chen (Bosch) | Zachary Manchester (Carnegie Mellon University) | Zico Kolter (Carnegie Mellon University)",,,,,,,
Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion,,,,Lucas Nunes (University Of Bonn) | Rodrigo Marcuzzi (University Of Bonn) | Benedikt Mersch (University Of Bonn) | Jens Behley (University Of Bonn) | Cyrill Stachniss (University Of Bonn),,,,,,,
The Science of Data Filtering: Data Curation cannot be Compute Agnostic,,,,Sachin Goyal (Carnegie Mellon University) | Pratyush Maini (Carnegie Mellon University) | Zachary Lipton (Carnegie Mellon University) | Aditi Raghunathan (Carnegie Mellon University) | Zico Kolter (Carnegie Mellon University),,,,,,,
Efficient Test-Time Adaptation of Vision-Language Models,,,,Adilbek Karmanov (Mohamed Bin Zayed University Of Artificial Intelligence) | Dayan Guan (Nanyang Technological University) | Shijian Lu (Nanyang Technological University) | Abdulmotaleb El Saddik (Mohamed Bin Zayed University Of Artificial Intelligence) | Eric P. Xing (Mohamed Bin Zayed Univeristy Of AI),,,,,,,
"Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications","We consider the task of animating 3D facial geometry from speech signal. Existing works are primarily deterministic, focusing on learning a one-to-one mapping from speech signal to 3D face meshes on small datasets with limited speakers. While these models can achieve high-quality lip articulation for speakers in the training set, they are unable to capture the full and diverse distribution of 3D facial motions that accompany speech in the real world. Importantly, the relationship between speech and facial motion is one-to-many, containing both inter-speaker and intra-speaker variations and necessitating a probabilistic approach. In this paper, we identify and address key challenges that have so far limited the development of probabilistic models: lack of datasets and metrics that are suitable for training and evaluating them, as well as the difficulty of designing a model that generates diverse results while remaining faithful to a strong conditioning signal as speech. We first propose large-scale benchmark datasets and metrics suitable for probabilistic modeling. Then, we demonstrate a probabilistic model that achieves both diversity and fidelity to speech, outperforming other methods across the proposed benchmarks. Finally, we showcase useful applications of probabilistic models trained on these large-scale datasets: we can generate diverse speech-driven 3D facial motion that matches unseen speaker styles extracted from reference clips; and our synthetic meshes can be used to improve the performance of downstream audio-visual models.",http://arxiv.org/abs/2311.18168v1,,Karren Yang (Apple) | Anurag Ranjan (Apple) | Jen-Hao Rick Chang (Apple) | Raviteja Vemulapalli (None) | Oncel Tuzel (Apple),2023-11-30 01:14:43+00:00,,,,,,
Improving Visual Recognition via Visual-Semantic Hierarchy Mapping,,,,"Hyeongjun Kwon (None) | Jinhyun Jang (Yonsei University) | Jin Kim (Yonsei University, Seoul, South Korea) | Kwonyoung Kim (None) | Kwanghoon Sohn (Yonsei University)",,,,,,,
MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training,"Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3$\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10$\times$-1000$\times$ improved learning efficiency when compared with non-reinforced CLIP training.",http://arxiv.org/abs/2311.17049v1,,Pavan Kumar Anasosalu Vasu (Apple) | Hadi Pouransari (Apple) | Fartash Faghri (None) | Raviteja Vemulapalli (None) | Oncel Tuzel (Apple),2023-11-28 18:55:42+00:00,,,,,,
Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis,"Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at https://github.com/YanzuoLu/CFLD.",http://arxiv.org/abs/2402.18078v1,,Yanzuo Lu (SUN YAT-SEN UNIVERSITY) | Manlin Zhang (SUN YAT-SEN UNIVERSITY) | Jinhua Ma (SUN YAT-SEN UNIVERSITY) | Xiaohua Xie (SUN YAT-SEN UNIVERSITY) | Jianhuang Lai (SUN YAT-SEN UNIVERSITY),2024-02-28 06:07:07+00:00,,,,,,
Loopy-SLAM: Dense Neural SLAM with Loop Closures,"Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.",http://arxiv.org/abs/2402.09944v1,,Lorenzo Liso (ETHZ - ETH Zurich) | Erik Sandstr??m (ETH Zurich) | Vladimir Yugay (University Of Amsterdam) | Luc Van Gool (ETH Zurich) | Martin R. Oswald (University Of Amsterdam),2024-02-14 18:18:32+00:00,,,,,,
View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network,,,,Quan Zhang (SUN YAT-SEN UNIVERSITY) | Lei Wang (SUN YAT-SEN UNIVERSITY) | Vishal M. Patel (Johns Hopkins University) | Xiaohua Xie (SUN YAT-SEN UNIVERSITY) | Jianhuang Lai (SUN YAT-SEN UNIVERSITY),,,,,,,
COCONut: Modernizing COCO Segmentation,,,,Xueqing Deng (ByteDance Research) | Qihang Yu (Johns Hopkins University) | Peng Wang (Bytedance US AILab) | Xiaohui Shen (ByteDance) | Liang-Chieh Chen (None),,,,,,,
Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on S E ( 3 )  ,"Addressing accuracy limitations and pose ambiguity in 6D object pose estimation from single RGB images presents a significant challenge, particularly due to object symmetries or occlusions. In response, we introduce a novel score-based diffusion method applied to the $SE(3)$ group, marking the first application of diffusion models to $SE(3)$ within the image domain, specifically tailored for pose estimation tasks. Extensive evaluations demonstrate the method's efficacy in handling pose ambiguity, mitigating perspective-induced ambiguity, and showcasing the robustness of our surrogate Stein score formulation on $SE(3)$. This formulation not only improves the convergence of Langevin dynamics but also enhances computational efficiency. Thus, we pioneer a promising strategy for 6D object pose estimation.",http://arxiv.org/abs/2305.15873v1,,Tsu-Ching Hsiao (Woven By Toyota) | Hao-Wei Chen (National Tsing Hua University) | Hsuan-Kung Yang (National Tsinghua University) | Chun-Yi Lee (National Tsing Hua University),2023-05-25 09:09:32+00:00,,,,,,
Transferable and Principled Efficiency for Open-Vocabulary Segmentation,,,,Jingxuan Xu (Beijing Jiao Tong University) | Wuyang Chen (University Of Texas At Austin) | Yao Zhao (Beijing Jiao Tong University) | Yunchao Wei (Beijing Jiao Tong University),,,,,,,
Deep Generative Model based Rate-Distortion for Image Downscaling Assessment,,,,Yuanbang Liang (Cardiff Univeristy) | Bhavesh Garg (IIT Bombay) | Paul L. Rosin (Cardiff University) | Yipeng Qin (Cardiff University),,,,,,,
From a Bird??s Eye View to See: Joint Camera and Subject Registration without the Camera Calibration,"We tackle a new problem of multi-view camera and subject registration in the bird's eye view (BEV) without pre-given camera calibration. This is a very challenging problem since its only input is several RGB images from different first-person views (FPVs) for a multi-person scene, without the BEV image and the calibration of the FPVs, while the output is a unified plane with the localization and orientation of both the subjects and cameras in a BEV. We propose an end-to-end framework solving this problem, whose main idea can be divided into following parts: i) creating a view-transform subject detection module to transform the FPV to a virtual BEV including localization and orientation of each pedestrian, ii) deriving a geometric transformation based method to estimate camera localization and view direction, i.e., the camera registration in a unified BEV, iii) making use of spatial and appearance information to aggregate the subjects into the unified BEV. We collect a new large-scale synthetic dataset with rich annotations for evaluation. The experimental results show the remarkable effectiveness of our proposed method.",http://arxiv.org/abs/2212.09298v2,,"Zekun Qian (Tianjin University) | Ruize Han (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Wei Feng (Tianjin University) | Song Wang (University Of South Carolina)",2022-12-19 08:31:08+00:00,,,,,,
Customization Assistant for Text-to-image Generation,"Customizing pre-trained text-to-image generation model has attracted massive research interest recently, due to its huge potential in real-world applications. Although existing methods are able to generate creative content for a novel concept contained in single user-input image, their capability are still far from perfection. Specifically, most existing methods require fine-tuning the generative model on testing images. Some existing methods do not require fine-tuning, while their performance are unsatisfactory. Furthermore, the interaction between users and models are still limited to directive and descriptive prompts such as instructions and captions. In this work, we build a customization assistant based on pre-trained large language model and diffusion model, which can not only perform customized generation in a tuning-free manner, but also enable more user-friendly interactions: users can chat with the assistant and input either ambiguous text or clear instruction. Specifically, we propose a new framework consists of a new model design and a novel training strategy. The resulting assistant can perform customized generation in 2-5 seconds without any test time fine-tuning. Extensive experiments are conducted, competitive results have been obtained across different domains, illustrating the effectiveness of the proposed method.",http://arxiv.org/abs/2312.03045v1,,"Yufan Zhou (State University Of New York, Buffalo) | Ruiyi Zhang (None) | Jiuxiang Gu (Adobe Systems) | Tong Sun (Adobe Systems)",2023-12-05 16:54:42+00:00,,,,,,
JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups,,,,Simindokht Jahangard (None) | Zhixi Cai (None) | Shiki Wen (Monash University) | Hamid Rezatofighi (Monash University),,,,,,,
FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,"We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/",http://arxiv.org/abs/2312.08344v1,,Bowen Wen (NVIDIA) | Wei Yang (NVIDIA) | Jan Kautz (NVIDIA) | Stan Birchfield (NVIDIA),2023-12-13 18:28:09+00:00,,,,,,
Distraction is All You Need: Memory-Efficient Image Immunization against Diffusion-Based Image Editing,,,,Lo (None) | Cheng Yeo (National Chiao Tung University) | Hong-Han Shuai (National Yang Ming Chiao Tung University) | Wen-Huang Cheng (National Taiwan University),,,,,,,
Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection,"Recently, the proliferation of highly realistic synthetic images, facilitated through a variety of GANs and Diffusions, has significantly heightened the susceptibility to misuse. While the primary focus of deepfake detection has traditionally centered on the design of detection algorithms, an investigative inquiry into the generator architectures has remained conspicuously absent in recent years. This paper contributes to this lacuna by rethinking the architectures of CNN-based generators, thereby establishing a generalized representation of synthetic artifacts. Our findings illuminate that the up-sampling operator can, beyond frequency-based artifacts, produce generalized forgery artifacts. In particular, the local interdependence among image pixels caused by upsampling operators is significantly demonstrated in synthetic images generated by GAN or diffusion. Building upon this observation, we introduce the concept of Neighboring Pixel Relationships(NPR) as a means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. A comprehensive analysis is conducted on an open-world dataset, comprising samples generated by \tft{28 distinct generative models}. This analysis culminates in the establishment of a novel state-of-the-art performance, showcasing a remarkable \tft{11.6\%} improvement over existing methods. The code is available at https://github.com/chuangchuangtan/NPR-DeepfakeDetection.",http://arxiv.org/abs/2312.10461v2,,"Chuangchuang Tan (Beijing Jiao Tong University) | Huan Liu (Beijing Jiao Tong University) | Yao Zhao (Beijing Jiao Tong University) | Shikui Wei (Beijing Jiaotong University) | Guanghua Gu (Yan Shan University) | Ping Liu (Institute Of High Performance Computing, Singapore, A*STAR) | Yunchao Wei (Beijing Jiao Tong University)",2023-12-16 14:27:06+00:00,,,,,,
JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments,,,,Duy Tho Le (Monash University) | Chenhui Gou (Monash University) | Stavya Datta (Monash University) | Hengcan Shi (None) | Ian Reid (University Of Adelaide) | Jianfei Cai (Monash University) | Hamid Rezatofighi (Monash University),,,,,,,
Loose Inertial Poser: Motion Capture with IMU-attached Loose-Wear Jacket,,,,Chengxu Zuo (Xiamen University) | Yiming Wang (Xiamen University) | Lishuang Zhan (Xiamen University) | Shihui Guo (Xiamen University) | Xinyu Yi (Tsinghua University) | Feng Xu (Tsinghua University) | Yipeng Qin (Cardiff University),,,,,,,
EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning,,,,"Hongxia Xie (National Chiao Tung University) | Chu-Jun Peng (National Yang Ming Chiao Tung University) | Yu-Wen Tseng (Department Of Computer Science And Informational Engineering, National Taiwan University) | Hung-Jen Chen (National Yang Ming Chiao Tung University) | Chan-Feng Hsu (National Chiao Tung University) | Hong-Han Shuai (National Yang Ming Chiao Tung University) | Wen-Huang Cheng (National Taiwan University)",,,,,,,
TRINS: Towards Multimodal Language Models That Can Read,,,,"Ruiyi Zhang (None) | Yanzhe Zhang (Georgia Institute Of Technology) | Jian Chen (Mohamed Bin Zayed University Of Artificial Intelligence) | Yufan Zhou (State University Of New York, Buffalo) | Jiuxiang Gu (Adobe Systems) | Changyou Chen (State University Of New York, Buffalo) | Tong Sun (Adobe Systems)",,,,,,,
Bidirectional Autoregessive Diffusion Model for Dance Generation,"Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge. Recently, diffusion models have showcased remarkable generative abilities across various domains. They hold promise for human motion generation due to their adaptable many-to-many nature. Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement. When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions. To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions. To make the generated dance motion smoother, a local information decoder is built for local motion enhancement. The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions. To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements. Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation.",http://arxiv.org/abs/2402.04356v1,,Canyu Zhang (University Of South Carolina) | Youbao Tang (PAII INC.) | NING Zhang (PAII) | Ruei-Sung Lin (PAII Inc) | Mei Han (PAII) | Jing Xiao (Pingan Group) | Song Wang (University Of South Carolina),2024-02-06 19:42:18+00:00,,,,,,
Boosting Flow-based Generative Super-Resolution Models via Learned Prior,,,,"Li-Yuan Tsao (National Tsing Hua University) | Yi-Chen Lo (National Tsing Hua University) | Chia-Che Chang (MediaTek) | Hao-Wei Chen (National Tsing Hua University) | Roy Tseng (MediaTek) | Chien Feng (Department Of Computer Science, National Tsing Hua University, National Tsinghua University) | Chun-Yi Lee (National Tsing Hua University)",,,,,,,
Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects,,,,Yijia Weng (Stanford University) | Bowen Wen (NVIDIA) | Jonathan Tremblay (NVIDIA) | Valts Blukis (NVIDIA) | Dieter Fox (University Of Washington) | Leonidas Guibas (Stanford University) | Stan Birchfield (NVIDIA),,,,,,,
Learned Lossless Image Compression based on Bit Plane Slicing,,,,Zhe Zhang (Wuhan University) | Huairui Wang (Wuhan University) | Zhenzhong Chen (Wuhan University) | Shan Liu (Tencent Media Lab),,,,,,,
"Time-, Memory- and Parameter-Efficient Visual Adaptation","As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.",http://arxiv.org/abs/2402.02887v1,,Otniel-Bogdan Mercea (University Of T??bingen) | Alexey Gritsenko (Google) | Cordelia Schmid (Inria / Google) | Anurag Arnab (Google),2024-02-05 10:55:47+00:00,,,,,,
Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment,"No-reference point cloud quality assessment (NR-PCQA) aims to automatically evaluate the perceptual quality of distorted point clouds without available reference, which have achieved tremendous improvements due to the utilization of deep neural networks. However, learning-based NR-PCQA methods suffer from the scarcity of labeled data and usually perform suboptimally in terms of generalization. To solve the problem, we propose a novel contrastive pre-training framework tailored for PCQA (CoPA), which enables the pre-trained model to learn quality-aware representations from unlabeled data. To obtain anchors in the representation space, we project point clouds with different distortions into images and randomly mix their local patches to form mixed images with multiple distortions. Utilizing the generated anchors, we constrain the pre-training process via a quality-aware contrastive loss following the philosophy that perceptual quality is closely related to both content and distortion. Furthermore, in the model fine-tuning stage, we propose a semantic-guided multi-view fusion module to effectively integrate the features of projected images from multiple perspectives. Extensive experiments show that our method outperforms the state-of-the-art PCQA methods on popular benchmarks. Further investigations demonstrate that CoPA can also benefit existing learning-based PCQA models.",http://arxiv.org/abs/2403.10066v1,,Ziyu Shan (Shanghai Jiao Tong University) | Yujie Zhang (Shanghai Jiao Tong University) | Qi Yang (Tencent MediaLab) | Haichen Yang (Shanghai Jiao Tong University) | Yiling Xu (None) | Jenq-Neng Hwang (None) | Xiaozhong Xu (Tencent Media Lab) | Shan Liu (Tencent Media Lab),2024-03-15 07:16:07+00:00,,,,,,
End-to-End Spatio-Temporal Action Localisation with Video Transformers,"The most performant spatio-temporal action localisation models use external person proposals and complex external memory banks. We propose a fully end-to-end, purely-transformer based model that directly ingests an input video, and outputs tubelets -- a sequence of bounding boxes and the action classes at each frame. Our flexible model can be trained with either sparse bounding-box supervision on individual frames, or full tubelet annotations. And in both cases, it predicts coherent tubelets as the output. Moreover, our end-to-end model requires no additional pre-processing in the form of proposals, or post-processing in terms of non-maximal suppression. We perform extensive ablation experiments, and significantly advance the state-of-the-art results on four different spatio-temporal action localisation benchmarks with both sparse keyframes and full tubelet annotations.",http://arxiv.org/abs/2304.12160v1,,Alexey Gritsenko (Google) | Xuehan Xiong (Google) | Josip Djolonga (Google) | Mostafa Dehghani (Google DeepMind) | Chen Sun (Brown University) | Mario Lu??i?? (Google) | Cordelia Schmid (Inria / Google) | Anurag Arnab (Google),2023-04-24 15:14:01+00:00,,,,,,
Learning to Remove Wrinkled Transparent Film with Polarized Prior,"In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems. We first physically model the imaging of industrial materials covered by the film. Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film. We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework. To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight. The image with minimized specular highlight is set as a prior for supporting the reconstruction network. Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film. Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks. Our code will be released at \url{https://github.com/jqtangust/FilmRemoval}.",http://arxiv.org/abs/2403.04368v1,,Jiaqi Tang (Hong Kong University Of Science And Technology (Guangzhou)) | RUIZHENG WU (Smartmore Technology) | Xiaogang Xu (Zhejiang Lab) | Sixing Hu (Smartmore Corporation) | Ying-Cong Chen (The Hong Kong University Of Science And Technology),2024-03-07 09:56:56+00:00,,,,,,
Inversion-Free Image Editing with Natural Language,,,,"Sihan Xu (University Of Michigan - Ann Arbor) | Yidong Huang (University Of Michigan - Ann Arbor) | Jiayi Pan (University Of California, Berkeley) | Ziqiao Ma (University Of Michigan) | Joyce Chai (University Of Michigan)",,,,,,,
Bi-Causal: Group Activity Recognition via Bidirectional Causality,,,,Youliang Zhang (Wuhan University) | Wenxuan Liu (Wuhan University Of Technology) | Danni Xu (National University Of Singapore) | Zhuo Zhou (Wuhan University) | Zheng Wang (Wuhan University),,,,,,,
UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and Unfavorable Data Pairs,,,,Youngju Na (KAIST) | Woo Jae Kim (Korea Advanced Institute Of Science And Technology (KAIST)) | Kyu Han (Korea Advanced Institute Of Science & Technology) | Suhyeon Ha (Korea Advanced Institute Of Science And Technology) | Sung-Eui Yoon (KAIST),,,,,,,
CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation,"This paper presents a simple but performant semi-supervised semantic segmentation approach, called CorrMatch. Previous approaches mostly employ complicated training strategies to leverage unlabeled data but overlook the role of correlation maps in modeling the relationships between pairs of locations. We observe that the correlation maps not only enable clustering pixels of the same category easily but also contain good shape information, which previous works have omitted. Motivated by these, we aim to improve the use efficiency of unlabeled data by designing two novel label propagation strategies. First, we propose to conduct pixel propagation by modeling the pairwise similarities of pixels to spread the high-confidence pixels and dig out more. Then, we perform region propagation to enhance the pseudo labels with accurate class-agnostic masks extracted from the correlation maps. CorrMatch achieves great performance on popular segmentation benchmarks. Taking the DeepLabV3+ with ResNet-101 backbone as our segmentation model, we receive a 76%+ mIoU score on the Pascal VOC 2012 dataset with only 92 annotated images. Code is available at https://github.com/BBBBchan/CorrMatch.",http://arxiv.org/abs/2306.04300v3,,"Bo-Yuan Sun (Nankai University) | Yuqi Yang (Nankai University) | Le Zhang (University Of Electronic Science And Technology Of China) | Ming-Ming Cheng (Nankai University, Tsinghua University) | Qibin Hou (Nankai University)",2023-06-07 10:02:29+00:00,,,,,,
Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners,"Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/",http://arxiv.org/abs/2402.17723v1,,Yazhou Xing (The Hong Kong University Of Science And Technology) | Yingqing He (HKUST) | Zeyue Tian (Hong Kong University Of Science And Technology) | Xintao Wang (Tencent) | Qifeng Chen (Hong Kong University Of Science And Technology),2024-02-27 17:57:04+00:00,,,,,,
AnyScene: Customized Image Synthesis with Composited Foreground,,,,Ruidong Chen (Tianjin University) | Lanjun Wang (Tianjin University) | Weizhi Nie (Tianjin University) | Yongdong Zhang (University Of Science And Technology Of China) | Anan Liu (Tianjin University),,,,,,,
Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse,,,,Yining Wang (Fudan University) | Junjie Sun (Fudan University) | Chenyue Wang (Fudan University) | Mi Zhang (Fudan University) | Min Yang (Fudan University),,,,,,,
Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance,,,,Dazhong Shen (Shanghai Artificial Intelligence Laboratory) | Guanglu Song (Sensetime X-Lab) | Zeyue Xue (The University Of Hong Kong) | Fu-Yun Wang (The Chinese University Of Hong Kong) | Yu Liu (The Chinese University Of Hong Kong),,,,,,,
GenesisTex: Adapting Image Denoising Diffusion to Texture Space,,,,Chenjian Gao (Tencent Games) | Boyan Jiang (Fudan University) | Xinghui Li (Tsinghua University) | YingPeng Zhang (South China University Of Technology) | Qian Yu (Beihang University),,,,,,,
Attribute-Guided Pedestrian Retrieval: Bridging Person Re-ID with Internal Attribute Variability,,,,"Yan Huang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Qiang Wu (University Of Technology Sydney) | Yi Zhong (Beijing Institute Of Technology) | Liang Wang (CASIA)",,,,,,,
Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution,,,,Longguang Wang (National University Of Defense Technology) | Juncheng Li (Shanghai University) | Yingqian Wang (None) | Qingyong Hu (University Of Oxford) | Yulan Guo (SUN YAT-SEN UNIVERSITY),,,,,,,
SfmCAD: Unsupervised CAD Reconstruction by Learning Sketch-based Feature Modeling Operations,,,,"Pu Li (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Jianwei Guo (Institute Of Automation, Chinese Academy Of Sciences) | HUIBIN LI (None) | Bedrich Benes (Purdue University) | Dong-Ming Yan (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
Multimodal Aerial Visual RECognition (MAVREC) Dataset: Can Multi-view Improve Aerial Visual Perception?,"Despite the commercial abundance of UAVs, aerial data acquisition remains challenging, and the existing Asia and North America-centric open-source UAV datasets are small-scale or low-resolution and lack diversity in scene contextuality. Additionally, the color content of the scenes, solar-zenith angle, and population density of different geographies influence the data diversity. These two factors conjointly render suboptimal aerial-visual perception of the deep neural network (DNN) models trained primarily on the ground-view data, including the open-world foundational models.   To pave the way for a transformative era of aerial detection, we present Multiview Aerial Visual RECognition or MAVREC, a video dataset where we record synchronized scenes from different perspectives -- ground camera and drone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard 2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million annotated bounding boxes. This makes MAVREC the largest ground and aerial-view dataset, and the fourth largest among all drone-based datasets across all modalities and tasks. Through our extensive benchmarking on MAVREC, we recognize that augmenting object detectors with ground-view images from the corresponding geographical location is a superior pre-training strategy for aerial detection. Building on this strategy, we benchmark MAVREC with a curriculum-based semi-supervised object detection approach that leverages labeled (ground and aerial) and unlabeled (only aerial) images to enhance the aerial detection. We publicly release the MAVREC dataset: https://mavrec.github.io.",http://arxiv.org/abs/2312.04548v1,,Aritra Dutta (University Of Central Florida) | Srijan Das (University Of North Carolina At Charlotte) | Jacob Nielsen (University Of Southern Denmark - SDU) | RAJATSUBHRA CHAKRABORTY (University Of North Carolina At Charlotte) | Mubarak Shah (University Of Central Florida),2023-12-07 18:59:14+00:00,,,,,,
InstaGen: Enhancing Object Detection with Text-to-Image Diffusion Model,,,,Chengjian Feng (Meituan) | Yujie Zhong (Meituan) | Zequn Jie (Meituan) | Weidi Xie (Shanghai Jiao Tong University) | Lin Ma (Meituan),,,,,,,
Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors,"We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus shifting the focus from the static background scene to the foreground objects. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on four benchmarks: Avenue, ShanghaiTech, UBnormal and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy, obtaining competitive AUC scores, while processing 1655 FPS. Hence, our model is between 8 and 70 times faster than competing methods. We also conduct an ablation study to justify our design. Our code is freely available at: https://github.com/ristea/aed-mae.",http://arxiv.org/abs/2306.12041v2,,Nicolae Ristea (University Politehnica Of Bucharest) | Florinel Croitoru (University Of Bucharest) | Radu Tudor Ionescu (None) | Marius Popescu (University Of Bucharest) | Fahad Shahbaz Khan (MBZUAI; Link??ping University) | Mubarak Shah (University Of Central Florida),2023-06-21 06:18:05+00:00,,,,,,
Misalignment-Robust Frequency Distribution Loss for Image Transformation,"This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image style transfer that relies solely on completely misaligned data. Our code is available at: https://github.com/eezkni/FDL",http://arxiv.org/abs/2402.18192v1,,Zhangkai Ni (Tongji University) | Juncheng Wu (Tongji University) | Zian Wang (Tongji University) | Wenhan Yang (Peng Cheng Lab) | Hanli Wang (Tongji University) | Lin Ma (Meituan),2024-02-28 09:27:41+00:00,,,,,,
SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction,,,,Yang Zhou (SenseTime Research) | Hao Shao (None) | Letian Wang (University Of Toronto) | Steven L. Waslander (University Of Toronto) | Hongsheng Li (The Chinese University Of Hong Kong) | Yu Liu (The Chinese University Of Hong Kong),,,,,,,
LoS: Local Structure Guided Stereo Matching,,,,"Kunhong Li (SUN YAT-SEN UNIVERSITY) | Longguang Wang (National University Of Defense Technology) | Ye Zhang (SUN YAT-SEN UNIVERSITY) | Kaiwen Xue (Huawei Cloud Computing Technologies Co., Ltd.) | Shunbo Zhou (Huawei Technologies Ltd.) | Yulan Guo (SUN YAT-SEN UNIVERSITY)",,,,,,,
DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation,"We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",http://arxiv.org/abs/2401.04747v1,,Junming Chen (Hong Kong University Of Science And Technology) | Yunfei Liu (International Digital Economy Academy (IDEA)) | Jianan Wang (None) | Ailing Zeng (IDEA) | Yu Li (International Digital Economy Academy) | Qifeng Chen (Hong Kong University Of Science And Technology),2024-01-09 11:38:18+00:00,,,,,,
LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching,"The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.",http://arxiv.org/abs/2311.11284v3,,Yixun Liang (Hong Kong University Of Science And Technology) | Xin Yang (The Hong Kong University Of Science And Technology) | Jiantao Lin (Hong Kong University Of Science And Technology) | Haodong LI (Hong Kong University Of Science And Technology) | Xiaogang Xu (Zhejiang Lab) | Ying-Cong Chen (The Hong Kong University Of Science And Technology),2023-11-19 09:59:09+00:00,,,,,,
Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis,,,,"Mingyang Zhao (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Jiang Jingen (Shandong University) | Lei Ma (Peking University) | Shiqing Xin (Shandong University) | Gaofeng Meng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Dong-Ming Yan (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
SemCity: Semantic Scene Generation with Triplane Diffusion,"We present ""SemCity,"" a 3D diffusion model for semantic scene generation in real-world outdoor environments. Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model. Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model. The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at https://github.com/zoomin-lee/SemCity.",http://arxiv.org/abs/2403.07773v3,,Jumin Lee (Korea Advanced Institute Of Science And Technology) | Sebin Lee (Korea Advanced Institute Of Science And Technology (KAIST)) | Changho Jo (Neosapience) | Woobin Im (Korea Advanced Institute Of Science And Technology) | Ju-Hyeong Seon (Korea Advanced Institute Of Science & Technology) | Sung-Eui Yoon (KAIST),2024-03-12 15:59:08+00:00,,,,,,
GROUNDHOG: Grounding Large Language Models to Holistic Segmentation,"Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.",http://arxiv.org/abs/2402.16846v1,,Yichi Zhang (University Of Michigan) | Ziqiao Ma (University Of Michigan) | Xiaofeng Gao (Amazon AGI) | Suhaila Shakiah (Amazon) | Qiaozi Gao (Amazon) | Joyce Chai (University Of Michigan),2024-02-26 18:59:33+00:00,,,,,,
CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model,"Image-based virtual try-on enables users to virtually try on different garments by altering original clothes in their photographs. Generative Adversarial Networks (GANs) dominate the research field in image-based virtual try-on, but have not resolved problems such as unnatural deformation of garments and the blurry generation quality. Recently, diffusion models have emerged with surprising performance across various image generation tasks. While the generative quality of diffusion models is impressive, achieving controllability poses a significant challenge when applying it to virtual try-on tasks and multiple denoising iterations limit its potential for real-time applications. In this paper, we propose Controllable Accelerated virtual Try-on with Diffusion Model called CAT-DM. To enhance the controllability, a basic diffusion-based virtual try-on network is designed, which utilizes ControlNet to introduce additional control conditions and improves the feature extraction of garment images. In terms of acceleration, CAT-DM initiates a reverse denoising process with an implicit distribution generated by a pre-trained GAN-based model. Compared with previous try-on methods based on diffusion models, CAT-DM not only retains the pattern and texture details of the in-shop garment but also reduces the sampling steps without compromising generation quality. Extensive experiments demonstrate the superiority of CAT-DM against both GAN-based and diffusion-based methods in producing more realistic images and accurately reproducing garment patterns. Our code and models will be publicly released.",http://arxiv.org/abs/2311.18405v1,,Jianhao Zeng (Tianjin University) | Dan Song (Tianjin University) | Weizhi Nie (Tianjin University) | Hongshuo Tian (Tianjin University) | Tongtong Wang (Tencent LightSpeed Studio) | Anan Liu (Tianjin University),2023-11-30 09:56:17+00:00,,,,,,
CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection,"Knowledge Distillation (KD) has been validated as an effective model compression technique for learning compact object detectors. Existing state-of-the-art KD methods for object detection are mostly based on feature imitation, which is generally observed to be better than prediction mimicking. In this paper, we show that the inconsistency of the optimization objectives between the ground-truth signals and distillation targets is the key reason for the inefficiency of prediction mimicking. To alleviate this issue, we present a simple yet effective distillation scheme, termed CrossKD, which delivers the intermediate features of the student's detection head to the teacher's detection head. The resulting cross-head predictions are then forced to mimic the teacher's predictions. Such a distillation manner relieves the student's head from receiving contradictory supervision signals from the ground-truth annotations and the teacher's predictions, greatly improving the student's detection performance. On MS COCO, with only prediction mimicking losses applied, our CrossKD boosts the average precision of GFL ResNet-50 with 1x training schedule from 40.2 to 43.7, outperforming all existing KD methods for object detection. Code is available at https://github.com/jbwang1997/CrossKD.",http://arxiv.org/abs/2306.11369v1,,"JiaBao Wang (Nankai University) | Yuming Chen (None) | Zhaohui Zheng (Nankai University) | Xiang Li (Nankai University) | Ming-Ming Cheng (Nankai University, Tsinghua University) | Qibin Hou (Nankai University)",2023-06-20 08:19:51+00:00,,,,,,
HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses,"We present HumanNeRF-SE, which can synthesize diverse novel pose images with simple input. Previous HumanNeRF studies require large neural networks to fit the human appearance and prior knowledge. Subsequent methods build upon this approach with some improvements. Instead, we reconstruct this approach, combining explicit and implicit human representations with both general and specific mapping processes. Our key insight is that explicit shape can filter the information used to fit implicit representation, and frozen general mapping combined with point-specific mapping can effectively avoid overfitting and improve pose generalization performance. Our explicit and implicit human represent combination architecture is extremely effective. This is reflected in our model's ability to synthesize images under arbitrary poses with few-shot input and increase the speed of synthesizing images by 15 times through a reduction in computational complexity without using any existing acceleration modules. Compared to the state-of-the-art HumanNeRF studies, HumanNeRF-SE achieves better performance with fewer learnable parameters and less training time (see Figure 1).",http://arxiv.org/abs/2312.02232v1,,Caoyuan Ma (Wuhan University) | Yu-Lun Liu (National Yang Ming Chiao Tung University) | Zhixiang Wang (The University Of Tokyo) | Wu Liu (None) | Xinchen Liu (None) | Zheng Wang (Wuhan University),2023-12-04 06:37:11+00:00,,,,,,
CausalPC: Improving the Robustness of Point Cloud Classification by Causal Effect Identification,,,,Yuanmin Huang (Fudan University) | Mi Zhang (Fudan University) | Daizong Ding (Huawei Technologies Ltd.) | Erling Jiang (Fudan University) | Zhaoxiang Wang (Fudan University) | Min Yang (Fudan University),,,,,,,
SVGDreamer: Text Guided SVG Generation with Diffusion Model,"Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD convergence and improve aesthetic appeal. Extensive experiments have been conducted to validate the effectiveness of SVGDreamer, demonstrating its superiority over baseline methods in terms of editability, visual quality, and diversity. The code and demo of SVGDreamer can be found at https://ximinng.github.io/SVGDreamer-project/",http://arxiv.org/abs/2312.16476v3,,XiMing Xing (Beihang University) | Chuang Wang (Beihang University) | Haitao Zhou (Beihang University) | Jing Zhang (Beihang University) | Dong Xu (University Of Hong Kong) | Qian Yu (Beihang University),2023-12-27 08:50:01+00:00,,,,,,
Investigating Compositional Challenges in Vision-Language Models for Visual Grounding,,,,"Yunan Zeng (None) | Yan Huang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Jinjin Zhang (None) | Zequn Jie (Meituan) | Zhenhua Chai (Meituan) | Liang Wang (CASIA)",,,,,,,
Grounding Everything: Emerging Localization Properties in Vision-Language Transformers,"Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. It shows that GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark.",http://arxiv.org/abs/2312.00878v3,,Walid Bousselham (Johann Wolfgang Goethe Universit??t Frankfurt Am Main) | Felix Petersen (Stanford University) | Vittorio Ferrari (Synthesia) | Hilde Kuehne (University Of Bonn MIT-IBM Watson AI Lab),2023-12-01 19:06:12+00:00,,,,,,
Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle,"We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a $5\times$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow",http://arxiv.org/abs/2312.03431v1,,Youtian Lin (Nanjing University) | Zuozhuo Dai (Alibaba Group) | Siyu Zhu (Fudan University) | Yao Yao (Nanjing University),2023-12-06 11:25:52+00:00,,,,,,
Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis,"Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU.",http://arxiv.org/abs/2312.16812v1,,Zhan Li (OPPO US Research Center & Portland State University) | Zhang Chen (OPPO US Research Center) | Zhong Li (InnoPeak Technology) | Yi Xu (OPPO US Research Center),2023-12-28 04:14:55+00:00,,,,,,
ScanFormer: Referring Expression Comprehension by Iteratively Scanning,,,,Wei Su (Zhejiang University) | Peihan Miao (Zhejiang University) | Huanzhang Dou (Zhejiang University) | Xi Li (Zhejiang University),,,,,,,
Image Processing GNN: Breaking Rigidity in Super-Resolution,,,,Yuchuan Tian (Peking University) | Hanting Chen (Huawei Technologies Ltd.) | Chao Xu (Peking University) | Yunhe Wang (Huawei Noah's Ark Lab),,,,,,,
Text-to-3D using Gaussian Splatting,,,,Zilong Chen (Tsinghua University) | Feng Wang (Tsinghua University) | Yikai Wang (Tsinghua University) | Huaping Liu (Tsinghua University),,,,,,,
Distilling Semantic Priors from SAM to Efficient Image Restoration Models,,,,Quan Zhang (Tsinghua University) | Xiaoyu Liu (University Of Science And Technology Of China) | Wei Li (Huawei Noah's Ark Lab) | Hanting Chen (Huawei Technologies Ltd.) | Junchao Liu (Huawei Noah's Ark Lab) | Jie Hu (Huawei Technologies Ltd.) | Zhiwei Xiong (None) | Chun Yuan (Tsinghua University) | Yunhe Wang (Huawei Noah's Ark Lab),,,,,,,
"What, when, and where? -- Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions",,,,"Brian Chen (Samsung) | Nina Shvetsova (None) | Andrew Rouditchenko (Massachusetts Institute Of Technology) | Daniel Kondermann (Heidelberg University, Ruprecht-Karls-Universit??t Heidelberg) | Samuel Thomas (IBM Research) | Shih-Fu Chang (Columbia University) | Rogerio Feris (International Business Machines) | James Glass (Massachusetts Institute Of Technology) | Hilde Kuehne (University Of Bonn MIT-IBM Watson AI Lab)",,,,,,,
NARUTO: Neural Active Reconstruction from Uncertain Target Observations,"We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.",http://arxiv.org/abs/2402.18771v1,,"Ziyue Feng (Clemson University) | Huangying Zhan (OPPO US Research Center) | Zheng Chen (Indiana University, Bloomington) | Qingan Yan (OPPO US Research Center) | Xiangyu Xu (InnoPeak Technology,) | Changjiang Cai (None) | Bing Li (Clemson University) | Qilun Zhu (Clemson University) | Yi Xu (OPPO US Research Center)",2024-02-29 00:25:26+00:00,,,,,,
BEVSpread: Spread Voxel Pooling for Bird??s-Eye-View Representation in Vision-based Roadside 3D Object Detection,,,,Wenjie Wang (Zhejiang University) | Yehao Lu (Zhejiang University) | Guangcong Zheng (None) | Shuigenzhan (Zhejiang University) | Xiaoqing Ye (Baidu) | Zichang Tan (Baidu) | Jingdong Wang (Baidu) | Gaoang Wang (Zhejiang University) | Xi Li (Zhejiang University),,,,,,,
Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion,"Recent advances in generative AI have unveiled significant potential for the creation of 3D content. However, current methods either apply a pre-trained 2D diffusion model with the time-consuming score distillation sampling (SDS), or a direct 3D diffusion model trained on limited 3D data losing generation diversity. In this work, we approach the problem by employing a multi-view 2.5D diffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D diffusion directly models the structural distribution of 3D data, while still maintaining the strong generalization ability of the original 2D diffusion model, filling the gap between 2D diffusion-based and direct 3D diffusion-based methods for 3D content generation. During inference, multi-view normal maps are generated using the 2.5D diffusion, and a novel differentiable rasterization scheme is introduced to fuse the almost consistent multi-view normal maps into a consistent 3D model. We further design a normal-conditioned multi-view image generation module for fast appearance generation given the 3D geometry. Our method is a one-pass diffusion process and does not require any SDS optimization as post-processing. We demonstrate through extensive experiments that, our direct 2.5D generation with the specially-designed fusion scheme can achieve diverse, mode-seeking-free, and high-fidelity 3D content generation in only 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.",http://arxiv.org/abs/2311.15980v1,,Yuanxun Lu (Nanjing University) | Jingyang Zhang (Apple) | Shiwei Li (Apple) | Tian Fang (Hong Kong University Of Science And Technology) | David McKinnon (Apple) | Yanghai Tsin (Apple) | Long Quan (The Hong Kong University Of Science And Technology) | Xun Cao (Nanjing University) | Yao Yao (Nanjing University),2023-11-27 16:26:54+00:00,,,,,,
Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Debluring,"Coarse-to-fine schemes are widely used in traditional single-image motion deblur; however, in the context of deep learning, existing multi-scale algorithms not only require the use of complex modules for feature fusion of low-scale RGB images and deep semantics, but also manually generate low-resolution pairs of images that do not have sufficient confidence. In this work, we propose a multi-scale network based on single-input and multiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of algorithms based on a coarse-to-fine scheme. To alleviate restoration defects impacting detail information brought about by using a multi-scale architecture, we combine the characteristics of real-world blurring trajectories with a learnable wavelet transform module to focus on the directional continuity and frequency features of the step-by-step transitions between blurred images to sharp images. In conclusion, we propose a multi-scale network with a learnable discrete wavelet transform (MLWNet), which exhibits state-of-the-art performance on multiple real-world deblurred datasets, in terms of both subjective and objective quality as well as computational efficiency.",http://arxiv.org/abs/2401.00027v2,,Xin Gao (None) | Tianheng Qiu (University Of Science And Technology Of China) | Xinyu Zhang (None) | Hanlin Bai (China University Of Mining Technology - Beijing) | Kang Liu (None) | Xuan Huang (Chinese Academy Of Sciences) | Hu Wei (Chinese Academy Of Sciences) | Guoying Zhang (China University Of Mining Technology - Beijing) | Huaping Liu (Tsinghua University),2023-12-29 02:59:40+00:00,,,,,,
ZeroRF: Fast Sparse View 360?? Reconstruction with Zero Pretraining,"We present ZeroRF, a novel per-scene optimization method addressing the challenge of sparse view 360{\deg} reconstruction in neural field representations. Current breakthroughs like Neural Radiance Fields (NeRF) have demonstrated high-fidelity image synthesis but struggle with sparse input views. Existing methods, such as Generalizable NeRFs and per-scene optimization approaches, face limitations in data dependency, computational cost, and generalization across diverse scenarios. To overcome these challenges, we propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into a factorized NeRF representation. Unlike traditional methods, ZeroRF parametrizes feature grids with a neural network generator, enabling efficient sparse view 360{\deg} reconstruction without any pretraining or additional regularization. Extensive experiments showcase ZeroRF's versatility and superiority in terms of both quality and speed, achieving state-of-the-art results on benchmark datasets. ZeroRF's significance extends to applications in 3D content generation and editing. Project page: https://sarahweiii.github.io/zerorf/",http://arxiv.org/abs/2312.09249v1,,"Ruoxi Shi (University Of California, San Diego) | Xinyue Wei (University Of California, San Diego) | Cheng Wang (University Of California, San Diego) | Hao Su (UCSD)",2023-12-14 18:59:32+00:00,,,,,,
Bridging Sources in Geospatial Sensing with Cross Sensor Pretraining,,,,Boran Han (Amazon/AWS) | Shuai Zhang (Amazon) | Xingjian Shi (Boson AI) | Markus Reichstein (Max-Planck Institute),,,,,,,
A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection,,,,"Hanshi Wang (Institute Of Automation, Chinese Academy Of Science) | Zhipeng Zhang (Didi Research) | Jin Gao (Institute Of Automation, Chinese Academy Of Sciences) | Weiming Hu (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion,"Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus_page.",http://arxiv.org/abs/2311.07885v1,,"Minghua Liu (University Of California, San Diego) | Ruoxi Shi (University Of California, San Diego) | Linghao Chen (None) | Zhuoyang Zhang (IIIS, Tsinghua University) | Chao Xu (University Of California, Los Angeles) | Xinyue Wei (University Of California, San Diego) | Hansheng Chen (Stanford University) | Chong Zeng (Zhejiang University) | Jiayuan Gu (University Of California, San Diego) | Hao Su (UCSD)",2023-11-14 03:40:25+00:00,,,,,,
How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?,,,,"Yuxin Chen (None) | Zongyang Ma (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Ziqi Zhang (None) | Zhongang Qi (Tencent PCG ARC Lab) | Chunfeng Yuan (, Institute Of Automation, Chinese Academy Of Science) | Bing Li (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Junfu Pu (Tencent ARC Lab) | Ying Shan (Tencent) | Xiaojuan Qi (University Of Oxford) | Weiming Hu (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
Self-supervised learning for geospatial vegetation forecasting,"The innovative application of precise geospatial vegetation forecasting holds immense potential across diverse sectors, including agriculture, forestry, humanitarian aid, and carbon accounting. To leverage the vast availability of satellite imagery for this task, various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However, the important area of vegetation dynamics has not been thoroughly explored. Our study breaks new ground by introducing GreenEarthNet, the first dataset specifically designed for high-resolution vegetation forecasting, and Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Europe. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameter-efficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibility with the existing satellite imagery forecasting dataset EarthNet2021, enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-the-art models on EarthNet2021, as well as adapted models from time series forecasting and video prediction. To the best of our knowledge, this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes.",http://arxiv.org/abs/2303.16198v2,,Vitus Benson (Max-Planck-Institute For Biogeochemistry) | Claire Robin (Max Planck Institute For Biogeochemistry) | Christian Requena-Mesa (Max-Planck Institute For Biogeochemistry) | LAZARO ALONSO SILVA (Max-Planck Institute) | M??lanie Weynants (Max Planck Institute For Biogeochemistry) | Nora Linscheid (Max Planck Institute For Biogeochemistry) | Jose Cortes (Max-Planck Institute) | Zhihan Gao (The Hong Kong University Of Science And Technology) | Nuno Carvalhais (Max-Planck Institute) | Markus Reichstein (Max-Planck Institute),2023-03-28 17:59:05+00:00,,,,,,
TokenCompose: Grounding Diffusion with Token-level Supervision,"We present TokenCompose, a Latent Diffusion Model for text-to-image generation that achieves enhanced consistency between user-specified text prompts and model-generated images. Despite its tremendous success, the standard denoising process in the Latent Diffusion Model takes text prompts as conditions only, absent explicit constraint for the consistency between the text prompts and the image contents, leading to unsatisfactory results for composing multiple object categories. TokenCompose aims to improve multi-category instance composition by introducing the token-wise consistency terms between the image content and object segmentation maps in the finetuning stage. TokenCompose can be applied directly to the existing training pipeline of text-conditioned diffusion models without extra human labeling information. By finetuning Stable Diffusion, the model exhibits significant improvements in multi-category instance composition and enhanced photorealism for its generated images.",http://arxiv.org/abs/2312.03626v1,,"Zirui Wang (Princeton University) | Zhizhou Sha (Tsinghua University) | Zheng Ding (University Of California, San Diego) | Yilin Wang (Tsinghua University) | Zhuowen Tu (University Of California, San Diego)",2023-12-06 17:13:15+00:00,,,,,,
Putting the Object Back into Video Object Segmentation,"We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries for restructuring and interacting with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with a similar running time and improves by 4.2 J&F over DeAOT while running three times as fast. Code is available at: https://hkchengrex.github.io/Cutie",http://arxiv.org/abs/2310.12982v1,,Ho Kei Cheng (University Of Illinois Urbana-Champaign) | Seoung Wug Oh (Adobe Systems) | Brian Price (Adobe Research) | Joon-Young Lee (Adobe Research) | Alexander G. Schwing (UIUC),2023-10-19 17:59:56+00:00,,,,,,
Multi-Level Neural Scene Graphs for Dynamic Urban Environments,,,,Tobias Fischer (ETH Zurich) | Lorenzo Porzi (Facebook) | Samuel Rota Bul?? (Meta) | Marc Pollefeys (ETH Zurich / Microsoft) | Peter Kontschieder (Meta),,,,,,,
Spherical Mask: Coarse-to-Fine 3D Point Cloud Instance Segmentation with Spherical Representation,"Coarse-to-fine 3D instance segmentation methods show weak performances compared to recent Grouping-based, Kernel-based and Transformer-based methods. We argue that this is due to two limitations: 1) Instance size overestimation by axis-aligned bounding box(AABB) 2) False negative error accumulation from inaccurate box to the refinement phase. In this work, we introduce Spherical Mask, a novel coarse-to-fine approach based on spherical representation, overcoming those two limitations with several benefits. Specifically, our coarse detection estimates each instance with a 3D polygon using a center and radial distance predictions, which avoids excessive size estimation of AABB. To cut the error propagation in the existing coarse-to-fine approaches, we virtually migrate points based on the polygon, allowing all foreground points, including false negatives, to be refined. During inference, the proposal and point migration modules run in parallel and are assembled to form binary masks of instances. We also introduce two margin-based losses for the point migration to enforce corrections for the false positives/negatives and cohesion of foreground points, significantly improving the performance. Experimental results from three datasets, such as ScanNetV2, S3DIS, and STPLS3D, show that our proposed method outperforms existing works, demonstrating the effectiveness of the new instance representation with spherical coordinates.",http://arxiv.org/abs/2312.11269v1,,"Sangyun Shin (University Of Oxford) | Kaichen Zhou (Department Of Computer Science, University Of Oxford) | Madhu Vankadari (Department Of Computer Science, University Of Oxford) | Andrew Markham (University Of Oxford) | Niki Trigoni (University Of Oxford)",2023-12-18 15:14:07+00:00,,,,,,
NetTrack: Tracking Highly Dynamic Objects with a Net,,,,Guangze Zheng (The University Of Hong Kong) | Shijie Lin (None) | Haobo Zuo (University Of Hong Kong) | Changhong Fu (Tongji University) | Jia Pan (University Of Hong Kong),,,,,,,
Logit Standardization in Knowledge Distillation,"Knowledge distillation involves transferring soft labels from a teacher to a student using a shared temperature-based softmax function. However, the assumption of a shared temperature between teacher and student implies a mandatory exact match between their logits in terms of logit range and variance. This side-effect limits the performance of student, considering the capacity discrepancy between them and the finding that the innate logit relations of teacher are sufficient for student to learn. To address this issue, we propose setting the temperature as the weighted standard deviation of logit and performing a plug-and-play Z-score pre-process of logit standardization before applying softmax and Kullback-Leibler divergence. Our pre-process enables student to focus on essential logit relations from teacher rather than requiring a magnitude match, and can improve the performance of existing logit-based distillation methods. We also show a typical case where the conventional setting of sharing temperature between teacher and student cannot reliably yield the authentic distillation evaluation; nonetheless, this challenge is successfully alleviated by our Z-score. We extensively evaluate our method for various student and teacher models on CIFAR-100 and ImageNet, showing its significant superiority. The vanilla knowledge distillation powered by our pre-process can achieve favorable performance against state-of-the-art methods, and other distillation variants can obtain considerable gain with the assistance of our pre-process.",http://arxiv.org/abs/2403.01427v1,,"Shangquan Sun (University Of Chinese Academy Of Sciences) | Wenqi Ren (Sun Yat-Sen University) | Jingzhi Li (Institute Information Of Engineering, Chinese Academy Of Sciences) | Rui Wang (Institute Of Information Engineering) | Xiaochun Cao (SUN YAT-SEN UNIVERSITY)",2024-03-03 07:54:03+00:00,,,,,,
Multi-agent Collaborative Perception via Motion-aware Robust Communication Network,,,,"Shixin Hong (Tsinghua University) | Yu LIU (Tsinghua University) | Zhi Li (Shenzhen International Graduate School, Tsinghua University) | Shaohui Li ( Tsinghua University) | You He (Tsinghua University)",,,,,,,
Friendly Sharpness-Aware Minimization,,,,"Tao Li (Shanghai Jiao Tong University) | Pan Zhou (Sea Group) | Zhengbao He (Department Of Automation, Shanghai Jiao Tong University) | Xinwen Cheng (Shanghai Jiao Tong University) | Xiaolin Huang (Shanghai Jiao Tong University, Tsinghua University)",,,,,,,
PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving,,,,Xinshuo Weng (NVIDIA) | Boris Ivanovic (NVIDIA) | Yan Wang (NVIDIA) | Yue Wang (Massachusetts Institute Of Technology) | Marco Pavone (NVIDIA),,,,,,,
Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval,"Collecting well-matched multimedia datasets is crucial for training cross-modal retrieval models. However, in real-world scenarios, massive multimodal data are harvested from the Internet, which inevitably contains Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data will remarkably harm the cross-modal retrieval performance. Previous efforts tend to mitigate this problem by estimating a soft correspondence to down-weight the contribution of PMPs. In this paper, we aim to address this challenge from a new perspective: the potential semantic similarity among unpaired samples makes it possible to excavate useful knowledge from mismatched pairs. To achieve this, we propose L2RM, a general framework based on Optimal Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to generate refined alignments by seeking a minimal-cost transport plan across different modalities. To formalize the rematching idea in OT, first, we propose a self-supervised cost function that automatically learns from explicit similarity-cost mapping relation. Second, we present to model a partial OT problem while restricting the transport among false positives to further boost refined alignments. Extensive experiments on three benchmarks demonstrate our L2RM significantly improves the robustness against PMPs for existing models. The code is available at https://github.com/hhc1997/L2RM.",http://arxiv.org/abs/2403.05105v1,,Haochen Han (Xi'an Jiao Tong University) | Qinghua Zheng (Xi'an Jiao Tong University) | Guang Dai (SGIT AI) | Minnan Luo (None) | Jingdong Wang (Baidu),2024-03-08 07:09:30+00:00,,,,,,
LEMON: Learning 3D Human-Object Interaction Relation from 2D Images,"Learning 3D human-object interaction relation is pivotal to embodied AI and interaction modeling. Most existing methods approach the goal by learning to predict isolated interaction elements, e.g., human contact, object affordance, and human-object spatial relation, primarily from the perspective of either the human or the object. Which underexploit certain correlations between the interaction counterparts (human and object), and struggle to address the uncertainty in interactions. Actually, objects' functionalities potentially affect humans' interaction intentions, which reveals what the interaction is. Meanwhile, the interacting humans and objects exhibit matching geometric structures, which presents how to interact. In light of this, we propose harnessing these inherent correlations between interaction counterparts to mitigate the uncertainty and jointly anticipate the above interaction elements in 3D space. To achieve this, we present LEMON (LEarning 3D huMan-Object iNteraction relation), a unified model that mines interaction intentions of the counterparts and employs curvatures to guide the extraction of geometric correlations, combining them to anticipate the interaction elements. Besides, the 3D Interaction Relation dataset (3DIR) is collected to serve as the test bed for training and evaluation. Extensive experiments demonstrate the superiority of LEMON over methods estimating each element in isolation.",http://arxiv.org/abs/2312.08963v1,,Yuhang Yang (University Of Science And Technology Of China) | Wei Zhai (University Of Science And Technology Of China) | Hongchen Luo (University Of Science And Technology Of China) | Yang Cao (University Of Science And Technology Of China) | Zheng-Jun Zha (University Of Science And Technology Of China),2023-12-14 14:10:57+00:00,,,,,,
MS-DETR: Efficient DETR Training with Mixed Supervision,"DETR accomplishes end-to-end object detection through iteratively generating multiple object candidates based on image features and promoting one candidate for each ground-truth object. The traditional training procedure using one-to-one supervision in the original DETR lacks direct supervision for the object detection candidates.   We aim at improving the DETR training efficiency by explicitly supervising the candidate generation procedure through mixing one-to-one supervision and one-to-many supervision. Our approach, namely MS-DETR, is simple, and places one-to-many supervision to the object queries of the primary decoder that is used for inference. In comparison to existing DETR variants with one-to-many supervision, such as Group DETR and Hybrid DETR, our approach does not need additional decoder branches or object queries. The object queries of the primary decoder in our approach directly benefit from one-to-many supervision and thus are superior in object candidate prediction. Experimental results show that our approach outperforms related DETR variants, such as DN-DETR, Hybrid DETR, and Group DETR, and the combination with related DETR variants further improves the performance.",http://arxiv.org/abs/2401.03989v1,,Chuyang Zhao (Baidu) | Yifan Sun (Baidu Research) | Wenhao Wang (University Of Technology Sydney) | Qiang Chen (Baidu) | Errui Ding (Baidu) | Yi Yang (Zhejiang University) | Jingdong Wang (Baidu),2024-01-08 16:08:53+00:00,,,,,,
OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning,,,,"Geng Xinyu (None) | Jiaming Wang (Harbin Institute Of Technology) | Jiawei Gong (Harbin Institute Of Technology) | Yuerong Xue (Harbin Institute Of Technology) | Jun Xu (Harbin Institute Of Technology) | Fanglin Chen (Harbin Institute Of Technology (Shenzhen)) | Xiaolin Huang (Shanghai Jiao Tong University, Tsinghua University)",,,,,,,
NeRFDeformer: NeRF Transformation from a Single View via 3D Scene Flows,,,,Zhenggang Tang (UIUC) | Jason Ren (Apple) | Xiaoming Zhao (UIUC) | Bowen Wen (NVIDIA) | Jonathan Tremblay (NVIDIA) | Stan Birchfield (NVIDIA) | Alexander G. Schwing (UIUC),,,,,,,
Learning Continuous 3D Words for Text-to-Image Generation,"Current controls over diffusion models (e.g., through text or ControlNet) for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change. In this paper, we present an approach for allowing users of text-to-image models to have fine-grained control of several attributes in an image. We do this by engineering special sets of input tokens that can be transformed in a continuous manner -- we call them Continuous 3D Words. These attributes can, for example, be represented as sliders and applied jointly with text prompts for fine-grained control over image generation. Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses. Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process. Project Page: https://ttchengab.github.io/continuous_3d_words",http://arxiv.org/abs/2402.08654v1,,"Ta-Ying Cheng (Department Of Computer Science, University Of Oxford) | Matheus Gadelha (Adobe Systems) | Thibault Groueix (Adobe Systems) | Matthew Fisher (Adobe Research) | Radomir Mech (University Of Calgary) | Andrew Markham (University Of Oxford) | Niki Trigoni (University Of Oxford)",2024-02-13 18:34:10+00:00,,,,,,
Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds,"Adversarial attack methods based on point manipulation for 3D point cloud classification have revealed the fragility of 3D models, yet the adversarial examples they produce are easily perceived or defended against. The trade-off between the imperceptibility and adversarial strength leads most point attack methods to inevitably introduce easily detectable outlier points upon a successful attack. Another promising strategy, shape-based attack, can effectively eliminate outliers, but existing methods often suffer significant reductions in imperceptibility due to irrational deformations. We find that concealing deformation perturbations in areas insensitive to human eyes can achieve a better trade-off between imperceptibility and adversarial strength, specifically in parts of the object surface that are complex and exhibit drastic curvature changes. Therefore, we propose a novel shape-based adversarial attack method, HiT-ADV, which initially conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. Additionally, HiT-ADV is extendable to physical attack. We propose that by employing benign resampling and benign rigid transformations, we can further enhance physical adversarial strength with little sacrifice to imperceptibility. Extensive experiments have validated the superiority of our method in terms of adversarial and imperceptible properties in both digital and physical spaces. Our code is avaliable at: https://github.com/TRLou/HiT-ADV.",http://arxiv.org/abs/2403.05247v1,,"Tianrui Lou (None) | Xiaojun Jia (, Chinese Academy Of Sciences) | Jindong Gu (University Of Oxford & Google Research) | Li Liu (University Of Oulu) | Siyuan Liang (National University Of Singapore) | Bangyan He (Institute Of Information Engineering, CAS) | Xiaochun Cao (SUN YAT-SEN UNIVERSITY)",2024-03-08 12:08:06+00:00,,,,,,
Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters,,,,Jiazuo Yu (Dalian University Of Technology) | Yunzhi Zhuge (Dalian University Of Technology) | Lu Zhang (Dalian University Of Technology) | Ping Hu (University Of Electronic Science And Technology Of China) | Dong Wang (Dalian University Of Technology) | Huchuan Lu (Dalian University Of Technology) | You He (Tsinghua University),,,,,,,
Driving Everywhere with Large Language Model Policy Adaptation,"Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.",http://arxiv.org/abs/2402.05932v1,,Boyi Li (UC Berkeley / NVIDIA) | Yue Wang (Massachusetts Institute Of Technology) | Jiageng Mao (CUHK) | Boris Ivanovic (NVIDIA) | Sushant Veer (NVIDIA) | Karen Leung (University Of Washington) | Marco Pavone (NVIDIA),2024-02-08 18:59:03+00:00,,,,,,
HomoFormer: Homogenized Transformer for Image Shadow Removal,,,,Jie Xiao (University Of Science And Technology Of China) | Xueyang Fu (University Of Science And Technology Of China) | Yurui Zhu (University Of Science And Technology Of China) | Dong Li (University Of Science And Technology Of China) | Jie Huang (University Of Science And Technology Of China) | Kai Zhu (University Of Science And Technology Of China) | Zheng-Jun Zha (University Of Science And Technology Of China),,,,,,,
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation,,,,"Ke Guo (HKU) | Zhenwei Miao (Alibaba Group) | Wei Jing (NetEase,) | Weiwei Liu (Huzhou Institute Of Zhejiang University) | Weizi Li (University Of Tennessee, Knoxville) | Dayang Hao (Cainiao) | Jia Pan (University Of Hong Kong)",,,,,,,
Bayesian Diffusion Models for 3D Shape Reconstruction,"We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction.",http://arxiv.org/abs/2403.06973v1,,"Haiyang Xu (University Of Science And Technology Of China) | Yu Lei (Shanghai Jiao Tong University) | Zeyuan Chen (University Of California, San Diego) | Xiang Zhang (University Of California, San Diego) | Yue Zhao (Tsinghua University ) | Yilin Wang (Tsinghua University) | Zhuowen Tu (University Of California, San Diego)",2024-03-11 17:55:53+00:00,,,,,,
MultiDiff: Consistent Novel View Synthesis from a Single Image,,,,Norman M??ller (Meta) | Katja Schwarz (University Of Tuebingen) | Barbara Roessle (Technische Universit??t M??nchen) | Lorenzo Porzi (Facebook) | Samuel Rota Bul?? (Meta) | Matthias Nie??ner (Technical University Of Munich) | Peter Kontschieder (Meta),,,,,,,
Language Model Guided Interpretable Video Action Reasoning,,,,Ning Wang (Xidian University) | Guangming Zhu (Xidian University) | Hongsheng Li (Xi'an University Of Electronic Science And Technology) | Liang Zhang (Xidian University) | Syed Afaq Ali Shah (Edith Cowan University) | Mohammed Bennamoun (University Of Western Australia),,,,,,,
CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment,,,,"Sajid Javed (Khalifa University Of Science And Technology) | Arif Mahmood (Information Technology University, Lahore) | IYYAKUTTI IYAPPAN GANAPATHI (Khalifa University Of Science, Technology And Research) | Fayaz Ali (Khalifa University Of Science, Technology And Research) | Naoufel Werghi (Khalifa University) | Mohammed Bennamoun (University Of Western Australia)",,,,,,,
Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation,"Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS",http://arxiv.org/abs/2403.06946v1,,Xinyao Li (University Of Electronic Science And Technology Of China) | Yuke Li (Wuhan University) | Zhekai Du (University Of Electronic Science And Technology Of China) | Fengling Li (University Of Technology Sydney) | Ke Lu (University Of Electronic Science And Technology Of China) | Jingjing Li (University Of Electronic Science And Technology Of China),2024-03-11 17:33:12+00:00,,,,,,
Poly Kernel Inception Network for Remote Sensing Detection,"Object detection in remote sensing images (RSIs) often suffers from several increasing challenges, including the large variation in object scales and the diverse-ranging context. Prior methods tried to address these challenges by expanding the spatial receptive field of the backbone, either through large-kernel convolution or dilated convolution. However, the former typically introduces considerable background noise, while the latter risks generating overly sparse feature representations. In this paper, we introduce the Poly Kernel Inception Network (PKINet) to handle the above challenges. PKINet employs multi-scale convolution kernels without dilation to extract object features of varying scales and capture local context. In addition, a Context Anchor Attention (CAA) module is introduced in parallel to capture long-range contextual information. These two components work jointly to advance the performance of PKINet on four challenging remote sensing detection benchmarks, namely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.",http://arxiv.org/abs/2403.06258v1,,Xinhao Cai (Nanjing University Of Science And Technology) | Qiuxia Lai (Communication University Of China) | Yuwei Wang (Nanjing University Of Science And Technology) | Wenguan Wang (Zhejiang University) | Zeren Sun (Nanjing University Of Science And Technology) | Yazhou Yao (Nanjing University Of Science And Technology),2024-03-10 16:56:44+00:00,,,,,,
In2SET: Inner-Inter Similarity Exploiting Transformer for Dual-Camera Compressive Hyperspectral Imaging,"Dual-Camera Compressed Hyperspectral Imaging (DCCHI) offers the capability to reconstruct 3D Hyperspectral Image (HSI) by fusing compressive and Panchromatic (PAN) image, which has shown great potential for snapshot hyperspectral imaging in practice. In this paper, we introduce a novel DCCHI reconstruction network, the Intra-Inter Similarity Exploiting Transformer (In2SET). Our key insight is to make full use of the PAN image to assist the reconstruction. To this end, we propose using the intra-similarity within the PAN image as a proxy for approximating the intra-similarity in the original HSI, thereby offering an enhanced content prior for more accurate HSI reconstruction. Furthermore, we aim to align the features from the underlying HSI with those of the PAN image, maintaining semantic consistency and introducing new contextual information for the reconstruction process. By integrating In2SET into a PAN-guided unrolling framework, our method substantially enhances the spatial-spectral fidelity and detail of the reconstructed images, providing a more comprehensive and accurate depiction of the scene. Extensive experiments conducted on both real and simulated datasets demonstrate that our approach consistently outperforms existing state-of-the-art methods in terms of reconstruction quality and computational complexity. Code will be released.",http://arxiv.org/abs/2312.13319v1,,Xin Wang (Beijing Institute Of Technology) | Lizhi Wang (None) | Xiangtian Ma (Beijing Institute Of Technology) | Maoqing Zhang (Beijing Institute Of Technoloy) | Lin Zhu (Beijing Institute Of Technology) | Hua Huang (Beijing Normal University),2023-12-20 13:55:25+00:00,,,,,,
VideoMAC: Video Masked Autoencoders Meet ConvNets,"Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\textbf{5.2\%} / \textbf{6.4\%} $\mathcal{J}\&\mathcal{F}$), body part propagation (+\textbf{6.3\%} / \textbf{3.1\%} mIoU), and human pose tracking (+\textbf{10.2\%} / \textbf{11.1\%} PCK@0.1).",http://arxiv.org/abs/2402.19082v1,,Gensheng Pei (Nanjing University Of Science And Technology) | Tao Chen (None) | Xiruo Jiang (None) | ????????? Liu (Nanjing University Of Science And Technology) | Zeren Sun (Nanjing University Of Science And Technology) | Yazhou Yao (Nanjing University Of Science And Technology),2024-02-29 12:09:25+00:00,,,,,,
CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing,,,,Guiwei Zhang (Beijing University Of Aeronautics And Astronautics) | Tianyu Zhang (Du Xiaoman Financial) | Guanglin Niu (Beihang University) | Zichang Tan (Baidu) | Yalong Bai (JD AI Research) | Qing Yang (Du Xiaoman Technology(BeiJing)),,,,,,,
Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation.,,,,Dong Zhao (Xi'an University Of Electronic Science And Technology) | Shuang Wang (Xidian University) | Qi Zang (Xidian University) | Licheng Jiao (Xidian University) | Nicu Sebe (University Of Trento) | Zhun Zhong (University Of Nottingham),,,,,,,
Federated Generalized Category Discovery,"Generalized category discovery (GCD) aims at grouping unlabeled samples from known and unknown classes, given labeled data of known classes. To meet the recent decentralization trend in the community, we introduce a practical yet challenging task, namely Federated GCD (Fed-GCD), where the training data are distributively stored in local clients and cannot be shared among clients. The goal of Fed-GCD is to train a generic GCD model by client collaboration under the privacy-protected constraint. The Fed-GCD leads to two challenges: 1) representation degradation caused by training each client model with fewer data than centralized GCD learning, and 2) highly heterogeneous label spaces across different clients. To this end, we propose a novel Associated Gaussian Contrastive Learning (AGCL) framework based on learnable GMMs, which consists of a Client Semantics Association (CSA) and a global-local GMM Contrastive Learning (GCL). On the server, CSA aggregates the heterogeneous categories of local-client GMMs to generate a global GMM containing more comprehensive category knowledge. On each client, GCL builds class-level contrastive learning with both local and global GMMs. The local GCL learns robust representation with limited local data. The global GCL encourages the model to produce more discriminative representation with the comprehensive category relationships that may not exist in local data. We build a benchmark based on six visual datasets to facilitate the study of Fed-GCD. Extensive experiments show that our AGCL outperforms the FedAvg-based baseline on all datasets.",http://arxiv.org/abs/2305.14107v1,,Nan Pu (University Of Trento) | Wenjing Li (University Of Science And Technology Of China) | Xinyuan Ji (Leiden University) | Yalan Qin (Shanghai University) | Nicu Sebe (University Of Trento) | Zhun Zhong (University Of Nottingham),2023-05-23 14:27:41+00:00,,,,,,
Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation,"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches.",http://arxiv.org/abs/2403.02899v1,,Zhekai Du (University Of Electronic Science And Technology Of China) | Xinyao Li (University Of Electronic Science And Technology Of China) | Fengling Li (University Of Technology Sydney) | Ke Lu (University Of Electronic Science And Technology Of China) | Lei Zhu (Shandong Normal University) | Jingjing Li (University Of Electronic Science And Technology Of China),2024-03-05 12:06:48+00:00,,,,,,
MoML: Online Meta Adaptation for 3D Human Motion Prediction,,,,Xiaoning Sun (Nanjing University Of Science And Technology) | Huaijiang Sun (Nanjing University Of Science And Technology) | Bin Li (Nanjing University Of Science And Technology) | Dong Wei (Nanjing University Of Science And Technology) | Weiqing Li (Nanjing University Of Science And Technology) | Jianfeng Lu (Nanjing University Of Science And Technology),,,,,,,
Fast Adaptation for Human Pose Estimation via Meta-Optimization,,,,Shengxiang Hu (None) | Huaijiang Sun (Nanjing University Of Science And Technology) | Bin Li (Nanjing University Of Science And Technology) | Dong Wei (Nanjing University Of Science And Technology) | Weiqing Li (Nanjing University Of Science And Technology) | Jianfeng Lu (Nanjing University Of Science And Technology),,,,,,,
SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream,,,,Lin Zhu (Beijing Institute Of Technology) | Kangmin Jia (Beijing Institute Of Technology) | Yifan Zhao (Peking University) | Yunshan Qi (BeiHang University) | Lizhi Wang (None) | Hua Huang (Beijing Normal University),,,,,,,
Dynamic Prompt Optimizing for Text-to-Image Generation,,,,Wenyi Mo (Renmin University Of China) | Tianyu Zhang (Du Xiaoman Financial) | Yalong Bai (JD AI Research) | Bing Su (None) | Ji-Rong Wen (Renmin University Of China) | Qing Yang (Du Xiaoman Technology(BeiJing)),,,,,,,
Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans,"We propose an unsupervised method for parsing large 3D scans of real-world scenes into interpretable parts. Our goal is to provide a practical tool for analyzing 3D scenes with unique characteristics in the context of aerial surveying and mapping, without relying on application-specific user annotations. Our approach is based on a probabilistic reconstruction model that decomposes an input 3D point cloud into a small set of learned prototypical shapes. Our model provides an interpretable reconstruction of complex scenes and leads to relevant instance and semantic segmentations. To demonstrate the usefulness of our results, we introduce a novel dataset of seven diverse aerial LiDAR scans. We show that our method outperforms state-of-the-art unsupervised methods in terms of decomposition accuracy while remaining visually interpretable. Our method offers significant advantage over existing approaches, as it does not require any manual annotations, making it a practical and efficient tool for 3D scene analysis. Our code and dataset are available at https://imagine.enpc.fr/~loiseaur/learnable-earth-parser",http://arxiv.org/abs/2304.09704v1,,Romain Loiseau (Ecole Nationale Des Ponts Et Chausees) | Elliot Vincent (Imagine (LIGM) - Willow (Inria)) | Mathieu Aubry (ENPC) | Loic Landrieu (ENPC),2023-04-19 14:49:31+00:00,,,,,,
OpenStreetView-5M: The Many Roads to Global Visual Geolocation,,,,"Guillaume Astruc (ENPC, Ecole Nationale Des Ponts Et Chausees) | Nicolas Dufour (Ecole Nationale Des Ponts Et Chausees) | Ioannis Siglidis (Ecole Nationale Des Ponts Et Chausees) | Constantin Aronssohn (ENPC, Ecole Nationale Des Ponts Et Chaus??es) | Nacim Bouia (Ecole Normale Superieure) | Stephanie Fu (University Of California, Berkeley) | Romain Loiseau (Ecole Nationale Des Ponts Et Chausees) | Van Nguyen Nguyen (Ecole Des Ponts ParisTech) | Charles Raude (ENPC, Ecole Nationale Des Ponts Et Chausees) | Elliot Vincent (Imagine (LIGM) - Willow (Inria)) | Lintao XU (Universit?? Gustave Eiffel) | Hongyu Zhou (Ecole Nationale Des Ponts Et Chausees) | Loic Landrieu (ENPC)",,,,,,,
3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation,"3D Morphable Models (3DMMs) provide promising 3D face reconstructions in various applications. However, existing methods struggle to reconstruct faces with extreme expressions due to deficiencies in supervisory signals, such as sparse or inaccurate landmarks. Segmentation information contains effective geometric contexts for face reconstruction. Certain attempts intuitively depend on differentiable renderers to compare the rendered silhouettes of reconstruction with segmentation, which is prone to issues like local optima and gradient instability. In this paper, we fully utilize the facial part segmentation geometry by introducing Part Re-projection Distance Loss (PRDL). Specifically, PRDL transforms facial part segmentation into 2D points and re-projects the reconstruction onto the image plane. Subsequently, by introducing grid anchors and computing different statistical distances from these anchors to the point sets, PRDL establishes geometry descriptors to optimize the distribution of the point sets for face reconstruction. PRDL exhibits a clear gradient compared to the renderer-based methods and presents state-of-the-art reconstruction performance in extensive quantitative and qualitative experiments. The project will be publicly available.",http://arxiv.org/abs/2312.00311v2,,"Zidu Wang (Institute Of Automation, Chinese Academy Of Sciences) | Xiangyu Zhu (None) | Tianshuo Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Baiqin Wang (None) | Zhen Lei (Institute Of Automation, Chinese Academy Of Sciences)",2023-12-01 03:05:21+00:00,,,,,,
REACTO: Reconstructing Articulated Objects from a Single Video,,,,"Chaoyue Song (Nanyang Technological University) | Jiacheng Wei (Nanyang Technological University) | Chuan-Sheng Foo (Centre For Frontier AI Research, A*STAR) | Guosheng Lin (Nanyang Technological University) | Fayao Liu (Institute For Infocomm Research, A*STAR)",,,,,,,
Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning,"In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.",http://arxiv.org/abs/2403.01781v1,,"Tung Le (University Of California, Irvine) | Khai Nguyen (UT Austin) | Shanlin Sun (University Of California, Irvine) | Nhat Ho (University Of Texas, Austin) | Xiaohui Xie (University Of California, Irvine)",2024-03-04 07:21:07+00:00,,,,,,
VTimeLLM: Empower LLM to Grasp Video Moments,"Large language models (LLMs) have shown remarkable text understanding capabilities, which have been extended as Video LLMs to handle video data for comprehending visual details. However, existing Video LLMs can only provide a coarse description of the entire video, failing to capture the precise start and end time boundary of specific events. In this paper, we solve this issue via proposing VTimeLLM, a novel Video LLM designed for fine-grained video moment understanding and reasoning with respect to time boundary. Specifically, our VTimeLLM adopts a boundary-aware three-stage training strategy, which respectively utilizes image-text pairs for feature alignment, multiple-event videos to increase temporal-boundary awareness, and high-quality video-instruction tuning to further improve temporal understanding ability as well as align with human intents. Extensive experiments demonstrate that in fine-grained time-related comprehension tasks for videos such as Temporal Video Grounding and Dense Video Captioning, VTimeLLM significantly outperforms existing Video LLMs. Besides, benefits from the fine-grained temporal understanding of the videos further enable VTimeLLM to beat existing Video LLMs in video dialogue benchmark, showing its superior cross-modal understanding and reasoning abilities.",http://arxiv.org/abs/2311.18445v1,,Bin Huang (Tsinghua University) | Xin Wang (None) | Hong Chen (None) | Zihan Song (Tsinghua University) | Wenwu Zhu (Tsinghua University),2023-11-30 10:49:56+00:00,,,,,,
Deep Video Inverse Tone Mapping Based on Temporal Clues,,,,Yuyao Ye (Peking University) | Ning Zhang (None) | Yang Zhao (Hefei University Of Technology) | Hongbin Cao (ByteDance) | Ronggang Wang (Peking University Shenzhen Graduate School),,,,,,,
Prompt-enhanced Multiple Instance Learning for Weakly Supervised Anomaly Detection,,,,Junxi Chen (None) | Liang Li (None) | Li Su (University Of Chinese Academy Of Sciences) | Zheng-Jun Zha (University Of Science And Technology Of China) | Qingming Huang (University Of Chinese Academy Of Sciences),,,,,,,
CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing,,,,"A Liu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Shuai Xue (Beijing Institute Of Technology) | Gan Jianwen (Macao University Of Science And Techonology) | Jun Wan (None) | Yanyan Liang (Macau University Of Science And Technology) | Jiankang Deng (Imperial College London & Huawei UKRD) | Sergio Escalera (Computer Vision Center) | Zhen Lei (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers,"Recent advances in generative AI have significantly enhanced image and video editing, particularly in the context of text prompt control. State-of-the-art approaches predominantly rely on diffusion models to accomplish these tasks. However, the computational demands of diffusion-based methods are substantial, often necessitating large-scale paired datasets for training, and therefore challenging the deployment in practical applications. This study addresses this challenge by breaking down the text-based video editing process into two separate stages. In the first stage, we leverage an existing text-to-image diffusion model to simultaneously edit a few keyframes without additional fine-tuning. In the second stage, we introduce an efficient model called MaskINT, which is built on non-autoregressive masked generative transformers and specializes in frame interpolation between the keyframes, benefiting from structural guidance provided by intermediate frames. Our comprehensive set of experiments illustrates the efficacy and efficiency of MaskINT when compared to other diffusion-based methodologies. This research offers a practical solution for text-based video editing and showcases the potential of non-autoregressive masked generative transformers in this domain.",http://arxiv.org/abs/2312.12468v1,,"Haoyu Ma (University Of California, Irvine) | Shahin Mahdizadehaghdam (Meta) | Bichen Wu (Facebook) | Zhipeng Fan (Facebook) | Yuchao Gu (None) | Wenliang Zhao (Meta Inc) | Lior Shapira (Meta) | Xiaohui Xie (University Of California, Irvine)",2023-12-19 07:05:39+00:00,,,,,,
DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling,,,,Xiaoyun Zheng (Peking University Shenzhen Graduate School) | Liwei Liao (Peking University) | Xufeng Li (Cityu) | Jianbo Jiao (University Of Birmingham) | Rongjie Wang (PengCheng Laboratory) | Feng Gao (Peking University) | Shiqi Wang (City University Of Hong Kong) | Ronggang Wang (Peking University Shenzhen Graduate School),,,,,,,
Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior,"Recent works on text-to-3d generation show that using only 2D diffusion supervision for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this issue by retraining diffusion models with images rendered from 3D data to ensure multi-view consistency while struggling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D diffusion model. Specifically, we demonstrate that high-quality and diverse 3D geometry can be guaranteed by keypoints supervision through a sparse ray sampling approach. Moreover, to ensure accurate appearances of different views, we further modulate the output of the 2D diffusion model to the correct patterns of the template views without altering the generated object's style. These two decoupled designs effectively harness 3D information from reference objects to generate 3D objects while preserving the generation quality of the 2D diffusion model. Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our project page is available at: https://stellarcheng.github.io/Sculpt3D/.",http://arxiv.org/abs/2403.09140v1,,"Chen Cheng (Nanyang Technological University) | Xiaofeng Yang (Nanyang Technological University) | Fan Yang (None) | Chengzeng Feng (Nanyang Technological University) | ZHOUJIE FU (Nanyang Technological University) | Chuan-Sheng Foo (Centre For Frontier AI Research, A*STAR) | Guosheng Lin (Nanyang Technological University) | Fayao Liu (Institute For Infocomm Research, A*STAR)",2024-03-14 07:39:59+00:00,,,,,,
Retraining-free Model Quantization via One-Shot Weight-Coupling Learning,,,,"Chen Tang (Tsinghua University) | Yuan Meng (Tsinghua University) | Jiacheng Jiang (Tsinghua University) | Shuzhao Xie (Tsinghua University) | Rongwei Lu (Tsinghua University) | Xinzhu Ma (University Of Sydney) | Zhi Wang (SIGS, Tsinghua University) | Wenwu Zhu (Tsinghua University)",,,,,,,
Weakly Supervised Video Individual Counting,,,,Xinyan Liu (None) | Guorong Li (University Of Chinese Academy Of Sciences) | Yuankai Qi (The University Of Adelaide) | Ziheng Yan (University Of Chinese Academy Of Sciences) | Zhenjun Han (University Of The Chinese Academy Of Sciences) | Anton Van Den Hengel (University Of Adelaide) | Ming-Hsuan Yang (University Of California At Merced) | Qingming Huang (University Of Chinese Academy Of Sciences),,,,,,,
EASE-DETR: Easing the Competition among Object Queries,,,,Yulu Gao (Beijing University Of Aeronautics And Astronautics) | Yifan Sun (Baidu Research) | Xudong Ding (Beijing University Of Aeronautics And Astronautics) | Chuyang Zhao (Baidu) | Si Liu (Beihang University),,,,,,,
SocialCircle: Learning the Angle-based Social Interaction Representation for Pedestrian Trajectory Prediction,"Analyzing and forecasting trajectories of agents like pedestrians and cars in complex scenes has become more and more significant in many intelligent systems and applications. The diversity and uncertainty in socially interactive behaviors among a rich variety of agents make this task more challenging than other deterministic computer vision tasks. Researchers have made a lot of efforts to quantify the effects of these interactions on future trajectories through different mathematical models and network structures, but this problem has not been well solved. Inspired by marine animals that localize the positions of their companions underwater through echoes, we build a new anglebased trainable social representation, named SocialCircle, for continuously reflecting the context of social interactions at different angular orientations relative to the target agent. We validate the effect of the proposed SocialCircle by training it along with several newly released trajectory prediction models, and experiments show that the SocialCircle not only quantitatively improves the prediction performance, but also qualitatively helps better consider social interactions when forecasting pedestrian trajectories in a way that is consistent with human intuitions.",http://arxiv.org/abs/2310.05370v1,,Conghao Wong (Huazhong University Of Science And Technology) | Beihao Xia (Huazhong University Of Science And Technology) | Ziqian Zou (Huazhong University Of Science And Technology) | Yulong Wang (Huazhong Agricultural University) | Xinge You (Huazhong University Of Science And Technology),2023-10-09 02:59:21+00:00,,,,,,
Learning Dense Visual Correspondence for Category-level Garment Manipulation,,,,Ruihai Wu (Peking University) | Haoran Lu (Peking University) | Yiyan Wang (Beijing Institute Of Technology) | Yubo Wang (Peking University) | Hao Dong (None),,,,,,,
"Point, Segment and Count: A Generalized Framework for Object Counting","Class-agnostic object counting aims to count all objects in an image with respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot counting. Current state-of-the-art methods highly rely on density maps to predict object counts, which lacks model interpretability. In this paper, we propose a generalized framework for both few-shot and zero-shot object counting based on detection. Our framework combines the superior advantages of two foundation models without compromising their zero-shot capability: (\textbf{i}) SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP to classify proposals to obtain accurate object counts. However, this strategy meets the obstacles of efficiency overhead and the small crowded objects that cannot be localized and distinguished. To address these issues, our framework, termed PseCo, follows three steps: point, segment, and count. Specifically, we first propose a class-agnostic object localization to provide accurate but least point prompts for SAM, which consequently not only reduces computation costs but also avoids missing small objects. Furthermore, we propose a generalized object classification that leverages CLIP image/text embeddings as the classifier, following a hierarchical knowledge distillation to obtain discriminative classifications among hierarchical mask proposals. Extensive experimental results on FSC-147 dataset demonstrate that PseCo achieves state-of-the-art performance in both few-shot/zero-shot object counting/detection, with additional results on large-scale COCO and LVIS datasets. The source code is available at \url{https://github.com/Hzzone/PseCo}.",http://arxiv.org/abs/2311.12386v2,,Zhizhong Huang (Fudan University) | Mingliang Dai (Fudan University) | Yi Zhang (Sichuan University) | Junping Zhang (Fudan University) | Hongming Shan (None),2023-11-21 06:55:21+00:00,,,,,,
RepViT: Revisiting Mobile CNN From ViT Perspective,"Recently, lightweight Vision Transformers (ViTs) demonstrate superior performance and lower latency, compared with lightweight Convolutional Neural Networks (CNNs), on resource-constrained mobile devices. Researchers have discovered many structural connections between lightweight ViTs and lightweight CNNs. However, the notable architectural disparities in the block structure, macro, and micro designs between them have not been adequately examined. In this study, we revisit the efficient design of lightweight CNNs from ViT perspective and emphasize their promising prospect for mobile devices. Specifically, we incrementally enhance the mobile-friendliness of a standard lightweight CNN, \ie, MobileNetV3, by integrating the efficient architectural designs of lightweight ViTs. This ends up with a new family of pure lightweight CNNs, namely RepViT. Extensive experiments show that RepViT outperforms existing state-of-the-art lightweight ViTs and exhibits favorable latency in various vision tasks. Notably, on ImageNet, RepViT achieves over 80\% top-1 accuracy with 1.0 ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge. Besides, when RepViT meets SAM, our RepViT-SAM can achieve nearly 10$\times$ faster inference than the advanced MobileSAM. Codes and models are available at \url{https://github.com/THU-MIG/RepViT}.",http://arxiv.org/abs/2307.09283v8,,Ao Wang (Tsinghua University) | Hui Chen (Tsinghua University) | Zijia Lin (Kuaishou Technology) | Jungong Han (Aberystwyth University) | Guiguang Ding (Tsinghua University),2023-07-18 14:24:33+00:00,,,,,,
Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning,,,,Wenjin Hou (Huazhong University Of Science And Technology) | Shiming Chen (Carnegie Mellon University) | Shuhuang Chen (Huazhong University Of Science And Technology) | Ziming Hong (The University Of Sydney) | Yan Wang (Alibaba Group) | Xuetao Feng (Alibaba Group) | Salman Khan (Mohamed Bin Zayed University Of Artificial Intelligence) | Fahad Shahbaz Khan (MBZUAI; Link??ping University) | Xinge You (Huazhong University Of Science And Technology),,,,,,,
"One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications","The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications. The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and meantime the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios. During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to respond to different input prompts, further minimizing the impact on other concepts. Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM. Our code and pre-tuned SPMs are available on the project page https://lyumengyao.github.io/projects/spm.",http://arxiv.org/abs/2312.16145v2,,"Mengyao Lyu (Tsinghua University) | Yuhong Yang (None) | Haiwen Hong (Alibaba Group) | Hui Chen (Tsinghua University) | Xuan Jin (University Of Science And Technology Of China) | Yuan He (Alibaba Group) | Hui Xue (Zhejiang University, Tsinghua University) | Jungong Han (Aberystwyth University) | Guiguang Ding (Tsinghua University)",2023-12-26 18:08:48+00:00,,,,,,
DreamVideo: Composing Your Dream Videos with Customized Subject and Motion,"Customized generation using diffusion models has made impressive progress in image generation, but remains unsatisfactory in the challenging video generation task, as it requires the controllability of both subjects and motions. To that end, we present DreamVideo, a novel approach to generating personalized videos from a few static images of the desired subject and a few videos of target motion. DreamVideo decouples this task into two stages, subject learning and motion learning, by leveraging a pre-trained video diffusion model. The subject learning aims to accurately capture the fine appearance of the subject from provided images, which is achieved by combining textual inversion and fine-tuning of our carefully designed identity adapter. In motion learning, we architect a motion adapter and fine-tune it on the given videos to effectively model the target motion pattern. Combining these two lightweight and efficient adapters allows for flexible customization of any subject with any motion. Extensive experimental results demonstrate the superior performance of our DreamVideo over the state-of-the-art methods for customized video generation. Our project page is at https://dreamvideo-t2v.github.io.",http://arxiv.org/abs/2312.04433v1,,"Yujie Wei (Fudan University) | Shiwei Zhang (Alibaba Group) | Zhiwu Qing (Huazhong University Of Science And Technology, Tsinghua University) | Hangjie Yuan (Nanyang Technological University) | Zhiheng Liu (University Of Science And Technology Of China) | Yu Liu (Alibaba Group) | Yingya Zhang (Alibaba Group) | Jingren Zhou (Alibaba Group) | Hongming Shan (None)",2023-12-07 16:57:26+00:00,,,,,,
ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation,"Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.",http://arxiv.org/abs/2312.16217v1,,Xiaoqi Li (Peking University) | Mingxu Zhang (Beijing University Of Posts And Telecommunications) | Yiran Geng (Peking University) | Haoran Geng (Peking University) | Yuxing Long (Beijing University Of Posts And Telecommunications) | Yan Shen (Peking University) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Jiaming Liu (Peking University) | Hao Dong (None),2023-12-24 06:38:11+00:00,,,,,,
Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training,"In this paper, we target the adaptive source driven 3D scene editing task by proposing a CustomNeRF model that unifies a text description or a reference image as the editing prompt. However, obtaining desired editing results conformed with the editing prompt is nontrivial since there exist two significant challenges, including accurate editing of only foreground regions and multi-view consistency given a single-view reference image. To tackle the first challenge, we propose a Local-Global Iterative Editing (LGIE) training scheme that alternates between foreground region editing and full-image editing, aimed at foreground-only manipulation while preserving the background. For the second challenge, we also design a class-guided regularization that exploits class priors within the generation model to alleviate the inconsistency problem among different views in image-driven editing. Extensive experiments show that our CustomNeRF produces precise editing results under various real scenes for both text- and image-driven settings.",http://arxiv.org/abs/2312.01663v1,,"Runze He (Institute Of Information Engineering, Chinese Academy Of Sciences) | Shaofei Huang (Institute Of Information Engineering, Chinese Academy Of Sciences) | Xuecheng Nie (National University Of Singaore, National University Of Singapore) | Tianrui Hui (Hefei University Of Technology) | Luoqi Liu (None) | Jiao Dai (Institute Of Information Engineering,Chinese Academy Of Sciences) | Jizhong Han (Institute Of Information Engineering) | Guanbin Li (Sun Yat-Sen University) | Si Liu (Beihang University)",2023-12-04 06:25:06+00:00,,,,,,
Towards Detailed and Robust 3D Clothed Human Reconstruction with High-Frequency and Low-Frequency Information of Parametric Body Models,,,,Yifan Yang (South China University Of Technology) | Dong Liu (South China University Of Technology) | Shuhai Zhang (South China University Of Technology) | Zeshuai Deng (SCUT) | Zixiong Huang (South China University Of Technology) | Mingkui Tan (South China University Of Technology),,,,,,,
SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution,"Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior following abstract instructions across diverse tasks. Experiments on multi-task robotic manipulation benchmarks like Meta-World and LOReL demonstrate state-of-the-art performance and human-interpretable skill representations from SkillDiffuser. More visualization results and information could be found on our website.",http://arxiv.org/abs/2312.11598v2,,"Zhixuan Liang (The University Of Hong Kong) | Yao Mu (The University Of Hong Kong) | Hengbo Ma (None) | Masayoshi Tomizuka (University Of California, Berkeley) | Mingyu Ding (UC Berkeley) | Ping Luo (The University Of Hong Kong)",2023-12-18 18:16:52+00:00,,,,,,
Video Interpolation with Diffusion Models,,,,"Siddhant Jain (Google Research) | Daniel Watson (Google DeepMind) | Aleksander Holynski (UC Berkeley & Google Research) | Eric Tabellion (Google) | Ben Poole (Google) | Janne Kontkanen (Research, Google)",,,,,,,
FSC: Few-point Shape Completion,"While previous studies have demonstrated successful 3D object shape completion with a sufficient number of points, they often fail in scenarios when a few points, e.g. tens of points, are observed. Surprisingly, via entropy analysis, we find that even a few points, e.g. 64 points, could retain substantial information to help recover the 3D shape of the object. To address the challenge of shape completion with very sparse point clouds, we then propose Few-point Shape Completion (FSC) model, which contains a novel dual-branch feature extractor for handling extremely sparse inputs, coupled with an extensive branch for maximal point utilization with a saliency branch for dynamic importance assignment. This model is further bolstered by a two-stage revision network that refines both the extracted features and the decoder output, enhancing the detail and authenticity of the completed point cloud. Our experiments demonstrate the feasibility of recovering 3D shapes from a few points. The proposed Few-point Shape Completion (FSC) model outperforms previous methods on both few-point inputs and many-point inputs, and shows good generalizability to different object categories.",http://arxiv.org/abs/2403.07359v2,,Xianzu Wu (Jianghan University) | Xianfeng Wu (Jianghan University) | Tianyu Luan (State University Of New York At Buffalo) | Yajing Bai (Jianghan University) | Zhongyuan Lai (Jianghan University) | Junsong Yuan (State University Of New York At Buffalo),2024-03-12 06:45:34+00:00,,,,,,
PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion,"In this paper, we introduce PI3D, a novel and efficient framework that utilizes the pre-trained text-to-image diffusion models to generate high-quality 3D shapes in minutes. On the one hand, it fine-tunes a pre-trained 2D diffusion model into a 3D diffusion model, enabling both 3D generative capabilities and generalization derived from the 2D model. On the other, it utilizes score distillation sampling of 2D diffusion models to quickly improve the quality of the sampled 3D shapes. PI3D enables the migration of knowledge from image to triplane generation by treating it as a set of pseudo-images. We adapt the modules in the pre-training model to enable hybrid training using pseudo and real images, which has proved to be a well-established strategy for improving generalizability. The efficiency of PI3D is highlighted by its ability to sample diverse 3D models in seconds and refine them in minutes. The experimental results confirm the advantages of PI3D over existing methods based on either 3D diffusion models or lifting 2D diffusion models in terms of fast generation of 3D consistent and high-quality models. The proposed PI3D stands as a promising advancement in the field of text-to-3D generation, and we hope it will inspire more research into 3D generation leveraging the knowledge in both 2D and 3D data.",http://arxiv.org/abs/2312.09069v1,,Ying-Tian Liu (Tsinghua University) | Yuan-Chen Guo (Tsinghua University) | Guan Luo (Tsinghua University) | Heyi Sun (Tsinghua University) | Wei Yin ( Shenzhen DJI Sciences And Technologies Ltd.) | Song-Hai Zhang (Tsinghua University),2023-12-14 16:04:34+00:00,,,,,,
SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes,"We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. We will release the codebase for research purposes.",http://arxiv.org/abs/2308.10638v1,,"Soubhik Sanyal (None) | Partha Ghosh (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Jinlong Yang (Google) | Michael J. Black (University Of T??bingen) | Justus Thies (Max-Planck Institute For Intelligent Systems) | Timo Bolkart (Google)",2023-08-21 11:23:25+00:00,,,,,,
Geometry Transfer for Stylizing Radiance Fields,"Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.",http://arxiv.org/abs/2402.00863v2,,Hyunyoung Jung (Seoul National University) | Seonghyeon Nam (Facebook) | Nikolaos Sarafianos (Meta Reality Labs) | Sungjoo Yoo (None) | Alexander Sorkine-Hornung (Meta) | Rakesh Ranjan (None),2024-02-01 18:58:44+00:00,,,,,,
Prompt-Driven Referring Image Segmentation with Instance Contrasting,,,,"Chao Shang (None) | Zichen Song (University Of Electronic Science And Technology Of China) | Heqian Qiu (University Of Electronic Science And Technology Of China) | Lanxiao Wang (University Of Electronic Science And Technology Of China) | Fanman Meng (University Of Electronic Science And Technology Of China) | Hongliang Li (University Of Electronic Science And Technology Of China, Tsinghua University)",,,,,,,
Guided Slot Attention for Unsupervised Video Object Segmentation,"Unsupervised video object segmentation aims to segment the most prominent object in a video sequence. However, the existence of complex backgrounds and multiple foreground objects make this task challenging. To address this issue, we propose a guided slot attention network to reinforce spatial structural information and obtain better foreground--background separation. The foreground and background slots, which are initialized with query guidance, are iteratively refined based on interactions with template information. Furthermore, to improve slot--template interaction and effectively fuse global and local features in the target and reference frames, K-nearest neighbors filtering and a feature aggregation transformer are introduced. The proposed model achieves state-of-the-art performance on two popular datasets. Additionally, we demonstrate the robustness of the proposed model in challenging scenes through various comparative experiments.",http://arxiv.org/abs/2303.08314v2,,Minhyeok Lee (Yonsei University) | Suhwan Cho (Yonsei University) | Dogyoon Lee (Yonsei University) | Chaewon Park (Yonsei University) | Jungho Lee (None) | Sangyoun Lee (Yonsei University),2023-03-15 02:08:20+00:00,,,,,,
Class Incremental Learning with Multi-Teacher Distillation,,,,"Haitao Wen (University Of Electronic Science And Technology Of China) | Lili Pan (University Of Electronic Science And Technology Of China) | Yu Dai (University Of Electronic Science And Technology Of China) | Heqian Qiu (University Of Electronic Science And Technology Of China) | Lanxiao Wang (University Of Electronic Science And Technology Of China) | Qingbo Wu (University Of Electronic Science And Technology Of China) | Hongliang Li (University Of Electronic Science And Technology Of China, Tsinghua University)",,,,,,,
ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models,,,,"Meng-Li Shih (University Of Washington) | Wei-Chiu Ma (Cornell University) | Lorenzo Boyice (Google) | Aleksander Holynski (UC Berkeley & Google Research) | Forrester Cole (Google) | Brian Curless (University Of Washington) | Janne Kontkanen (Research, Google)",,,,,,,
G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images,,,,Zixiong Huang (South China University Of Technology) | Qi Chen (The University Of Adelaide) | Libo Sun (University Of Adelaide) | Yifan Yang (South China University Of Technology) | Naizhou Wang (CVTE Research) | Qi Wu (University Of Adelaide) | Mingkui Tan (South China University Of Technology),,,,,,,
OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance. Our dataset will be made publicly available.",http://arxiv.org/abs/2402.09181v1,,"Yutao Hu (University Of Hong Kong) | Tianbin (None) | Quanfeng Lu (Shanghai AI Laboratory) | Wenqi Shao (The Chinese University Of Hong Kong) | Junjun He (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Ping Luo (The University Of Hong Kong)",2024-02-14 13:51:56+00:00,,,,,,
Spectrum AUC Difference (SAUCD): Human Aligned 3D Shape Evaluation,"Existing 3D mesh shape evaluation metrics mainly focus on the overall shape but are usually less sensitive to local details. This makes them inconsistent with human evaluation, as human perception cares about both overall and detailed shape. In this paper, we propose an analytic metric named Spectrum Area Under the Curve Difference (SAUCD) that demonstrates better consistency with human evaluation. To compare the difference between two shapes, we first transform the 3D mesh to the spectrum domain using the discrete Laplace-Beltrami operator and Fourier transform. Then, we calculate the Area Under the Curve (AUC) difference between the two spectrums, so that each frequency band that captures either the overall or detailed shape is equitably considered. Taking human sensitivity across frequency bands into account, we further extend our metric by learning suitable weights for each frequency band which better aligns with human perception. To measure the performance of SAUCD, we build a 3D mesh evaluation dataset called Shape Grading, along with manual annotations from more than 800 subjects. By measuring the correlation between our metric and human evaluation, we demonstrate that SAUCD is well aligned with human evaluation, and outperforms previous 3D mesh metrics.",http://arxiv.org/abs/2403.01619v1,,"Tianyu Luan (State University Of New York At Buffalo) | Zhong Li (InnoPeak Technology) | Lele Chen (University Of Rochester) | Xuan Gong (Harvard University) | Lichang Chen (Department Of Computer Science, University Of Maryland, College Park) | Yi Xu (OPPO US Research Center) | Junsong Yuan (State University Of New York At Buffalo)",2024-03-03 21:35:00+00:00,,,,,,
PlatoNeRF: 3D Reconstruction in Plato??s Cave via Single-View Two-Bounce Lidar,"3D reconstruction from a single-view is challenging because of the ambiguity from monocular cues and lack of information about occluded regions. Neural radiance fields (NeRF), while popular for view synthesis and 3D reconstruction, are typically reliant on multi-view images. Existing methods for single-view 3D reconstruction with NeRF rely on either data priors to hallucinate views of occluded regions, which may not be physically accurate, or shadows observed by RGB cameras, which are difficult to detect in ambient light and low albedo backgrounds. We propose using time-of-flight data captured by a single-photon avalanche diode to overcome these limitations. Our method models two-bounce optical paths with NeRF, using lidar transient data for supervision. By leveraging the advantages of both NeRF and two-bounce light measured by lidar, we demonstrate that we can reconstruct visible and occluded geometry without data priors or reliance on controlled ambient lighting or scene albedo. In addition, we demonstrate improved generalization under practical constraints on sensor spatial- and temporal-resolution. We believe our method is a promising direction as single-photon lidars become ubiquitous on consumer devices, such as phones, tablets, and headsets.",http://arxiv.org/abs/2312.14239v1,,Tzofi Klinghoffer (Massachusetts Institute Of Technology) | Xiaoyu Xiang (Meta) | Siddharth Somasundaram (Massachusetts Institute Of Technology) | Yuchen Fan (Facebook) | Christian Richardt (Meta Reality Labs) | Ramesh Raskar (Massachusetts Institute Of Technology) | Rakesh Ranjan (None),2023-12-21 18:59:53+00:00,,,,,,
Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers,"Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.",http://arxiv.org/abs/2312.09147v2,,Zi-Xin Zou (None) | Zhipeng Yu (University Of The Chinese Academy Of Sciences) | Yuan-Chen Guo (Tsinghua University) | Yangguang Li (Shanghai AI Laboratory) | Yan-Pei Cao (Tencent ARC Lab) | Ding Liang (Tsinghua University) | Song-Hai Zhang (Tsinghua University),2023-12-14 17:18:34+00:00,,,,,,
Dual Prototype Attention for Unsupervised Video Object Segmentation,"Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study.",http://arxiv.org/abs/2211.12036v2,,Suhwan Cho (Yonsei University) | Minhyeok Lee (Yonsei University) | Seunghoon Lee (Yonsei University) | Dogyoon Lee (Yonsei University) | Heeseung Choi (None) | Ig-Jae Kim (Korea Institute Of Science And Technology) | Sangyoun Lee (Yonsei University),2022-11-22 06:19:17+00:00,,,,,,
Emotional Speech-Driven 3D Body Animation via Disentangled Latent Diffusion,"Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech. Our project website is amuse.is.tue.mpg.de.",http://arxiv.org/abs/2312.04466v1,,"Kiran Chhatre (KTH Royal Institute Of Technology) | Radek Danecek (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Nikos Athanasiou (None) | Giorgio Becherini (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Christopher Peters (KTH Royal Institute Of Technology) | Michael J. Black (University Of T??bingen) | Timo Bolkart (Google)",2023-12-07 17:39:25+00:00,,,,,,
Open-World Human-Object Interaction Detection via Multi-modal Prompts,,,,"Jie Yang (The Chinese University Of Hong Kong, Shenzhen) | Bingliang Li (The Chinese University Of Hong Kong (Shenzhen)) | Ailing Zeng (IDEA) | Lei Zhang (International Digital Economy Academy (IDEA)) | Ruimao Zhang (The Chinese University Of Hong Kong (Shenzhen))",,,,,,,
S2 MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering ,"Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding features to ensure inter-view semantic consistency. Experimental results on six large-scale multi-view datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art algorithms in terms of clustering performance and CPU execution time, especially when handling massive data. The code of S^2MVTC is publicly available at https://github.com/longzhen520/S2MVTC.",http://arxiv.org/abs/2403.09107v1,,Zhen Long (University Of Electronic Science And Technology Of China) | Qiyuan Wang (None) | Yazhou Ren (University Of Electronic Science And Technology Of China) | Yipeng Liu (University Of Electronic Science And Technology Of China) | Ce Zhu (University Of Electronic Science And Technology Of China),2024-03-14 05:00:29+00:00,,,,,,
LEOD: Label-Efficient Object Detection for Event Cameras,"Object detection with event cameras enjoys the property of low latency and high dynamic range, making it suitable for safety-critical scenarios such as self-driving. However, labeling event streams with high temporal resolutions for supervised training is costly. We address this issue with LEOD, the first framework for label-efficient event-based detection. Our method unifies weakly- and semi-supervised object detection with a self-training mechanism. We first utilize a detector pre-trained on limited labels to produce pseudo ground truth on unlabeled events, and then re-train the detector with both real and generated labels. Leveraging the temporal consistency of events, we run bi-directional inference and apply tracking-based post-processing to enhance the quality of pseudo labels. To stabilize training, we further design a soft anchor assignment strategy to mitigate the noise in labels. We introduce new experimental protocols to evaluate the task of label-efficient event-based detection on Gen1 and 1Mpx datasets. LEOD consistently outperforms supervised baselines across various labeling ratios. For example, on Gen1, it improves mAP by 8.6% and 7.8% for RVT-S trained with 1% and 2% labels. On 1Mpx, RVT-S with 10% labels even surpasses its fully-supervised counterpart using 100% labels. LEOD maintains its effectiveness even when all labeled data are available, reaching new state-of-the-art results. Finally, we show that our method readily scales to improve larger detectors as well.",http://arxiv.org/abs/2311.17286v1,,Ziyi Wu (University Of Toronto) | Mathias Gehrig (University Of Zurich) | Qing Lyu (University Of Toronto) | Xudong Liu (None) | Igor Gilitschenski (University Of Toronto),2023-11-29 00:09:45+00:00,,,,,,
SPAD: Spatially Aware Multiview Diffusers,"We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: https://yashkant.github.io/spad",http://arxiv.org/abs/2402.05235v1,,Yash Kant (University Of Toronto / Snap Research) | Aliaksandr Siarohin (Snap) | Ziyi Wu (University Of Toronto) | Michael Vasilkovsky (Snap) | Guocheng Qian (KAUST) | Jian Ren (Snap) | Riza Alp Guler (Snap) | Bernard Ghanem (KAUST) | Sergey Tulyakov (Snap) | Igor Gilitschenski (University Of Toronto),2024-02-07 20:16:09+00:00,,,,,,
RCBEVDet: Radar-camera Fusion in Bird??s Eye View for 3D Object Detection,,,,"Zhiwei Lin (Peking University) | Zhe Liu (University Of Electronic Science And Technology Of China) | Zhongyu Xia (Peking University) | Xinhao Wang (Peking University) | Yongtao Wang (Peking University) | Shengxiang Qi (Chongqing Changan Automobile Co., Ltd) | Yang Dong (Chongqing Changan Automobile Co., Ltd.) | Nan Dong (Changan) | Le Zhang (University Of Electronic Science And Technology Of China) | Ce Zhu (University Of Electronic Science And Technology Of China)",,,,,,,
FreeMan: Towards benchmarking 3D human pose estimation under Real-World Conditions,,,,Jiong WANG (Fudan University) | Fengyu Yang (Chinese University Of Hong Kong(Shenzhen)) | Bingliang Li (The Chinese University Of Hong Kong (Shenzhen)) | Wenbo Gou (Carnegie Mellon University) | Danqi Yan (The Chinese University Of Hong Kong Shenzhen) | Ailing Zeng (IDEA) | Yijun Gao (Tencent Turing Lab) | Junle Wang (Tencent) | Yanqing Jing (Tencent) | Ruimao Zhang (The Chinese University Of Hong Kong (Shenzhen)),,,,,,,
CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention,"Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.",http://arxiv.org/abs/2402.17678v1,,"Mohammad Sadil Khan (University Of Luxembourg) | Elona Dupont (SnT, University Of Luxemburg) | Sk Aziz Ali (DFKI GmbH) | Kseniya Cherenkova (University Of Luxemburg) | Anis Kacem (University Of Luxemburg) | Djamila Aouada (SnT, University Of Luxembourg)",2024-02-27 16:53:16+00:00,,,,,,
Residual Denoising Diffusion Models,"We propose residual denoising diffusion models (RDDM), a novel dual diffusion process that decouples the traditional single denoising diffusion process into residual diffusion and noise diffusion. This dual diffusion framework expands the denoising-based diffusion models, initially uninterpretable for image restoration, into a unified and interpretable model for both image generation and restoration by introducing residuals. Specifically, our residual diffusion represents directional diffusion from the target image to the degraded input image and explicitly guides the reverse generation process for image restoration, while noise diffusion represents random perturbations in the diffusion process. The residual prioritizes certainty, while the noise emphasizes diversity, enabling RDDM to effectively unify tasks with varying certainty or diversity requirements, such as image generation and restoration. We demonstrate that our sampling process is consistent with that of DDPM and DDIM through coefficient transformation, and propose a partially path-independent generation process to better understand the reverse process. Notably, our RDDM enables a generic UNet, trained with only an $\ell _1$ loss and a batch size of 1, to compete with state-of-the-art image restoration methods. We provide code and pre-trained models to encourage further exploration, application, and development of our innovative framework (https://github.com/nachifur/RDDM).",http://arxiv.org/abs/2308.13712v2,,"Jiawei Liu (Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Qiang Wang (Shenyang University) | Huijie Fan (None) | Yinong Wang (University Of Hong Kong) | Yandong Tang (Shenyang Institue Of Automation) | Liangqiong Qu (The University Of Hong Kong)",2023-08-25 23:54:15+00:00,,,,,,
TraffiX - A V2X Dataset for Multi-Modal Cooperative 3D Object Detection of Traffic Participants Using Onboard and Roadside Sensors,,,,"Walter Zimmer (Technical University Of Munich (TUM)) | Gerhard Arya Wardana (Department Of Informatics, Technische Universit??t M??nchen) | Suren Sritharan (Technische Universit??t M??nchen) | Xingcheng Zhou (None) | Rui Song (Technical University Of Munich) | Alois Knoll (Technical University Munich)",,,,,,,
Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D,,,,"Mukund Varma T (University Of California, San Diego) | Peihao Wang (University Of Texas, Austin) | Zhiwen Fan (University Of Texas, Austin) | Zhangyang Wang (University Of Texas At Austin) | Hao Su (UCSD) | Ravi Ramamoorthi (None)",,,,,,,
Dynamic Support Information Mining for Category-Agnostic Pose Estimation,,,,"Pengfei Ren (Beijing University Of Posts And Telecommunications) | Yuanyuan Gao (Beijing University Of Posts And Telecommunications) | Haifeng Sun (Beijing University Of Posts And Telecommunications) | Qi Qi (Beijing University Of Posts And Telecommunications) | Jingyu Wang (Beijing University Of Post And Telecommunication, Tsinghua University) | Jianxin Liao (Beijing University Of Posts And Telecommunications)",,,,,,,
Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation,,,,Qinghe Ma (Nanjing University) | Jian Zhang (Nanjing University) | Lei Qi (Southeast University) | Qian Yu (Shandong Women's University) | Yinghuan Shi (Nanjing University) | Yang Gao (Nanjing University),,,,,,,
Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis,"Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them ""unlearnable"" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic perturbation. Furthermore, by incorporating an additional transformation process, we design a simple denoising-error maximization loss that is sufficient for causing transformation-robust semantic distortion and degradation in a personalized generation. Extensive experiments on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing approaches. Notably, MetaCloak can successfully fool online training services like Replicate, in a black-box manner, demonstrating the effectiveness of MetaCloak in real-world scenarios. Our code is available at https://github.com/liuyixin-louis/MetaCloak.",http://arxiv.org/abs/2311.13127v2,,Yixin Liu (Lehigh Universisty) | Chenrui Fan (Huazhong University Of Science And Technology) | Yutong Dai (Lehigh University) | Xun Chen (Samsung Research America) | Pan Zhou (Huazhong University Of Science And Technology) | Lichao Sun (Lehigh University),2023-11-22 03:31:31+00:00,,,,,,
From Coarse to Fine-Grained Open-Set Recognition,,,,Nico Lang (None) | V??steinn Sn??bjarnarson (Copenhagen University) | Elijah Cole (Altos Labs) | Oisin Mac Aodha (University Of Edinburgh) | Christian Igel (University Of Copenhagen) | Serge Belongie (None),,,,,,,
General Object Foundation Model for Images and Videos at Scale,"We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formulate general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multi-modal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tackling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foundational model to provide universal object-level information for multi-modal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The model and code will be released at https://glee-vision.github.io .",http://arxiv.org/abs/2312.09158v1,,Junfeng Wu (Huazhong University Of Science And Technology) | Yi Jiang (Bytedance) | Qihao Liu (Johns Hopkins University) | Zehuan Yuan (Nanjing University) | Xiang Bai (Huazhong University Of Science And Technology) | Song Bai (ByteDance),2023-12-14 17:26:00+00:00,,,,,,
WaveMo: Learning Wavefront Modulations to See Through Scattering,,,,"Mingyang Xie (University Of Maryland, College Park) | Haiyun Guo (Rice University) | Brandon Y. Feng (Massachusetts Institute Of Technology) | Lingbo Jin (Rice University) | Ashok Veeraraghavan (William Marsh Rice University) | Christopher Metzler (University Of Maryland, College Park)",,,,,,,
Open-Vocabulary Semantic Segmentation with Image Embedding Balancing,,,,Xiangheng Shan (Huazhong University Of Science And Technology) | Dongyue Wu (None) | Guilin Zhu (Huazhong University Of Science And Technology) | Yuanjie Shao (Huazhong University Of Science And Technology) | Nong Sang (Huazhong University Of Science And Technology) | Changxin Gao (Huazhong University Of Science And Technology),,,,,,,
Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints,"3D human generation is increasingly significant in various applications. However, the direct use of 2D generative methods in 3D generation often results in significant loss of local details, while methods that reconstruct geometry from generated images struggle with global view consistency. In this work, we introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. To achieve this, we employ the Fourier occupancy field (FOF) representation, enabling the direct production of 3D shapes as preliminary results using 2D generative models. With the proposed high-frequency enhancer and the multi-view recarving strategy, our method can seamlessly integrate the details from different views into a uniform global shape.To better utilize the 3D human prior and enhance control over the generated geometry, we introduce a compact spherical embedding of 3D joints. This allows for effective application of pose guidance during the generation process. Additionally, our method is capable of generating 3D humans guided by textual inputs. Our experimental results demonstrate the capability of our method to ensure global structure, local details, high resolution, and low computational cost, simultaneously. More results and code can be found on our project page at http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.",http://arxiv.org/abs/2312.08591v1,,Muxin Zhang (Tianjin University) | Qiao Feng (None) | Zhuo Su (ByteDance) | Chao Wen (ByteDance) | Zhou Xue (Li Auto) | Kun Li (None),2023-12-14 01:24:22+00:00,,,,,,
UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity,"Existing text-based person retrieval datasets often have relatively coarse-grained text annotations. This hinders the model to comprehend the fine-grained semantics of query texts in real scenarios. To address this problem, we contribute a new benchmark named \textbf{UFineBench} for text-based person retrieval with ultra-fine granularity.   Firstly, we construct a new \textbf{dataset} named UFine6926. We collect a large number of person images and manually annotate each image with two detailed textual descriptions, averaging 80.8 words each. The average word count is three to four times that of the previous datasets. In addition of standard in-domain evaluation, we also propose a special \textbf{evaluation paradigm} more representative of real scenarios. It contains a new evaluation set with cross domains, cross textual granularity and cross textual styles, named UFine3C, and a new evaluation metric for accurately measuring retrieval ability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a more efficient \textbf{algorithm} especially designed for text-based person retrieval with ultra fine-grained texts. It achieves fine granularity mining by adopting a shared cross-modal granularity decoder and hard negative match mechanism.   With standard in-domain evaluation, CFAM establishes competitive performance across various datasets, especially on our ultra fine-grained UFine6926. Furthermore, by evaluating on UFine3C, we demonstrate that training on our UFine6926 significantly improves generalization to real scenarios compared with other coarse-grained datasets. The dataset and code will be made publicly available at \url{https://github.com/Zplusdragon/UFineBench}.",http://arxiv.org/abs/2312.03441v2,,Jialong Zuo (Huazhong University Of Science And Technology) | Hanyu Zhou (Huazhong University Of Science And Technology) | Ying Nie (Huawei Noah's Ark Lab) | Feng Zhang (Huazhong University Of Science And Technology) | Tianyu Guo (Peking University) | Nong Sang (Huazhong University Of Science And Technology) | Yunhe Wang (Huawei Noah's Ark Lab) | Changxin Gao (Huazhong University Of Science And Technology),2023-12-06 11:50:14+00:00,,,,,,
Multi-Scale Video Anomaly Detection by Multi-Grained Spatio-Temporal Representation Learning,,,,"Menghao Zhang (Beijing University Of Posts And Telecommunications) | Jingyu Wang (Beijing University Of Post And Telecommunication, Tsinghua University) | Qi Qi (Beijing University Of Posts And Telecommunications) | Haifeng Sun (Beijing University Of Posts And Telecommunications) | Zirui Zhuang (Beijing University Of Posts And Telecommunications) | Pengfei Ren (Beijing University Of Posts And Telecommunications) | Ruilong Ma (Beijing University Of Posts And Telecommunications) | Jianxin Liao (Beijing University Of Posts And Telecommunications)",,,,,,,
LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes Detection,"This paper introduces a novel approach for high-quality deepfake detection called Localized Artifact Attention Network (LAA-Net). Existing methods for high-quality deepfake detection are mainly based on a supervised binary classifier coupled with an implicit attention mechanism. As a result, they do not generalize well to unseen manipulations. To handle this issue, two main contributions are made. First, an explicit attention mechanism within a multi-task learning framework is proposed. By combining heatmap-based and self-consistency attention strategies, LAA-Net is forced to focus on a few small artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid Network (E-FPN) is proposed as a simple and effective mechanism for spreading discriminative low-level features into the final feature output, with the advantage of limiting redundancy. Experiments performed on several benchmarks show the superiority of our approach in terms of Area Under the Curve (AUC) and Average Precision (AP). The code will be released soon.",http://arxiv.org/abs/2401.13856v1,,"Dat NGUYEN (University Of Luxembourg) | Nesryne Mejri (SnT, University Of Luxembourg) | Inder Pal Singh (University Of Luxemburg) | Polina Kuleshova (University Of Luxemburg) | Marcella Astrid (University Of Luxemburg) | Anis Kacem (University Of Luxemburg) | Enjie Ghorbel (CRISTAL Laboratory, ENSI, University Of Manouba) | Djamila Aouada (SnT, University Of Luxembourg)",2024-01-24 23:42:08+00:00,,,,,,
Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation,,,,"Wenxiao Deng (Nanjing University) | Wenbin Li (Nanjing University) | Tianyu Ding (Microsoft) | Lei Wang (University Of Wollonong) | Hongguang Zhang (Systems Engineering Institute, AMS) | Kuihua Huang (National University Of Defense Technology) | Jing Huo (Nanjing University) | Yang Gao (Nanjing University)",,,,,,,
Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles,"Collaborative perception in automated vehicles leverages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collaborative 3D perception methods typically employ 3D bounding boxes or bird's eye views as representations of the environment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) compressed orthogonal attention features shared between vehicles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30%, and (ii) models anchored on semantic occupancy outpace state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments.",http://arxiv.org/abs/2402.07635v1,,Rui Song (Technical University Of Munich) | Chenwei Liang (Fraunhofer) | Hu Cao (Technical University Of Munich) | Zhiran Yan (Technische Hochschule Ingolstadt) | Walter Zimmer (Technical University Of Munich (TUM)) | Markus Gross (Fraunhofer IVI) | Andreas Festag (Technische Hochschule Ingolstadt) | Alois Knoll (Technical University Munich),2024-02-12 13:19:08+00:00,,,,,,
CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras,,,,"Sachin Shah (University Of Maryland, College Park) | Matthew Chan (Department Of Computer Science, University Of Maryland, College Park) | Haoming Cai (University Of Maryland, College Park) | Jingxi Chen (University Of Maryland College Park) | Sakshum Kulshrestha (University Of Maryland, College Park) | Chahat Deep Singh (University Of Maryland, College Park) | Yiannis Aloimonos (University Of Maryland, College Park) | Christopher Metzler (University Of Maryland, College Park)",,,,,,,
Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling,,,,"Liwen Wu (Computer Science And Engineering Department, University Of California, San Diego) | Sai Bi (Adobe Systems) | Zexiang Xu (Adobe Research) | Fujun Luan (Adobe Systems) | Kai Zhang (Adobe Systems) | Iliyan Georgiev (Adobe) | Kalyan Sunkavalli (Adobe Research) | Ravi Ramamoorthi (None)",,,,,,,
Temperature-based Backdoor Attacks on Thermal Infrared Object Detection,,,,Wen Yin (Huazhong University Of Science And Technology) | Jian Lou (Zhejiang University) | Pan Zhou (Huazhong University Of Science And Technology) | Yulai Xie (Huazhong University Of Science And Technology) | Dan Feng (Huazhong University Of Science And Technology) | Yuhua Sun (None) | Tailai Zhang (Huazhong University Of Science And Technology) | Lichao Sun (Lehigh University),,,,,,,
LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging,,,,Haoyang Ge (Tianjin University) | Qiao Feng (None) | Hailong Jia (Tianjin University) | Xiongzheng Li (None) | Xiangjun Yin (None) | You Zhou (Nanjing University) | Jingyu Yang (Tianjin University) | Kun Li (None),,,,,,,
FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning,,,,Junyuan Zhang (University Of Hong Kong) | Shuang Zeng (The University Of Hong Kong) | Miao Zhang (New York University) | Runxi Wang (Beijing University Of Aeronautics And Astronautics) | Feifei Wang (Stanford University) | Yuyin Zhou (UC Santa Cruz) | Paul Pu Liang (Carnegie Mellon University) | Liangqiong Qu (The University Of Hong Kong),,,,,,,
DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing,"Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DragGAN is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach DragDiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Our approach involves optimizing the diffusion latents to achieve precise spatial control. The supervision signal of this optimization process is from the diffusion model's UNet features, which are known to contain rich semantic and geometric information. Moreover, we introduce two additional techniques, namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DragBench -- the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.",http://arxiv.org/abs/2306.14435v5,,"Yujun Shi (National University Of Singaore, National University Of Singapore) | Chuhui Xue (ByteDance) | Jun Hao Liew (ByteDance) | Jiachun Pan (National University Of Singapore) | Hanshu Yan (ByteDance) | Wenqing Zhang (Huazhong University Of Science And Technology) | Vincent Y. F. Tan (National University Of Singapore) | Song Bai (ByteDance)",2023-06-26 06:04:09+00:00,,,,,,
Rethinking Few-shot 3D Point Cloud Semantic Segmentation,"This paper revisits few-shot 3D point cloud semantic segmentation (FS-PCS), with a focus on two significant issues in the state-of-the-art: foreground leakage and sparse point distribution. The former arises from non-uniform point sampling, allowing models to distinguish the density disparities between foreground and background for easier segmentation. The latter results from sampling only 2,048 points, limiting semantic information and deviating from the real-world practice. To address these issues, we introduce a standardized FS-PCS setting, upon which a new benchmark is built. Moreover, we propose a novel FS-PCS model. While previous methods are based on feature optimization by mainly refining support features to enhance prototypes, our method is based on correlation optimization, referred to as Correlation Optimization Segmentation (COSeg). Specifically, we compute Class-specific Multi-prototypical Correlation (CMC) for each query point, representing its correlations to category prototypes. Then, we propose the Hyper Correlation Augmentation (HCA) module to enhance CMC. Furthermore, tackling the inherent property of few-shot training to incur base susceptibility for models, we propose to learn non-parametric prototypes for the base classes during training. The learned base prototypes are used to calibrate correlations for the background class through a Base Prototypes Calibration (BPC) module. Experiments on popular datasets demonstrate the superiority of COSeg over existing methods. The code is available at: https://github.com/ZhaochongAn/COSeg",http://arxiv.org/abs/2403.00592v1,,"Zhaochong An (University Of Copenhagen) | Guolei Sun (None) | Yun Liu (Institute For Infocomm Research, A*STAR) | Fayao Liu (Institute For Infocomm Research, A*STAR) | Zongwei Wu (Bayerische Julius-Maximilians-Universit??t W??rzburg) | Dan Wang (University Of Copenhagen) | Luc Van Gool (ETH Zurich) | Serge Belongie (None)",2024-03-01 15:14:47+00:00,,,,,,
Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes,"Neural implicit representation of geometric shapes has witnessed considerable advancements in recent years. However, common distance field based implicit representations, specifically signed distance field (SDF) for watertight shapes or unsigned distance field (UDF) for arbitrary shapes, routinely suffer from degradation of reconstruction accuracy when converting to explicit surface points and meshes. In this paper, we introduce a novel neural implicit representation based on unsigned orthogonal distance fields (UODFs). In UODFs, the minimal unsigned distance from any spatial point to the shape surface is defined solely in one orthogonal direction, contrasting with the multi-directional determination made by SDF and UDF. Consequently, every point in the 3D UODFs can directly access its closest surface points along three orthogonal directions. This distinctive feature leverages the accurate reconstruction of surface points without interpolation errors. We verify the effectiveness of UODFs through a range of reconstruction examples, extending from simple watertight or non-watertight shapes to complex shapes that include hollows, internal or assembling structures.",http://arxiv.org/abs/2403.01414v1,,"YuJie Lu (Donghua University, Shanghai) | Long Wan (Donghua University, Shanghai) | Nayu Ding (Donghua University, Shanghai) | Yulong Wang (Donghua University, Shanghai) | Shuhan Shen (Institute Of Automation, Chinese Academy Of Science) | Shen Cai (Donghua University) | Lin Gao (University Of Chinese Academy Of Sciences)",2024-03-03 06:58:35+00:00,,,,,,
Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark,,,,"Ziyang Chen (University Of Michigan) | Israel D. Gebru (Facebook) | Christian Richardt (Meta Reality Labs) | Anurag Kumar (Facebook) | William Laney (Meta) | Andrew Owens (University Of Michigan) | Alexander Richard (Reality Labs Research, Meta)",,,,,,,
Real-time 3D-aware Portrait Video Relighting,,,,Ziqi Cai (Chinese Academy Of Sciences & Beijing Jiao Tong University) | Kaiwen Jiang (None) | Shu-Yu Chen (Chinese Academy Of Sciences) | Yu-Kun Lai (Cardiff University) | Hongbo Fu (City University Of Hong Kong) | Boxin Shi (Peking University) | Lin Gao (University Of Chinese Academy Of Sciences),,,,,,,
From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations,"We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, we output multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind our method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. We visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, we introduce a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show our model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, our perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available online.",http://arxiv.org/abs/2401.01885v1,,"Evonne Ng (University Of California, Berkeley) | Javier Romero (None) | Timur Bagautdinov (Reality Labs Research) | Shaojie Bai (Meta) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Angjoo Kanazawa (UC Berkeley) | Alexander Richard (Reality Labs Research, Meta)",2024-01-03 18:55:16+00:00,,,,,,
Learned representation-guided diffusion models for large-image generation,,,,"Alexandros Graikos (Stony Brook University) | Srikar Yellapragada (None) | Minh-Quan Le (State University Of New York At Stony Brook) | Saarthak Kapse (State University Of New York At Stony Brook) | Prateek Prasanna (State University Of New York, Stony Brook) | Joel Saltz (State University Of New York At Stony Brook) | Dimitris Samaras (Stony Brook University)",,,,,,,
Unifying Top-down and Bottom-up Scanpath Prediction using Transformers,,,,"Zhibo Yang (State University Of New York, Stony Brook) | Sounak Mondal (State University Of New York, Stony Brook) | Seoyoung Ahn (State University Of New York, Stony Brook) | Ruoyu Xue (State University Of New York At Stony Brook) | Gregory Zelinsky (State University Of New York At Stony Brook) | Minh Hoai (State University Of New York, Stony Brook) | Dimitris Samaras (Stony Brook University)",,,,,,,
Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture,,,,Juanwu Lu (Purdue University) | Can Cui (Purdue University) | Yunsheng Ma (Purdue University) | Aniket Bera (Purdue University) | Ziran Wang (Purdue University),,,,,,,
LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs,"We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives. This approach aims to address the challenge of interpreting and executing spontaneous user instructions such as ""overtake the car ahead,"" which have typically posed difficulties for existing frameworks. We introduce the LaMPilot benchmark specifically designed to quantitatively evaluate the efficacy of Large Language Models (LLMs) in translating human directives into actionable driving policies. We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark. The results of the experiments showed that GPT-4, with human feedback, achieved an impressive task completion rate of 92.7% and a minimal collision rate of 0.9%. To encourage further investigation in this area, our code and dataset will be made available.",http://arxiv.org/abs/2312.04372v1,,Yunsheng Ma (Purdue University) | Can Cui (Purdue University) | Xu Cao (University Of Illinois Urbana-Champaign) | Wenqian Ye (University Of Virginia) | Peiran Liu (Purdue University) | Juanwu Lu (Purdue University) | Amr Abdelraouf (None) | Rohit Gupta (Toyota Motor Corporation) | Kyungtae Han (Toyota Motor North America) | Aniket Bera (Purdue University) | James Rehg (None) | Ziran Wang (Purdue University),2023-12-07 15:43:52+00:00,,,,,,
Improve Transformers with Irrelevant Data from Other Modalities,"We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.",http://arxiv.org/abs/2401.14405v1,,Yiyuan Zhang (The Chinese University Of Hong Kong) | Xiaohan Ding (Tencent AI Lab) | Kaixiong Gong (None) | Yixiao Ge (Tencent) | Ying Shan (Tencent) | Xiangyu Yue (None),2024-01-25 18:59:58+00:00,,,,,,
OneLLM: One Framework to Align All Modalities with Language,"Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM",http://arxiv.org/abs/2312.03700v1,,Jiaming Han (The Chinese University Of Hong Kong) | Kaixiong Gong (None) | Yiyuan Zhang (The Chinese University Of Hong Kong) | Jiaqi Wang (Shanghai AI Laboratory) | Kaipeng Zhang (Shanghai AI Laboratory) | Dahua Lin (The Chinese University Of Hong Kong) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Peng Gao (The Chinese University Of Hong Kong) | Xiangyu Yue (None),2023-12-06 18:59:19+00:00,,,,,,
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction,"While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.",http://arxiv.org/abs/2402.16174v1,,Xiao Chen (The Chinese University Of Hong Kong) | Quanyi Li (University Of Edinburgh) | Tai Wang (Shanghai AI Laboratory) | Tianfan Xue (The Chinese University Of Hong Kong) | Jiangmiao Pang (Shanghai AI Laboratory ),2024-02-25 18:59:29+00:00,,,,,,
EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI,"In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions. This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction. However, traditional research focuses more on scene-level input and output setups from a global view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding. It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories. Building upon this database, we introduce a baseline framework named Embodied Perceptron. It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",http://arxiv.org/abs/2312.16170v1,,"Tai Wang (Shanghai AI Laboratory) | Xiaohan Mao (Shanghai Jiao Tong University) | Chenming Zhu (The Chinese University Of Hong Kong, Shenzhen) | Runsen Xu (The Chinese University Of Hong Kong) | Ruiyuan Lyu (Shanghai AI Laboratory) | Peisen Li (Tsinghua University) | Xiao Chen (The Chinese University Of Hong Kong) | Wenwei Zhang (None) | Kai Chen (Shanghai AI Laboratory) | Tianfan Xue (The Chinese University Of Hong Kong) | Xihui Liu (The University Of Hong Kong) | Cewu Lu (Shanghai Jiao Tong University) | Dahua Lin (The Chinese University Of Hong Kong) | Jiangmiao Pang (Shanghai AI Laboratory )",2023-12-26 18:59:11+00:00,,,,,,
Diffusion-ES: Generative Evolutionary Search with Diffusion Models for Trajectory Optimization,,,,"Brian Yang (School Of Computer Science, Carnegie Mellon University) | Huangyuan Su (Computer Science, School Of Engineering And Applied Sciences, Harvard University) | Nikolaos Gkanatsios (Carnegie Mellon University) | Tsung-Wei Ke (CMU, Carnegie Mellon University) | Ayush Jain (Carnegie Mellon University) | Jeff Schneider (Carnegie Mellon University) | Katerina Fragkiadaki (CMU)",,,,,,,
Enhancing Visual Continual Learning with Language-Guided Supervision,,,,"Bolin Ni (Institute Of Automation, Chinese Academy Of Sciences) | Hongbo Zhao (Institute Of Automation, Chinese Academy Of Sciences) | Chenghao Zhang (Alibaba Group) | Ke Hu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Gaofeng Meng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Shiming Xiang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",,,,,,,
Improving Image Restoration through Removing Degradations in Textual Representations,"In this paper, we introduce a new perspective for improving image restoration by removing degradation in the textual representations of a given degraded image. Intuitively, restoration is much easier on text modality than image one. For example, it can be easily conducted by removing degradation-related words while keeping the content-aware words. Hence, we combine the advantages of images in detail description and ones of text in degradation removal to perform restoration. To address the cross-modal assistance, we propose to map the degraded images into textual representations for removing the degradations, and then convert the restored textual representations into a guidance image for assisting image restoration. In particular, We ingeniously embed an image-to-text mapper and text restoration module into CLIP-equipped text-to-image models to generate the guidance. Then, we adopt a simple coarse-to-fine approach to dynamically inject multi-scale information from guidance to image restoration networks. Extensive experiments are conducted on various image restoration tasks, including deblurring, dehazing, deraining, and denoising, and all-in-one image restoration. The results showcase that our method outperforms state-of-the-art ones across all these tasks. The codes and models are available at \url{https://github.com/mrluin/TextualDegRemoval}.",http://arxiv.org/abs/2312.17334v1,,"Jingbo Lin (Harbin Institute Of Technology) | Zhilu Zhang (Harbin Institute Of Technology) | Yuxiang Wei (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Dongwei Ren (Harbin Institute Of Technology) | Dongsheng Jiang (Huawei Technologies Ltd.) | Qi Tian (Huawei Technologies Ltd.) | Wangmeng Zuo (Harbin Institute Of Technology)",2023-12-28 19:18:17+00:00,,,,,,
Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning,,,,Zichen Miao (Purdue University) | Jiang Wang (Microsoft) | Ze Wang (Purdue University) | Zhengyuan Yang (Microsoft) | Lijuan Wang (Microsoft) | Qiang Qiu (Purdue University) | Zicheng Liu (Microsoft),,,,,,,
Segment and Caption Anything,"We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions. SAM presents strong generalizability to segment anything while is short for semantic understanding. By introducing a lightweight query-based feature mixer, we align the region-specific features with the embedding space of language models for later caption generation. As the number of trainable parameters is small (typically in the order of tens of millions), it costs less computation, less memory usage, and less communication bandwidth, resulting in both fast and scalable training. To address the scarcity problem of regional caption data, we propose to first pre-train our model on objection detection and segmentation tasks. We call this step weak supervision pretraining since the pre-training data only contains category names instead of full-sentence descriptions. The weak supervision pretraining allows us to leverage many publicly available object detection and segmentation datasets. We conduct extensive experiments to demonstrate the superiority of our method and validate each design choice. This work serves as a stepping stone towards scaling up regional captioning data and sheds light on exploring efficient ways to augment SAM with regional semantics. The project page, along with the associated code, can be accessed via the following https://xk-huang.github.io/segment-caption-anything/.",http://arxiv.org/abs/2312.00869v1,,"Xiaoke Huang (Shenzhen International Graduate School, Tsinghua University) | Jianfeng Wang (Microsoft) | Yansong Tang (Tsinghua University) | Zheng Zhang (Microsoft) | Han Hu (Microsft Research Asia) | Jiwen Lu (Tsinghua University) | Lijuan Wang (Microsoft) | Zicheng Liu (Microsoft)",2023-12-01 19:00:17+00:00,,,,,,
Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation,"Recently, an audio-visual segmentation (AVS) task has been introduced, aiming to group pixels with sounding objects within a given video. This task necessitates a first-ever audio-driven pixel-level understanding of the scene, posing significant challenges. In this paper, we propose an innovative audio-visual transformer framework, termed COMBO, an acronym for COoperation of Multi-order Bilateral relatiOns. For the first time, our framework explores three types of bilateral entanglements within AVS: pixel entanglement, modality entanglement, and temporal entanglement. Regarding pixel entanglement, we employ a Siam-Encoder Module (SEM) that leverages prior knowledge to generate more precise visual features from the foundational model. For modality entanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to align corresponding visual and auditory signals bi-directionally. As for temporal entanglement, we introduce an innovative adaptive inter-frame consistency loss according to the inherent rules of temporal. Comprehensive experiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou on MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that COMBO surpasses previous state-of-the-art methods. Code and more results will be publicly available at https://combo-avs.github.io/.",http://arxiv.org/abs/2312.06462v1,,"Qi Yang (School Of Artificial Intelligence, University Of Chinese Academy Of Sciences.) | Xing Nie (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Tong Li (Meituan) | Gaopengfei (Beijing SanKuai Online Technology Co., Ltd.) | Ying Guo (Meituan) | Cheng Zhen (Meituan) | Pengfei Yan (Meituan) | Shiming Xiang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2023-12-11 15:51:38+00:00,,,,,,
DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior,"3D generation has raised great attention in recent years. With the success of text-to-image diffusion models, the 2D-lifting technique becomes a promising route to controllable 3D generation. However, these methods tend to present inconsistent geometry, which is also known as the Janus problem. We observe that the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D diffusion models and overfitting of the optimization objective. To address it, we propose a two-stage 2D-lifting framework, namely DreamControl, which optimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained objects with control-based score distillation. Specifically, adaptive viewpoint sampling and boundary integrity metric are proposed to ensure the consistency of generated priors. The priors are then regarded as input conditions to maintain reasonable geometries, in which conditional LoRA and weighted score are further proposed to optimize detailed textures. DreamControl can generate high-quality 3D content in terms of both geometry consistency and texture fidelity. Moreover, our control-based optimization guidance is applicable to more downstream tasks, including user-guided generation and 3D animation. The project page is available at https://github.com/tyhuang0428/DreamControl.",http://arxiv.org/abs/2312.06439v2,,Tianyu Huang (Harbin Institute Of Technology & City University Of Hong Kong) | Yihan Zeng (Huawei Technologies Ltd.) | Zhilu Zhang (Harbin Institute Of Technology) | Wan Xu (Harbin Institute Of Technology) | Hang Xu (Huawei Noah??S Ark Lab) | Songcen Xu (Huawei Noah's Ark Lab) | Rynson W.H. Lau (City University Of Hong Kong) | Wangmeng Zuo (Harbin Institute Of Technology),2023-12-11 15:12:50+00:00,,,,,,
ODIN: A Single Model for 2D and 3D Perception,"State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It outperforms all previous works by a wide margin when the sensed 3D point cloud is used in place of the point cloud sampled from 3D mesh. When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and checkpoints can be found at the project website: https://odin-seg.github.io.",http://arxiv.org/abs/2401.02416v1,,Ayush Jain (Carnegie Mellon University) | Pushkal Katara (Carnegie Mellon University) | Nikolaos Gkanatsios (Carnegie Mellon University) | Adam Harley (Ryerson University) | Gabriel Sarch (Carnegie Mellon University) | Kriti Aggarwal (Microsoft) | Vishrav Chaudhary (Microsoft) | Katerina Fragkiadaki (CMU),2024-01-04 18:59:25+00:00,,,,,,
LAMP: Learn A Motion Pattern for Few-Shot Video Generation,,,,Rui-Qi Wu (Nankai University) | Liangyu Chen (Megvii Technology) | Tong Yang (Fudan University) | Chun-Le Guo (None) | Chongyi Li (None) | Xiangyu Zhang (MEGVII Technology),,,,,,,
SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation,,,,Kejia Yin (None) | Varshanth Rao (ModiFace - A L'Oreal Group Company) | Ruowei Jiang (ModiFace) | Xudong Liu (None) | Parham Aarabi (Toronto University) | David B. Lindell (University Of Toronto),,,,,,,
Panacea: Panoramic and Controllable Video Generation for Autonomous Driving,"The field of autonomous driving increasingly demands high-quality annotated training data. In this paper, we propose Panacea, an innovative approach to generate panoramic and controllable videos in driving scenarios, capable of yielding an unlimited numbers of diverse, annotated samples pivotal for autonomous driving advancements. Panacea addresses two critical challenges: 'Consistency' and 'Controllability.' Consistency ensures temporal and cross-view coherence, while Controllability ensures the alignment of generated content with corresponding annotations. Our approach integrates a novel 4D attention and a two-stage generation pipeline to maintain coherence, supplemented by the ControlNet framework for meticulous control by the Bird's-Eye-View (BEV) layouts. Extensive qualitative and quantitative evaluations of Panacea on the nuScenes dataset prove its effectiveness in generating high-quality multi-view driving-scene videos. This work notably propels the field of autonomous driving by effectively augmenting the training dataset used for advanced BEV perception techniques.",http://arxiv.org/abs/2311.16813v1,,Yuqing Wen (Megvii Technology) | Yucheng Zhao (University Of Science And Technology Of China) | Yingfei Liu (Megvii Technology) | Fan Jia (Megvii Technology) | Yanhui Wang (None) | Chong Luo (Microsoft Research Asia) | Chi Zhang (Columbia University) | Tiancai Wang (Megvii Technology) | Xiaoyan Sun (University Of Science And Technology Of China) | Xiangyu Zhang (MEGVII Technology),2023-11-28 14:22:24+00:00,,,,,,
4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling,"Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, text-to-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure -- but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.",http://arxiv.org/abs/2311.17984v1,,"Sherwin Bahmani (None) | Ivan Skorokhodov (KAUST) | Victor Rong (University Of Toronto) | Gordon Wetzstein (Stanford University) | Leonidas Guibas (Stanford University) | Peter Wonka (KAUST) | Sergey Tulyakov (Snap) | Jeong Joon Park (Stanford University) | Andrea Tagliasacchi (Simon Fraser University, Google Brain) | David B. Lindell (University Of Toronto)",2023-11-29 18:58:05+00:00,,,,,,
Semantic Human Mesh Reconstruction with Textures,"The field of 3D detailed human mesh reconstruction has made significant progress in recent years. However, current methods still face challenges when used in industrial applications due to unstable results, low-quality meshes, and a lack of UV unwrapping and skinning weights. In this paper, we present SHERT, a novel pipeline that can reconstruct semantic human meshes with textures and high-precision details. SHERT applies semantic- and normal-based sampling between the detailed surface (eg mesh and SDF) and the corresponding SMPL-X model to obtain a partially sampled semantic mesh and then generates the complete semantic mesh by our specifically designed self-supervised completion and refinement networks. Using the complete semantic mesh as a basis, we employ a texture diffusion model to create human textures that are driven by both images and texts. Our reconstructed meshes have stable UV unwrapping, high-quality triangle meshes, and consistent semantic information. The given SMPL-X model provides semantic information and shape priors, allowing SHERT to perform well even with incorrect and incomplete inputs. The semantic information also makes it easy to substitute and animate different body parts such as the face, body, and hands. Quantitative and qualitative experiments demonstrate that SHERT is capable of producing high-fidelity and robust semantic meshes that outperform state-of-the-art methods.",http://arxiv.org/abs/2403.02561v1,,Xiaoyu Zhan (None) | Jianxin Yang (Nanjing University) | Yuanqi Li (Nanjing University) | Jie Guo (Nanjing University) | Yanwen Guo (Nanjing University) | Wenping Wang (Texas A&M University - College Station),2024-03-05 00:34:05+00:00,,,,,,
Wonder3D: Single Image to 3D using Cross-Domain Diffusion,"In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.",http://arxiv.org/abs/2310.15008v3,,"Xiaoxiao Long (The University Of Hong Kong) | Yuan-Chen Guo (Tsinghua University) | Cheng Lin (Tencent) | Yuan Liu (The University Of Hong Kong) | Zhiyang Dou (The University Of Hong Kong) | Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) | Yuexin Ma (ShanghaiTech University) | Song-Hai Zhang (Tsinghua University) | Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) | Christian Theobalt (MPI Informatik) | Wenping Wang (Texas A&M University - College Station)",2023-10-23 15:02:23+00:00,,,,,,
Question Aware Vision Transformer for Multimodal Reasoning,"Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",http://arxiv.org/abs/2402.05472v1,,"Roy Ganz (Technion - Israel Institute Of Technology, Technion) | Yair Kittenplon (AWS AI Labs) | Aviad Aberdam (Amazon AWS AI) | Elad Ben Avraham (Amazon) | Oren Nuriel (Amazon) | Shai Mazor (Amazon) | Ron Litman (Amazon AI Labs)",2024-02-08 08:03:39+00:00,,,,,,
Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation,"Referring expression segmentation (RES) aims at segmenting the foreground masks of the entities that match the descriptive natural language expression. Previous datasets and methods for classic RES task heavily rely on the prior assumption that one expression must refer to object-level targets. In this paper, we take a step further to finer-grained part-level RES task. To promote the object-level RES task towards finer-grained vision-language understanding, we put forward a new multi-granularity referring expression segmentation (MRES) task and construct an evaluation benchmark called RefCOCOm by manual annotations. By employing our automatic model-assisted data engine, we build the largest visual grounding dataset namely MRES-32M, which comprises over 32.2M high-quality masks and captions on the provided 1M images. Besides, a simple yet strong model named UniRES is designed to accomplish the unified object-level and part-level grounding task. Extensive experiments on our RefCOCOm for MRES and three datasets (i.e., RefCOCO(+/g) for classic RES task demonstrate the superiority of our method over previous state-of-the-art methods. To foster future research into fine-grained visual grounding, our benchmark RefCOCOm, the MRES-32M dataset and model UniRES will be publicly available at https://github.com/Rubics-Xuan/MRES",http://arxiv.org/abs/2312.08007v1,,"Wenxuan Wang (National Lab Of Pattern Recognition, Institute Of Automation,Chinese Academy Of Sciences) | Tongtian Yue (, Institute Of Automation, Chinese Academy Of Science) | Yisi Zhang (University Of Science And Technology Beijing) | Longteng Guo (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xingjian He (, Institute Of Automation, Chinese Academy Of Science) | Xinlong Wang (Beijing Academy Of Artificial Intelligence) | Jing Liu (Institute Of Automation, Chinese Academy Of Science)",2023-12-13 09:29:45+00:00,,,,,,
Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis,"Point cloud analysis has achieved outstanding performance by transferring point cloud pre-trained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal trade-off between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pre-trained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.",http://arxiv.org/abs/2403.01439v1,,Xin Zhou (Huazhong University Of Science And Technology) | Dingkang Liang (Huazhong University Of Science And Technology) | Wei Xu (Huazhong University Of Science And Technology) | Xingkui Zhu (Huazhong University Of Science And Technology) | Yihan Xu (Huazhong University Of Science And Technology) | Zhikang Zou (Huazhong University Of Science And Technology) | Xiang Bai (Huazhong University Of Science And Technology),2024-03-03 08:25:04+00:00,,,,,,
NEAT: Distilling 3D Wireframes from Neural Attraction Fields,,,,Nan Xue (Ant Group) | Bin Tan (Wuhan University) | Yuxi Xiao (Zhejiang University) | Liang Dong (Google) | Gui-Song Xia (Wuhan University) | Tianfu Wu (None) | Yujun Shen (The Chinese University Of Hong Kong),,,,,,,
CoDeF: Content Deformation Fields for Temporally Consistent Video Processing,"We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis.Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline.We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video.With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field.We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training.More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.Project page can be found at https://qiuyu96.github.io/CoDeF/.",http://arxiv.org/abs/2308.07926v1,,"Hao Ouyang (Department Of Computer Science And Engineering, Hong Kong University Of Science And Technology) | Qiuyu Wang (Ant Group) | Yuxi Xiao (Zhejiang University) | Qingyan Bai (Hong Kong University Of Science And Technology) | Juntao Zhang (Hong Kong University Of Science And Technology) | Kecheng Zheng (Ant Group) | Xiaowei Zhou (None) | Qifeng Chen (Hong Kong University Of Science And Technology) | Yujun Shen (The Chinese University Of Hong Kong)",2023-08-15 17:59:56+00:00,,,,,,
SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models,,,,"Tongtian Yue (, Institute Of Automation, Chinese Academy Of Science) | Jie Cheng (State Key Laboratory Of Multimodal Artificial Intelligence Systems, CASIA) | Longteng Guo (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xingyuan Dai (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zijia Zhao (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xingjian He (, Institute Of Automation, Chinese Academy Of Science) | Gang Xiong (Institute Of Automation, Chinese Academy Of Science) | Yisheng Lv (Institute Of Automation, Chinese Academy Of Science) | Jing Liu (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models,"Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative results validate the effectiveness of our designs. Additionally, experiments on 18 datasets further demonstrate that Monkey surpasses existing LMMs in many tasks like Image Captioning and various Visual Question Answering formats. Specially, in qualitative tests focused on dense text question answering, Monkey has exhibited encouraging results compared with GPT4V. Code is available at https://github.com/Yuliang-Liu/Monkey.",http://arxiv.org/abs/2311.06607v3,,Zhang Li (None) | Biao Yang (Huazhong University Of Science And Technology) | Qiang Liu (Kingsoft Office) | Zhiyin Ma (Huazhong University Of Science And Technology) | Shuo Zhang (Huazhong University Of Science And Technology) | Jingxu Yang (Kingsoft Office Corporation Limited) | Yabo Sun (Kingsoft Office) | Yuliang Liu (Huazhong University Of Science And Technology) | Xiang Bai (Huazhong University Of Science And Technology),2023-11-11 16:37:41+00:00,,,,,,
GRAM: Global Reasoning for Multi-Page VQA,"The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document-level tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our C-Former model, which reduces the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM's state-of-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach.",http://arxiv.org/abs/2401.03411v1,,"Itshak Blau (Electrical Engineering Department, Technion ?? Israel Institute Of Technology, Technion - Israel Institute Of Technology) | Sharon Fogel (Amazon) | Roi Ronen (Technion - Israel Institute Of Technology) | Alona Golts (Amazon) | Shahar Tsiper (Amazon) | Elad Ben Avraham (Amazon) | Aviad Aberdam (Amazon AWS AI) | Roy Ganz (Technion - Israel Institute Of Technology, Technion) | Ron Litman (Amazon AI Labs)",2024-01-07 08:03:06+00:00,,,,,,
MatSynth: A Modern PBR Materials Dataset,"We introduce MatSynth, a dataset of 4,000+ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.",http://arxiv.org/abs/2401.06056v2,,Giuseppe Vecchio (University Of Catania) | Valentin Deschaintre (Adobe Research),2024-01-11 17:20:34+00:00,,,,,,
VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning,"Salient object detection (SOD) and camouflaged object detection (COD) are related yet distinct binary mapping tasks. These tasks involve multiple modalities, sharing commonalities and unique cues. Existing research often employs intricate task-specific specialist models, potentially leading to redundancy and suboptimal results. We introduce VSCode, a generalist model with novel 2D prompt learning, to jointly address four SOD tasks and three COD tasks. We utilize VST as the foundation model and introduce 2D prompts within the encoder-decoder architecture to learn domain and task-specific knowledge on two separate dimensions. A prompt discrimination loss helps disentangle peculiarities to benefit model optimization. VSCode outperforms state-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot generalization to unseen tasks by combining 2D prompts, such as RGB-D COD.",http://arxiv.org/abs/2311.15011v1,,"Ziyang Luo (None) | Nian Liu (Mohamed Bin Zayed University Of Artificial Intelligence) | Wangbo Zhao (National University Of Singapore) | Xuguang Yang (Northwestern Polytechnical University Xi'an) | Dingwen Zhang (Northwestern Polytechnical University) | Deng-Ping Fan (ETH Zurich) | Fahad Shahbaz Khan (MBZUAI; Link??ping University) | Junwei Han (Northwestern Polytechnical University, Tsinghua University)",2023-11-25 12:34:02+00:00,,,,,,
GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding,"Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \textit{i.e.}, the ""label rendering"" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%, 11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",http://arxiv.org/abs/2311.11863v1,,"Hao Li (Northwest Polytechnical University) | Dingwen Zhang (Northwestern Polytechnical University) | Yalun Dai (Nanyang Technological University) | Nian Liu (Mohamed Bin Zayed University Of Artificial Intelligence) | Lechao Cheng (Hefei University Of Technology) | Li Jingfeng (Northwest Polytechnical University Xi'an) | Jingdong Wang (Baidu) | Junwei Han (Northwestern Polytechnical University, Tsinghua University)",2023-11-20 15:59:41+00:00,,,,,,
HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces,"Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15-30% while achieving real-time framerates (at least 36 FPS) for virtual-reality resolutions (2Kx2K).",http://arxiv.org/abs/2312.03160v1,,Haithem Turki (Carnegie Mellon University) | Vasu Agrawal (Meta Reality Labs Research) | Samuel Rota Bul?? (Meta) | Lorenzo Porzi (Facebook) | Peter Kontschieder (Meta) | Deva Ramanan (Carnegie Mellon University) | Michael Zollhoefer (Meta) | Christian Richardt (Meta Reality Labs),2023-12-05 22:04:49+00:00,,,,,,
SpecNeRF: Gaussian Directional Encoding for Specular Reflections,"Neural radiance fields have achieved remarkable performance in modeling the appearance of 3D scenes. However, existing approaches still struggle with the view-dependent appearance of glossy surfaces, especially under complex lighting of indoor environments. Unlike existing methods, which typically assume distant lighting like an environment map, we propose a learnable Gaussian directional encoding to better model the view-dependent effects under near-field lighting conditions. Importantly, our new directional encoding captures the spatially-varying nature of near-field lighting and emulates the behavior of prefiltered environment maps. As a result, it enables the efficient evaluation of preconvolved specular color at any 3D location with varying roughness coefficients. We further introduce a data-driven geometry prior that helps alleviate the shape radiance ambiguity in reflection modeling. We show that our Gaussian directional encoding and geometry prior significantly improve the modeling of challenging specular reflections in neural radiance fields, which helps decompose appearance into more physically meaningful components.",http://arxiv.org/abs/2312.13102v1,,Li Ma (None) | Vasu Agrawal (Meta Reality Labs Research) | Haithem Turki (Carnegie Mellon University) | Changil Kim (Facebook) | Chen Gao (Meta) | Pedro V. Sander (Hong Kong University Of Science And Technology) | Michael Zollhoefer (Meta) | Christian Richardt (Meta Reality Labs),2023-12-20 15:20:25+00:00,,,,,,
What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation,,,,"Yihua Cheng (University Of Birmingham) | Yaning Zhu (Huazhong University Of Science And Technology) | Zongji Wang (Key Laboratory Of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy Of Sciences) | Hongquan Hao (Calmcar) | Liu Wei (Jiangsu University Of Science And Technology) | Shiqing Cheng (Zhejiang University) | Xi Wang (CalmCar Vision System) | Hyung Jin Chang (None)",,,,,,,
GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinemen,,,,"Linfang Zheng (University Of Birmingham) | Tze Ho Elden Tse (University Of Birmingham) | Chen Wang (Department Of Computer Science, The University Of Hong Kong) | Yinghan Sun (Southern University Of Science And Technology) | Hua Chen (Southern University Of Science And Technology) | Ale?? Leonardis (University Of Birmingham) | Wei Zhang (Southern University Of Science And Technology Of China) | Hyung Jin Chang (None)",,,,,,,
MANUS: Markerless Grasp Capture using Articulated 3D Gaussians,"Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that can cause misalignments resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.",http://arxiv.org/abs/2312.02137v1,,"Chandradeep Pokhariya (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Ishaan Shah (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Angela Xing (Brown University) | Zekun Li (Tencent AI Lab) | Kefan Chen (Brown University) | Avinash Sharma (International Institute Of Information Technology Hyderabad) | Srinath Sridhar (None)",2023-12-04 18:56:22+00:00,,,,,,
MaxQ: Multi-Axis Query for N:M Sparsity Network,"N:M sparsity has received increasing attention due to its remarkable performance and latency trade-off compared with structured and unstructured sparsity. However, existing N:M sparsity methods do not differentiate the relative importance of weights among blocks and leave important weights underappreciated. Besides, they directly apply N:M sparsity to the whole network, which will cause severe information loss. Thus, they are still sub-optimal. In this paper, we propose an efficient and effective Multi-Axis Query methodology, dubbed as MaxQ, to rectify these problems. During the training, MaxQ employs a dynamic approach to generate soft N:M masks, considering the weight importance across multiple axes. This method enhances the weights with more importance and ensures more effective updates. Meanwhile, a sparsity strategy that gradually increases the percentage of N:M weight blocks is applied, which allows the network to heal from the pruning-induced damage progressively. During the runtime, the N:M soft masks can be precomputed as constants and folded into weights without causing any distortion to the sparse pattern and incurring additional computational overhead. Comprehensive experiments demonstrate that MaxQ achieves consistent improvements across diverse CNN architectures in various computer vision tasks, including image classification, object detection and instance segmentation. For ResNet50 with 1:16 sparse pattern, MaxQ can achieve 74.6\% top-1 accuracy on ImageNet and improve by over 2.8\% over the state-of-the-art. Codes and checkpoints are available at \url{https://github.com/JingyangXiang/MaxQ}.",http://arxiv.org/abs/2312.07061v2,,Jingyang Xiang (Zhejiang University) | Siqi Li (Zhejiang University) | Junhao Chen (Zhejiang University) | Zhuangzhi Chen (Zhejiang University Of Technology) | Tianxin Huang (Tencent Youtu Lab) | Linpeng Peng (Zhejiang University) | Yong Liu (Zhejiang University),2023-12-12 08:28:29+00:00,,,,,,
Pixel-level Semantic Correspondence through Layout-aware Representation Learning and Multi-scale Matching Integration,,,,Yixuan Sun (Fudan University) | Zhangyue Yin (Fudan University) | Haibo Wang (None) | Yan Wang (Fudan University) | Xipeng Qiu (Fudan University) | Weifeng Ge (Fudan University) | Wenqiang Zhang (None),,,,,,,
DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields,"Advances in neural fields are enabling high-fidelity capture of the shape and appearance of static and dynamic scenes. However, their capabilities lag behind those offered by representations such as pixels or meshes due to algorithmic challenges and the lack of large-scale real-world datasets. We address the dataset limitation with DiVA-360, a real-world 360 dynamic visual-audio dataset with synchronized multimodal visual, audio, and textual information about table-scale scenes. It contains 46 dynamic scenes, 30 static scenes, and 95 static objects spanning 11 categories captured using a new hardware system using 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M image frames and 1360 s of dynamic data. We provide detailed text descriptions for all scenes, foreground-background segmentation masks, category-specific 3D pose alignment for static objects, as well as metrics for comparison. Our data, hardware and software, and code are available at https://diva360.github.io/.",http://arxiv.org/abs/2307.16897v1,,"Cheng-You Lu (University Of Technology Sydney) | Peisen Zhou (Brown University) | Angela Xing (Brown University) | Chandradeep Pokhariya (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Arnab Dey (Universit?? De Nice-Sophia Antipolis) | Ishaan Shah (International Institute Of Information Technology, Hyderabad, International Institute Of Information Technology Hyderabad) | Rugved Mavidipalli (Brown University) | Dylan Hu (Brown University) | Andrew Comport (CNRS) | Kefan Chen (Brown University) | Srinath Sridhar (None)",2023-07-31 17:59:48+00:00,,,,,,
OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning,"Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.",http://arxiv.org/abs/2403.09634v1,,Lingyi Hong (Fudan University) | Shilin Yan (Fudan University) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Wanyun Li (Fudan University) | Xinyu Zhou (None) | Pinxue Guo (Fudan University) | Kaixun Jiang (Fudan University) | Yiting Cheng (None) | Jinglun Li (None) | Zhaoyu Chen (Fudan University) | Wenqiang Zhang (None),2024-03-14 17:59:13+00:00,,,,,,
SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking,,,,Xiaojun Hou (Zhejiang University) | Jiazheng Xing (Zhejiang University) | Yijie Qian (Zhejiang University) | Yaowei Guo (Zhejiang University) | Shuo Xin (Zhejiang University Of Technology) | Junhao Chen (Zhejiang University) | Kai Tang (Zhejiang University) | Mengmeng Wang (Zhejiang University) | Zhengkai Jiang (Tencent) | Liang Liu (Tencent Youtu Lab) | Yong Liu (Zhejiang University),,,,,,,
PointOBB: Learning Oriented Object Detection via Single Point Supervision,"Single point-supervised object detection is gaining attention due to its cost-effectiveness. However, existing approaches focus on generating horizontal bounding boxes (HBBs) while ignoring oriented bounding boxes (OBBs) commonly used for objects in aerial images. This paper proposes PointOBB, the first single Point-based OBB generation method, for oriented object detection. PointOBB operates through the collaborative utilization of three distinctive views: an original view, a resized view, and a rotated/flipped (rot/flp) view. Upon the original view, we leverage the resized and rot/flp views to build a scale augmentation module and an angle acquisition module, respectively. In the former module, a Scale-Sensitive Consistency (SSC) loss is designed to enhance the deep network's ability to perceive the object scale. For accurate object angle predictions, the latter module incorporates self-supervised learning to predict angles, which is associated with a scale-guided Dense-to-Sparse (DS) matching strategy for aggregating dense angles corresponding to sparse objects. The resized and rot/flp views are switched using a progressive multi-view switching strategy during training to achieve coupled optimization of scale and angle. Experimental results on the DIOR-R and DOTA-v1.0 datasets demonstrate that PointOBB achieves promising performance, and significantly outperforms potential point-supervised baselines.",http://arxiv.org/abs/2311.14757v1,,Junwei Luo (Wuhan University) | Xue Yang (Shanghai AI Laboratory) | Yi Yu (Southeast University) | Qingyun Li (Harbin Institute Of Technology) | Junchi Yan (Shanghai Jiao Tong University) | Yansheng Li (Wuhan University),2023-11-23 15:51:50+00:00,,,,,,
SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery,"Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense potential towards a generic model for Earth Observation. Nevertheless, these works primarily focus on a single modality without temporal and geo-context modeling, hampering their capabilities for diverse tasks. In this study, we present SkySense, a generic billion-scale model, pre-trained on a curated multi-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal sequences. SkySense incorporates a factorized multi-modal spatiotemporal encoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR) data as input. This encoder is pre-trained by our proposed Multi-Granularity Contrastive Learning to learn representations across different modal and spatial granularities. To further enhance the RSI representations by the geo-context clue, we introduce Geo-Context Prototype Learning to learn region-aware prototypes upon RSI's multi-modal spatiotemporal features. To our best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules can be flexibly combined or used individually to accommodate various tasks. It demonstrates remarkable generalization capabilities on a thorough evaluation encompassing 16 datasets over 7 tasks, from single- to multi-modal, static to temporal, and classification to localization. SkySense surpasses 18 recent RSFMs in all test scenarios. Specifically, it outperforms the latest models such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and 3.61% on average respectively. We will release the pre-trained weights to facilitate future research and Earth Observation applications.",http://arxiv.org/abs/2312.10115v1,,"Xin Guo (Ant Group) | Jiangwei Lao (Ant Group) | Bo Dang (Wuhan University) | Yingying Zhang (Hikvision Research Institute) | Lei Yu (Antgroup) | Lixiang Ru (Ant Group) | Liheng Zhong (Ant Group) | Ziyuan Huang (National University Of Singapore) | Kang Wu (Wuhan University) | Dingxiang Hu (Mybank) | HUIMEI HE (Ant Group) | Jian Wang (, Institute Of Automation, Chinese Academy Of Science) | Jingdong Chen (Ant Group) | Ming Yang (Ant Group) | Yongjun Zhang (None) | Yansheng Li (Wuhan University)",2023-12-15 09:57:21+00:00,,,,,,
Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset,,,,Yiming Li (New York University) | Zhiheng Li (New York University) | Nuo Chen (New York University) | Moonjun Gong (New York University) | Zonglin Lyu (New York University) | Zehong Wang (New York University) | Peili Jiang (New York University) | Chen Feng (New York University),,,,,,,
KD-DETR: Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling,"DETR is a novel end-to-end transformer architecture object detector, which significantly outperforms classic detectors when scaling up the model size. In this paper, we focus on the compression of DETR with knowledge distillation. While knowledge distillation has been well-studied in classic detectors, there is a lack of researches on how to make it work effectively on DETR. We first provide experimental and theoretical analysis to point out that the main challenge in DETR distillation is the lack of consistent distillation points. Distillation points refer to the corresponding inputs of the predictions for student to mimic, and reliable distillation requires sufficient distillation points which are consistent between teacher and student. Based on this observation, we propose a general knowledge distillation paradigm for DETR(KD-DETR) with consistent distillation points sampling. Specifically, we decouple detection and distillation tasks by introducing a set of specialized object queries to construct distillation points. In this paradigm, we further propose a general-to-specific distillation points sampling strategy to explore the extensibility of KD-DETR. Extensive experiments on different DETR architectures with various scales of backbones and transformer layers validate the effectiveness and generalization of KD-DETR. KD-DETR boosts the performance of DAB-DETR with ResNet-18 and ResNet-50 backbone to 41.4$\%$, 45.7$\%$ mAP, respectively, which are 5.2$\%$, 3.5$\%$ higher than the baseline, and ResNet-50 even surpasses the teacher model by $2.2\%$.",http://arxiv.org/abs/2211.08071v2,,Yu Wang (Baidu) | Li (None) | Shengzhao Wen (Baidu) | Gang Zhang (Baidu) | Haixiao Yue (Baidu) | Haocheng Feng (Baidu) | Junyu Han (Baidu) | Errui Ding (Baidu),2022-11-15 11:52:30+00:00,,,,,,
Towards Better Vision-Inspired Vision-Language Models,,,,"Yun-Hao Cao (Nanjing University) | Kaixiang Ji (Ant Group) | Ziyuan Huang (National University Of Singapore) | Chuanyang Zheng (Ant Group) | Jiajia Liu (Alibaba Group) | Jian Wang (, Institute Of Automation, Chinese Academy Of Science) | Jingdong Chen (Ant Group) | Ming Yang (Ant Group)",,,,,,,
LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images,,,,Jing Zhang (New York University) | Irving Fang (New York University) | Hao Wu (New York University) | Akshat Kaushik (New York University) | Alice Rodriguez (New York University) | Hanwen Zhao (New York University) | Juexiao Zhang (New York University) | Zhuo Zheng (Stanford University) | Radu Iovita (New York University) | Chen Feng (New York University),,,,,,,
TexOct: Generating Textures of 3D Models with Octree-based Diffusion,,,,Jialun Liu (Baidu) | Chenming Wu (None) | Xinqi Liu (Baidu Inc) | Xing Liu (Baidu) | Jinbo Wu (Baidu) | Haotian Peng (Baidu) | Chen Zhao (None) | Haocheng Feng (Baidu) | Jingtuo Liu (Baidu) | Errui Ding (Baidu),,,,,,,
Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis,"Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.",http://arxiv.org/abs/2402.17364v1,,Zicheng Zhang (None) | RUOBING ZHENG (None) | Bonan Li (None) | Congying Han (University Of Chinese Academy Of Sciences) | Tianqi Li (Ant Group) | Meng Wang (Ant Group) | Tiande Guo (University Of The Chinese Academy Of Sciences) | Jingdong Chen (Ant Group) | Ziwen Liu (University Of The Chinese Academy Of Sciences) | Ming Yang (Ant Group),2024-02-27 09:56:15+00:00,,,,,,
Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts,"Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance.",http://arxiv.org/abs/2312.00968v1,,"Jialin Wu (Google) | Xia Hu (Research, Google) | Yaqing Wang (Research, Google) | Bo Pang (Google) | Radu Soricut (Google)",2023-12-01 23:04:27+00:00,,,,,,
On Scaling up a Multilingual Vision and Language Model,,,,"Xi Chen (Google) | Josip Djolonga (Google) | Piotr Padlewski (Google) | Basil Mustafa (Google) | Soravit Changpinyo (Google Research) | Jialin Wu (Google) | Carlos Riquelme Ruiz (Google) | Sebastian Goodman (Google) | Xiao Wang (Google DeepMind) | Yi Tay (Google) | Siamak Shakeri (Research, Google) | Mostafa Dehghani (Google DeepMind) | Daniel Salz (Google) | Mario Lu??i?? (Google) | Michael Tschannen (Google DeepMind) | Arsha Nagrani (Google ) | Hexiang Hu (Google Deepmind) | Mandar Joshi (Google DeepMind) | Bo Pang (Google) | Ceslee Montgomery (Google) | Paulina Pietrzyk (Google) | Marvin Ritter (Google DeepMind) | AJ Piergiovanni (Google) | Matthias Minderer (Google) | Filip Pavetic (Google) | Austin Waters (Google) | Gang Li (Google) | Ibrahim Alabdulmohsin (Google) | Lucas Beyer (Google Brain/DM Z??rich) | Julien Amelot (Research, Google) | Kenton Lee (Google Research) | Andreas Steiner (Google) | Yang Li (Google) | Daniel Keysers (Google DeepMind) | Anurag Arnab (Google) | Yuanzhong Xu (Google) | Keran Rong (Google Deepmind) | Alexander Kolesnikov (Google) | Mojtaba Seyedhosseini (Google) | Anelia Angelova (Google) | Xiaohua Zhai (Google) | Neil Houlsby (Google) | Radu Soricut (Google)",,,,,,,
Learning Occupancy for Monocular 3D Object Detection,"Monocular 3D detection is a challenging task due to the lack of accurate 3D information. Existing approaches typically rely on geometry constraints and dense depth estimates to facilitate the learning, but often fail to fully exploit the benefits of three-dimensional feature extraction in frustum and 3D space. In this paper, we propose \textbf{OccupancyM3D}, a method of learning occupancy for monocular 3D detection. It directly learns occupancy in frustum and 3D space, leading to more discriminative and informative 3D features and representations. Specifically, by using synchronized raw sparse LiDAR point clouds, we define the space status and generate voxel-based occupancy labels. We formulate occupancy prediction as a simple classification problem and design associated occupancy losses. Resulting occupancy estimates are employed to enhance original frustum/3D features. As a result, experiments on KITTI and Waymo open datasets demonstrate that the proposed method achieves a new state of the art and surpasses other methods by a significant margin. Codes and pre-trained models will be available at \url{https://github.com/SPengLiang/OccupancyM3D}.",http://arxiv.org/abs/2305.15694v1,,"Liang Peng (FABU Inc) | Junkai Xu (Zhejiang University) | Haoran Cheng (College Of Computer Science And Technology, Zhejiang University) | Zheng Yang (Fabu Inc) | Xiaopei Wu (Zhejiang University) | Wei Qian (Fabu) | Wenxiao Wang (Zhejiang University) | Boxi Wu (Zhejiang University) | Deng Cai (Zhejiang University)",2023-05-25 04:03:46+00:00,,,,,,
Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection,,,,Zhanwei Zhang (None) | Minghao Chen (Zhejiang University) | Shuai Xiao (Alibaba Group) | Liang Peng (FABU Inc) | Hengjia Li (FABU Inc) | Binbin Lin (Zhejiang University) | Ping Li (Hangzhou Dianzi University) | Wenxiao Wang (Zhejiang University) | Boxi Wu (Zhejiang University) | Deng Cai (Zhejiang University),,,,,,,
"Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action","We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.",http://arxiv.org/abs/2312.17172v1,,Jiasen Lu (None) | Christopher Clark (None) | Sangho Lee (Allen Institute For Artificial Intelligence) | Zichen Zhang (Allen Institute For Artificial Intelligence) | Savya Khosla (University Of Illinois Urbana-Champaign) | Ryan Marten (None) | Derek Hoiem (University Of Illinois At Urbana-Champaign) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence),2023-12-28 17:57:06+00:00,,,,,,
Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World,"Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets. Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced.",http://arxiv.org/abs/2312.02976v1,,Kiana Ehsani (Allen Institute For Artificial Intelligence) | Tanmay Gupta (Allen Institute For Artificial Intelligence) | Rose Hendrix (Allen Institute For Artificial Intelligence) | Jordi Salvador (Allen Institute For AI) | Luca Weihs (Allen Institute For Artificial Intelligence) | Kuo-Hao Zeng (Allen Institute For Artificial Intelligence) | Kunal Singh Singh (None) | Yejin Kim (Allen Institute For Artificial Intelligence) | Winson Han (Allen Institute For Artificial Intelligence) | Alvaro Herrasti (Allen Institute For Artificial Intelligence) | Ranjay Krishna (University Of Washington) | Dustin Schwenk (Allen Institute For Artificial Intelligence) | Eli VanderBilt (University Of Idaho) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence),2023-12-05 18:59:45+00:00,,,,,,
UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation,,,,"Hong Li (Beijing University Of Aeronautics And Astronautics) | Yutang Feng (Beijing University Of Aeronautics And Astronautics) | Song Xue (Baidu) | Xuhui Liu (Beihang University) | Boyu Liu (Beijing University Of Aeronautics And Astronautics) | Bohan Zeng (Beijing University Of Aeronautics And Astronautics) | Shanglin Li (Beijing University Of Aeronautics And Astronautics) | Jianzhuang Liu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Shumin Han (Baidu) | Baochang Zhang (Beihang University)",,,,,,,
ZONE: Zero-Shot Instruction-Guided Local Editing,"Recent advances in vision-language models like Stable Diffusion have shown remarkable power in creative image synthesis and editing.However, most existing text-to-image editing methods encounter two obstacles: First, the text prompt needs to be carefully crafted to achieve good results, which is not intuitive or user-friendly. Second, they are insensitive to local edits and can irreversibly affect non-edited regions, leaving obvious editing traces. To tackle these problems, we propose a Zero-shot instructiON-guided local image Editing approach, termed ZONE. We first convert the editing intent from the user-provided instruction (e.g., ``make his tie blue"") into specific image editing regions through InstructPix2Pix. We then propose a Region-IoU scheme for precise image layer extraction from an off-the-shelf segment model. We further develop an edge smoother based on FFT for seamless blending between the layer and the image.Our method allows for arbitrary manipulation of a specific region with a single instruction while preserving the rest. Extensive experiments demonstrate that our ZONE achieves remarkable local editing results and user-friendliness, outperforming state-of-the-art methods.",http://arxiv.org/abs/2312.16794v1,,"Shanglin Li (Beijing University Of Aeronautics And Astronautics) | Bohan Zeng (Beijing University Of Aeronautics And Astronautics) | Yutang Feng (Beijing University Of Aeronautics And Astronautics) | Sicheng Gao (Bayerische Julius-Maximilians-Universit??t W??rzburg) | Xuhui Liu (Beihang University) | Jiaming Liu (Xiaohongshu) | Li Lin (Xiamen University) | Xu Tang (Shanghaitech University) | Yao Hu (Zhejiang University, Tsinghua University) | Jianzhuang Liu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Baochang Zhang (Beihang University)",2023-12-28 02:54:34+00:00,,,,,,
Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models,,,,"Weiwei Cao (University Of Science And Technology Of China) | Jianpeng Zhang (None) | Yingda Xia (Alibaba Group) | Tony C. W. MOK (Alibaba DAMO Academy) | Zi Li (Alibaba DAMO Academy) | Xianghua Ye (Zhejiang University) | Le Lu (Alibaba Group) | Jian Zheng (Suzhou Institute Of Biomedical Engineering And Technology, Chinese Academy Of Sciences) | Yuxing Tang (Alibaba Group) | Ling Zhang (Alibaba Group)",,,,,,,
Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration,,,,"Tony C. W. MOK (Alibaba DAMO Academy) | Zi Li (Alibaba DAMO Academy) | Yunhao Bai (None) | Jianpeng Zhang (None) | Wei Liu (Alibaba Group) | Yan-Jie Zhou (DAMO Academy, Alibaba Group) | Ke Yan (Alibaba DAMO Academy) | Dakai Jin (Alibaba Group) | Yu Shi (China Medical University Shenyang) | Xiaoli Yin (China Medical University Shenyang) | Le Lu (Alibaba Group) | Ling Zhang (Alibaba Group)",,,,,,,
Taming Mode Collapse in Score Distillation for Text-to-3D Generation,"Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as ""Janus"" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing in entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitigating the Janus problem. Based on this new objective, we derive a new update rule for 3D score distillation, dubbed Entropic Score Distillation (ESD). We theoretically reveal that ESD can be simplified and implemented by just adopting the classifier-free guidance trick upon variational score distillation. Although embarrassingly straightforward, our extensive experiments successfully demonstrate that ESD can be an effective treatment for Janus artifacts in score distillation.",http://arxiv.org/abs/2401.00909v1,,"Peihao Wang (University Of Texas, Austin) | Dejia Xu (University Of Texas At Austin) | Zhiwen Fan (University Of Texas, Austin) | Dilin Wang (Facebook) | Sreyas Mohan (Meta) | Forrest Iandola (Meta) | Rakesh Ranjan (None) | Yilei Li (Facebook) | Qiang Liu (University Of Texas, Austin) | Zhangyang Wang (University Of Texas At Austin) | Vikas Chandra (Facebook)",2023-12-31 22:47:06+00:00,,,,,,
TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation,,,,Xiaopei Wu (Zhejiang University) | Yuenan Hou (Shanghai AI Laboratory) | Xiaoshui Huang (Shanghai AI Laboratory) | Binbin Lin (Zhejiang University) | Tong He (Shanghai AI Lab) | Xinge Zhu (The Chinese University Of Hong Kong) | Yuexin Ma (ShanghaiTech University) | Boxi Wu (Zhejiang University) | Haifeng Liu (Zhejiang University) | Deng Cai (Zhejiang University) | Wanli Ouyang (University Of Sydney),,,,,,,
UniPAD: A Universal Pre-training Paradigm for Autonomous Driving,"In the context of autonomous driving, the significance of effective feature learning is widely acknowledged. While conventional 3D self-supervised pre-training methods have shown widespread success, most methods follow the ideas originally designed for 2D images. In this paper, we present UniPAD, a novel self-supervised learning paradigm applying 3D volumetric differentiable rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction of continuous 3D shape structures and the intricate appearance characteristics of their 2D projections. The flexibility of our method enables seamless integration into both 2D and 3D frameworks, enabling a more holistic comprehension of the scenes. We manifest the feasibility and effectiveness of UniPAD by conducting extensive experiments on various downstream 3D tasks. Our method significantly improves lidar-, camera-, and lidar-camera-based baseline by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic segmentation on the nuScenes validation set, achieving state-of-the-art results in comparison with previous methods. The code will be available at https://github.com/Nightmare-n/UniPAD.",http://arxiv.org/abs/2310.08370v1,,Honghui Yang (Zhejiang University) | Sha Zhang (None) | Di Huang (University Of Sydney) | Xiaoyang Wu (The University Of Hong Kong) | Haoyi Zhu (University Of Science And Technology Of China) | Tong He (Shanghai AI Lab) | SHIXIANG TANG (The Chinese University Of Hong Kong) | Hengshuang Zhao (The University Of Hong Kong) | Qibo Qiu (Zhejiang Lab) | Binbin Lin (Zhejiang University) | Xiaofei He (Zhejiang University) | Wanli Ouyang (University Of Sydney),2023-10-12 14:39:58+00:00,,,,,,
EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything,"Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.",http://arxiv.org/abs/2312.00863v1,,"Yunyang Xiong (Facebook) | Balakrishnan Varadarajan (Meta) | Lemeng Wu (University Of Texas, Austin) | Xiaoyu Xiang (Meta) | Fanyi Xiao (Meta) | Chenchen Zhu (Meta AI) | Xiaoliang Dai (Facebook) | Dilin Wang (Facebook) | Fei Sun (Meta) | Forrest Iandola (Meta) | Raghuraman Krishnamoorthi (Facebook) | Vikas Chandra (Facebook)",2023-12-01 18:31:00+00:00,,,,,,
ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation,"This work introduces ArtAdapter, a transformative text-to-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multi-level style encoder with our proposed explicit adaptation mechanism enables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions. Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.",http://arxiv.org/abs/2312.02109v1,,Dar-Yen Chen (None) | Hamish Tennent (PicCollage) | Ching-Wen Hsu (PicCollage),2023-12-04 18:39:00+00:00,,,,,,
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers,"Deployment of Transformer models on edge devices is becoming increasingly challenging due to the exponentially growing inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require computationally expensive fine-tuning, which is undesirable in many edge deployment cases. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. It leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm. This distribution further guides token partitioning for efficient similarity-based pruning. Due to the elimination of the fine-tuning overhead, Zero-TPrune can prune large models at negligible computational cost, switch between different pruning configurations at no computational cost, and perform hyperparameter tuning efficiently. We evaluate the performance of Zero-TPrune on vision tasks by applying it to various vision Transformer backbones and testing them on ImageNet. Without any fine-tuning, Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7\% and improves its throughput by 45.3\% with only 0.4\% accuracy loss. Compared with state-of-the-art pruning methods that require fine-tuning, Zero-TPrune not only eliminates the need for fine-tuning after pruning but also does so with only 0.1\% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning methods, Zero-TPrune reduces accuracy loss by up to 49\% with the same or higher throughput.",http://arxiv.org/abs/2305.17328v2,,Hongjie Wang (Princeton University) | Bhishma Dedhia (Princeton University) | Niraj Jha (Princeton University),2023-05-27 02:08:51+00:00,,,,,,
Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models,,,,MATTHEW KOWAL (None) | Richard P. Wildes (York University) | Kosta Derpanis (York University/Samsung),,,,,,,
Language-driven All-in-one Adverse Weather Removal,,,,Hao Yang (Beijing Institute Of Technology) | Liyuan Pan (Beijing Institute Of Technology) | Yan Yang (ANU) | Wei Liang (Beijing Institute Of Technology),,,,,,,
MatFuse: Controllable Material Generation with Diffusion Models,"Creating high-quality materials in computer graphics is a challenging and time-consuming task, which requires great expertise. To simplify this process, we introduce MatFuse, a unified approach that harnesses the generative power of diffusion models for creation and editing of 3D materials. Our method integrates multiple sources of conditioning, including color palettes, sketches, text, and pictures, enhancing creative possibilities and granting fine-grained control over material synthesis. Additionally, MatFuse enables map-level material editing capabilities through latent manipulation by means of a multi-encoder compression model which learns a disentangled latent representation for each map. We demonstrate the effectiveness of MatFuse under multiple conditioning settings and explore the potential of material editing. Finally, we assess the quality of the generated materials both quantitatively in terms of CLIP-IQA and FID scores and qualitatively by conducting a user study. Source code for training MatFuse and supplemental materials are publicly available at https://gvecchio.com/matfuse.",http://arxiv.org/abs/2308.11408v3,,Giuseppe Vecchio (University Of Catania) | Renato Sortino (University Of Catania) | Simone Palazzo (University Of Catania) | Concetto Spampinato (University Of Catania),2023-08-22 12:54:48+00:00,,,,,,
SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining,,,,Chull Hwan Song (Dealicious Inc) | Taebaek Hwang (None) | Jooyoung Yoon (Dealicious Inc) | Shunghyun Choi (None) | Yeong Hyeon Gu (Sejong University),,,,,,,
PointInfinity: Resolution-Invariant Point Diffusion Models,,,,"Zixuan Huang (University Of Illinois Urbana-Champaign) | Justin Johnson (University Of Michigan) | Shoubhik Debnath (FAIR, Meta) | James Rehg (None) | Chao-Yuan Wu (Meta)",,,,,,,
LightIt: Illumination Modeling and Control for Diffusion Models,,,,Peter Kocsis (None) | Kalyan Sunkavalli (Adobe Research) | Julien Philip (Adobe Research) | Matthias Nie??ner (Technical University Of Munich) | Yannick Hold-Geoffroy (Adobe Research),,,,,,,
Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation,"In the digital era, QR codes serve as a linchpin connecting virtual and physical realms. Their pervasive integration across various applications highlights the demand for aesthetically pleasing codes without compromised scannability. However, prevailing methods grapple with the intrinsic challenge of balancing customization and scannability. Notably, stable-diffusion models have ushered in an epoch of high-quality, customizable content generation. This paper introduces Text2QR, a pioneering approach leveraging these advancements to address a fundamental challenge: concurrently achieving user-defined aesthetics and scanning robustness. To ensure stable generation of aesthetic QR codes, we introduce the QR Aesthetic Blueprint (QAB) module, generating a blueprint image exerting control over the entire generation process. Subsequently, the Scannability Enhancing Latent Refinement (SELR) process refines the output iteratively in the latent space, enhancing scanning robustness. This approach harnesses the potent generation capabilities of stable-diffusion models, navigating the trade-off between image aesthetics and QR code scannability. Our experiments demonstrate the seamless fusion of visual appeal with the practical utility of aesthetic QR codes, markedly outperforming prior methods. Codes are available at \url{https://github.com/mulns/Text2QR}",http://arxiv.org/abs/2403.06452v2,,Guangyang Wu (Shanghai Jiao Tong University) | Xiaohong Liu (Shanghai Jiao Tong University) | Jun Jia (Shanghai Jiao Tong University) | Xuehao Cui (University Of Michigan - Ann Arbor) | Guangtao Zhai (Shanghai Jiao Tong University),2024-03-11 06:03:31+00:00,,,,,,
DUSt3R: Dense Unstructured Stereo 3D Reconstruction,,,,Shuzhe Wang (Aalto University) | Vincent Leroy (Naver Labs Europe) | Yohann Cabon (Naver Labs Europe) | Boris Chidlovskii (None) | Jerome Revaud (Naver Labs Europe),,,,,,,
NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation,,,,Vikas Thamizharasan (University Of Massachusetts Amherst) | Difan Liu (Adobe Research) | Matthew Fisher (Adobe Research) | Nanxuan Zhao (Adobe Research) | Evangelos Kalogerakis (UMass Amherst) | Michal Luk???? (Adobe Systems),,,,,,,
Real-Time Neural BRDF with Spherically Distributed Primitives,"We propose a novel compact and efficient neural BRDF offering highly versatile material representation, yet with very-light memory and neural computation consumption towards achieving real-time rendering. The results in Figure 1, rendered at full HD resolution on a current desktop machine, show that our system achieves real-time rendering with a wide variety of appearances, which is approached by the following two designs. On the one hand, noting that bidirectional reflectance is distributed in a very sparse high-dimensional subspace, we propose to project the BRDF into two low-dimensional components, i.e., two hemisphere feature-grids for incoming and outgoing directions, respectively. On the other hand, learnable neural reflectance primitives are distributed on our highly-tailored spherical surface grid, which offer informative features for each component and alleviate the conventional heavy feature learning network to a much smaller one, leading to very fast evaluation. These primitives are centrally stored in a codebook and can be shared across multiple grids and even across materials, based on the low-cost indices stored in material-specific spherical surface grids. Our neural BRDF, which is agnostic to the material, provides a unified framework that can represent a variety of materials in consistent manner. Comprehensive experimental results on measured BRDF compression, Monte Carlo simulated BRDF acceleration, and extension to spatially varying effect demonstrate the superior quality and generalizability achieved by the proposed scheme.",http://arxiv.org/abs/2310.08332v1,,Yishun Dou (Huawei) | Zhong Zheng (Huawei.Com) | Qiaoqiao Jin (Shanghai Jiao Tong University) | Bingbing Ni (Shanghai Jiao Tong University) | Yugang Chen (Hisilicon) | Junxiang Ke (Huawei Technologies Ltd.),2023-10-12 13:46:36+00:00,,,,,,
Understanding Video Transfomers via Universal Concept Discovery,"This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models. Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers. Finally, we demonstrate that VTCDcan be used to improve model performance for fine-grained tasks.",http://arxiv.org/abs/2401.10831v1,,MATTHEW KOWAL (None) | Achal Dave (None) | Rares Andrei Ambrus (Toyota Research Institute) | Adrien Gaidon (Toyota Research Institute (TRI)) | Kosta Derpanis (York University/Samsung) | Pavel Tokmakov (Toyota Research Institute),2024-01-19 17:27:21+00:00,,,,,,
On Train-Test Class Overlap and Detection for Image Retrieval,,,,Chull Hwan Song (Dealicious Inc) | Jooyoung Yoon (Dealicious Inc) | Taebaek Hwang (None) | Shunghyun Choi (None) | Yeong Hyeon Gu (Sejong University) | Yannis Avrithis (IARAI),,,,,,,
BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation,"Generating large-scale 3D scenes cannot simply apply existing 3D object synthesis technique since 3D scenes usually hold complex spatial configurations and consist of a number of objects at varying scales. We thus propose a practical and efficient 3D representation that incorporates an equivariant radiance field with the guidance of a bird's-eye view (BEV) map. Concretely, objects of synthesized 3D scenes could be easily manipulated through steering the corresponding BEV maps. Moreover, by adequately incorporating positional encoding and low-pass filters into the generator, the representation becomes equivariant to the given BEV map. Such equivariance allows us to produce large-scale, even infinite-scale, 3D scenes via synthesizing local scenes and then stitching them with smooth consistency. Extensive experiments on 3D scene datasets demonstrate the effectiveness of our approach. Our project website is at https://zqh0253.github.io/BerfScene/.",http://arxiv.org/abs/2312.02136v1,,"Qihang Zhang (The Chinese University Of Hong Kong) | Yinghao Xu (Chinese University Of Hong Kong) | Yujun Shen (The Chinese University Of Hong Kong) | Bo Dai (Shanghai AI Laboratory) | Bolei Zhou (University Of California, Los Angeles) | Ceyuan Yang (The Chinese University Of Hong Kong)",2023-12-04 18:56:10+00:00,,,,,,
Perceptual-Oriented Video Frame Interpolation Via Asymmetric Synergistic Blending,,,,Guangyang Wu (Shanghai Jiao Tong University) | Xin Tao (Kuaishou) | Changlin Li (SeeKoo) | Wenyi Wang (University Of Electronic Science And Technology Of China) | Xiaohong Liu (Shanghai Jiao Tong University) | Qingqing Zheng (None),,,,,,,
Visual Fact Checker: Enabling High Fidelity Detailed Caption Generation,,,,"Yunhao Ge (University Of Southern California) | Xiaohui Zeng (Department Of Computer Science, University Of Toronto) | Jacob Huffman (NVIDIA) | Tsung-Yi Lin (NVIDIA) | Ming-Yu Liu (NVIDIA) | Yin Cui (Google)",,,,,,,
Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models,,,,Hongjie Wang (Princeton University) | Difan Liu (Adobe Research) | Yan Kang (None) | Yijun Li (Adobe Research) | Zhe Lin (Adobe Research) | Niraj Jha (Princeton University) | Yuchen Liu (None),,,,,,,
Generating Enhanced Negatives for Training Language-Based Object Detectors,"The recent progress in language-based open-vocabulary object detection can be largely attributed to finding better ways of leveraging large-scale data with free-form text annotations. Training such models with a discriminative objective function has proven successful, but requires good positive and negative samples. However, the free-form nature and the open vocabulary of object descriptions make the space of negatives extremely large. Prior works randomly sample negatives or use rule-based techniques to build them. In contrast, we propose to leverage the vast knowledge built into modern generative models to automatically build negatives that are more relevant to the original data. Specifically, we use large-language-models to generate negative text descriptions, and text-to-image diffusion models to also generate corresponding negative images. Our experimental analysis confirms the relevance of the generated negative data, and its use in language-based detectors improves performance on two complex benchmarks.",http://arxiv.org/abs/2401.00094v1,,"Shiyu Zhao (Rutgers University, New Brunswick) | Long Zhao (Google Research) | Vijay Kumar BG (NEC Laboratories America) | Yumin Suh (NEC Labs America) | Dimitris N. Metaxas (Rutgers) | Manmohan Chandraker (UC San Diego) | Samuel Schulter (None)",2023-12-29 23:04:00+00:00,,,,,,
Learning to Segment Referred Objects from Narrated Egocentric Videos,,,,Yuhan Shen (Northeastern University) | Huiyu Wang (Facebook) | Xitong Yang (Meta) | Matt Feiszli (Meta AI) | Ehsan Elhamifar (None) | Lorenzo Torresani (Facebook) | Effrosyni Mavroudi (None),,,,,,,
Learning Multi-dimensional Human Preference for Text-to-Image Generation,,,,Sixian Zhang (None) | Bohan Wang (Kuaishou) | Junqiang Wu (Kuaishou) | Yan Li (Kuaishou) | Tingting Gao (China Agricultural University) | Di ZHANG (Kuaishou Technology) | Zhongyuan Wang (Kuaishou),,,,,,,
FaceCom: Towards High-fidelity 3D Facial Shape Completion via Optimization and Inpainting Guidance,,,,Yinglong Li (None) | Hongyu Wu (None) | Wang (None) | Qingzhao Qin (Peking University) | Yijiao Zhao (None) | Yong Wang (None) | Aimin Hao (None),,,,,,,
ReGenNet: Towards Human Action-Reaction Synthesis,,,,"Liang Xu (Shanghai Jiao Tong University) | Yizhou Zhou (WeChat AI) | Yichao Yan (Shanghai Jiao Tong University) | Xin Jin (Eastern Institute For Advanced Study) | Wenhan Zhu (None) | Fengyun Rao (WeChat, Tencent) | Xiaokang Yang (Shanghai Jiao Tong University, China) | Wenjun Zeng (None)",,,,,,,
VecFusion: Vector Font Generation with Diffusion,"We present VecFusion, a new neural architecture that can generate vector fonts with varying topological structures and precise control point positions. Our approach is a cascaded diffusion model which consists of a raster diffusion model followed by a vector diffusion model. The raster model generates low-resolution, rasterized fonts with auxiliary control point information, capturing the global style and shape of the font, while the vector model synthesizes vector fonts conditioned on the low-resolution raster fonts from the first stage. To synthesize long and complex curves, our vector diffusion model uses a transformer architecture and a novel vector representation that enables the modeling of diverse vector geometry and the precise prediction of control points. Our experiments show that, in contrast to previous generative models for vector graphics, our new cascaded vector diffusion model generates higher quality vector fonts, with complex structures and diverse styles.",http://arxiv.org/abs/2312.10540v1,,Vikas Thamizharasan (University Of Massachusetts Amherst) | Difan Liu (Adobe Research) | Shantanu Agarwal (Balbix) | Matthew Fisher (Adobe Research) | Micha??l Gharbi (Massachusetts Institute Of Technology) | Oliver Wang (Adobe Research) | Alec Jacobson (University Of Toronto And Adobe Systems) | Evangelos Kalogerakis (UMass Amherst),2023-12-16 20:49:00+00:00,,,,,,
"See, Say, and Segment: Correcting False Premises with LMMs",,,,"Tsung-Han Wu (University Of California, Berkeley) | Giscard Biamby (University Of California, Berkeley) | David Chan (University Of California Berkeley) | Lisa Dunlap (University Of California, Berkeley) | Ritwik Gupta (Defense Innovation Unit) | Xudong Wang (Electrical Engineering & Computer Science Department, University Of California Berkeley) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Joseph Gonzalez (University Of California - Berkeley)",,,,,,,
GPLD3D: Latent Diffusion of 3D Shape Generative Models by Enforcing Geometric and Physical Priors,,,,Yuan Dong (Alibaba Group) | Qi Zuo (Alibaba Group) | Xiaodong Gu (Alibaba Group) | Weihao Yuan (Alibaba Group) | Zhengyi Zhao (Alibaba Group) | Zilong Dong (Alibaba Group) | Liefeng Bo (None) | Qixing Huang (University Of Texas At Austin),,,,,,,
Adaptive Slot Attention: Object Discovery with Dynamic Slot Number,,,,"Ke Fan (Fudan University) | Zechen Bai (Show Lab, National University Of Singapore) | Tianjun Xiao (Amazon) | Tong He (Amazon Web Services) | Max Horn (GSK Plc) | Yanwei Fu (Fudan University) | Francesco Locatello (ISTA) | Zheng Zhang (New York University)",,,,,,,
VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,"Current diffusion-based video editing primarily focuses on structure-preserved editing by utilizing various dense correspondences to ensure temporal consistency and motion alignment. However, these approaches are often ineffective when the target edit involves a shape change. To embark on video editing with shape change, we explore customized video subject swapping in this work, where we aim to replace the main subject in a source video with a target subject having a distinct identity and potentially different shape. In contrast to previous methods that rely on dense correspondences, we introduce the VideoSwap framework that exploits semantic point correspondences, inspired by our observation that only a small number of semantic points are necessary to align the subject's motion trajectory and modify its shape. We also introduce various user-point interactions (\eg, removing points and dragging points) to address various semantic point correspondence. Extensive experiments demonstrate state-of-the-art video subject swapping results across a variety of real-world videos.",http://arxiv.org/abs/2312.02087v2,,Yuchao Gu (None) | Yipin Zhou (Facebook) | Bichen Wu (Facebook) | Licheng Yu (None) | Jia-Wei Liu (National University Of Singapore) | Rui Zhao (None) | Jay Zhangjie Wu (National University Of Singapore) | David Junhao Zhang (National University Of Singapore) | Mike Zheng Shou (National University Of Singapore) | Kevin Tang (Meta),2023-12-04 17:58:06+00:00,,,,,,
GOAT-Bench: A Benchmark for Multi-modal Lifelong Navigation,,,,"Mukul Khanna (Georgia Institute Of Technology) | Ram Ramrakhya (None) | Gunjan Chhablani (Georgia Institute Of Technology) | Sriram Yenamandra (Georgia Institute Of Technology) | Theo Gervet (Carnegie Mellon University) | Matthew Chang (University Of Illinois, Urbana Champaign) | Zsolt Kira (Georgia Institute Of Technology) | Devendra Singh Chaplot (Carnegie Mellon University) | Dhruv Batra (FAIR (Meta) And Georgia Tech) | Roozbeh Mottaghi (Meta)",,,,,,,
Towards Text-guided 3D Scene Composition,"We are witnessing significant breakthroughs in the technology for generating 3D objects from text. Existing approaches either leverage large text-to-image models to optimize a 3D representation or train 3D generators on object-centric datasets. Generating entire scenes, however, remains very challenging as a scene contains multiple 3D objects, diverse and scattered. In this work, we introduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes from text. We marry the locality of objects with globality of scenes by introducing a hybrid 3D representation: explicit for objects and implicit for scenes. Remarkably, an object, being represented explicitly, can be either generated from text using conventional text-to-3D approaches, or provided by users. To configure the layout of the scene and automatically place objects, we apply the Particle Swarm Optimization technique during the optimization process. Furthermore, it is difficult for certain parts of the scene (e.g., corners, occlusion) to receive multi-view supervision, leading to inferior geometry. We incorporate an RGBD panorama diffusion model to mitigate it, resulting in high-quality geometry. Extensive evaluation supports that our approach achieves superior quality over previous approaches, enabling the generation of detailed and view-consistent 3D scenes.",http://arxiv.org/abs/2312.08885v1,,"Qihang Zhang (The Chinese University Of Hong Kong) | Chaoyang Wang (Snap Inc) | Aliaksandr Siarohin (Snap) | Peiye Zhuang (Snap) | Yinghao Xu (Chinese University Of Hong Kong) | Ceyuan Yang (The Chinese University Of Hong Kong) | Dahua Lin (The Chinese University Of Hong Kong) | Bolei Zhou (University Of California, Los Angeles) | Sergey Tulyakov (Snap) | Hsin-Ying Lee (Snap)",2023-12-13 18:59:30+00:00,,,,,,
Test-Time Linear Out-of-Distribution Detection,,,,"Ke Fan (Fudan University) | Tong Liu (BOE Technology Group Co., Ltd) | Xingyu Qiu (Fudan University) | Yikai Wang (None) | Lian Huai (BOE Technology Group Co., Ltd) | Zeyu Shangguan (BOE TECHNOLOGY GROUP CO., LTD) | Shuang Gou (BOE Technology Group Co., Ltd) | FENGJIAN LIU (BOE Technology Group Co., Ltd ) | Yuqian Fu (Fudan University) | Yanwei Fu (Fudan University) | Xingqun Jiang (BOE Technology Group Co., LTD)",,,,,,,
Lane2Seq: Towards Unified Lane Detection via Sequence Generation,"In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain transformer-based encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on reinforcement learning to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on benchmarks. For example, Lane2Seq gets 97.95\% and 97.42\% F1 score on Tusimple and LLAMAS datasets, establishing a new state-of-the-art result for two benchmarks.",http://arxiv.org/abs/2402.17172v1,,Kunyang Zhou (Southeast University),2024-02-27 03:13:48+00:00,,,,,,
Construct to Associate: Cooperative Context Learning for Domain Adaptive Point Cloud Segmentation,,,,Guangrui Li (University Of Technology Sydney),,,,,,,
Mudslide: A Universal Nuclear Instance Segmentation Method,,,,Jun Wang (Peking University),,,,,,,
TransNeXt: Robust Foveal Visual Perception for Vision Transformers,"Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridges the gap between GLU and SE mechanism, which empowers each token to have channel attention based on its nearest neighbor image features, enhancing local modeling capability and model robustness. We combine aggregated attention and convolutional GLU to create a new visual backbone called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves state-of-the-art performance across multiple model sizes. At a resolution of $224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of $384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic segmentation mIoU of 54.7.",http://arxiv.org/abs/2311.17132v1,,Dai Shi (Independent Researcher),2023-11-28 18:03:27+00:00,,,,,,
Improving Physics-Augmented Continuum Neural Radiance Fields-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization,,,,Takuhiro Kaneko (None),,,,,,,
An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains,"3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.",http://arxiv.org/abs/2402.17562v1,,George Eskandar (Universit??t Stuttgart),2024-02-27 15:02:17+00:00,,,,,,
LED: A Large-scale Real-world Paired Dataset for Event Camera Denoising,,,,Yuxing Duan (None),,,,,,,
Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation,"Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.",http://arxiv.org/abs/2311.17117v2,,Li Hu (Alibaba),2023-11-28 12:27:15+00:00,,,,,,
Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation,"Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.",http://arxiv.org/abs/2402.17614v1,,Jonas Herzog (None),2024-02-27 15:43:53+00:00,,,,,,
Mean-Shift Feature Transformer,,,,Takumi Kobayashi (National Institute Of Advanced Industrial Science And Technology (AIST)),,,,,,,
Gradient Reweighting: Towards Imbalanced Class-Incremental Learning,"Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.",http://arxiv.org/abs/2402.18528v1,,Jiangpeng He (Purdue University),2024-02-28 18:08:03+00:00,,,,,,
DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation,,,,Haonan Lin (None),,,,,,,
Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation,,,,Yunhe Gao (Rutgers University),,,,,,,
AdaShift: Learning Discriminative Self-Gated Neural Feature Activation With an Adaptive Shift Factor,,,,Sudong Cai (Kyoto University),,,,,,,
CORE-MPI: Consistency Object Removal with Embedding MultiPlane Image,,,,Donggeun Yoon (Chungnam National University / KETI) | Donghyeon Cho (Hanyang University),,,,,,,
Text-guided Explorable Image Super-resolution,,,,Kanchana Vaishnavi Gandikota (None) | Paramanand Chandramouli (Universit??t Siegen),,,,,,,
Revisiting Counterfactual Problems in Referring Expression Comprehension,,,,Zhihan Yu (Beijing University Of Posts And Telecommunications) | Ruifan Li (Beijing University Of Post And Telecommunication),,,,,,,
JoAPR: Cleaning the Lens of Prompt Learning for Visual-Language Models,,,,YUNCHENG GUO (None) | Xiaodong Gu (Fudan University),,,,,,,
Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting,,,,"Haiwei Chen (USC-ICT, Vision And Graphics Lab) | Yajie Zhao (University Of Southern California)",,,,,,,
NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging,"Layout-aware text-to-image generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware text-to-image diffusion models still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware text-to-image diffusion model called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with ControlNet to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of ControlNet. The code is available at https://github.com/univ-esuty/noisecollage.",http://arxiv.org/abs/2403.03485v1,,Takahiro Shirakawa (Kyushu University) | Seiichi Uchida (Kyushu University),2024-03-06 05:56:31+00:00,,,,,,
Hierarchical Correlation Clustering and Tree Preserving Embedding,"We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.",http://arxiv.org/abs/2002.07756v2,,Morteza Haghir Chehreghani (Chalmers University Of Technology) | Mostafa Haghir Chehreghani (Amirkabir University Of Technology),2020-02-18 17:44:25+00:00,,,,,,
Leveraging Camera Triplets for Efficient and Accurate Structure-from-Motion,,,,Lalit Manam (Indian Institute Of Science) | Venu Madhav Govindu (Indian Institute Of Science),,,,,,,
Curriculum Point Prompting for Weakly-Supervised Referring Segmentation,,,,Qiyuan Dai (ShanghaiTech University) | Sibei Yang (None),,,,,,,
G-FARS: Gradient-Field-based Auto-Regressive Sampling for 3D Part Grouping,,,,Junfeng Cheng (Imperial College London) | Tania Stathaki (Imperial College London),,,,,,,
A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning,,,,Siddharth Srivastava (TensorTour Inc) | Gaurav Sharma (TensorTour),,,,,,,
Pose-Transformed Equivariant Network for 3D Point Trajectory Prediction,,,,Ruixuan Yu (Shandong University) | Jian Sun (Xi'an Jiao Tong University),,,,,,,
Universal Robustness via Median Random Smoothing for Real-World Super-Resolution,,,,"Zakariya Chaouai (Paris-Saclay University, CEA, List) | Mohamed Tamaazousti (CEA/LIST)",,,,,,,
VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning,"The ideal form of Visual Question Answering requires understanding, grounding and reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most existing VQA benchmarks are limited to just picking the answer from a pre-defined set of options and lack attention to text. We present a new challenge with a dataset that contains 23,781 questions based on 10124 image-text pairs. Specifically, the task requires the model to align multimedia representations of the same entity to implement multi-hop reasoning between image and text and finally use natural language to answer the question. The aim of this challenge is to develop and benchmark models that are capable of multimedia entity alignment, multi-step reasoning and open-ended answer generation.",http://arxiv.org/abs/2303.02635v1,,Kang Chenkang (Huaihai Institute Of Technology) | Xiangqian Wu (Harbin Institute Of Technology),2023-03-05 10:32:26+00:00,,,,,,
"Spurious Generalization Indicators - A Sanity Check on Shape Bias, Spectral Bias, and the Critical Band",,,,"Paul Gavrikov (Offenburg University) | Janis Keuper (Institute For Machine Learning And Analytics, Offenburg University)",,,,,,,
Online Task-Free Continual Generative and Discriminative Learning via Dynamic Cluster Memory,,,,??? ??? (University Of York) | Adrian Bors (University Of York),,,,,,,
MESA: Matching Everything by Segmenting Anything,"Feature matching is a crucial task in the field of computer vision, which involves finding correspondences between images. Previous studies achieve remarkable performance using learning-based feature comparison. However, the pervasive presence of matching redundancy between images gives rise to unnecessary and error-prone computations in these methods, imposing limitations on their accuracy. To address this issue, we propose MESA, a novel approach to establish precise area (or region) matches for efficient matching redundancy reduction. MESA first leverages the advanced image understanding capability of SAM, a state-of-the-art foundation model for image segmentation, to obtain image areas with implicit semantic. Then, a multi-relational graph is proposed to model the spatial structure of these areas and construct their scale hierarchy. Based on graphical models derived from the graph, the area matching is reformulated as an energy minimization task and effectively resolved. Extensive experiments demonstrate that MESA yields substantial precision improvement for multiple point matchers in indoor and outdoor downstream tasks, e.g. +13.61% for DKM in indoor pose estimation.",http://arxiv.org/abs/2401.16741v1,,Yesheng Zhang (Shanghai Jiao Tong University) | Xu Zhao (Shanghai Jiao Tong University),2024-01-30 04:39:32+00:00,,,,,,
Video-Based Human Pose Regression via Decoupled Space-Time Aggregation,,,,Jijie He (Zhejiang Gongshang University) | Wenwu Yang (Zhejiang Gongshang University),,,,,,,
One-Prompt to Segment All Medical Images,,,,Wu (None) | Min Xu (Carnegie Mellon University),,,,,,,
Fair-VPT: Fair Visual Prompt Tuning for Image Classification,,,,Sungho Park (Yonsei University) | Hyeran Byun (Yonsei University),,,,,,,
CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning,"Pseudo-label-based semi-supervised learning (SSL) algorithms trained on a class-imbalanced set face two cascading challenges: 1) Classifiers tend to be biased towards majority classes, and 2) Biased pseudo-labels are used for training. It is difficult to appropriately re-balance the classifiers in SSL because the class distribution of an unlabeled set is often unknown and could be mismatched with that of a labeled set. We propose a novel class-imbalanced SSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD). For each iteration of training, CDMAD first assesses the classifier's biased degree towards each class by calculating the logits on an image without any patterns (e.g., solid color image), which can be considered irrelevant to the training set. CDMAD then refines biased pseudo-labels of the base SSL algorithm by ensuring the classifier's neutrality. CDMAD uses these refined pseudo-labels during the training of the base SSL algorithm to improve the quality of the representations. In the test phase, CDMAD similarly refines biased class predictions on test samples. CDMAD can be seen as an extension of post-hoc logit adjustment to address a challenge of incorporating the unknown class distribution of the unlabeled set for re-balancing the biased classifier under class distribution mismatch. CDMAD ensures Fisher consistency for the balanced error. Extensive experiments verify the effectiveness of CDMAD.",http://arxiv.org/abs/2403.10391v1,,Hyuck Lee (Korea Advanced Institute Of Science And Technology) | Heeyoung Kim (Korea Advanced Institute Of Science And Technology),2024-03-15 15:22:13+00:00,,,,,,
Variance-guided and Parameter-Efficient Feature Space Adaptation for Cross-domain Few-Shot Learning,,,,Rashindrie Perera (University Of Melbourne) | Saman Halgamuge (University Of Melbourne),,,,,,,
Self-Supervised Facial Representation Learning with Facial Region Awareness,"Self-supervised pre-training has been proved to be effective in learning transferable representations that benefit various visual tasks. This paper asks this question: can self-supervised pre-training learn general facial representations for various facial analysis tasks? Recent efforts toward this goal are limited to treating each face image as a whole, i.e., learning consistent facial representations at the image-level, which overlooks the consistency of local facial representations (i.e., facial regions like eyes, nose, etc). In this work, we make a first attempt to propose a novel self-supervised facial representation learning framework to learn consistent global and local facial representations, Facial Region Awareness (FRA). Specifically, we explicitly enforce the consistency of facial regions by matching the local facial representations across views, which are extracted with learned heatmaps highlighting the facial regions. Inspired by the mask prediction in supervised semantic segmentation, we obtain the heatmaps via cosine similarity between the per-pixel projection of feature maps and facial mask embeddings computed from learnable positional embeddings, which leverage the attention mechanism to globally look up the facial image for facial regions. To learn such heatmaps, we formulate the learning of facial mask embeddings as a deep clustering problem by assigning the pixel features from the feature maps to them. The transfer learning results on facial classification and regression tasks show that our FRA outperforms previous pre-trained models and more importantly, using ResNet as the unified backbone for various tasks, our FRA achieves comparable or even better performance compared with SOTA methods in facial analysis tasks.",http://arxiv.org/abs/2403.02138v1,,"Zheng Gao (Queen Mary, University Of London) | Ioannis Patras (Queen Mary University Of London)",2024-03-04 15:48:56+00:00,,,,,,
Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting,"We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder summarizes joint heatmaps into effective feature embedding using self-attention. Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9\% reduction of error in an MPJPE metric. Our source code is available in GitHub.",http://arxiv.org/abs/2402.18330v1,,Taeho Kang (Seoul National University) | Youngki Lee (Seoul National University),2024-02-28 13:50:39+00:00,,,,,,
Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation,"Recently, diffusion-based methods, like InstructPix2Pix (IP2P), have achieved effective instruction-based image editing, requiring only natural language instructions from the user. However, these methods often inadvertently alter unintended areas and struggle with multi-instruction editing, resulting in compromised outcomes. To address these issues, we introduce the Focus on Your Instruction (FoI), a method designed to ensure precise and harmonious editing across multiple instructions without extra training or test-time optimization. In the FoI, we primarily emphasize two aspects: (1) precisely extracting regions of interest for each instruction and (2) guiding the denoising process to concentrate within these regions of interest. For the first objective, we identify the implicit grounding capability of IP2P from the cross-attention between instruction and image, then develop an effective mask extraction method. For the second objective, we introduce a cross attention modulation module for rough isolation of target editing regions and unrelated regions. Additionally, we introduce a mask-guided disentangle sampling strategy to further ensure clear region isolation. Experimental results demonstrate that FoI surpasses existing methods in both quantitative and qualitative evaluations, especially excelling in multi-instruction editing task.",http://arxiv.org/abs/2312.10113v1,,Guo (None) | Tianwei Lin (Horizon Robotics),2023-12-15 09:10:35+00:00,,,,,,
Efficient Scene Recovery Using Luminous Flux Prior,,,,ZhongYu Li (University Of Science And Technology Of China) | Lei Zhang (University Of Science And Technology Of China),,,,,,,
Anatomically Constrained Implicit Face Models,"Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor specific anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-fidelity, and can readily be used as a drop in replacement to conventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape fitting, shape editing, and performance retargeting.",http://arxiv.org/abs/2312.07538v1,,"Prashanth Chandran (None) | Gaspard Zoss (Disney Research, Disney)",2023-12-12 18:59:21+00:00,,,,,,
Masked and Shuffled Blind Spot Denoising for Real-World Images,,,,"Hamadi Chihaoui (University Of Bern) | Paolo Favaro (Institute F??r Informatik, University Of Bern)",,,,,,,
SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design,"Recently, efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally, they use 4x4 patch embeddings and a 4-stage structure at the macro level, while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also achieves competitive performance by leveraging token representations with reduced spatial redundancy from the early stages. Furthermore, our preliminary analyses suggest that attention layers in the early stages can be substituted with convolutions, and several attention heads in the latter stages are computationally redundant. To handle this, we introduce a single-head attention module that inherently prevents head redundancy and simultaneously boosts accuracy by parallelly combining global and local information. Building upon our solutions, we introduce SHViT, a Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy tradeoff. For example, on ImageNet-1k, our SHViT-S4 is 3.3x, 8.1x, and 2.4x faster than MobileViTv2 x1.0 on GPU, CPU, and iPhone12 mobile device, respectively, while being 1.3% more accurate. For object detection and instance segmentation on MS COCO using Mask-RCNN head, our model achieves performance comparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone latency on GPU and mobile device, respectively.",http://arxiv.org/abs/2401.16456v1,,Seokju Yun (University Of Seoul) | Youngmin Ro (University Of Seoul),2024-01-29 09:12:23+00:00,,,,,,
Activity-Biometrics: Person Identification from Daily Activities,,,,Shehreen Azad (University Of Central Florida) | Yogesh S. Rawat (University Of Central Florida),,,,,,,
Make Adapters Great Again,,,,Jan-Martin Steitz (None) | Stefan Roth (None),,,,,,,
Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models,"Diffusion models have achieved remarkable results in generating high-quality, diverse, and creative images. However, when it comes to text-based image generation, they often fail to capture the intended meaning presented in the text. For instance, a specified object may not be generated, an unnecessary object may be generated, and an adjective may alter objects it was not intended to modify. Moreover, we found that relationships indicating possession between objects are often overlooked. While users' intentions in text are diverse, existing methods tend to specialize in only some aspects of these. In this paper, we propose Predicated Diffusion, a unified framework to express users' intentions. We consider that the root of the above issues lies in the text encoder, which often focuses only on individual words and neglects the logical relationships between them. The proposed method does not solely rely on the text encoder, but instead, represents the intended meaning in the text as propositions using predicate logic and treats the pixels in the attention maps as the fuzzy predicates. This enables us to obtain a differentiable loss function that makes the image fulfill the proposition by minimizing it. When compared to several existing methods, we demonstrated that Predicated Diffusion can generate images that are more faithful to various text prompts, as verified by human evaluators and pretrained image-text models.",http://arxiv.org/abs/2311.16117v1,,Kota Sueyoshi (Osaka University) | Takashi Matsubara (Osaka Universiry),2023-10-03 15:45:50+00:00,,,,,,
Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation,,,,Jingyun Wang (None) | Guoliang Kang (Beihang University),,,,,,,
Image Restoration by Denoising Diffusion Models With Iteratively Preconditioned Guidance,"Training deep neural networks has become a common approach for addressing image restoration problems. An alternative for training a ""task-specific"" network for each observation model is to use pretrained deep denoisers for imposing only the signal's prior within iterative algorithms, without additional training. Recently, a sampling-based variant of this approach has become popular with the rise of diffusion/score-based generative models. Using denoisers for general purpose restoration requires guiding the iterations to ensure agreement of the signal with the observations. In low-noise settings, guidance that is based on back-projection (BP) has been shown to be a promising strategy (used recently also under the names ""pseudoinverse"" or ""range/null-space"" guidance). However, the presence of noise in the observations hinders the gains from this approach. In this paper, we propose a novel guidance technique, based on preconditioning that allows traversing from BP-based guidance to least squares based guidance along the restoration scheme. The proposed approach is robust to noise while still having much simpler implementation than alternative methods (e.g., it does not require SVD or a large number of iterations). We use it within both an optimization scheme and a sampling-based scheme, and demonstrate its advantages over existing methods for image deblurring and super-resolution.",http://arxiv.org/abs/2312.16519v1,,Tomer Garber (Open University Of Israel) | Tom Tirer (Bar-Ilan University),2023-12-27 10:57:03+00:00,,,,,,
Latent Modulated Function for Computational Optimal Continuous Image Representation,,,,Zongyao He (Sun Yat-Sen University) | Zhi Jin (Sun Yat-Sen University),,,,,,,
Towards Understanding and Improving Adversarial Robustness of Vision Transformers,,,,Samyak Jain (None) | Tanima Dutta (IIT BHU),,,,,,,
Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling,"Humans possess a remarkable ability to integrate auditory and visual information, enabling a deeper understanding of the surrounding environment. This early fusion of audio and visual cues, demonstrated through cognitive psychology and neuroscience research, offers promising potential for developing multimodal perception models. However, training early fusion architectures poses significant challenges, as the increased model expressivity requires robust learning frameworks to harness their enhanced capabilities. In this paper, we address this challenge by leveraging the masked reconstruction framework, previously successful in unimodal settings, to train audio-visual encoders with early fusion. Additionally, we propose an attention-based fusion module that captures interactions between local audio and visual representations, enhancing the model's ability to capture fine-grained interactions. While effective, this procedure can become computationally intractable, as the number of local representations increases. Thus, to address the computational complexity, we propose an alternative procedure that factorizes the local representations before representing audio-visual interactions. Extensive evaluations on a variety of datasets demonstrate the superiority of our approach in audio-event classification, visual sound localization, sound separation, and audio-visual segmentation. These contributions enable the efficient training of deeply integrated audio-visual models and significantly advance the usefulness of early fusion architectures.",http://arxiv.org/abs/2312.01017v1,,"Shentong Mo (CMU, Carnegie Mellon University) | Pedro Morgado (None)",2023-12-02 03:38:49+00:00,,,,,,
Single Domain Generalization for Crowd Counting,,,,Zhuoxuan Peng (The Hong Kong University Of Science And Technology) | S.-H. Gary Chan (The Hong Kong University Of Science And Technology),,,,,,,
Specularity Factorization for Low Light Enhancement,,,,Saurabh Saini (International Institute Of Information Technology Hyderabad) | P. J. Narayanan (IIIT Hyderabad),,,,,,,
Just Add ? ! Pose Induced Video Transformers for Understanding Activities of Daily Living ,,,,Dominick Reilly (UNC Charlotte) | Srijan Das (University Of North Carolina At Charlotte),,,,,,,
FADES: Fair Disentanglement with Sensitive Relevance,,,,Taeuk Jang (Purdue University) | Xiaoqian Wang (Purdue University),,,,,,,
Global Latent Neural Rendering,"A recent trend among generalizable novel view synthesis methods is to learn a rendering operator acting over single camera rays. This approach is promising because it removes the need for explicit volumetric rendering, but it effectively treats target images as collections of independent pixels. Here, we propose to learn a global rendering operator acting over all camera rays jointly. We show that the right representation to enable such rendering is a 5-dimensional plane sweep volume consisting of the projection of the input images on a set of planes facing the target camera. Based on this understanding, we introduce our Convolutional Global Latent Renderer (ConvGLR), an efficient convolutional architecture that performs the rendering operation globally in a low-resolution latent space. Experiments on various datasets under sparse and generalizable setups show that our approach consistently outperforms existing methods by significant margins.",http://arxiv.org/abs/2312.08338v2,,Thomas Tanay (Huawei Technologies Ltd.) | Matteo Maggioni (Huawei Technologies Ltd.),2023-12-13 18:14:13+00:00,,,,,,
Language-conditioned Detection Transformer,,,,"Jang Hyun Cho (University Of Texas, Austin) | Philipp Kr??henb??hl (University Of Texas At Austin)",,,,,,,
A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark,"Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We describe our evaluation protocol in detail and provide specific examples of how the protocol should be used. We utilize the protocol to offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL as the backbone model and demonstrate its efficiency. The results emphasize the importance of consistent data preprocessing practices for reliable and meaningful comparisons. We make our source code public at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.",http://arxiv.org/abs/2307.04570v2,,"Jakub Paplham (Czech Technical University In Prague) | Vojtech Franc (Czech Technical University In Prague, Faculty Of Electrical Engineering)",2023-07-10 14:02:31+00:00,,,,,,
Density-Guided Semi-Supervised 3D Semantic Segmentation with Dual-Space Hardness Sampling,,,,Jianan Li (None) | Qiulei Dong (None),,,,,,,
Interference-Free Low-Rank Adaptation for Continual Learning,,,,Yan-Shuo Liang (Nanjing University) | Wu-Jun Li (Nanjing University),,,,,,,
CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective,"Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers. Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance. The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task. WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs. Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing. The code is available at https://github.com/snskysk/CAM-Back-Again.",http://arxiv.org/abs/2403.06676v1,,Shunsuke Yasuki (Rikkyo University) | Masato Taki (Rikkyo University),2024-03-11 12:48:22+00:00,,,,,,
Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices,,,,"Huancheng Chen (University Of Texas At Austin) | Haris Vikalo (University Of Texas, Austin)",,,,,,,
Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer,"Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a new target domain by actively selecting a limited number of target data to annotate.This setting neglects the more practical scenario where training data are collected from multiple sources. This motivates us to target a new and challenging setting of knowledge transfer that extends ADA from a single source domain to multiple source domains, termed Multi-source Active Domain Adaptation (MADA). Not surprisingly, we find that most traditional ADA methods cannot work directly in such a setting, mainly due to the excessive domain gap introduced by all the source domains and thus their uncertainty-aware sample selection can easily become miscalibrated under the multi-domain shifts. Considering this, we propose a Dynamic integrated uncertainty valuation framework(Detective) that comprehensively consider the domain shift between multi-source domains and target domain to detect the informative target samples. Specifically, the leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the model's parameters to fit the union of multi-source domains. This enables an approximate single-source domain modeling by the dynamic model. We then comprehensively measure both domain uncertainty and predictive uncertainty in the target domain to detect informative target samples using evidential deep learning, thereby mitigating uncertainty miscalibration. Furthermore, we introduce a contextual diversity-aware calculator to enhance the diversity of the selected samples. Experiments demonstrate that our solution outperforms existing methods by a considerable margin on three domain adaptation benchmarks.",http://arxiv.org/abs/2311.12905v1,,Wenqiao Zhang (National University Of Singapore) | Zheqi Lv (Zhejiang University),2023-11-21 13:12:21+00:00,,,,,,
Fun with Flags: Robust Principal Directions via Flag Manifolds,"Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, creating novel robust and dual geodesic PCA variations. The remarkable flexibility offered by the 'flagification' introduced here enables even more algorithmic variants identified by specific flag types. Last but not least, we propose an effective convergent solver for these flag-formulations employing the Stiefel manifold. Our empirical results on both real-world and synthetic scenarios, demonstrate the superiority of our novel algorithms, especially in terms of robustness to outliers on manifolds.",http://arxiv.org/abs/2401.04071v1,,Tolga Birdal (Imperial College London) | Nathan Mankovich (None),2024-01-08 18:18:02+00:00,,,,,,
Instance-based Max-margin for Practical Few-shot Recognition,,,,Minghao Fu (None) | Ke Zhu (Nanjing University),,,,,,,
SuperNormal: Neural Surface Reconstruction via Multi-View Normal Integration,"We present SuperNormal, a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. With a few minutes, SuperNormal produces detailed surfaces on par with 3D scanners. We harness volume rendering to optimize a neural signed distance function (SDF) powered by multi-resolution hash encoding. To accelerate training, we propose directional finite difference and patch-based ray marching to approximate the SDF gradients numerically. While not compromising reconstruction quality, this strategy is nearly twice as efficient as analytical gradients and about three times faster than axis-aligned finite difference. Experiments on the benchmark dataset demonstrate the superiority of SuperNormal in efficiency and accuracy compared to existing multi-view photometric stereo methods. On our captured objects, SuperNormal produces more fine-grained geometry than recent neural 3D reconstruction methods.",http://arxiv.org/abs/2312.04803v1,,Xu Cao (Osaka University) | Takafumi Taketomi (CyberAgent),2023-12-08 02:34:30+00:00,,,,,,
Fixed Point Diffusion Models,"We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an implicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efficiency. Compared to the state-of-the-art DiT model, FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation quality in situations where sampling computation or time is limited. Our code and pretrained models are available at https://lukemelas.github.io/fixed-point-diffusion-models.",http://arxiv.org/abs/2401.08741v1,,"Luke Melas-Kyriazi (VGG, University Of Oxford) | Xingjian Bai (University Of Oxford)",2024-01-16 18:55:54+00:00,,,,,,
CHAIN: Enhancing Generalization in Data-Efficient GAN Training through lipsCHitz continuity constrAIned Normalization,,,,Yao Ni (Australian National University) | Piotr Koniusz (Australian National University),,,,,,,
ICP-Flow: LiDAR Scene Flow Estimation with ICP,"Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including supervised models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, supervised by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.",http://arxiv.org/abs/2402.17351v1,,Yancong Lin (Delft University Of Technology) | Zimin Xia (Motional),2024-02-27 09:41:59+00:00,,,,,,
Unsupervised Occupancy Learning from Sparse Point Cloud,,,,Amine Ouasfi (INRIA) | Adnane Boukhayma (INRIA),,,,,,,
SRTube: Video-Language Pre-Training with Action-Centric Video Tube Features and Semantic Role Labeling,,,,Juhee Lee (None) | Jewon Kang (Ewha Womans Univrsity),,,,,,,
Defense without Forgetting: Continual Adversarial Defense with Anisotropic & Isotropic Pseudo Replay,,,,Yuhang Zhou (Harbin Institute Of Technology) | Zhongyun Hua (Harbin Institute Of Technology Shenzhen),,,,,,,
RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation,,,,Oded Bialer (General Motors) | Yuval Haitman (Ben-Gurion University Of The Negev),,,,,,,
Adaptive Hyper-graph Aggregation for Modality-Agnostic Federated Learning,,,,Fan Qi (Tianjin University Of Technology) | Shuai Li (Tianjin University Of Technology),,,,,,,
Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment,,,,Alvi Md Ishmam (Virginia Polytechnic Institute And State University) | Chris Thomas (Virginia Polytechnic Institute And State University),,,,,,,
SAOR: Single-View Articulated Object Reconstruction,"We introduce SAOR, a novel approach for estimating the 3D shape, texture, and viewpoint of an articulated object from a single image captured in the wild. Unlike prior approaches that rely on pre-defined category-specific 3D templates or tailored 3D skeletons, SAOR learns to articulate shapes from single-view image collections with a skeleton-free part-based model without requiring any 3D object shape priors. To prevent ill-posed solutions, we propose a cross-instance consistency loss that exploits disentangled object shape deformation and articulation. This is helped by a new silhouette-based sampling mechanism to enhance viewpoint diversity during training. Our method only requires estimated object silhouettes and relative depth maps from off-the-shelf pre-trained networks during training. At inference time, given a single-view image, it efficiently outputs an explicit mesh representation. We obtain improved qualitative and quantitative results on challenging quadruped animals compared to relevant existing work.",http://arxiv.org/abs/2303.13514v2,,Mehmet Aygun (University Of Edinburgh) | Oisin Mac Aodha (University Of Edinburgh),2023-03-23 17:59:35+00:00,,,,,,
Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval,"In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class distances and maximize the inter-class distances in the dual representation spaces. We demonstrate the power of our approach with three challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With our dual approach, for single-view object recognition, we outperform the previous best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On the other hand, for single-view object retrieval, we outperform the previous best by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.",http://arxiv.org/abs/2403.00272v1,,Rohan Sarkar (Purdue University) | Avinash Kak (Purdue University),2024-03-01 04:20:13+00:00,,,,,,
A2XP: Towards Private Domain Generalization,"Deep Neural Networks (DNNs) have become pivotal in various fields, especially in computer vision, outperforming previous methodologies. A critical challenge in their deployment is the bias inherent in data across different domains, such as image style, and environmental conditions, leading to domain gaps. This necessitates techniques for learning general representations from biased training data, known as domain generalization. This paper presents Attend to eXpert Prompts (A2XP), a novel approach for domain generalization that preserves the privacy and integrity of the network architecture. A2XP consists of two phases: Expert Adaptation and Domain Generalization. In the first phase, prompts for each source domain are optimized to guide the model towards the optimal direction. In the second phase, two embedder networks are trained to effectively amalgamate these expert prompts, aiming for an optimal output. Our extensive experiments demonstrate that A2XP achieves state-of-the-art results over existing non-private domain generalization methods. The experimental results validate that the proposed approach not only tackles the domain generalization challenge in DNNs but also offers a privacy-preserving, efficient solution to the broader field of computer vision.",http://arxiv.org/abs/2311.10339v1,,Geunhyeok Yu (Kyung Hee University) | Hyoseok Hwang (Kyung Hee University),2023-11-17 05:49:50+00:00,,,,,,
DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction,"We propose a novel concept of dual and integrated latent topologies (DITTO in short) for implicit 3D reconstruction from noisy and sparse point clouds. Most existing methods predominantly focus on single latent type, such as point or grid latents. In contrast, the proposed DITTO leverages both point and grid latents (i.e., dual latent) to enhance their strengths, the stability of grid latents and the detail-rich capability of point latents. Concretely, DITTO consists of dual latent encoder and integrated implicit decoder. In the dual latent encoder, a dual latent layer, which is the key module block composing the encoder, refines both latents in parallel, maintaining their distinct shapes and enabling recursive interaction. Notably, a newly proposed dynamic sparse point transformer within the dual latent layer effectively refines point latents. Then, the integrated implicit decoder systematically combines these refined latents, achieving high-fidelity 3D reconstruction and surpassing previous state-of-the-art methods on object- and scene-level datasets, especially in thin and detailed structures.",http://arxiv.org/abs/2403.05005v1,,Jaehyeok Shim (UNIST) | Kyungdon Joo (None),2024-03-08 03:03:41+00:00,,,,,,
Gradient Alignment for Cross-domain Face Anti-Spoofing,,,,Binh M. Le (Sungkyunkwan University) | Simon Woo (Sungkyunkwan University),,,,,,,
Federated Online Adaptation for Deep Stereo,,,,Matteo Poggi (Universit?? Di Bologna) | Fabio Tosi (University Of Bologna),,,,,,,
Step differences in instructional video,,,,Tushar Nagarajan (Meta) | Lorenzo Torresani (Facebook),,,,,,,
LaneCPP: Continuous 3D Lane Detection using Physical Priors,,,,"Maximilian Pittner (Bosch) | Joel Janai (Robert Bosch GmbH, Bosch) | Alexandru Paul Condurache (None)",,,,,,,
Efficient Privacy-Preserving Visual Localization Using 3D Ray Clouds,,,,Heejoon Moon (HANYANG University) | Chunghwan Lee (Hanyang University) | Je Hyeong Hong (Hanyang University),,,,,,,
Towards Accurate and Robust Architectures via Neural Architecture Search,,,,Yuwei Ou (Sichuan University) | Yuqi Feng (Sichuan University) | Yanan Sun (Sichuan University),,,,,,,
WaveFace: Authentic Face Restoration with Efficient Frequency Recovery,,,,Yunqi Miao (The University Of Warwick) | Jiankang Deng (Imperial College London & Huawei UKRD) | Jungong Han (Aberystwyth University),,,,,,,
Transfer CLIP for Generalizable Image Denoising,,,,Jun Cheng (Huazhong University Of Science And Technology) | Dong Liang (Huazhong University Of Science And Technology) | Shan Tan (Huazhong University Of Science And Technology),,,,,,,
CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition,"To achieve greater accuracy, hypergraph matching algorithms require exponential increases in computational resources. Recent kd-tree-based approximate nearest neighbor (ANN) methods, despite the sparsity of their compatibility tensor, still require exhaustive calculations for large-scale graph matching. This work utilizes CUR tensor decomposition and introduces a novel cascaded second and third-order hypergraph matching framework (CURSOR) for efficient hypergraph matching. A CUR-based second-order graph matching algorithm is used to provide a rough match, and then the core of CURSOR, a fiber-CUR-based tensor generation method, directly calculates entries of the compatibility tensor by leveraging the initial second-order match result. This significantly decreases the time complexity and tensor density. A probability relaxation labeling (PRL)-based matching algorithm, especially suitable for sparse tensors, is developed. Experiment results on large-scale synthetic datasets and widely-adopted benchmark sets demonstrate the superiority of CURSOR over existing methods. The tensor generation method in CURSOR can be integrated seamlessly into existing hypergraph matching methods to improve their performance and lower their computational costs.",http://arxiv.org/abs/2402.16594v3,,Qixuan Zheng (City University Of Hong Kong) | Ming Zhang (Hong Kong Applied Science And Technology Research Institute (ASTRI)) | Hong Yan (City University Of Hong Kong),2024-02-26 14:18:12+00:00,,,,,,
NeLF-Pro: Neural Light Field Probes,"We present NeLF-Pro, a novel representation for modeling and reconstructing light fields in diverse natural scenes that vary in extend and spatial granularity. In contrast to previous fast reconstruction methods that represent the 3D scene globally, we model the light field of a scene as a set of local light field feature probes, parameterized with position and multi-channel 2D feature maps. Our central idea is to bake the scene's light field into spatially varying learnable representations and to query point features by weighted blending of probes close to the camera - allowing for mipmap representation and rendering. We introduce a novel vector-matrix-matrix (VMM) factorization technique that effectively represents the light field feature probes as products of core factors (i.e., VM) shared among local feature probes, and a basis factor (i.e., M) - efficiently encoding internal relationships and patterns within the scene.Experimentally, we demonstrate that NeLF-Pro significantly boosts the performance of feature grid-based representations, and achieves fast reconstruction with better rendering quality while maintaining compact modeling.",http://arxiv.org/abs/2312.13328v1,,"Zinuo You (ETH Zurich) | Andreas Geiger (University Of T??bingen) | Anpei Chen (Department Of Computer Science, ETHZ - ETH Zurich)",2023-12-20 17:18:44+00:00,,,,,,
Simple but Effective Text-to-Video Generation with Grid Diffusion Models,,,,Taegyeong Lee (Ulsan National Institute Of Science And Technology) | Soyeong Kwon (Ulsan National Institute Of Science And Technology) | Taehwan Kim (UNIST),,,,,,,
Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow,"Single RGB or LiDAR is the mainstream sensor for the challenging scene flow, which relies heavily on visual features to match motion features. Compared with single modality, existing methods adopt a fusion strategy to directly fuse the cross-modal complementary knowledge in motion space. However, these direct fusion methods may suffer the modality gap due to the visual intrinsic heterogeneous nature between RGB and LiDAR, thus deteriorating motion features. We discover that event has the homogeneous nature with RGB and LiDAR in both visual and motion spaces. In this work, we bring the event as a bridge between RGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework for scene flow, which explores a homogeneous space to fuse the cross-modal complementary knowledge for physical interpretation. In visual fusion, we discover that event has a complementarity (relative v.s. absolute) in luminance space with RGB for high dynamic imaging, and has a complementarity (local boundary v.s. global shape) in scene structure space with LiDAR for structure integrity. In motion fusion, we figure out that RGB, event and LiDAR are complementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to each other in correlation space, which motivates us to fuse their motion correlations for motion continuity. The proposed hierarchical fusion can explicitly fuse the multimodal knowledge to progressively improve scene flow from visual space to motion space. Extensive experiments have been performed to verify the superiority of the proposed method.",http://arxiv.org/abs/2403.07432v1,,Hanyu Zhou (Huazhong University Of Science And Technology) | Yi Chang (Huazhong University Of Science And Technology) | Zhiwei Shi (Huazhong University Of Science And Technology),2024-03-12 09:15:19+00:00,,,,,,
Single Mesh Diffusion Models with Field Latents for Texture Generation,"We introduce a framework for intrinsic latent diffusion models operating directly on the surfaces of 3D shapes, with the goal of synthesizing high-quality textures. Our approach is underpinned by two contributions: field latents, a latent representation encoding textures as discrete vector fields on the mesh vertices, and field latent diffusion models, which learn to denoise a diffusion process in the learned latent space on the surface. We consider a single-textured-mesh paradigm, where our models are trained to generate variations of a given texture on a mesh. We show the synthesized textures are of superior fidelity compared those from existing single-textured-mesh generative models. Our models can also be adapted for user-controlled editing tasks such as inpainting and label-guided generation. The efficacy of our approach is due in part to the equivariance of our proposed framework under isometries, allowing our models to seamlessly reproduce details across locally similar regions and opening the door to a notion of generative texture transfer.",http://arxiv.org/abs/2312.09250v1,,Thomas W. Mitchel (PlayStation) | Carlos Esteves (Google Research) | Ameesh Makadia (Google Research),2023-12-14 18:59:36+00:00,,,,,,
MaskPLAN: Masked Generative Layout Planning from Partial Input,,,,Hang Zhang (ETHZ - ETH Zurich) | Anton Savov (ETHZ - ETH Zurich) | Benjamin Dillenburger (ETHZ - ETH Zurich),,,,,,,
Label Propagation for Zero-shot Classification with Vision-Language Models,,,,Vladan Stojni?? (Czech Technical University In Prague) | Yannis Kalantidis (NAVER LABS Europe) | Giorgos Tolias (CTU In Prague),,,,,,,
Consistent Prompting for Rehearsal-Free Continual Learning,"Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning. In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks. Detailed analysis shows that improvements come from more consistent training and testing.",http://arxiv.org/abs/2403.08568v2,,Zhanxin Gao (Sun Yat-Sen University) | Jun Cen (None) | Xiaobin Chang (SUN YAT-SEN UNIVERSITY),2024-03-13 14:24:09+00:00,,,,,,
In-distribution Public Data Synthesis with Diffusion Models for Differentially Private Image Classification,,,,Jinseong Park (Seoul National University) | Yujin Choi (Seoul National University) | Jaewook Lee (Seoul National University),,,,,,,
Omni-Q: Omni-Directional Scene Understanding for Unsupervised Visual Grounding,,,,Sai Wang (Wuhan University) | Yutian Lin (Wuhan University) | Yu Wu (Wuhan University),,,,,,,
Hierarchical Histogram Threshold Segmentation ?? Auto-terminating High-detail Oversegmentation,,,,Thomas Chang (Nuremberg Institute Of Technology) | Simon Seibt (Georg-Simon-Ohm-Fachhochschule N??rnberg) | Bartosz Von Rymon Lipinski (Technical University OAS Nuremberg),,,,,,,
HDQMF: Holographic Feature Decomposition Using Quantum Algorithms,,,,"Prathyush Poduval (University Of California, Irvine) | Zhuowen Zou (University Of California, Irvine) | Mohsen Imani (University Of California, Irvine)",,,,,,,
SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation,,,,Keqi Chen (Universit?? De Strasbourg) | Vinkle Srivastav (University Of Strasbourg) | Nicolas Padoy (University Of Strasbourg),,,,,,,
LAN: Learning to Adapt Noise for Image Denoising,,,,Changjin Kim (Hanyang University) | Tae Hyun Kim (Hanyang Univ.) | Sungyong Baik (Hanyang University),,,,,,,
Object Pose Estimation via the Aggregation of Diffusion Features,,,,Tianfu Wang (University Of Chinese Academy Of Sciences) | Guosheng Hu (Oosto) | Hongguang Wang (Shenyang Institute Of Automation),,,,,,,
VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models,,,,Xiang Li (National University Of Singapore) | Qianli Shen (National University Of Singapore) | Kenji Kawaguchi (National University Of Singapore),,,,,,,
Intraoperative 2D/3D Image Registration via Differentiable X-ray Rendering,"Surgical decisions are informed by aligning rapid portable 2D intraoperative images (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g., CT). 2D/3D image registration often fails in practice: conventional optimization methods are prohibitively slow and susceptible to local minima, while neural networks trained on small datasets fail on new patients or require impractical landmark supervision. We present DiffPose, a self-supervised approach that leverages patient-specific simulation and differentiable physics-based rendering to achieve accurate 2D/3D registration without relying on manually labeled data. Preoperatively, a CNN is trained to regress the pose of a randomly oriented synthetic X-ray rendered from the preoperative CT. The CNN then initializes rapid intraoperative test-time optimization that uses the differentiable X-ray renderer to refine the solution. Our work further proposes several geometrically principled methods for sampling camera poses from $\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving registration in the tangent space $\mathfrak{se}(3)$ with geodesic and multiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy across surgical datasets at intraoperative speeds, improving upon existing unsupervised methods by an order of magnitude and even outperforming supervised baselines. Our code is available at https://github.com/eigenvivek/DiffPose.",http://arxiv.org/abs/2312.06358v1,,Vivek Gopalakrishnan (MIT) | Neel Dey (Massachusetts Institute Of Technology) | Polina Golland (Massachusetts Institute Of Technology),2023-12-11 13:05:54+00:00,,,,,,
Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors,"Multi-camera-based 3D object detection has made notable progress in the past several years. However, we observe that there are cases (e.g. faraway regions) in which popular 2D object detectors are more reliable than state-of-the-art 3D detectors. In this paper, to improve the performance of query-based 3D object detectors, we present a novel query generating approach termed QAF2D, which infers 3D query anchors from 2D detection results. A 2D bounding box of an object in an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth, yaw angle, and size candidates. Then, the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box, and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of prompt parameters. We integrate QAF2D into three popular query-based 3D object detectors and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is $2.3\%$ NDS and $2.7\%$ mAP. Code is available at https://github.com/nullmax-vision/QAF2D.",http://arxiv.org/abs/2403.06093v1,,Haoxuanye Ji (Xi'an Jiao Tong University) | Pengpeng Liang (Zhengzhou University) | Erkang Cheng (Nullmax),2024-03-10 04:38:27+00:00,,,,,,
Estimating Extreme 3D Image Rotations using Cascaded Attention,,,,Shay Dekel (Bar Ilan University) | Yosi Keller (Bar Ilan University) | Martin ??ad??k (Brno University Of Technology),,,,,,,
CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation,,,,Lingjun Zhao (University Of Michigan - Ann Arbor) | Jingyu Song (University Of Michigan - Ann Arbor) | Katherine Skinner (University Of Michigan - Ann Arbor),,,,,,,
Fitting Flats to Flats,,,,Gabriel Dogadov (Technische Universit??t Berlin) | Ugo Finnendahl (Technische Universit??t Berlin) | Marc Alexa (TU Berlin),,,,,,,
QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction,"Inverse problems span across diverse fields. In medical contexts, computed tomography (CT) plays a crucial role in reconstructing a patient's internal structure, presenting challenges due to artifacts caused by inherently ill-posed inverse problems. Previous research advanced image quality via post-processing and deep unrolling algorithms but faces challenges, such as extended convergence times with ultra-sparse data. Despite enhancements, resulting images often show significant artifacts, limiting their effectiveness for real-world diagnostic applications. We aim to explore deep second-order unrolling algorithms for solving imaging inverse problems, emphasizing their faster convergence and lower time complexity compared to common first-order methods like gradient descent. In this paper, we introduce QN-Mixer, an algorithm based on the quasi-Newton approach. We use learned parameters through the BFGS algorithm and introduce Incept-Mixer, an efficient neural architecture that serves as a non-local regularization term, capturing long-range dependencies within images. To address the computational demands typically associated with quasi-Newton algorithms that require full Hessian matrix computations, we present a memory-efficient alternative. Our approach intelligently downsamples gradient information, significantly reducing computational requirements while maintaining performance. The approach is validated through experiments on the sparse-view CT problem, involving various datasets and scanning protocols, and is compared with post-processing and deep unrolling state-of-the-art approaches. Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, all while reducing the number of unrolling iterations required.",http://arxiv.org/abs/2402.17951v2,,"Ishak Ayad (ETIS & AGM, CY Cergy Paris University, ENSEA, CNRS) | Nicolas Larue (ETIS , CY Cergy Paris University, ENSEA, CNRS, University Of Ljubljana) | Mai K. Nguyen (ETIS , CY Cergy Paris University, ENSEA, CNRS)",2024-02-28 00:20:25+00:00,,,,,,
Model Adaptation for Time Constrained Embodied Control,,,,Jaehyun Song (Sungkyunkwan University) | Minjong Yoo (Sungkyunkwan University) | Honguk Woo (Sungkyunkwan University),,,,,,,
Cyclic Learning for Binaural Audio Generation and Localization,,,,Zhaojian Li (Northwest Polytechnical University) | Bin Zhao (Northwest Polytechnical University Xi'an) | Yuan Yuan (Northwest Polytechnical University Xi'an),,,,,,,
Multi-Modal Proxy Learning Towards Personalized Visual Multiple Clustering,,,,Jiawei Yao (University Of Washington) | Qi Qian (Alibaba Group) | Juhua Hu (University Of Washington),,,,,,,
Beyond Average: Individualized Visual Scanpath Prediction,,,,"Xianyu Chen (University Of Minnesota) | Ming Jiang (University Of Minnesota, Minneapolis) | Qi Zhao (University Of Minnesota, Minneapolis)",,,,,,,
Diffusion Models Without Attention,"In recent advancements in high-fidelity image generation, Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a key player. However, their application at high resolutions presents significant computational challenges. Current methods, such as patchifying, expedite processes in UNet and Transformer architectures but at the expense of representational capacity. Addressing this, we introduce the Diffusion State Space Model (DiffuSSM), an architecture that supplants attention mechanisms with a more scalable state space model backbone. This approach effectively handles higher resolutions without resorting to global compression, thus preserving detailed image representation throughout the diffusion process. Our focus on FLOP-efficient architectures in diffusion training marks a significant step forward. Comprehensive evaluations on both ImageNet and LSUN datasets at two resolutions demonstrate that DiffuSSMs are on par or even outperform existing diffusion models with attention modules in FID and Inception Score metrics while significantly reducing total FLOP usage.",http://arxiv.org/abs/2311.18257v1,,Jing Nathan Yan (Cornell University) | Jiatao Gu (Apple (MLR)) | Alexander Rush (Cornell Tech),2023-11-30 05:15:35+00:00,,,,,,
CFAT: Unleashing Triangular Windows for Image Super-resolution,,,,"Abhisek Ray (Indian Institute Of Technology, Patna) | Gaurav Kumar (Indian Institute Of Technology (IIT), Patna) | Maheshkumar Kolekar (Indian Institute Of Technology, Patna)",,,,,,,
MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning,,,,Ahmed Agiza (None) | Marina Neseem (Brown University) | Sherief Reda (Brown University),,,,,,,
Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches,,,,Qing Yu (LY Corporation) | Mikihiro Tanaka (LY Corporation) | Kent Fujiwara (LY Corporation),,,,,,,
IBD-SLAM: Learning Image-Based Depth Fusion for Generalizable SLAM,,,,Minghao Yin (The University Of Hong Kong) | Shangzhe Wu (Stanford University) | Kai Han (The University Of Hong Kong),,,,,,,
DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling,"In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.",http://arxiv.org/abs/2402.08876v1,,Miguel Fainstein (Universidad De Buenos Aires) | Viviana Siless (Universidad Torcuato Di Tella) | Emmanuel Iarussi (Universidad Torcuato Di Tella / Conicet),2024-02-14 00:42:19+00:00,,,,,,
Beyond Text: Frozen Large Language Models in Visual Signal Comprehension,"In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a ``foreign language'' with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rigorous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2L-Tokenizer.",http://arxiv.org/abs/2403.07874v1,,Lei Zhu (Peking University) | Fangyun Wei (None) | Yanye Lu (Peking University),2024-03-12 17:59:51+00:00,,,,,,
Viewpoint-Aware Visual Grounding in 3D Scenes,,,,Xiangxi Shi (Oregon State University) | Zhonghua Wu (SenseTime) | Stefan Lee (Oregon State University),,,,,,,
On the Estimation of Image-matching Uncertainty in Visual Place Recognition,,,,Mubariz Zaffar (Delft University Of Technology) | Liangliang Nan (Delft University Of Technology) | Julian F. P. Kooij (Delft University Of Technology),,,,,,,
RankED: Addressing Imbalance and Uncertainty in Edge Detection Using Ranking-based Losses,"Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement between different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annotators. In this paper, we propose RankED, a unified ranking-based approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high confidence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUD-v2, BSDS500 and Multi-cue datasets. Code is available at https://ranked-cvpr24.github.io.",http://arxiv.org/abs/2403.01795v2,,Bedrettin Cetinkaya (Middle East Technical University) | Sinan Kalkan (Middle East Technical University) | Emre Akbas (METU),2024-03-04 07:35:36+00:00,,,,,,
Parameter Efficient Self-Supervised Geospatial Domain Adaptation,,,,Linus Scheibenreif (University Of St.Gallen) | Michael Mommert (Stuttgart University Of Applied Sciences) | Damian Borth (University Of St.Gallen),,,,,,,
Adaptive Softassign via Hadamard-Equipped Sinkhorn,"Softassign is a crucial step in several popular algorithms for graph matching or other learning targets. Such softassign-based algorithms perform very well for small graph matching tasks. However, the performance of such algorithms is sensitive to a parameter in the softassign in large-scale problems, especially when handling noised data. Turning the parameter is difficult and almost done empirically. This paper constructs an adaptive softassign method by delicately taking advantage of Hadamard operations in Sinkhorn. Compared with the previous state-of-the-art algorithms such as the scalable Gromov-Wasserstein Learning (S-GWL), the resulting algorithm enjoys both a higher accuracy and a significant improvement in efficiency for large graph matching problems. In particular, on the protein network matching benchmark problems (1004 nodes), our algorithm can improve the accuracy from $56.3\%$ by the S-GWL to $75.1\%$, at the same time, it can achieve 3X+ speedup in efficiency.",http://arxiv.org/abs/2309.13855v1,,Binrui Shen (Xi'an Jiaotong-Liverpool University) | Qiang Niu (Xi'an Jiaotong-Liverpool University) | Shengxin Zhu (Beijing Normal Unversity),2023-09-25 03:47:32+00:00,,,,,,
Continual Motion Prediction Learning Framework via Meta-Representation Learning and Optimal Memory Buffer Retention Strategy,,,,Dae Jun Kang (None) | Dongsuk Kum (Korea Advanced Institute Of Science And Technology) | Sanmin Kim (KAIST),,,,,,,
Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models,,,,Peifei Zhu (None) | Tsubasa Takahashi (LY Corporation) | Hirokatsu Kataoka (LY Corporation),,,,,,,
AEROBLAD??E: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error,"With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions.",http://arxiv.org/abs/2401.17879v1,,Jonas Ricker (Ruhr University Bochum) | Denis Lukovnikov (Ruhr University Bochum) | Asja Fischer (Ruhr-Universit??t Bochum),2024-01-31 14:36:49+00:00,,,,,,
"IDGuard: Robust, General, Identity-centric POI Proactive Defense Against Face Editing Abuse",,,,Yunshu Dai (SUN YAT-SEN UNIVERSITY) | Jianwei Fei (Nanjing University Of Information Science And Technology) | Fangjun Huang (SUN YAT-SEN UNIVERSITY),,,,,,,
Contrastive Learning for DeepFake Classification and Localization via Multi-Label Ranking,,,,"Cheng-Yao Hong (Academia Sinica) | Yen-Chi Hsu (Department Of Computer Science And Informational Engineering, National Taiwan University) | Tyng-Luh Liu (IIS/Academia Sinica)",,,,,,,
Pre-training Vision Models with Mandelbulb Variations,,,,Benjamin N. Chiche (Rist) | Yuto Horikawa (Rist) | Ryo Fujita (Kyoto University),,,,,,,
Anomaly Score: Evaluating Generative Models and Individual Generated Images based on Complexity and Vulnerability,"With the advancement of generative models, the assessment of generated images becomes more and more important. Previous methods measure distances between features of reference and generated images from trained vision models. In this paper, we conduct an extensive investigation into the relationship between the representation space and input space around generated images. We first propose two measures related to the presence of unnatural elements within images: complexity, which indicates how non-linear the representation space is, and vulnerability, which is related to how easily the extracted feature changes by adversarial input changes. Based on these, we introduce a new metric to evaluating image-generative models called anomaly score (AS). Moreover, we propose AS-i (anomaly score for individual images) that can effectively evaluate generated images individually. Experimental results demonstrate the validity of the proposed approach.",http://arxiv.org/abs/2312.10634v1,,Jaehui Hwang (Yonsei University) | Junghyuk Lee (Yonsei University) | Jong-Seok Lee (Yonsei University),2023-12-17 07:33:06+00:00,,,,,,
Structured Gradient-based Interpretations via Norm-Regularized Adversarial Training,,,,"Shizhan Gong (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Qi Dou (The Chinese University Of Hong Kong) | Farzan Farnia (The Chinese University Of Hong Kong)",,,,,,,
HumMUSS: Human Motion Understanding using State Space Models,,,,Arnab Mondal (McGill University) | Stefano Alletto (Apple) | Denis Tome (Apple),,,,,,,
Dual-scale Transformer for Large-scale Single-Pixel Imaging,,,,Gang Qu (Westlake University) | Ping Wang (Zhejiang University) | Xin Yuan (Westlake University),,,,,,,
Active Prompt Learning in Vision Language Models,,,,Jihwan Bang (KAIST) | Sumyeong Ahn (Michigan State University) | Jae-Gil Lee (Korea Advanced Institute Of Science And Technology),,,,,,,
Robust Self-calibration of Focal Lengths from the Fundamental Matrix,"The problem of self-calibration of two cameras from a given fundamental matrix is one of the basic problems in geometric computer vision. Under the assumption of known principal points and square pixels, the well-known Bougnoux formula offers a means to compute the two unknown focal lengths. However, in many practical situations, the formula yields inaccurate results due to commonly occurring singularities. Moreover, the estimates are sensitive to noise in the computed fundamental matrix and to the assumed positions of the principal points. In this paper, we therefore propose an efficient and robust iterative method to estimate the focal lengths along with the principal points of the cameras given a fundamental matrix and priors for the estimated camera parameters. In addition, we study a computationally efficient check of models generated within RANSAC that improves the accuracy of the estimated models while reducing the total computational time. Extensive experiments on real and synthetic data show that our iterative method brings significant improvements in terms of the accuracy of the estimated focal lengths over the Bougnoux formula and other state-of-the-art methods, even when relying on inaccurate priors.",http://arxiv.org/abs/2311.16304v1,,Viktor Kocur (Comenius University In Bratislava) | Daniel Kyselica (Comenius University In Bratislava) | Zuzana Kukelova (Czech Technical University In Prague),2023-11-27 20:36:00+00:00,,,,,,
Fusing Personal and Environmental Cues for Identification and Segmentation of First-Person Camera Wearers in Third-Person View,,,,"Ziwei Zhao (Indiana University) | Yuchen Wang (Indiana University) | Chuhua Wang (Indiana University, Bloomington)",,,,,,,
Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts,"Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a ""good"" solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.",http://arxiv.org/abs/2402.19215v1,,Cansu Korkmaz (Koc University) | Ahmet Murat Tekalp (Ko?? University) | Zafer Dogan (Koc University),2024-02-29 14:45:00+00:00,,,,,,
Revamping Federated Learning Security from a Defender's Perspective: A Unified Defense with Homomorphic Encrypted Data Space,,,,"Naveen Kumar Kummari (Indian Institute Of Technology Hyderabad, India) | Reshmi Mitra (Southeast Missouri State University) | Krishna Mohan Chalavadi (Indian Institute Of Technology Hyderabad)",,,,,,,
Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing,"Recovering a clear image from a single hazy image is an open inverse problem. Although significant research progress has been made, most existing methods ignore the effect that downstream tasks play in promoting upstream dehazing. From the perspective of the haze generation mechanism, there is a potential relationship between the depth information of the scene and the hazy image. Based on this, we propose a dual-task collaborative mutual promotion framework to achieve the dehazing of a single image. This framework integrates depth estimation and dehazing by a dual-task interaction mechanism and achieves mutual enhancement of their performance. To realize the joint optimization of the two tasks, an alternative implementation mechanism with the difference perception is developed. On the one hand, the difference perception between the depth maps of the dehazing result and the ideal image is proposed to promote the dehazing network to pay attention to the non-ideal areas of the dehazing. On the other hand, by improving the depth estimation performance in the difficult-to-recover areas of the hazy image, the dehazing network can explicitly use the depth information of the hazy image to assist the clear image recovery. To promote the depth estimation, we propose to use the difference between the dehazed image and the ground truth to guide the depth estimation network to focus on the dehazed unideal areas. It allows dehazing and depth estimation to leverage their strengths in a mutually reinforcing manner. Experimental results show that the proposed method can achieve better performance than that of the state-of-the-art approaches.",http://arxiv.org/abs/2403.01105v1,,Yafei Zhang (Kunmimg University Of Science And Technology) | Shen Zhou (Kunmimg University Of Science And Technology) | Huafeng Li (Kunmimg University Of Science And Technology),2024-03-02 06:29:44+00:00,,,,,,
Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding,"Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text comprehension abilities, facilitating advances in several downstream tasks such as zero-shot image classification, image-text retrieval, and text-to-image generation. However, the compositional reasoning abilities of existing VLMs remains subpar. The root of this limitation lies in the inadequate alignment between the images and captions in the pretraining datasets. Additionally, the current contrastive learning objective fails to focus on fine-grained grounding components like relations, actions, and attributes, resulting in ""bag-of-words"" representations. We introduce a simple and effective method to improve compositional reasoning in VLMs. Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework. Our approach does not require specific annotations and does not incur extra parameters. When integrated with CLIP, our technique yields notable improvement over state-of-the-art baselines across five vision-language compositional benchmarks. We open-source our code at https://github.com/lezhang7/Enhance-FineGrained.",http://arxiv.org/abs/2306.08832v3,,Le Zhang (Mila-Quebec AI Institute) | Rabiul Awal (None) | Aishwarya Agrawal (None),2023-06-15 03:26:28+00:00,,,,,,
BoQ: A Place is Worth a Bag of learnable Queries,,,,Amar Ali-Bey (Universit?? Laval) | Brahim Chaib-Draa (Laval University) | Philippe Gigu??re (Universit?? Laval),,,,,,,
"LCD: Towards Hierarchical Embeddings with Localizability, Composability, and Decomposability Learned from Anatomy",,,,Mohammad Reza Hosseinzadeh Taher (Arizona State University) | Michael Gotway (Mayo Clinic) | Jianming Liang (Arizona State University),,,,,,,
Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization,,,,Jimyeong Kim (Seoul National University) | Jungwon Park (Seoul National University) | Wonjong Rhee (Seoul National University),,,,,,,
StyLitGAN: Image-based Relighting via Latent Control,,,,Anand Bhattad (None) | James Soole (University Of Illinois Urbana-Champaign) | David Forsyth (University Of Illinois At Urbana-Champaign),,,,,,,
Utility-Fairness Trade-Offs and How to Find Them,,,,Sepehr Dehdashtian (Michigan State University) | Bashir Sadeghi (Michigan State University) | Vishnu Naresh Boddeti (None),,,,,,,
Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning,,,,Leonardo Iurada (Polytechnic Institute Of Turin) | Marco Ciccone (Politecnico Di Torino) | Tatiana Tommasi (Politecnico Di Torino),,,,,,,
ManiFPT: Defining and Analyzing Fingerprints of Generative Models,"Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.",http://arxiv.org/abs/2402.10401v2,,Hae Jin Song (University Of Southern California) | Mahyar Khayatkhoei (USC/ISI) | Wael AbdAlmageed (Clemson University),2024-02-16 01:58:35+00:00,,,,,,
Your Transferability Barrier is Fragile: Free-Lunch for Transferring the Non-Transferable Learning,,,,Ziming Hong (The University Of Sydney) | Li Shen (JD Explore Academy) | Tongliang Liu (Mohamed Bin Zayed University Of Artificial Intelligence),,,,,,,
High-Quality Facial Geometry and Appearance Capture at Home,"Facial geometry and appearance capture have demonstrated tremendous success in 3D scanning real humans in studios. Recent works propose to democratize this technique while keeping the results high quality. However, they are still inconvenient for daily usage. In addition, they focus on an easier problem of only capturing facial skin. This paper proposes a novel method for high-quality face capture, featuring an easy-to-use system and the capability to model the complete face with skin, mouth interior, hair, and eyes. We reconstruct facial geometry and appearance from a single co-located smartphone flashlight sequence captured in a dim room where the flashlight is the dominant light source (e.g. rooms with curtains or at night). To model the complete face, we propose a novel hybrid representation to effectively model both eyes and other facial regions, along with novel techniques to learn it from images. We apply a combined lighting model to compactly represent real illuminations and exploit a morphable face albedo model as a reflectance prior to disentangle diffuse and specular. Experiments show that our method can capture high-quality 3D relightable scans.",http://arxiv.org/abs/2312.03442v1,,"Yuxuan Han (Tsinghua University) | Junfeng Lyu (School Of Software, Tsinghua University) | Feng Xu (Tsinghua University)",2023-12-06 11:51:06+00:00,,,,,,
Towards CLIP-driven Language-free 3D Visual Grounding via 2D-3D Relational Enhancement and Consistency,,,,Yuqi Zhang (Sichuan University) | Han Luo (Sichuan University) | Yinjie Lei (Sichuan University),,,,,,,
MR-VNet: Media Restoration using Volterra Networks,,,,Siddharth Roheda (Samsung Research) | Amit Unde (SRIB Bangalore) | Loay Rashid (Samsung Research Institute Bangalore),,,,,,,
Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis,"Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to $31\times$ on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to $4\times$ higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.",http://arxiv.org/abs/2401.02436v2,,Simon Niedermayr (Technical University Of Munich) | Josef Stumpfegger (Technische Universit??t M??nchen) | R??diger Westermann (Technische Universit??t M??nchen),2023-11-17 14:40:43+00:00,,,,,,
Learning Degradation Independent Representations for Camera ISP Pipelines,"Image signal processing (ISP) pipeline plays a fundamental role in digital cameras, which converts raw Bayer sensor data to RGB images. However, ISP-generated images usually suffer from imperfections due to the compounded degradations that stem from sensor noises, demosaicing noises, compression artifacts, and possibly adverse effects of erroneous ISP hyperparameter settings such as ISO and gamma values. In a general sense, these ISP imperfections can be considered as degradations. The highly complex mechanisms of ISP degradations, some of which are even unknown, pose great challenges to the generalization capability of deep neural networks (DNN) for image restoration and to their adaptability to downstream tasks. To tackle the issues, we propose a novel DNN approach to learn degradation-independent representations (DiR) through the refinement of a self-supervised learned baseline representation. The proposed DiR learning technique has remarkable domain generalization capability and consequently, it outperforms state-of-the-art methods across various downstream tasks, including blind image restoration, object detection, and instance segmentation, as verified in our experiments.",http://arxiv.org/abs/2307.00761v3,,Yanhui Guo (McMaster University) | Fangzhou Luo (McMaster University) | Xiaolin Wu (McMaster University),2023-07-03 05:38:28+00:00,,,,,,
Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning,"Over the past year, a large body of multimodal research has emerged around zero-shot evaluation using GPT descriptors. These studies boost the zero-shot accuracy of pretrained VL models with an ensemble of label-specific text generated by GPT. A recent study, WaffleCLIP, demonstrated that similar zero-shot accuracy can be achieved with an ensemble of random descriptors. However, both zero-shot methods are un-trainable and consequently sub-optimal when some few-shot out-of-distribution (OOD) training data is available. Inspired by these prior works, we present two more flexible methods called descriptor and word soups, which do not require an LLM at test time and can leverage training data to increase OOD target accuracy. Descriptor soup greedily selects a small set of textual descriptors using generic few-shot training data, then calculates robust class embeddings using the selected descriptors. Word soup greedily assembles a chain of words in a similar manner. Compared to existing few-shot soft prompt tuning methods, word soup requires fewer parameters by construction and less GPU memory, since it does not require backpropagation. Both soups outperform current published few-shot methods, even when combined with SoTA zero-shot methods, on cross-dataset and domain generalization benchmarks. Compared with SoTA prompt and descriptor ensembling methods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy with fewer ensemble members. Please checkout our code: github.com/Chris210634/word_soups",http://arxiv.org/abs/2311.13612v1,,"Christopher Liao (None) | Theodoros Tsiligkaridis (MIT Lincoln Laboratory, Massachusetts Institute Of Technology) | Brian Kulis (Boston University)",2023-11-21 23:30:01+00:00,,,,,,
Rethinking Multi-domain Generalization with A General Learning Objective,"Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.",http://arxiv.org/abs/2402.18853v1,,Zhaorui Tan (None) | Xi Yang (Xi'an Jiaotong-Liverpool University) | Kaizhu Huang (Duke Kunshan University),2024-02-29 05:00:30+00:00,,,,,,
"FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features",,,,Andre Rochow (Rheinische Friedrich-Wilhelms Universit??t Bonn) | Max Schwarz (University Of Bonn) | Sven Behnke (University Of Bonn),,,,,,,
ASAM: Boosting Segment Anything Model with Adversarial Tuning,,,,"Bo Li (Vivo Mobile Communication Co.,Ltd.) | Haoke Xiao (Xiamen University) | Lv Tang (Vivo Mobile Communication Co., Ltd)",,,,,,,
Uncertainty Visualization via Low-Dimensional Posterior Projections,"In ill-posed inverse problems, it is commonly desirable to obtain insight into the full spectrum of plausible solutions, rather than extracting only a single reconstruction. Information about the plausible solutions and their likelihoods is encoded in the posterior distribution. However, for high-dimensional data, this distribution is challenging to visualize. In this work, we introduce a new approach for estimating and visualizing posteriors by employing energy-based models (EBMs) over low-dimensional subspaces. Specifically, we train a conditional EBM that receives an input measurement and a set of directions that span some low-dimensional subspace of solutions, and outputs the probability density function of the posterior within that space. We demonstrate the effectiveness of our method across a diverse range of datasets and image restoration problems, showcasing its strength in uncertainty quantification and visualization. As we show, our method outperforms a baseline that projects samples from a diffusion-based posterior sampler, while being orders of magnitude faster. Furthermore, it is more accurate than a baseline that assumes a Gaussian posterior.",http://arxiv.org/abs/2312.07804v1,,"Omer Yair (Technion) | Tomer Michaeli (Technion) | Elias Nehme (Electrical Engineering Department, Technion ?? Israel Institute Of Technology, Technion - Israel Institute Of Technology)",2023-12-12 23:51:07+00:00,,,,,,
CSTA: CNN-based Spatiotemporal Attention for Video Summarization,,,,"Jaewon Son (None) | Jaehun Park (Sung Kyun Kwan University) | Kwangsu Kim (Department Of Computer Science & Engineering, College Of Computing, Sungkyunkwan University)",,,,,,,
Animating General Image with Large Visual Motion Model,,,,Dengsheng Chen (Meituan) | Xiaoming Wei (Meituan) | Xiaolin Wei (Meituan),,,,,,,
Flexible Depth Completion for Sparse and Varying Point Densities,,,,Jinhyung Park (Carnegie Mellon University) | Yu-Jhe Li (Carnegie Mellon University) | Kris Kitani (Carnegie Mellon University),,,,,,,
Improving Generalization via Meta-Learning on Hard Samples,,,,"Nishant Jain (Indian Institute Of Technology, Roorkee, Dhirubhai Ambani Institute Of Information And Communication Technology) | Arun Suggala (Google) | Pradeep Shenoy (Google)",,,,,,,
Learning from Observer Gaze: Zero-shot Attention Prediction Oriented by Human-Object Interaction Recognition,,,,Yuchen Zhou (Sun Yat-Sen University) | Linkai Liu (Sun Yat-Sen University) | Chao Gou (Sun Yat-Sen University),,,,,,,
ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning,,,,Beomyoung Kim (NAVER Cloud / KAIST) | Joonsang Yu (NAVER) | Sung Ju Hwang (Korea Advanced Institute Of Science And Technology),,,,,,,
Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models,"3D reconstruction of dynamic scenes is a long-standing problem in computer graphics and increasingly difficult the less information is available. Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry from RGB images or video sequences, often leveraging just a single monocular camera without depth information, such as regular smartphone recordings. Unfortunately, existing reconstruction methods are either unphysical and noisy or slow in optimization. To solve this problem, we propose a novel SfT reconstruction algorithm for cloth using a pre-trained neural surrogate model that is fast to evaluate, stable, and produces smooth reconstructions due to a regularizing physics simulation. Differentiable rendering of the simulated mesh enables pixel-wise comparisons between the reconstruction and a target video sequence that can be used for a gradient-based optimization procedure to extract not only shape information but also physical parameters such as stretching, shearing, or bending stiffness of the cloth. This allows to retain a precise, stable, and smooth reconstructed geometry while reducing the runtime by a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-based SfT approach.",http://arxiv.org/abs/2311.12796v1,,David Stotko (Rheinische Friedrich-Wilhelms-Universit??t Bonn) | Nils Wandel (University Of Bonn) | Reinhard Klein (University Of Bonn),2023-11-21 18:59:58+00:00,,,,,,
VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection,,,,Zihua Liu (Tokyo Institute Of Technology) | Hiroki Sakuma (SenseTime Japan Ltd.) | Masatoshi Okutomi (Tokyo Institute Of Technology),,,,,,,
NICE: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning,,,,Mustafa B Gurbuz (Georgia Institute Of Technology) | Jean Moorman (Georgia Institute Of Technology) | Constantine Dovrolis (Georgia Institute Of Technology),,,,,,,
LightOctree: Lightweight 3D Spatially-Coherent Indoor Lighting Estimation,,,,Xuecan Wang (None) | Shibang Xiao (Beijing University Of Aeronautics And Astronautics) | Xiaohui Liang (Zhongguancun Laboratory),,,,,,,
ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles,,,,"Jiawei Zhang (University Of Illinois, Urbana Champaign) | Chejian Xu (University Of Illinois At Urbana-Champaign) | Bo Li (UIUC)",,,,,,,
Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining,,,,Xiang Chen (Nanjing University Of Science And Technology) | Jinshan Pan (Nanjing University Of Science And Technology) | Jiangxin Dong (Nanjing University Of Science And Technology),,,,,,,
A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?,,,,Galadrielle Humblot-Renaux (Aalborg University) | Sergio Escalera (Computer Vision Center) | Thomas B. Moeslund (Aalborg University),,,,,,,
Learning with Structural Labels for Learning with Noisy Labels,,,,Noo-Ri Kim (Sungkyunkwan University) | Jin-Seop Lee (Sungkyunkwan University) | Jee-Hyong Lee (Sungkyunkwan University),,,,,,,
Seamless Human Motion Composition with Blended Positional Encodings,"Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",http://arxiv.org/abs/2402.15509v1,,German Barquero (Universitat De Barcelona) | Sergio Escalera (Computer Vision Center) | Cristina Palmero (Universitat De Barcelona),2024-02-23 18:59:40+00:00,,,,,,
MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes,,,,Bor Shiun Wang (None) | Chien-Yi Wang (NVIDIA) | Wei-Chen Chiu (None),,,,,,,
Reconstruction-free Cascaded Adaptive Compressive Sensing,,,,Chenxi Qiu (Nanjing University) | Tao Yue (Nanjing University) | Xuemei Hu (Nanjing University),,,,,,,
RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection,"Self-supervised feature reconstruction methods have shown promising advances in industrial image anomaly detection and localization. Despite this progress, these methods still face challenges in synthesizing realistic and diverse anomaly samples, as well as addressing the feature redundancy and pre-training bias of pre-trained feature. In this work, we introduce RealNet, a feature reconstruction network with realistic synthetic anomaly and adaptive feature selection. It is incorporated with three key innovations: First, we propose Strength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion process-based synthesis strategy capable of generating samples with varying anomaly strengths that mimic the distribution of real anomalous samples. Second, we develop Anomaly-aware Features Selection (AFS), a method for selecting representative and discriminative pre-trained feature subsets to improve anomaly detection performance while controlling computational costs. Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that adaptively selects discriminative residuals for comprehensive identification of anomalous regions across multiple levels of granularity. We assess RealNet on four benchmark datasets, and our results demonstrate significant improvements in both Image AUROC and Pixel AUROC compared to the current state-o-the-art methods. The code, data, and models are available at https://github.com/cnulab/RealNet.",http://arxiv.org/abs/2403.05897v1,,Ximiao Zhang (Capital Normal University) | Min Xu (Capital Normal University) | Xiuzhuang Zhou (Beijing University Of Posts And Telecommunications),2024-03-09 12:25:01+00:00,,,,,,
Normalizing Flows on the Product Space of SO(3) Manifolds for Probabilistic Human Pose Modeling,,,,"Olaf D??nkel (Saarland Informatics Campus, Max-Planck Institute) | Tim Salzmann (Technische Universit??t M??nchen) | Florian Pfaff (University Of Stuttgart)",,,,,,,
Fourier-basis functions to bridge augmentation gap: Rethinking frequency augmentation in image classification,,,,Mei Vaish (TNO) | Shunxin Wang (University Of Twente) | Nicola Strisciuglio (University Of Twente),,,,,,,
Generating Illustrated Instructions,,,,Sachit Menon (Columbia University) | Ishan Misra (Facebook) | Rohit Girdhar (Meta),,,,,,,
COSMO: Converting and Smoothing False Negatives for Vision-Language Pre-training,"We consider the critical issue of false negatives in Vision-Language Pre-training (VLP), a challenge that arises from the inherent many-to-many correspondence of image-text pairs in large-scale web-crawled datasets. The presence of false negatives can impede achieving optimal performance and even lead to learning failures. To address this challenge, we propose a method called COSMO (COnverting and SMOoothing false negatives) that manages the false negative issues, especially powerful in hard negative sampling. Building upon the recently developed GRouped mIni-baTch sampling (GRIT) strategy, our approach consists of two pivotal components: 1) an efficient connection mining process that identifies and converts false negatives into positives, and 2) label smoothing for the image-text contrastive loss (ITC). Our comprehensive experiments verify the effectiveness of COSMO across multiple downstream tasks, emphasizing the crucial role of addressing false negatives in VLP, potentially even surpassing the importance of addressing false positives. In addition, the compatibility of COSMO with the recent BLIP-family model is also demonstrated.",http://arxiv.org/abs/2312.06112v1,,Jaeseok Byun (Seoul National University) | Dohoon Kim (Seoul National University) | Taesup Moon (Seoul National University),2023-12-11 04:33:35+00:00,,,,,,
Discriminative Pattern Calibration Mechanism for Source-Free Domain Adaptation,,,,Haifeng Xia (Southeast University) | Siyu Xia (Southeast University) | Zhengming Ding (Tulane University),,,,,,,
MaskCLR: Attention-Guided Contrastive Learning for Robust Action Representation Learning,,,,"Mohamed Abdelfattah (VITA, EPFL) | Mariam Hassan (EPFL - EPF Lausanne) | Alex Alahi (None)",,,,,,,
Open Vocabulary Semantic Scene Sketch Understanding,"We study the underexplored but fundamental vision problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that results in semantically-aware feature space, which we evaluate by testing its performance on a semantic sketch segmentation task. To train our model we rely only on the availability of bitmap sketches with their brief captions and do not require any pixel-level annotations. To obtain generalization to a large set of sketches and categories, we build on a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. Firstly, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical network design that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. We, then, in the second level of the hierarchy, introduce a cross-attention between textual and visual branches. Our method outperforms zero-shot CLIP pixel accuracy of segmentation results by 37 points, reaching an accuracy of $85.5\%$ on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of scene sketches.",http://arxiv.org/abs/2312.12463v1,,"Ahmed Bourouis (None) | Judith Fan (Stanford University) | Yulia Gryaditskaya (CVSSP, PAI, University Of Surrey)",2023-12-18 19:02:07+00:00,,,,,,
Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning,"Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve state-of-the-art performance with improved efficiency in various computer vision tasks. This suggests a promising paradigm shift of adapting pre-trained ViT models to Federated Learning (FL) settings. However, the challenge of data heterogeneity among FL clients presents a significant hurdle in effectively deploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL) methods have limitations in balancing performance across both global and local data distributions. In this paper, we present a novel algorithm, SGPT, that integrates GFL and PFL approaches by employing a unique combination of both shared and group-specific prompts. This design enables SGPT to capture both common and group-specific features. A key feature of SGPT is its prompt selection module, which facilitates the training of a single global model capable of automatically adapting to diverse local client data distributions without the need for local fine-tuning. To effectively train the prompts, we utilize block coordinate descent (BCD), learning from common feature information (shared prompts), and then more specialized knowledge (group prompts) iteratively. Theoretically, we justify that learning the proposed prompts can reduce the gap between global and local performance. Empirically, we conduct experiments on both label and feature heterogeneity settings in comparison with state-of-the-art baselines, along with extensive ablation studies, to substantiate the superior performance of SGPT.",http://arxiv.org/abs/2310.18285v4,,Wenlong Deng (University Of British Columbia) | Christos Thrampoulidis (None) | Xiaoxiao Li (University Of British Columbia),2023-10-27 17:22:09+00:00,,,,,,
SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks,,,,Xinyu Shi (None) | Zecheng Hao (Peking University) | Zhaofei Yu (Peking University),,,,,,,
Extreme Point Supervised Instance Segmentation,,,,Hyeonjun Lee (POSTECH) | Sehyun Hwang (POSTECH) | Suha Kwak (POSTECH),,,,,,,
Deep Unsupervised Unrolling Networks for Phase Unwrapping,,,,Zhile Chen (South China University Of Technology) | Yuhui Quan (South China University Of Technology) | Hui Ji (National University Of Singapore),,,,,,,
Rapid Motor Adaptation for Robotic Manipulator Arms,"Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and orientation, and operating a variety of faucets and handles, with customized environment variations. Empirical results demonstrate that our agents surpass state-of-the-art methods like automatic domain randomization and vision-based policies, obtaining better generalization performance and sample efficiency.",http://arxiv.org/abs/2312.04670v1,,Yichao Liang (None) | Kevin Ellis (Cornell University) | Jo??o F. Henriques (University Of Oxford),2023-12-07 20:11:03+00:00,,,,,,
Fully Geometric Panoramic Localization,,,,Junho Kim (None) | Jiwon Jeong (Stanford University) | Young Min Kim (Seoul National University),,,,,,,
Discontinuity-preserving Normal Integration with Auxiliary Edges,,,,Hyomin Kim (POSTECH) | Yucheol Jung (POSTECH) | Seungyong Lee (POSTECH),,,,,,,
PAPR in Motion: Seamless Point-level 3D Scene Interpolation,,,,Shichong Peng (None) | Yanshu Zhang (None) | Ke Li (Simon Fraser University),,,,,,,
Dense Vision Transformer Compression with Few Samples,,,,Hanxiao Zhang (Nanjing University) | Yifan Zhou (Nanjing University) | Guo-Hua Wang (None),,,,,,,
Towards Realistic Scene Generation with LiDAR Diffusion Models,,,,Haoxi Ran (Carnegie Mellon University) | Vitor Guizilini (Toyota Research Institute) | Yue Wang (Massachusetts Institute Of Technology),,,,,,,
FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization,"Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However, both aspects are frequently overlooked by existing methods. To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.",http://arxiv.org/abs/2403.06375v2,,Shuai Tan (Shanghai Jiao Tong University) | Bin Ji (Shanghai Jiao Tong University) | Ye Pan (Shanghai Jiao Tong University),2024-03-11 01:58:04+00:00,,,,,,
Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection,,,,"Zhiwei Yang (Guangzhou Institute Of Technology, Xidian University) | Jing Liu (Guangzhou Institute Of Technology, Xidian University) | Peng Wu (Northwest Polytechnical University Xi'an)",,,,,,,
Learning to Control Camera Exposure via Reinforcement Learning,,,,"Kyunghyun Lee (LG AI Research) | Ukcheol Shin (CMU, Carnegie Mellon University) | Byeong-Uk Lee (KRAFTON,)",,,,,,,
Deciphering ??What?? and ??Where?? Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations,"We present an approach for analyzing grouping information contained within a neural network's activations, permitting extraction of spatial layout and semantic segmentation from the behavior of large pre-trained vision models. Unlike prior work, our method conducts a wholistic analysis of a network's activation state, leveraging features from all layers and obviating the need to guess which part of the model contains relevant information. Motivated by classic spectral clustering, we formulate this analysis in terms of an optimization objective involving a set of affinity matrices, each formed by comparing features within a different layer. Solving this optimization problem using gradient descent allows our technique to scale from single images to dataset-level analysis, including, in the latter, both intra- and inter-image relationships. Analyzing a pre-trained generative transformer provides insight into the computational strategy learned by such models. Equating affinity with key-query similarity across attention layers yields eigenvectors encoding scene spatial layout, whereas defining affinity by value vector similarity yields eigenvectors encoding object identity. This result suggests that key and query vectors coordinate attentional information flow according to spatial proximity (a `where' pathway), while value vectors refine a semantic category representation (a `what' pathway).",http://arxiv.org/abs/2312.06716v1,,Xiao Zhang (University Of Chicago) | David Yunis (Toyota Technological Institute At Chicago) | Michael Maire (None),2023-12-11 01:20:34+00:00,,,,,,
KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation,,,,Jihua Peng (The Hong Kong Polytechnic University) | Yanghong Zhou (The Hong Kong Polytechnic University) | Tracy Mok (None),,,,,,,
Analyzing and Explaining Image Classifiers via Diffusion Guidance,"While deep learning has led to huge progress in complex image classification tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call into question how reliably these classifiers work in the wild. Furthermore, for safety-critical tasks the black-box nature of their decisions is problematic, and explanations or at least methods which make decisions plausible are needed urgently. In this paper, we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the behavior and decisions of image classifiers by visual counterfactual explanations (VCEs), detection of systematic mistakes by analyzing images where classifiers maximally disagree, and visualization of neurons to verify potential spurious features. In this way, we validate existing observations, e.g. the shape bias of adversarially robust models, as well as novel failure modes, e.g. systematic errors of zero-shot CLIP classifiers, or identify harmful spurious features. Moreover, our VCEs outperform previous work while being more versatile.",http://arxiv.org/abs/2311.17833v1,,Maximilian Augustin (University Of Tuebingen) | Yannic Neuhaus (Eberhard-Karls-Universit??t T??bingen) | Matthias Hein (University Of T??bingen),2023-11-29 17:35:29+00:00,,,,,,
Learning to Produce Semi-dense Correspondences for Visual Localization,,,,Khang Truong Giang (Korea Advanced Institute Of Science And Technology) | Soohwan Song (Dongguk University) | Sungho Jo (Korea Advanced Institute Of Science & Technology),,,,,,,
Understanding and Improving Source-free Domain Adaptation from a Theoretical Perspective,,,,Yu Mitsuzumi (None) | Akisato Kimura (NTT Corporation) | Hisashi Kashima (Kyoto University),,,,,,,
SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System,"Accuracy and computational efficiency are the most important metrics to Visual Inertial Navigation System (VINS). The existing VINS algorithms with either high accuracy or low computational complexity, are difficult to provide the high precision localization in resource-constrained devices. To this end, we propose a novel filter-based VINS framework named SchurVINS, which could guarantee both high accuracy by building a complete residual model and low computational complexity with Schur complement. Technically, we first formulate the full residual model where Gradient, Hessian and observation covariance are explicitly modeled. Then Schur complement is employed to decompose the full model into ego-motion residual model and landmark residual model. Finally, Extended Kalman Filter (EKF) update is implemented in these two models with high efficiency. Experiments on EuRoC and TUM-VI datasets show that our method notably outperforms state-of-the-art (SOTA) methods in both accuracy and computational complexity. We will open source our experimental code to benefit the community.",http://arxiv.org/abs/2312.01616v1,,"Yunfei Fan (PICO, ByteDance) | Tianyu Zhao (Bytedance) | Guidong Wang (PICO)",2023-12-04 04:14:09+00:00,,,,,,
PPAL: A Simple Baseline for Active Learning in Object Detection,,,,Chenhongyi Yang (University Of Edinburgh) | Lichao Huang (Horizon Robotics ) | Elliot Crowley (University Of Edinburgh),,,,,,,
Sharingan: A Transformer Architecture for Multi-Person Gaze Following,"Gaze is a powerful form of non-verbal communication and social interaction that humans develop from an early age. As such, modeling this behavior is an important task that can benefit a broad set of application domains ranging from robotics to sociology. In particular, Gaze Following is defined as the prediction of the pixel-wise 2D location where a person in the image is looking. Prior efforts in this direction have focused primarily on CNN-based architectures to perform the task. In this paper, we introduce a novel transformer-based architecture for 2D gaze prediction. We experiment with 2 variants: the first one retains the same task formulation of predicting a gaze heatmap for one person at a time, while the second one casts the problem as a 2D point regression and allows us to perform multi-person gaze prediction with a single forward pass. This new architecture achieves state-of-the-art results on the GazeFollow and VideoAttentionTarget datasets. The code for this paper will be made publicly available.",http://arxiv.org/abs/2310.00816v1,,Samy Tafasca (EPFL) | Anshul Gupta (None) | Jean-Marc Odobez (Swiss Federal Institute Of Technology Lausanne),2023-10-01 23:14:54+00:00,,,,,,
Scale Decoupled Distillation,,,,Shicai Wei (University Of Electronic Science And Technology Of China) | Chunbo Luo (University Of Electronic Science And Technology Of China) | Yang Luo (University Of Electronic Science And Technology Of China),,,,,,,
Traceable Federated Continual Learning,,,,Qiang Wang (Beijing University Of Posts And Telecommunications) | Bingyan Liu (None) | Yawen Li (Beijing University Of Posts And Telecommunications),,,,,,,
ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations,,,,Rwiddhi Chakraborty (None) | Adrian De Sena Sletten (University Of Troms??) | Michael C. Kampffmeyer (UiT The Arctic University Of Norway),,,,,,,
Gaussian Shadow Casting for Neural Characters,"Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.",http://arxiv.org/abs/2401.06116v1,,Luis Bolanos (The University Of British Columbia) | Shih-Yang Su (University Of British Columbia) | Helge Rhodin (UBC),2024-01-11 18:50:31+00:00,,,,,,
Weakly-Supervised Audio-Visual Video Parsing with Prototype-based Pseudo-Labeling,,,,Kranthi Kumar Rachavarapu (Indian Institute Of Technology Madras) | Kalyan Ramakrishnan (University Of Oxford) | A. N. Rajagopalan (Indian Institute Of Technology Madras),,,,,,,
Identifying Important Group of Pixels using Interactions,"To better understand the behavior of image classifiers, it is useful to visualize the contribution of individual pixels to the model prediction. In this study, we propose a method, MoXI~($\textbf{Mo}$del e$\textbf{X}$planation by $\textbf{I}$nteractions), that efficiently and accurately identifies a group of pixels with high prediction confidence. The proposed method employs game-theoretic concepts, Shapley values and interactions, taking into account the effects of individual pixels and the cooperative influence of pixels on model confidence. Theoretical analysis and experiments demonstrate that our method better identifies the pixels that are highly contributing to the model outputs than widely-used visualization methods using Grad-CAM, Attention rollout, and Shapley value. While prior studies have suffered from the exponential computational cost in the computation of Shapley value and interactions, we show that this can be reduced to linear cost for our task.",http://arxiv.org/abs/2401.03785v1,,Kosuke Sumiyasu (Chiba University) | Kazuhiko Kawamoto (Chiba University) | Hiroshi Kera (Chiba University),2024-01-08 10:06:52+00:00,,,,,,
Diffusion-FOF: Single-view Clothed Human Reconstruction via Diffusion-based Fourier Occupancy Field,,,,Yuanzhen Li (Wuhan University) | Fei LUO (Wuhan University) | Chunxia Xiao (Wuhan University),,,,,,,
S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data,,,,"Xuyang Li (Chinese Academy Of Sciences) | Danfeng Hong (Chinese Academy Of Sciences, Aerospace Information Research Institute) | Jocelyn Chanussot (INRIA)",,,,,,,
VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis,"Self-Supervised Learning (SSL) has demonstrated promising results in 3D medical image analysis. However, the lack of high-level semantics in pre-training still heavily hinders the performance of downstream tasks. We observe that 3D medical images contain relatively consistent contextual position information, i.e., consistent geometric relations between different organs, which leads to a potential way for us to learn consistent semantic representations in pre-training. In this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework to leverage the contextual position priors for pre-training. Specifically, we first generate a group of base crops from different regions while enforcing feature discrepancy among them, where we employ them as class assignments of different regions. Then, we randomly crop sub-volumes and predict them belonging to which class (located at which region) by contrasting their similarity to different base crops, which can be seen as predicting contextual positions of different sub-volumes. Through this pretext task, VoCo implicitly encodes the contextual position priors into model representations without the guidance of annotations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics. Extensive experimental results on six downstream tasks demonstrate the superior effectiveness of VoCo. Code will be available at https://github.com/Luffy03/VoCo.",http://arxiv.org/abs/2402.17300v1,,"Linshan Wu (Hunan University) | Jia-Xin Zhuang (Department Of Computer Science And Engineering, Hong Kong University Of Science And Technology) | Hao Chen (The Hong Kong University Of Science And Technology)",2024-02-27 08:22:55+00:00,,,,,,
Learned Trajectory Embedding for Subspace Clustering,,,,Yaroslava Lochman (Chalmers University Of Technology) | Christopher Zach (Chalmers University) | Carl Olsson (Lund University),,,,,,,
An edit friendly ddpm noise space: inversion and manipulations,"Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g., shifting, color edits). Moreover, in text-conditional models, fixing those noise maps while changing the text prompt, modifies semantics while retaining structure. We illustrate how this property enables text-based editing of real images via the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM inversion). We also show how it can be used within existing diffusion-based editing methods to improve their quality and diversity.",http://arxiv.org/abs/2304.06140v2,,Inbar Huberman-Spiegelglas (Technion - Israel Institute Of Technology) | Vladimir Kulikov (Technion - Israel Institute Of Technology) | Tomer Michaeli (Technion),2023-04-12 19:47:13+00:00,,,,,,
PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos,,,,"Qi Zhao (None) | M. Salman Asif (University Of California, Riverside) | Zhan Ma (Nanjing University)",,,,,,,
Forecasting of 3D Whole-body Human Poses with Grasping Objects,,,,Yan Haitao (None) | Qiongjie Cui (Nanjing University Of Science And Technology) | Jiexin Xie (Fudan University) | Shijie Guo (Fudan University),,,,,,,
GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding,"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.",http://arxiv.org/abs/2403.03608v1,,Zi-Ting Chou (National Taiwan University) | Sheng-Yu Huang (National Taiwan University) | I-Jieh Liu (National Taiwan University) | Yu-Chiang Frank Wang (NVIDIA),2024-03-06 10:55:50+00:00,,,,,,
Making Vision Transformers Truly Shift-Equivariant,"For computer vision, Vision Transformers (ViTs) have become one of the go-to deep net architectures. Despite being inspired by Convolutional Neural Networks (CNNs), ViTs' output remains sensitive to small spatial shifts in the input, i.e., not shift invariant. To address this shortcoming, we introduce novel data-adaptive designs for each of the modules in ViTs, such as tokenization, self-attention, patch merging, and positional encoding. With our proposed modules, we achieve true shift-equivariance on four well-established ViTs, namely, Swin, SwinV2, CvT, and MViTv2. Empirically, we evaluate the proposed adaptive models on image classification and semantic segmentation tasks. These models achieve competitive performance across three different datasets while maintaining 100% shift consistency.",http://arxiv.org/abs/2305.16316v2,,Renan A. Rojas-Gomez (University Of Illinois At Urbana Champaign) | Teck-Yian Lim (DSO National Laboratories) | Minh Do (University Of Illinois At Urbana-Champaign) | Raymond A. Yeh (Purdue University),2023-05-25 17:59:40+00:00,,,,,,
Stationary Representations: Optimally Approximating Compatibility and Implications for Improved Model Replacements,,,,"Niccol?? Biondi (University Of Florence, Italy) | Federico Pernici (University Of Florence, Italy) | Simone Ricci (University Of Florence) | Alberto Bimbo (Universita Di Firenze)",,,,,,,
CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification,"Person re-identification (re-ID) is a challenging task that aims to learn discriminative features for person retrieval. In person re-ID, Jaccard distance is a widely used distance metric, especially in re-ranking and clustering scenarios. However, we discover that camera variation has a significant negative impact on the reliability of Jaccard distance. In particular, Jaccard distance calculates the distance based on the overlap of relevant neighbors. Due to camera variation, intra-camera samples dominate the relevant neighbors, which reduces the reliability of the neighbors by introducing intra-camera negative samples and excluding inter-camera positive samples. To overcome this problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that leverages camera information to enhance the reliability of Jaccard distance. Specifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs) to find k-reciprocal nearest neighbors on the intra-camera and inter-camera ranking lists, which improves the reliability of relevant neighbors and guarantees the contribution of inter-camera samples in the overlap. Moreover, we propose a camera-aware local query expansion (CLQE) to exploit camera variation as a strong constraint to mine reliable samples in relevant neighbors and assign these samples higher weights in overlap to further improve the reliability. Our CA-Jaccard distance is simple yet effective and can serve as a general distance metric for person re-ID methods with high reliability and low computational cost. Extensive experiments demonstrate the effectiveness of our method.",http://arxiv.org/abs/2311.10605v1,,Yiyu Chen (Beijing Institute Of Technology) | Zheyi Fan (Beijing Institute Of Technology) | Zhaoru Chen (Beijing Institute Of Technology) | Yixuan Zhu (Beijing Institute Of Technology),2023-11-17 16:01:06+00:00,,,,,,
Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth,,,,Zhaoyang Sun (Wuhan University Of Technology) | Shengwu Xiong (Wuhan University Of Technology) | Yaxiong Chen (Wuhan University Of Technology) | Yi Rong (Wuhan University Of Technology),,,,,,,
6-DoF Pose Estimation with MultiScale Residual Correlation,"We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image. Our method, dubbed MRC-Net, comprises two stages. The first performs pose classification and renders the 3D object in the classified pose. The second stage performs regression to predict fine-grained residual pose within class. Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage. MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images. To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage. We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O, YCB-V, and ITODD. Our method is non-iterative and requires no complex post-processing.",http://arxiv.org/abs/2403.08019v2,,Yuelong Li (Amazon) | Yafei Mao (Amazon) | Raja Bala (Amazon) | Sunil Hadap (Amazon),2024-03-12 18:36:59+00:00,,,,,,
Instance Tracking in 3D Scenes from Egocentric Videos,"Egocentric sensors such as AR/VR devices capture human-object interactions and offer the potential to provide task-assistance by recalling 3D locations of objects of interest in the surrounding environment. This capability requires instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We explore this problem by first introducing a new benchmark dataset, consisting of RGB and depth videos, per-frame camera pose, and instance-level annotations in both 2D camera and 3D world coordinates. We present an evaluation protocol which evaluates tracking performance in 3D coordinates with two settings for enrolling instances to track: (1) single-view online enrollment where an instance is specified on-the-fly based on the human wearer's interactions. and (2) multi-view pre-enrollment where images of an instance to be tracked are stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods from relevant areas, e.g., single object tracking (SOT) -- running SOT methods to track instances in 2D frames and lifting them to 3D using camera pose and depth. We also present a simple method that leverages pretrained segmentation and detection models to generate proposals from RGB frames and match proposals with enrolled instance images. Perhaps surprisingly, our extensive experiments show that our method (with no finetuning) significantly outperforms SOT-based approaches. We conclude by arguing that the problem of egocentric instance tracking is made easier by leveraging camera pose and using a 3D allocentric (world) coordinate representation.",http://arxiv.org/abs/2312.04117v1,,"Yunhan Zhao (University Of California, Irvine) | Haoyu Ma (University Of California, Irvine) | Shu Kong (None) | Charless Fowlkes (University Of California, Irvine)",2023-12-07 08:18:35+00:00,,,,,,
Sparse Semi-Detr: Sparse Learnable Queries for Semi-Supervised Object Detection,,,,Tahira Shehzadi (None) | Khurram Azeem Hashmi (DFKI - German Research Center For AI) | Didier Stricker (Universit??t Kaiserslautern) | Muhammad Zeshan Afzal (German Research Center For AI),,,,,,,
SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,"Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.",http://arxiv.org/abs/2403.03170v1,,Peng Qi (National University Of Singapore) | Zehong Yan (National University Of Singapore) | Wynne Hsu (National University Of Singapore) | Mong Li Lee (National University Of Singapore),2024-03-05 18:04:59+00:00,,,,,,
Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement,"Recently, dataset distillation has paved the way towards efficient machine learning, especially for image datasets. However, the distillation for videos, characterized by an exclusive temporal dimension, remains an underexplored domain. In this work, we provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression. Our investigation reveals that the temporal information is usually not well learned during distillation , and the temporal dimension of synthetic data contributes little. The observations motivate our unified framework of disentangling the dynamic and static information in the videos. It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block. Our method achieves state-of-the-art on video datasets at different scales, with notably smaller storage expenditure. Our code will be publicly available.",http://arxiv.org/abs/2312.00362v1,,Ziyu Wang (Shanghai Jiao Tong University) | Yue Xu (Shanghai Jiao Tong University) | Cewu Lu (Shanghai Jiao Tong University) | Yonglu Li (Shanghai Jiao Tong University),2023-12-01 05:59:08+00:00,,,,,,
JDEC: JPEG Decoding via Enhanced Continuous Cosine Coefficients,,,,Woo Kyoung Han (Korea University) | Sunghoon Im (DGIST) | Jaedeok Kim (NVIDIA) | Kyong Hwan Jin (Korea University),,,,,,,
GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects,,,,Sungphill Moon (Naver Labs) | Hyeontae Son (Naver Labs) | Dongcheol Hur (NAVER LABS) | Sangwook Kim (Naver Labs),,,,,,,
pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction,"We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.",http://arxiv.org/abs/2312.12337v2,,"David Charatan (None) | Sizhe Lester Li (Massachusetts Institute Of Technology) | Andrea Tagliasacchi (Simon Fraser University, Google Brain) | Vincent Sitzmann (Massachusetts Institute Of Technology)",2023-12-19 17:03:50+00:00,,,,,,
TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis,,,,"Pavlo Melnyk (Computer Vision Laboratory, Link??ping University) | Andreas Robinson (Link??ping University) | Michael Felsberg (Link??ping University) | M??rten Wadenb??ck (Link??ping University)",,,,,,,
Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models,,,,Pablo Marcos-Manch??n (University Of Barcelona) | Roberto Alcover (None) | Juan SanMiguel (Universidad Aut??noma De Madrid) | Jose M. Martinez (Universidad Aut??noma De Madrid),,,,,,,
Style Aligned Image Generation via Shared Attention,"Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.",http://arxiv.org/abs/2312.02133v2,,"Amir Hertz (Tel Aviv University) | Andrey Voynov (Google Research) | Shlomi Fruchter (Research, Google) | Daniel Cohen-Or (Google)",2023-12-04 18:55:35+00:00,,,,,,
Modality-agnostic Domain Generalizable Medical Image Segmentation by Multi-Frequency in Multi-Scale Attention,,,,Ju-Hyeon Nam (Inha University) | Nur Suriza Syazwany (Inha University) | Su Jung Kim (Inha University) | Sang-Chul Lee (Inha University),,,,,,,
Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains,,,,Eunsu Baek (Seoul National University) | Keondo Park (Seoul National University) | Ji-Yoon Kim (Seoul National University) | Hyung-Sin Kim (Seoul National University),,,,,,,
On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm,"Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours).",http://arxiv.org/abs/2312.03526v1,,"Peng Sun (None) | Bei Shi (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) | Daiwei Yu (Hangzhou City University) | Tao Lin (Westlake University)",2023-12-06 14:40:05+00:00,,,,,,
Distortion-Guided Domain Generalization for Blind Image Quality Assessment,,,,Aobo Li (Xidian University) | Jinjian Wu (Xidian University) | Yongxu Liu (Xidian University) | Leida Li (Xidian University),,,,,,,
OneFormer3D: One Transformer for Unified Point Cloud Segmentation,"Semantic, instance, and panoptic segmentation of 3D point clouds have been addressed using task-specific models of distinct design. Thereby, the similarity of all segmentation tasks and the implicit relationship between them have not been utilized effectively. This paper presents a unified, simple, and effective model addressing all these tasks jointly. The model, named OneFormer3D, performs instance and semantic segmentation consistently, using a group of learnable kernels, where each kernel is responsible for generating a mask for either an instance or a semantic category. These kernels are trained with a transformer-based decoder with unified instance and semantic queries passed as an input. Such a design enables training a model end-to-end in a single run, so that it achieves top performance on all three segmentation tasks simultaneously. Specifically, our OneFormer3D ranks 1st and sets a new state-of-the-art (+2.1 mAP50) in the ScanNet test leaderboard. We also demonstrate the state-of-the-art results in semantic, instance, and panoptic segmentation of ScanNet (+21 PQ), ScanNet200 (+3.8 mAP50), and S3DIS (+0.8 mIoU) datasets.",http://arxiv.org/abs/2311.14405v1,,Maksim Kolodiazhnyi (Samsung) | Anna Vorontsova (Samsung) | Anton Konushin (Samsung) | Danila Rukhovich (Samsung Research),2023-11-24 10:56:27+00:00,,,,,,
Attention Calibration for Disentangled Text-to-Image Personalization,,,,Yanbing Zhang (East China University Of Science And Technology) | Mengping Yang (East China University Of Science And Technology) | Qin Zhou (East China University Of Science And Technology) | Zhe Wang (East China University Of Science And Technology),,,,,,,
Compositional Video Understanding with Spatiotemporal Structure-based Transformers,,,,Hoyeoung Yun (Hanyang University) | Jinwoo Ahn (Hanyang University) | Minseo Kim (Hanyang University) | Eun-Sol Kim (Hanyang University),,,,,,,
BrainWash: A Poisoning Attack to Forget in Continual Learning,"Continual learning has gained substantial attention within the deep learning community, offering promising solutions to the challenging problem of sequential learning. Yet, a largely unexplored facet of this paradigm is its susceptibility to adversarial attacks, especially with the aim of inducing forgetting. In this paper, we introduce ""BrainWash,"" a novel data poisoning method tailored to impose forgetting on a continual learner. By adding the BrainWash noise to a variety of baselines, we demonstrate how a trained continual learner can be induced to forget its previously learned tasks catastrophically, even when using these continual learning baselines. An important feature of our approach is that the attacker requires no access to previous tasks' data and is armed merely with the model's current parameters and the data belonging to the most recent task. Our extensive experiments highlight the efficacy of BrainWash, showcasing degradation in performance across various regularization-based continual learning methods.",http://arxiv.org/abs/2311.11995v3,,"Ali Abbasi (Vanderbilt University) | Parsa Nooralinejad (University Of California, Davis) | Hamed Pirsiavash (University Of California, Davis) | Soheil Kolouri (Vanderbilt University)",2023-11-20 18:26:01+00:00,,,,,,
InNeRF360: Text-Guided 3D-Consistent Object Inpainting on Unbounded Neural Radiance Fields,"Neural Radiance Fields (NeRF) can generate highly realistic novel views. However, editing 3D scenes represented by NeRF across 360-degree views, particularly removing objects while preserving geometric and photometric consistency, remains a challenging problem due to NeRF's implicit scene representation. In this paper, we propose InpaintNeRF360, a unified framework that utilizes natural language instructions as guidance for inpainting NeRF-based 3D scenes.Our approach employs a promptable segmentation model by generating multi-modal prompts from the encoded text for multiview segmentation. We apply depth-space warping to enforce viewing consistency in the segmentations, and further refine the inpainted NeRF model using perceptual priors to ensure visual plausibility. InpaintNeRF360 is capable of simultaneously removing multiple objects or modifying object appearance based on text instructions while synthesizing 3D viewing-consistent and photo-realistic inpainting. Through extensive experiments on both unbounded and frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of our approach and showcase its potential to enhance the editability of implicit radiance fields.",http://arxiv.org/abs/2305.15094v1,,Dongqing Wang (EPFL) | Tong Zhang (EPFL) | Alaa Abboud (EPFL - EPF Lausanne) | Sabine S??sstrunk (None),2023-05-24 12:22:23+00:00,,,,,,
BiPer: Binary Neural Networks using a Periodic Function,,,,Edwin Vargas (None) | Claudia Correa (Universidad Industrial De Santander) | Carlos Hinojosa (KAUST) | Henry Arguello (Universidad Industrial De Santander),,,,,,,
Accurate Training Data for Occupancy Map Prediction in Automated Driving using Evidence Theory,,,,"Jonas K??lble (Bosch Center For Artificial Intelligence) | Sascha Wirges (Robert Bosch GmbH, Bosch) | Maxim Tatarchenko (Bosch) | Eddy Ilg (None)",,,,,,,
Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping,"The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies. We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples. At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.",http://arxiv.org/abs/2312.04521v1,,Alex Costanzino (University Of Bologna) | Pierluigi Zama Ramirez (University Of Bologna) | Giuseppe Lisanti (University Of Bologna) | Luigi Di Stefano (University Of Bologna),2023-12-07 18:41:21+00:00,,,,,,
Dual-Enhanced Coreset Selection with Class-wise Collaboration for Online Blurry Class Incremental Learning,,,,Yutian Luo (Renmin University Of China) | Shiqi Zhao (China Unicom Research Institute) | Haoran Wu (China Unicom Research Institute ) | Zhiwu Lu (Renmin University Of China),,,,,,,
SonicVisionLM: Playing Sound with Vision Language Models,"There has been a growing interest in the task of generating sound for silent videos, primarily because of its practicality in streamlining video post-production. However, existing methods for video-sound generation attempt to directly create sound from visual representations, which can be challenging due to the difficulty of aligning visual representations with audio representations. In this paper, we present SonicVisionLM, a novel framework aimed at generating a wide range of sound effects by leveraging vision language models. Instead of generating audio directly from video, we use the capabilities of powerful vision language models (VLMs). When provided with a silent video, our approach first identifies events within the video using a VLM to suggest possible sounds that match the video content. This shift in approach transforms the challenging task of aligning image and audio into more well-studied sub-problems of aligning image-to-text and text-to-audio through the popular diffusion models. To improve the quality of audio recommendations with LLMs, we have collected an extensive dataset that maps text descriptions to specific sound effects and developed temporally controlled audio adapters. Our approach surpasses current state-of-the-art methods for converting video to audio, resulting in enhanced synchronization with the visuals and improved alignment between audio and video components. Project page: https://yusiissy.github.io/SonicVisionLM.github.io/",http://arxiv.org/abs/2401.04394v2,,Zhifeng Xie (Shanghai University) | Shengye Yu (Shanghai University) | Qile He (Shanghai University) | Mengtian Li (Shanghai University),2024-01-09 07:30:10+00:00,,,,,,
Neural Redshift: Random Networks are not Random Functions,"Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent ""simplicity bias"". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inherit all these properties from their building blocks.   Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.",http://arxiv.org/abs/2403.02241v2,,Damien Teney (Idiap Research Institute) | Armand Nicolicioiu (ETHZ - ETH Zurich) | Valentin Hartmann (EPFL) | Ehsan Abbasnejad (University Of Adelaide),2024-03-04 17:33:20+00:00,,,,,,
Rethinking Visual Feature Extraction: Modeling Representatives from A Neural Clustering View,,,,"Guikun Chen (Zhejiang University) | Xia Li (Department Of Computer Science, ETH Zurich) | Yi Yang (Zhejiang University) | Wenguan Wang (Zhejiang University)",,,,,,,
GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds,"Sparse LiDAR point clouds cause severe loss of detail of static structures and reduce the density of static points available for navigation. Reduced density can be detrimental to navigation under several scenarios. We observe that despite high sparsity, in most cases, the global topology of LiDAR outlining the static structures can be inferred. We utilize this property to obtain a backbone skeleton of a static LiDAR scan in the form of a single connected component that is a proxy to its global topology. We utilize the backbone to augment new points along static structures to overcome sparsity. Newly introduced points could correspond to existing static structures or to static points that were earlier obstructed by dynamic objects. To the best of our knowledge, we are the first to use this strategy for sparse LiDAR point clouds. Existing solutions close to our approach fail to identify and preserve the global static LiDAR topology and generate sub-optimal points. We propose GLiDR, a Graph Generative network that is topologically regularized using 0-dimensional Persistent Homology (PH) constraints. This enables GLiDR to introduce newer static points along a topologically consistent global static LiDAR backbone. GLiDR generates precise static points using 32x sparser dynamic scans and performs better than the baselines across three datasets. The newly introduced static points allow GLiDR to outperform LiDAR-based navigation using SLAM in several settings. GLiDR generates a valuable byproduct - an accurate binary segmentation mask of static and dynamic objects that is helpful for navigation planning and safety in constrained environments.",http://arxiv.org/abs/2312.00068v1,,"Prashant Kumar (Indian Institute Of Technology Delhi) | Kshitij Madhav Bhat (Indian Institute Of Technology Indore) | Vedang Bhupesh Shenvi Nadkarni (Birla Institute Of Technology And Science Pilani (BITS Pilani)) | Prem Kalra (Indian Institute Of Technology, Delhi)",2023-11-29 20:59:00+00:00,,,,,,
MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model,"Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where an imperceptible perturbation is added to the image that can fool the DNNs. Diffusion-based adversarial purification focuses on using the diffusion model to generate a clean image against such adversarial attacks. Unfortunately, the generative process of the diffusion model is also inevitably affected by adversarial perturbation since the diffusion model is also a deep network where its input has adversarial perturbation. In this work, we propose MimicDiffusion, a new diffusion-based adversarial purification technique, that directly approximates the generative process of the diffusion model with the clean image as input. Concretely, we analyze the differences between the guided terms using the clean image and the adversarial sample. After that, we first implement MimicDiffusion based on Manhattan distance. Then, we propose two guidance to purify the adversarial perturbation and approximate the clean diffusion model. Extensive experiments on three image datasets including CIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including WideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that MimicDiffusion significantly performs better than the state-of-the-art baselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\%, 61.35\%, and 61.53\% average robust accuracy, which are 18.49\%, 13.23\%, and 17.64\% higher, respectively. The code is available in the supplementary material.",http://arxiv.org/abs/2312.04802v1,,Kaiyu Song (SUN YAT-SEN UNIVERSITY) | Hanjiang Lai (SUN YAT-SEN UNIVERSITY) | Yan Pan (SUN YAT-SEN UNIVERSITY) | Jian Yin (None),2023-12-08 02:32:47+00:00,,,,,,
Shadow-Enlightened Image Outpainting,,,,Hang Yu (Shanghai University) | Ruilin Li (None) | Shaorong Xie (Shanghai University) | Jiayan Qiu (Univerisity Of Leicester),,,,,,,
Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning,"Deep learning models for semantic segmentation often experience performance degradation when deployed to unseen target domains unidentified during the training phase. This is mainly due to variations in image texture (\ie style) from different data sources. To tackle this challenge, existing domain generalized semantic segmentation (DGSS) methods attempt to remove style variations from the feature. However, these approaches struggle with the entanglement of style and content, which may lead to the unintentional removal of crucial content information, causing performance degradation. This study addresses this limitation by proposing BlindNet, a novel DGSS approach that blinds the style without external modules or datasets. The main idea behind our proposed approach is to alleviate the effect of style in the encoder whilst facilitating robust segmentation in the decoder. To achieve this, BlindNet comprises two key components: covariance alignment and semantic consistency contrastive learning. Specifically, the covariance alignment trains the encoder to uniformly recognize various styles and preserve the content information of the feature, rather than removing the style-sensitive factor. Meanwhile, semantic consistency contrastive learning enables the decoder to construct discriminative class embedding space and disentangles features that are vulnerable to misclassification. Through extensive experiments, our approach outperforms existing DGSS methods, exhibiting robustness and superior performance for semantic segmentation on unseen target domains.",http://arxiv.org/abs/2403.06122v1,,Woo-Jin Ahn (Korea University) | Geun-Yeong Yang (Korea University) | Hyunduck Choi (Chonnam National University) | Myo-Taeg Lim (Korea University),2024-03-10 07:44:41+00:00,,,,,,
Harnessing Meta-Learning for Improving Full-Frame Video Stabilization,"Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of ""test-time adaptation"" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques. Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.",http://arxiv.org/abs/2403.03662v1,,Muhammad Kashif Ali (Hanyang University) | Eun Woo Im (Hanyang University) | Dongjin Kim (Hanyang University) | Tae Hyun Kim (Hanyang Univ.),2024-03-06 12:31:02+00:00,,,,,,
DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting,,,,Jer Pelhan (Universtiy Of Ljubljana) | Alan Lukezic (University Of Ljubljana) | Vitjan Zavrtanik (University Of Ljubljana) | Matej Kristan (University Of Ljubljana),,,,,,,
DiffusionRegPose: Enhancing Multi-Person Pose Estimation using a Diffusion-Based End-to-End Regression Approach,,,,Dayi Tan (Tongji University) | Hansheng Chen (Stanford University) | Wei Tian (Tongji University) | Lu Xiong (Tongji University),,,,,,,
Self-Calibrating Vicinal Risk Minimisation for Model Calibration,,,,Jiawei Liu (None) | Changkun Ye (Australian National University) | Ruikai Cui (Australian National University) | Nick Barnes (Australian National University),,,,,,,
Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing,"Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.",http://arxiv.org/abs/2402.17464v1,,Bi'an Du (None) | Xiang Gao (Peking University) | Wei Hu (None) | Renjie Liao (University Of British Columbia),2024-02-27 12:42:06+00:00,,,,,,
Selective nonlinearities removal from digital signals,"Many instruments performing optical and non-optical imaging and sensing, such as Optical Coherence Tomography (OCT), Magnetic Resonance Imaging or Fourier-transform spectrometry, produce digital signals containing modulations, sine-like components, which only after Fourier transformation give information about the structure or characteristics of the investigated object. Due to the fundamental physics-related limitations of such methods, the distribution of these signal components is often nonlinear and, when not properly compensated, leads to the resolution, precision or quality drop in the final image. Here, we propose an innovative approach that has the potential to allow cleaning of the signal from the nonlinearities but most of all, it now allows to switch the given order off, leaving all others intact. The latter provides a tool for more in-depth analysis of the nonlinearity-inducing properties of the investigated object, which can lead to applications in early disease detection or more sensitive sensing of chemical compounds. We consider OCT signals and nonlinearities up to the third order. In our approach, we propose two neural networks: one to remove solely the second-order nonlinearity and the other for removing solely the third-order nonlinearity. The input of the networks is a novel two-dimensional data structure with all the information needed for the network to infer a nonlinearity-free signal. We describe the developed networks and present the results for second-order and third-order nonlinearity removal in OCT data representing the images of various objects: a mirror, glass, and fruits.",http://arxiv.org/abs/2403.09731v1,,Krzysztof Maliszewski (University Of Canterbury) | Magdalena Urbanska (Massey University) | Varvara Vetrova (University Of Canterbury) | Sylwia Kolenderska (University Of Canterbury),2024-03-13 02:30:50+00:00,,,,,,
A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models,,,,"Julio Silva-Rodr??guez (ETS Montreal) | Sina Hajimiri (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Ismail Ben Ayed (ETS Montreal) | Jose Dolz (??cole De Technologie Sup??rieure)",,,,,,,
Don??t drop your samples! Coherence-aware training benefits Conditional diffusion,,,,"Nicolas Dufour (Ecole Nationale Des Ponts Et Chausees) | Victor Besnier (Valeo.Ai) | Vicky Kalogeiton (Ecole Polytechnique, IP Paris) | David Picard (None)",,,,,,,
Mosaic-SDF for 3D Generative Models,,,,Lior Yariv (Weizmann Institute Of Science) | Omri Puny (Weizmann Institute Of Science) | Oran Gafni (Meta AI) | Yaron Lipman (Facebook),,,,,,,
You Only Need Less Attention Each Stage in Vision Transformers,,,,Shuoxi Zhang (Huazhong University Of Science And Technology) | Hanpeng Liu (None) | Stephen Lin (Microsoft Research) | Kun He (Huazhong University Of Sceince And Technology),,,,,,,
TexVocab: Texture Vocabulary-conditioned Human Avatars,,,,Yuxiao Liu (None) | Zhe Li (Tsinghua University) | Yebin Liu (Tsinghua University) | Haoqian Wang (Tsinghua University),,,,,,,
Can??t make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models,,,,Himangi Mittal (None) | Nakul Agarwal (None) | Shao-Yuan Lo (Johns Hopkins University) | Kwonjoon Lee (Honda Research Institute USA),,,,,,,
DiaLoc: An Iterative Approach to Embodied Dialog Localization,"Multimodal learning has advanced the performance for many vision-language tasks. However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied. The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization. In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior. Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn. DiaLoc effectively utilizes the multimodal data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively. We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc narrows the gap between simulation and real-world applications, opening doors for future research on collaborative localization and navigation.",http://arxiv.org/abs/2403.06846v1,,Chao Zhang (Toshiba Europe Ltd) | Mohan Li (Toshiba Europe Ltd) | Ignas Budvytis (University Of Cambridge) | Stephan Liwicki (Toshiba Europe Ltd),2024-03-11 16:03:43+00:00,,,,,,
Restoration by Generation with Constrained Priors,"The inherent generative power of denoising diffusion models makes them well-suited for image restoration tasks where the objective is to find the optimal high-quality image within the generative space that closely resembles the input image. We propose a method to adapt a pretrained diffusion model for image restoration by simply adding noise to the input image to be restored and then denoise. Our method is based on the observation that the space of a generative model needs to be constrained. We impose this constraint by finetuning the generative model with a set of anchor images that capture the characteristics of the input image. With the constrained space, we can then leverage the sampling strategy used for generation to do image restoration. We evaluate against previous methods and show superior performances on multiple real-world restoration datasets in preserving identity and image quality. We also demonstrate an important and practical application on personalized restoration, where we use a personal album as the anchor images to constrain the generative space. This approach allows us to produce results that accurately preserve high-frequency details, which previous works are unable to do. Project webpage: https://gen2res.github.io.",http://arxiv.org/abs/2312.17161v1,,"Zheng Ding (University Of California, San Diego) | Xuaner Zhang (Adobe) | Zhuowen Tu (University Of California, San Diego) | Zhihao Xia (Adobe Systems)",2023-12-28 17:50:54+00:00,,,,,,
GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection,,,,"Xiaotian Li (Nanjing University Of Posts And Telecommunications) | Baojie Fan (Nanjing University Of Posts And Telecommunications) | Jiandong Tian (The Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Huijie Fan (None)",,,,,,,
In Search of a Data Transformation That Accelerates Neural Field Training,"Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hinder capturing fine details of the signal.",http://arxiv.org/abs/2311.17094v1,,Junwon Seo (None) | Sangyoon Lee (Pohang University Of Science And Technology) | Kwang In Kim (Pohang University Of Science And Technology) | Jaeho Lee (POSTECH),2023-11-28 06:17:49+00:00,,,,,,
X-3D: Explicit 3D Structure Modeling for Point Cloud Recognition,,,,Shuofeng Sun (Beijing University Of Posts And Telecommunications) | Yongming Rao (Tsinghua University) | Jiwen Lu (Tsinghua University) | Haibin Yan (Beijing University Of Posts And Telecommunications),,,,,,,
All Rivers Run to the Sea: Private Learning with Asymmetric Flows,"Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure privacy protection, the low-dimensional information-sensitive part is secured and fed to a small model in a private environment. On the other hand, the residual part is sent to fast cloud GPUs, and processed by a large model. To further enhance privacy and reduce the communication cost, Delta applies a random binary quantization technique along with a DP-based technique to the residuals before sharing them with the public platform. We theoretically show that Delta guarantees differential privacy in the public environment and greatly reduces the complexity in the private environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong privacy protection, fast training, and inference without significantly compromising the model utility.",http://arxiv.org/abs/2312.05264v1,,Yue Niu (USC) | Ramy E. Ali (Samsung) | Saurav Prakash (University Of Illinois At Urbana-Champaign) | Salman Avestimehr (University Of Southern California),2023-12-05 19:15:51+00:00,,,,,,
Deep-TROJ: An Inference Stage Trojan Insertion Algorithm through Efficient Weight Replacement Attack,,,,Sabbir Ahmed (State University Of New York At Binghamton) | RANYANG ZHOU (New Jersey Institute Of Technology) | Shaahin Angizi (New Jersey Institute Of Technology) | Adnan Rakin Rakin (None),,,,,,,
TULIP: A Multi-camera 3D Dataset for Precision Assessment of Parkinson's Disease,,,,Kyungdo Kim (Duke University) | Sihan Lyu (Duke University) | Sneha Mantri (Duke University) | Timothy DUNN (Duke University),,,,,,,
Learning to Localize Sound Sources from Mixtures without Prior Source Knowledge,,,,Dongjin Kim (Kyung Hee University) | Sung Jin Um (Kyung Hee University) | Sangmin Lee (University Of Illinois Urbana-Champaign) | Jung Uk Kim (Kyung Hee University),,,,,,,
ADA-Track: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association,,,,Shuxiao Ding (Mercedes-Benz AG & University Of Bonn) | Lukas Schneider (Mercedes Benz Research & Development) | Marius Cordts (Mercedes-Benz) | J??rgen Gall (University Of Bonn),,,,,,,
Towards Robust Learning to Optimize with Theoretical Guarantees,,,,Qingyu Song (The Chinese University Of Hong Kong) | Wei Lin (The Chinese University Of Hong Kong) | Juncheng Wang (Hong Kong Baptist University) | Hong Xu (CUHK),,,,,,,
UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures,"Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.",http://arxiv.org/abs/2401.11078v1,,"Mingyuan Zhou (Innopeak Technology) | Rakib Hyder (Oppo, Seattle, USA) | Ziwei Xuan (Innopeak Technology) | Guo-Jun Qi (University Of Central Florida)",2024-01-20 01:55:17+00:00,,,,,,
ReCoRe: Regularized Contrastive Representation Learning of World Model,"While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the naive integration of contrastive loss to world models fails due to a lack of supervisory signals to the visual encoder, as world-model-based RL methods independently optimize representation learning and agent policy. To overcome this issue, we propose an intervention-invariant regularizer in the form of an auxiliary task such as depth prediction, image denoising, etc., that explicitly enforces invariance to style-interventions. Our method outperforms current state-of-the-art model-based and model-free RL methods and significantly on out-of-distribution point navigation task evaluated on the iGibson benchmark. We further demonstrate that our approach, with only visual observations, outperforms recent language-guided foundation models for point navigation, which is essential for deployment on robots with limited computation capabilities. Finally, we demonstrate that our proposed model excels at the sim-to-real transfer of its perception module on Gibson benchmark.",http://arxiv.org/abs/2312.09056v1,,"Rudra Poudel (Toshiba Europe, Cambridge, UK) | Harit Pandya (Toshiba Europe) | Stephan Liwicki (Toshiba Europe Ltd) | Roberto Cipolla (University Of Cambridge)",2023-12-14 15:53:07+00:00,,,,,,
"Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning","Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech removal leads to worse performance. These findings highlight the importance of considering speech variation when learning scene-level audiovisual correspondences and suggest that dubbed audio can be a useful augmentation technique for training audiovisual models toward more robust performance.",http://arxiv.org/abs/2304.05600v1,,Nikhil Singh (Massachusetts Institute Of Technology) | Chih-Wei Wu (Netflix) | Iroro Orife (Netflix) | Kalayeh (None),2023-04-12 04:17:45+00:00,,,,,,
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",,,,Zhuowan Li (Johns Hopkins University) | Bhavan Jasani (Amazon) | Peng Tang (Amazon) | Shabnam Ghadar (Amazon),,,,,,,
BodyMAP - Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed,,,,Abhishek Tandon (Carnegie Mellon University) | Anujraaj Goyal (Carnegie Mellon University) | Henry M. Clever (NVIDIA) | Zackory Erickson (Carnegie Mellon University),,,,,,,
PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor,"Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneously in order to make the peer network specialized for defending the student network. We observe that such peer networks surpass the robustness of pretrained robust teacher network against student-attacked adversarial samples. With this peer network and adversarial distillation, PeerAiD achieves significantly higher robustness of the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the natural accuracy of the student network up to 4.72%p with ResNet-18 and TinyImageNet dataset.",http://arxiv.org/abs/2403.06668v1,,Jaewon Jung (Seoul National University) | Hongsun Jang (Seoul National University) | Jaeyong Song (Seoul National University) | Jinho Lee (Seoul National University),2024-03-11 12:36:14+00:00,,,,,,
CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers,"In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.",http://arxiv.org/abs/2403.07700v1,,Shahaf Arica (Technion - Israel Institute Of Technology) | Or Rubin (Technion - Israel Institute Of Technology) | Sapir Gershov (Technion - Israel Institute Of Technology) | Shlomi Laufer (Technion),2024-03-12 14:46:03+00:00,,,,,,
Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks,"While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamically updates the conditional distribution to follow the currently updated feature extractors and balance the distance between classes in feature spaces. Our experiments show that AdaRand outperforms the other fine-tuning regularization, which requires auxiliary source information and heavy computation costs.",http://arxiv.org/abs/2403.10097v1,,"Shin'ya Yamaguchi (Kyoto University) | Sekitoshi Kanai (NTT) | Kazuki Adachi (NTT) | Daiki Chijiwa (NTT, The University Of Tokyo)",2024-03-15 08:26:59+00:00,,,,,,
Evaluating Transferability in Retrieval Tasks: An Approach Using MMD and Kernel Methods,,,,Mengyu Dai (Florida State University) | Amir Hossein Raffiee (SalesForce.Com) | Aashish Jain (Salesforce) | Joshua Correa (SalesForce.Com),,,,,,,
Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion,,,,Sofia Casarin (Free University Of Bozen) | Cynthia Ugwu (Free University Of Bozen) | Sergio Escalera (Computer Vision Center) | Oswald Lanz (Free University Of Bozen-Bolzano),,,,,,,
Source-Free Domain Adaptation with Frozen Multimodal Foundation Model,"Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a target domain, with only access to unlabeled target training data and the source model pre-trained on a supervised source domain. Relying on pseudo labeling and/or auxiliary supervision, conventional methods are inevitably error-prone. To mitigate this limitation, in this work we for the first time explore the potentials of off-the-shelf vision-language (ViL) multimodal models (e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly applying the ViL model to the target domain in a zero-shot fashion is unsatisfactory, as it is not specialized for this particular task but largely generic. To make it task specific, we propose a novel Distilling multimodal Foundation model(DIFO)approach. Specifically, DIFO alternates between two steps during adaptation: (i) Customizing the ViL model by maximizing the mutual information with the target model in a prompt learning manner, (ii) Distilling the knowledge of this customized ViL model to the target model. For more fine-grained and reliable distillation, we further introduce two effective regularization terms, namely most-likely category encouragement and predictive consistency. Extensive experiments show that DIFO significantly outperforms the state-of-the-art alternatives. Code is here",http://arxiv.org/abs/2311.16510v3,,Song Tang (University Of Shanghai For Science And Technology) | Wenxin Su (University Of Shanghai For Science And Technology) | Mao Ye (University Of Electronic Science And Technology Of China) | Xiatian Zhu (University Of Surrey),2023-11-27 12:58:02+00:00,,,,,,
Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection,,,,Jin Yang (Xi'an Jiao Tong University) | Ping Wei (None) | Huan Li (Xi'an Jiao Tong University) | Ziyang Ren (Xi'an Jiao Tong University),,,,,,,
KPConvX: Modernizing Kernel Point Convolution with Kernel Attention,,,,Hugues Thomas (Apple) | Yao-Hung Hubert Tsai (Apple) | Timothy Barfoot (University Of Toronto) | Jian Zhang (Apple),,,,,,,
Equivariant plug-and-play image reconstruction,"Plug-and-play algorithms constitute a popular framework for solving inverse imaging problems that rely on the implicit definition of an image prior via a denoiser. These algorithms can leverage powerful pre-trained denoisers to solve a wide range of imaging tasks, circumventing the necessity to train models on a per-task basis. Unfortunately, plug-and-play methods often show unstable behaviors, hampering their promise of versatility and leading to suboptimal quality of reconstructed images. In this work, we show that enforcing equivariance to certain groups of transformations (rotations, reflections, and/or translations) on the denoiser strongly improves the stability of the algorithm as well as its reconstruction quality. We provide a theoretical analysis that illustrates the role of equivariance on better performance and stability. We present a simple algorithm that enforces equivariance on any existing denoiser by simply applying a random transformation to the input of the denoiser and the inverse transformation to the output at each iteration of the algorithm. Experiments on multiple imaging modalities and denoising networks show that the equivariant plug-and-play algorithm improves both the reconstruction performance and the stability compared to their non-equivariant counterparts.",http://arxiv.org/abs/2312.01831v1,,Matthieu Terris (INRIA) | Thomas Moreau (INRIA) | Nelly Pustelnik (CNRS) | Juli??n Tachella (CNRS),2023-12-04 12:07:39+00:00,,,,,,
"Selective, Interpretable and Motion Consistent Privacy Attribute Obfuscation for Action Recognition",,,,Filip Ilic (Technische Universit??t Graz) | He Zhao (York University) | Thomas Pock (Graz University Of Technology) | Richard P. Wildes (York University),,,,,,,
LAENeRF: Local Appearance Editing for Neural Radiance Fields,"Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest towards editable implicit 3D representations has surged over the last years. However, editing implicit or hybrid representations as used for NeRFs is difficult due to the entanglement of appearance and geometry encoded in the model parameters. Despite these challenges, recent research has shown first promising steps towards photorealistic and non-photorealistic appearance edits. The main open issues of related work include limited interactivity, a lack of support for local edits and large memory requirements, rendering them less useful in practice. We address these limitations with LAENeRF, a unified framework for photorealistic and non-photorealistic appearance editing of NeRFs. To tackle local editing, we leverage a voxel grid as starting point for region selection. We learn a mapping from expected ray terminations to final output color, which can optionally be supervised by a style loss, resulting in a framework which can perform photorealistic and non-photorealistic appearance editing of selected regions. Relying on a single point per ray for our mapping, we limit memory requirements and enable fast optimization. To guarantee interactivity, we compose the output color using a set of learned, modifiable base colors, composed with additive layer mixing. Compared to concurrent work, LAENeRF enables recoloring and stylization while keeping processing time low. Furthermore, we demonstrate that our approach surpasses baseline methods both quantitatively and qualitatively.",http://arxiv.org/abs/2312.09913v1,,Lukas Radl (Graz University Of Technology) | Michael Steiner (Technische Universit??t Graz) | Andreas Kurz (Technische Universit??t Graz) | Markus Steinberger (Technische Universit??t Graz),2023-12-15 16:23:42+00:00,,,,,,
TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint Video,"Neural Radiance Fields (NeRF) revolutionize the realm of visual media by providing photorealistic Free-Viewpoint Video (FVV) experiences, offering viewers unparalleled immersion and interactivity. However, the technology's significant storage requirements and the computational complexity involved in generation and rendering currently limit its broader application. To close this gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel technology that significantly reduces the storage size for Free-Viewpoint Video (FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a hybrid representation with tri-planes and voxel grids to support scaling up to long-duration sequences and scenes with complex motions or rapid changes. We propose a group training scheme tailored to achieving high training efficiency and yielding temporally consistent, low-entropy scene representations. Leveraging these properties of the representations, we introduce a compression pipeline with off-the-shelf video codecs, achieving an order of magnitude less storage size compared to the state-of-the-art. Our experiments demonstrate that TeTriRF can achieve competitive quality with a higher compression rate.",http://arxiv.org/abs/2312.06713v1,,"Minye Wu (KU Leuven) | Zehao Wang (KU Leuven) | Georgios Kouros (Department Of Electrical Engineering, KU Leuven, Belgium) | Tinne Tuytelaars (KU Leuven)",2023-12-10 23:00:24+00:00,,,,,,
IIRP-Net: Iterative Inference Residual Pyramid Network for Enhanced Image Registration,,,,Tai Ma (East China Normal University) | Zhangsuwei (East China Normal University) | Jiafeng Li (East China Normal University) | Ying Wen (East China Normal University),,,,,,,
Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features,"Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model's predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX's potential in advancing cancer diagnosis and treatment.",http://arxiv.org/abs/2403.07592v1,,Youngmin Chung (Sung Kyun Kwan University) | Ji Hun Ha (Sung Kyun Kwan University) | Kyeong Chan Im (Sungkyunkwan University) | Joo Sang Lee (Sungkyunkwan University),2024-03-12 12:25:38+00:00,,,,,,
Transferable Structural Sparse Adversarial Attack Via Exact Group Sparsity Training,,,,Di Ming (Chongqing University Of Technology) | Peng Ren (Chongqing University Of Technology) | Yunlong Wang (IQVIA) | Xin Feng (Chongqing University Of Technology),,,,,,,
LaRE2 : Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection ,,,,Yunpeng Luo (Tencent Youtu Lab) | Junlong Du (Tencent YouTu Lab) | Ke Yan (None) | Shouhong Ding (Tencent Youtu Lab),,,,,,,
CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoor Object Detection from Multi-view Images,"This paper introduces CN-RMA, a novel approach for 3D indoor object detection from multi-view images. We observe the key challenge as the ambiguity of image and 3D correspondence without explicit geometry to provide occlusion information. To address this issue, CN-RMA leverages the synergy of 3D reconstruction networks and 3D object detection networks, where the reconstruction network provides a rough Truncated Signed Distance Function (TSDF) and guides image features to vote to 3D space correctly in an end-to-end manner. Specifically, we associate weights to sampled points of each ray through ray marching, representing the contribution of a pixel in an image to corresponding 3D locations. Such weights are determined by the predicted signed distances so that image features vote only to regions near the reconstructed surface. Our method achieves state-of-the-art performance in 3D object detection from multi-view images, as measured by mAP@0.25 and mAP@0.5 on the ScanNet and ARKitScenes datasets. The code and models are released at https://github.com/SerCharles/CN-RMA.",http://arxiv.org/abs/2403.04198v1,,Guanlin Shen (Tsinghua University) | Jingwei Huang (Huawei Technologies Ltd.) | Zhihua Hu (Nanjing University Of Information Science And Technology) | Bin Wang (Tsinghua University),2024-03-07 03:59:47+00:00,,,,,,
EarthLoc: Astronaut Photography Localization by Indexing Earth from Space,"Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite its significance, accurately localizing the geographical extent of these images, crucial for effective utilization, poses substantial challenges. Current manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach - leveraging image retrieval - to address this challenge efficiently. We introduce innovative training techniques, including Year-Wise Data Augmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the development of a high-performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations data. Code and datasets are available at https://github.com/gmberton/EarthLoc",http://arxiv.org/abs/2403.06758v1,,Gabriele Berton (None) | Alex Stoken (University Of Texas At Austin) | Barbara Caputo (Politecnico Di Torino) | Carlo Masone (Politecnico Di Torino),2024-03-11 14:30:51+00:00,,,,,,
The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement,,,,Gabriele Trivigno (None) | Carlo Masone (Politecnico Di Torino) | Barbara Caputo (Politecnico Di Torino) | Torsten Sattler (Czech Technical University In Prague),,,,,,,
Event-based Structure-from-Orbit,,,,Ethan Elms (University Of Adelaide) | Yasir Latif (The University Of Adelaide) | Tae Ha Park (Stanford University) | Tat-Jun Chin (None),,,,,,,
Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining,,,,Qi Cui (Nanyang Technological University) | Ruohan Meng (Nanyang Technological University) | Chaohui Xu (Nanyang Technological University) | Chip Hong Chang (Nanyang Technological University),,,,,,,
Delving into the Trajectory Long-tail Distribution for Muti-object Tracking,"Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as ""pedestrians trajectory long-tail distribution"". Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.",http://arxiv.org/abs/2403.04700v1,,Sijia Chen (Huazhong University Of Science And Technology) | En Yu (Huazhong University Of Science And Technology) | Jinyang Li (Huazhong University Of Science And Technology) | Wenbing Tao (Huazhong University Of Science And Technology),2024-03-07 17:48:47+00:00,,,,,,
On The Vulnerability of Efficient Vision Transformers to Adversarial Computation Attacks,,,,"Navaneet K L (University Of California, Davis) | Soroush Abbasi Koohpayegani (University Of California, Davis) | Essam Sleiman (Harvard University) | Hamed Pirsiavash (University Of California, Davis)",,,,,,,
ATTA: Label-Free Accuracy Estimation for Test-Time Adaptation,,,,Taeckyung Lee (KAIST) | Sorn Chottananurak (KAIST) | Taesik Gong (Bell Labs) | Sung-Ju Lee (Korea Advanced Institute Of Science & Technology),,,,,,,
Enhancing Multi-modal Cooperation via Sample-level Modality Valuation,"One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this issue and improve cooperation between modalities by enhancing the discriminative ability of low-contributing modalities in a targeted manner. Overall, our methods reasonably observe the fine-grained uni-modal contribution at sample-level and achieve considerable improvement on different multi-modal models.",http://arxiv.org/abs/2309.06255v2,,Yake Wei (Renmin University Of China) | Ruoxuan Feng (Renmin University Of China) | Zihe Wang (Renmin University Of China) | Di Hu (Renmin University Of China),2023-09-12 14:16:34+00:00,,,,,,
FedKTL: An Upload-Efficient Knowledge Transfer Scheme With a Pre-trained Generator in Heterogeneous Federated Learning,,,,Jianqing Zhang (Shanghai Jiao Tong University & Tsinghua University) | Yang Liu (Tsinghua University) | Yang Hua (Queen's University Belfast) | Jian Cao (Shanghai Jiao Tong University),,,,,,,
Generalizable Face Landmarking Guided by Conditional Face Warping,,,,Jiayi Liang (Beijing Institute Of Technology) | Haotian Liu (Beijing Institute Of Technology) | Hongteng Xu (Renmin University Of China) | Dixin Luo (Beijing Institute Of Technology),,,,,,,
Correcting Diffusion Generation through Resampling,"Despite diffusion models' superior capabilities in modeling complex distributions, there are still non-trivial distributional discrepancies between generated and ground-truth images, which has resulted in several notable problems in image generation, including missing object errors in text-to-image generation and low image quality. Existing methods that attempt to address these problems mostly do not tend to address the fundamental cause behind these problems, which is the distributional discrepancies, and hence achieve sub-optimal results. In this paper, we propose a particle filtering framework that can effectively address both problems by explicitly reducing the distributional discrepancies. Specifically, our method relies on a set of external guidance, including a small set of real images and a pre-trained object detector, to gauge the distribution gap, and then design the resampling weight accordingly to correct the gap. Experiments show that our methods can effectively correct missing object errors and improve image quality in various image generation tasks. Notably, our method outperforms the existing strongest baseline by 5% in object occurrence and 1.0 in FID on MS-COCO. Our code is publicly available at https://github.com/UCSB-NLP-Chang/diffusion_resampling.git.",http://arxiv.org/abs/2312.06038v1,,"Yujian Liu (University Of California, Santa Barbara) | Yang Zhang (International Business Machines) | Tommi Jaakkola (Massachusetts Institute Of Technology) | Shiyu Chang (UC Santa Barbara)",2023-12-10 23:35:13+00:00,,,,,,
Byzantine-robust Decentralized Federated Learning via Dual-domain Clustering and Trust Bootstrapping,,,,Peng Sun (Hunan University) | Xinyang Liu (Hong Kong Polytechnic University) | Zhibo Wang (Zhejiang University) | Bo Liu (Shenzhen Institute Of Artificial Intelligence And Robotics For Society),,,,,,,
Hyper-MD: Mesh Denoising with Customized Parameters Aware of Noise Intensity and Geometric Characteristics,,,,Xingtao Wang (Harbin Institute Of Technology) | Hongliang Wei (Harbin Institute Of Technology) | Xiaopeng Fan (Harbin Institute Of Technology) | Debin Zhao (Harbin Institute Of Technology),,,,,,,
TE-TAD: Towards Fully End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression,,,,Ho-Joong Kim (Korea University) | Jung-Ho Hong (Korea University) | Heejo Kong (Korea University) | Seong-Whan Lee (Korea University),,,,,,,
Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos,,,,"Leonhard Sommer (University Of Freiburg, Albert-Ludwigs-Universit??t Freiburg) | Artur Jesslen (University Of Freiburg) | Eddy Ilg (None) | Adam Kortylewski (University Of Freiburg & MPI-INF)",,,,,,,
One-Shot Structure-Aware Stylized Image Synthesis,"While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models.",http://arxiv.org/abs/2402.17275v1,,Hansam Cho (Korea University) | Jonghyun Lee (Korea University) | Seunggyu Chang (NAVER Cloud) | Yonghyun Jeong (NAVER),2024-02-27 07:42:55+00:00,,,,,,
One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models,"Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",http://arxiv.org/abs/2403.01849v1,,"Lin Li (King's College London) | Haoyan Guan (King's College London, University Of London) | Jianing Qiu (Imperial College London) | Michael Spratling (King's College London And University Of Luxembourg)",2024-03-04 08:59:32+00:00,,,,,,
Efficient Hyperparameter Optimization with Adaptive Fidelity Identification,,,,Jiantong Jiang (The University Of Western Australia) | Zeyi Wen (Hong Kong University Of Science And Technology (Guangzhou)) | Atif Mansoor (University Of Western Australia) | Ajmal Mian (University Of Western Australia),,,,,,,
Multimodal Representation Learning by Alternating Unimodal Adaptation,"Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time uncertainty-based model fusion mechanism to integrate multimodal information. Extensive experiments are conducted on five diverse datasets, encompassing scenarios with complete modalities and scenarios with missing modalities. These experiments demonstrate the superiority of MLA over competing prior approaches.",http://arxiv.org/abs/2311.10707v1,,"Xiaohui Zhang (Beijing Jiao Tong University) | Jaehong Yoon (University Of North Carolina At Chapel Hill) | Mohit Bansal (University Of North Carolina At Chapel Hill) | Huaxiu Yao (Department Of Computer Science, University Of North Carolina At Chapel Hill)",2023-11-17 18:57:40+00:00,,,,,,
FedMef: Towards Memory-efficient Federated Dynamic Pruning,,,,Hong Huang (City University Of Hong Kong) | Weiming Zhuang (Sony Research) | Chen Chen (Sony AI) | Lingjuan Lyu (Sony AI),,,,,,,
iKUN: Speak to Trackers without Retraining,"Referring multi-object tracking (RMOT) aims to track multiple objects based on input textual descriptions. Previous works realize it by simply integrating an extra textual module into the multi-object tracker. However, they typically need to retrain the entire framework and have difficulties in optimization. In this work, we propose an insertable Knowledge Unification Network, termed iKUN, to enable communication with off-the-shelf trackers in a plug-and-play manner. Concretely, a knowledge unification module (KUM) is designed to adaptively extract visual features based on textual guidance. Meanwhile, to improve the localization accuracy, we present a neural version of Kalman filter (NKF) to dynamically adjust process noise and observation noise based on the current motion status. Moreover, to address the problem of open-set long-tail distribution of textual descriptions, a test-time similarity calibration method is proposed to refine the confidence score with pseudo frequency. Extensive experiments on Refer-KITTI dataset verify the effectiveness of our framework. Finally, to speed up the development of RMOT, we also contribute a more challenging dataset, Refer-Dance, by extending public DanceTrack dataset with motion and dressing descriptions. The codes and dataset are available at https://github.com/dyhBUPT/iKUN.",http://arxiv.org/abs/2312.16245v2,,Yunhao Du (Beijing University Of Posts And Telecommunications) | Cheng Lei (Beijing University Of Posts And Telecommunications) | Zhicheng Zhao (Beijing University Of Posts And Telecommunications) | Fei Su (Beijing University Of Posts And Telecommunications),2023-12-25 11:48:55+00:00,,,,,,
Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation,"This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.",http://arxiv.org/abs/2403.03890v1,,Xiao Ma (SEA AI Lab) | Sumit Patidar (Dyson) | Iain Haughton (Dyson Ltd) | Stephen James (Dyson),2024-03-06 17:50:26+00:00,,,,,,
Makeup Prior Models for 3D Facial Makeup Estimation and Applications,,,,Xingchao Yang (Cyberagent) | Takafumi Taketomi (CyberAgent) | Yuki Endo (University Of Tsukuba) | Yoshihiro Kanamori (University Of Tsukuba),,,,,,,
MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models,,,,Yanting Wang (Pennsylvania State University) | Hongye Fu (Zhejiang University) | Wei Zou (Pennsylvania State University) | Jinyuan Jia (Pennsylvania State University),,,,,,,
Data Poisoning based Backdoor Attacks to Contrastive Learning,,,,"Jinghuai Zhang (University Of California, Los Angeles (UCLA)) | Hongbin Liu (Duke University) | Jinyuan Jia (Pennsylvania State University) | Neil Zhenqiang Gong (Duke University)",,,,,,,
Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in a Neural Radiance Field,,,,Joshua Ahn (University Of Chicago) | Haochen Wang (Toyota Technological Institute At Chicago) | Raymond A. Yeh (Purdue University) | Greg Shakhnarovich (Toyota Technological Institute At Chicago),,,,,,,
FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication,,,,Eric Slyman (Oregon State University) | Stefan Lee (Oregon State University) | Scott Cohen (Adobe Systems) | Kushal Kafle (Adobe Systems),,,,,,,
GlitchBench: Can large multimodal models detect video game glitches?,"Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the capacity of LLMs for tasks requiring visual comprehension and reasoning. However, the extent and limitations of their enhanced abilities are not fully understood, especially when it comes to real-world tasks. To address this gap, we introduce GlitchBench, a novel benchmark derived from video game quality assurance tasks, to test and evaluate the reasoning capabilities of LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios from video games and aims to challenge both the visual and linguistic reasoning powers of LMMs in detecting and interpreting out-of-the-ordinary events. We evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents a new challenge for these models. Code and data are available at: https://glitchbench.github.io/",http://arxiv.org/abs/2312.05291v1,,Mohammad Reza Taesiri (University Of Alberta) | Tianjun Feng (University Of Alberta) | Cor-Paul Bezemer (University Of Alberta) | Anh Nguyen (Auburn University),2023-12-08 18:14:21+00:00,,,,,,
SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder,,,,Dihan Zheng (Tsinghua University) | Yihang Zou (Tsinghua University) | Xiaowen Zhang (Hisilicon) | Chenglong Bao (Tsinghua University),,,,,,,
A theory of volumetric representations for opaque solids,"We develop a theory for the representation of opaque solids as volumetric models. Starting from a stochastic representation of opaque solids as random indicator functions, we prove the conditions under which such solids can be modeled using exponential volumetric transport. We also derive expressions for the volumetric attenuation coefficient as a functional of the probability distributions of the underlying indicator functions. We generalize our theory to account for isotropic and anisotropic scattering at different parts of the solid, and for representations of opaque solids as implicit surfaces. We derive our volumetric representation from first principles, which ensures that it satisfies physical constraints such as reciprocity and reversibility. We use our theory to explain, compare, and correct previous volumetric representations, as well as propose meaningful extensions that lead to improved performance in 3D reconstruction tasks.",http://arxiv.org/abs/2312.15406v1,,"Bailey Miller (Carnegie Mellon University) | Hanyu Chen (CMU, Carnegie Mellon University) | Alice Lai (Carnegie Mellon University) | Ioannis Gkioulekas (Carnegie Mellon University)",2023-12-24 04:49:06+00:00,,,,,,
Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning,,,,Yun Li (CSIRO's Data61) | Zhe Liu (Tiktok) | Hang Chen (Snap) | Lina Yao (CSIRO's Data61 And University Of New South Wales),,,,,,,
Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image Registration,,,,Mingyuan Meng (The University Of Sydney) | Dagan Feng (University Of Sydney) | Lei Bi (The University Of Sydney) | Jinman Kim (University Of Sydney),,,,,,,
Advancing Chemical Structure Recognition in Hand-Drawn Images by Atom-Level Entity Localization,,,,Martijn Oldenhof (KU Leuven) | Edward De Brouwer (Yale University) | Adam Arany (KU Leuven) | Yves Moreau (University Of Leuven),,,,,,,
NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-efficient Scene Representation,,,,Sicheng Li (Zhejiang University) | Hao Li (None) | Yiyi Liao (Zhejiang University) | Lu Yu (Zhejiang University),,,,,,,
Uncertainty-Driven Continual Learning for Autonomous Driving,,,,"Lei Lai (Boston University) | Eshed Ohn-Bar (Boston University) | Sanjay Arora (Red Hat,) | John Yi (Boston University)",,,,,,,
Hyperbolic Anomaly Detection,,,,Huimin Li (Beihang University) | Zhentao Chen (Beihang University) | Yunhao Xu (Beihang University) | Junlin Hu (Beihang University),,,,,,,
Mind marginal non-crack regions: Clustering-inspired representation learning for crack segmentation,,,,Zhuangzhuang Chen (Shenzhen University) | Zhuonan Lai (Shenzhen University) | Jie Chen (Shenzhen University) | Jianqiang Li (Shenzhen University),,,,,,,
Pick-or-Mix: Dynamic Channel Sampling for ConvNets,,,,"Ashish Kumar (Indian Institute Of Technology, Kanpur) | Daneul Kim (Seoul National University) | Jaesik Park (Seoul National University) | Laxmidhar Behera (Indian Institute Of Technology , Kanpur)",,,,,,,
HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud,,,,WENCAN CHENG (None) | Hao Tang (ETH Zurich And CMU) | Luc Van Gool (ETH Zurich) | Jong Hwan Ko (Sungkyunkwan University (SKKU)),,,,,,,
DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling,"Recent methods such as Score Distillation Sampling (SDS) and Variational Score Distillation (VSD) using 2D diffusion models for text-to-3D generation have demonstrated impressive generation quality. However, the long generation time of such algorithms significantly degrades the user experience. To tackle this problem, we propose DreamPropeller, a drop-in acceleration algorithm that can be wrapped around any existing text-to-3D generation pipeline based on score distillation. Our framework generalizes Picard iterations, a classical algorithm for parallel sampling an ODE path, and can account for non-ODE paths such as momentum-based gradient updates and changes in dimensions during the optimization process as in many cases of 3D generation. We show that our algorithm trades parallel compute for wallclock time and empirically achieves up to 4.7x speedup with a negligible drop in generation quality for all tested frameworks.",http://arxiv.org/abs/2311.17082v2,,Linqi Zhou (Stanford University) | Andy Shih (Stanford University) | Chenlin Meng (None) | Stefano Ermon (Stanford University),2023-11-28 01:28:58+00:00,,,,,,
Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation,,,,"Hongwei Yan (Tsinghua University) | Liyuan Wang (Tsinghua University) | Kaisheng Ma (Institute For Interdisciplinary Information Sciences (IIIS), Tsinghua University) | Yi Zhong (Tsinghua University)",,,,,,,
Instance-level Expert Knowledge and Aggregate Discriminative Attention for Radiology Report Generation,,,,Shenshen Bu (Sun Yat-Sen University) | Taiji Li (SUN YAT-SEN UNIVERSITY) | Zhiming Dai (SUN YAT-SEN UNIVERSITY) | Yuedong Yang (SUN YAT-SEN UNIVERSITY),,,,,,,
Fast ODE-based Sampling for Diffusion Models in Around 5 Steps,"Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-based samplers. Extensive experiments on image synthesis with the resolution ranging from 32 to 256 demonstrate the effectiveness of our method. With only 5 NFE, we achieve 7.14 FID on CIFAR-10, 13.75 FID on ImageNet 64$\times$64, and 12.79 FID on LSUN Bedroom. Our code is available at https://github.com/zhyzhouu/amed-solver.",http://arxiv.org/abs/2312.00094v1,,Zhenyu Zhou (Zhejiang University) | Defang Chen (Zhejiang University) | Can Wang (Zhejiang University) | Chun Chen (Zhejiang University),2023-11-30 13:07:19+00:00,,,,,,
Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching,"Non-isometric shape correspondence remains a fundamental challenge in computer vision. Traditional methods using Laplace-Beltrami operator (LBO) eigenmodes face limitations in characterizing high-frequency extrinsic shape changes like bending and creases. We propose a novel approach of combining the non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in which we construct functional maps. To this end, we present a theoretical framework to effectively integrate non-orthogonal basis functions into descriptor- and learning-based functional map methods. Our approach can be incorporated easily into existing functional map pipelines across varying applications and is able to handle complex deformations beyond isometries. We show extensive evaluations across various supervised and unsupervised settings and demonstrate significant improvements. Notably, our approach achieves up to 15% better mean geodesic error for non-isometric correspondence settings and up to 45% improvement in scenarios with topological noise.",http://arxiv.org/abs/2312.03678v1,,Lennart Bastian (None) | Yizheng Xie (Technische Universit??t M??nchen) | Nassir Navab (TU Munich) | Zorah L??hner (Rheinische Friedrich-Wilhelms Universit??t Bonn),2023-12-06 18:41:01+00:00,,,,,,
Video Frame Interpolation via Direct Synthesis with the Event-based Reference,,,,Yuhan Liu (None) | Yongjian Deng (Beijing University Of Technology) | Hao Chen (Southeast University) | Zhen Yang (Beijing University Of Technology),,,,,,,
SIRA: Scalable Inter-frame Relation and Association for Radar Perception,,,,Ryoma Yataka (Mitsubishi Electric Research Laboratories) | Pu (Perry) Wang (None) | Petros Boufounos (Mitsubishi Electric Research Laboratories) | Ryuhei Takahashi (Mitsubishi Electric Corporation),,,,,,,
Learning Instance-Aware Correspondences for Robust Multi-Instance Point Cloud Registration in Cluttered Scenes,,,,Zhiyuan Yu (Na) | Zheng Qin (National University Of Defense Technology) | Lintao Zheng (National University Of Defense Technology) | Kai Xu (National University Of Defense Technology),,,,,,,
Fine-grained Prototypical Voting with Heterogeneous Mixup for Semi-supervised 2D-3D Cross-modal Retrieval,,,,"Fan Zhang (Georgia Institute Of Technology) | Xian-Sheng Hua (Terminus Group) | Chong Chen (Terminus Group) | Xiao Luo (University Of California, Los Angeles)",,,,,,,
PaintNeSF: Artistic Creation of Stylized Scenes with Vectorized 3D Strokes,,,,Haobin Duan (Beihang University) | Miao Wang (Beihang University) | Yanxun Li (Buaa Software Engineering) | Yong-Liang Yang (University Of Bath),,,,,,,
PIGEON: Predicting Image Geolocations,"Planet-scale image geolocalization remains a challenging problem due to the diversity of images originating from anywhere in the world. Although approaches based on vision transformers have made significant progress in geolocalization accuracy, success in prior literature is constrained to narrow distributions of images of landmarks, and performance has not generalized to unseen places. We present a new geolocalization system that combines semantic geocell creation, multi-task contrastive pretraining, and a novel loss function. Additionally, our work is the first to perform retrieval over location clusters for guess refinements. We train two models for evaluations on street-level data and general-purpose image geolocalization; the first model, PIGEON, is trained on data from the game of Geoguessr and is capable of placing over 40% of its guesses within 25 kilometers of the target location globally. We also develop a bot and deploy PIGEON in a blind experiment against humans, ranking in the top 0.01% of players. We further challenge one of the world's foremost professional Geoguessr players to a series of six matches with millions of viewers, winning all six games. Our second model, PIGEOTTO, differs in that it is trained on a dataset of images from Flickr and Wikipedia, achieving state-of-the-art results on a wide range of image geolocalization benchmarks, outperforming the previous SOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8 percentage points on the country level. Our findings suggest that PIGEOTTO is the first image geolocalization model that effectively generalizes to unseen places and that our approach can pave the way for highly accurate, planet-scale image geolocalization systems. Our code is available on GitHub.",http://arxiv.org/abs/2307.05845v4,,Lukas Haas (Stanford University) | Michal Skreta (Stanford University) | Silas Alberti (Stanford University) | Chelsea Finn (Stanford University),2023-07-11 23:36:49+00:00,,,,,,
Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes,"In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in a holistic understanding of both multiple road users' motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To address the limitation, we collect a synthetic dataset called TACO, which is four times larger than OATS and features a balanced distribution of atomic activities. To validate the effectiveness of our method, we conduct comprehensive experiments and ablation studies against various action recognition baselines. We also show that the performance of multi-label atomic activity recognition on real-world datasets can be improved by pretraining representations on TACO. We will release our source code and dataset. See the videos of visualization on the project page: https://hcis-lab.github.io/Action-slot/",http://arxiv.org/abs/2311.17948v1,,Chi-Hsi Kung (National Yang Ming Chiao Tung University) | ?????? ??? (National Yang Ming Chiao Tung University) | Yi-Hsuan Tsai (Google) | Yi-Ting Chen (National Yang Ming Chiao Tung University),2023-11-29 05:28:05+00:00,,,,,,
C3Net: Compound Conditioned ControlNet for Multimodal Content Generation,"We present Compound Conditioned ControlNet, C3Net, a novel generative neural architecture taking conditions from multiple modalities and synthesizing multimodal contents simultaneously (e.g., image, text, audio). C3Net adapts the ControlNet architecture to jointly train and make inferences on a production-ready diffusion model and its trainable copies. Specifically, C3Net first aligns the conditions from multi-modalities to the same semantic latent space using modality-specific encoders based on contrastive training. Then, it generates multimodal outputs based on the aligned latent space, whose semantic information is combined using a ControlNet-like architecture called Control C3-UNet. Correspondingly, with this system design, our model offers an improved solution for joint-modality generation through learning and explaining multimodal conditions instead of simply taking linear interpolations on the latent space. Meanwhile, as we align conditions to a unified latent space, C3Net only requires one trainable Control C3-UNet to work on multimodal semantic information. Furthermore, our model employs unimodal pretraining on the condition alignment stage, outperforming the non-pretrained alignment even on relatively scarce training data and thus demonstrating high-quality compound condition generation. We contribute the first high-quality tri-modal validation set to validate quantitatively that C3Net outperforms or is on par with first and contemporary state-of-the-art multimodal generation. Our codes and tri-modal dataset will be released.",http://arxiv.org/abs/2311.17951v1,,Juntao Zhang (Hong Kong University Of Science And Technology) | Yuehuai LIU (Hong Kong University Of Science And Technology) | Yu-Wing Tai (None) | Chi-Keung Tang (The Hong Kong University Of Science And Technology),2023-11-29 07:11:56+00:00,,,,,,
Iterated Learning Improves Compositionality in Large Vision-Language Models,,,,"Chenhao Zheng (University Of Michigan) | Jieyu Zhang (Department Of Computer Science, University Of Washington) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence) | Ranjay Krishna (University Of Washington)",,,,,,,
Domain Gap Embeddings for Generative Dataset Augmentation,,,,Yinong Wang (None) | Younjoon Chung (Carnegie Mellon University) | Chen Henry Wu (Carnegie Mellon University) | Fernando De La Torre (Carnegie Mellon),,,,,,,
SANeRF-HQ: Segment Anything for NeRF in High Quality,"Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Additional information can be found at https://lyclyc52.github.io/SANeRF-HQ/.",http://arxiv.org/abs/2312.01531v1,,Yichen Liu (HKUST) | Benran Hu (The Hong Kong University Of Science And Technology) | Chi-Keung Tang (The Hong Kong University Of Science And Technology) | Yu-Wing Tai (None),2023-12-03 23:09:38+00:00,,,,,,
OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation,,,,Ganlong Zhao (University Of Hong Kong) | Guanbin Li (Sun Yat-Sen University) | Weikai Chen (Tencent America) | Yizhou Yu (The University Of Hong Kong),,,,,,,
Unlocking Pretrained Image Backbones for Semantic Image Synthesis,"Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, which we dub DP-SIMS, achieves state-of-the-art results in terms of image quality and consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes, surpassing recent diffusion models while requiring two orders of magnitude less compute for inference.",http://arxiv.org/abs/2312.13314v2,,Tariq Berrada (Meta) | Jakob Verbeek (Meta AI) | Camille Couprie (Facebook) | Karteek Alahari (Inria),2023-12-20 09:39:19+00:00,,,,,,
Infer from What You Have Seen Before: Temporally-dependent Classifier for Semi-supervised Video Semantic Segmentation,,,,Jiafan Zhuang (Shantou University) | Zilei Wang (University Of Science And Technology Of China) | Yixin Zhang (University Of Science And Technology Of China) | Zhun Fan (Shantou University),,,,,,,
Unlocking the Potential of Pre-trained Vision Transformers for Few-Shot Semantic Segmentation through Relationship Descriptors,,,,Ziqin Zhou (None) | Hai-Ming Xu (The University Of Adelaide) | Yangyang Shu (None) | Lingqiao Liu (None),,,,,,,
Relational Matching for Weakly Semi-Supervised Oriented Object Detection,,,,Wenhao Wu (City University Of Hong Kong) | Hau San Wong (City University Of Hong Kong) | Si Wu (South China University Of Technology) | Tianyou Zhang (South China University Of Technology),,,,,,,
3D LiDAR Mapping in Dynamic Environments using a 4D Implicit Neural Representation,,,,Xingguang Zhong (Rheinische Friedrich-Wilhelms Universit??t Bonn) | Yue Pan (University Of Bonn) | Cyrill Stachniss (University Of Bonn) | Jens Behley (University Of Bonn),,,,,,,
ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models,,,,Xinyu Tian (Australian National University) | Shu Zou (Australian National University) | Zhaoyuan Yang (General Electric) | Jing Zhang (Australian National University),,,,,,,
Coherence As Texture -- Passive Textureless 3D Reconstruction by Self-interference,,,,Wei-Yu Chen (Carnegie Mellon University) | Aswin C. Sankaranarayanan (Carnegie Mellon University) | Anat Levin (Weizmann Institute Of Science) | Matthew O??Toole (Carnegie Mellon University),,,,,,,
A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives,"Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods.",http://arxiv.org/abs/2403.03037v1,,Simone Peirone (Polytechnic Institute Of Turin) | Francesca Pistilli (Polytechnic Institute Of Turin) | Antonio Alliegro (Politecnico Di Torino) | Giuseppe Averta (Polytechnic Of Turin),2024-03-05 15:18:02+00:00,,,,,,
Not All Classes Stand on Same Embeddings: Calibrating a Semantic Distance with Metric Tensor,,,,Jae Park Park (None) | Gyoomin Lee (Dongguk University) | Seunggi Park (Dongguk University) | Sung In Cho (Dongguk University),,,,,,,
Pose Adapted Shape Learning for Large-Pose Face Reenactment,,,,Gee-Sern Hsu (None) | Jie-Ying Zhang (National Taiwan University Of Science And Technology) | Yu-Hsiang Huang (National Taiwan University Of Science And Technology) | Wei-Jie Hong (National Taiwan University Of Science And Technology),,,,,,,
Improving Out-of-Distribution Generalization in Graphs via Hierarchical Semantic Environments,"Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attention mechanisms are employed to re-extract the subgraphs for regenerating global environments in a hierarchical manner. In addition, we introduce a new learning objective that guides our model to learn the diversity of environments within the same hierarchy while maintaining consistency across different hierarchies. This approach enables our model to consider the relationships between environments and facilitates robust graph invariant learning. Extensive experiments on real-world graph data have demonstrated the effectiveness of our framework. Particularly, in the challenging dataset DrugOOD, our method achieves up to 1.29\% and 2.83\% improvement over the best baselines on IC50 and EC50 prediction tasks, respectively.",http://arxiv.org/abs/2403.01773v1,,Yinhua Piao (Seoul National University) | Sangseon Lee (Seoul National University) | Yijingxiu Lu (Seoul National University) | Sun Kim (Seoul National University),2024-03-04 07:03:10+00:00,,,,,,
Retrieval-Augmented Embodied Agents,,,,"Yichen Zhu (Midea Group) | Zhicai Ou (AI Innovation Center, Midea Group) | Xiaofeng Mou (Midea Group) | Jian Tang (Midea Group)",,,,,,,
Differentiable Information Bottleneck for Deterministic Multi-view Clustering,,,,Xiaoqiang Yan (None) | Zhixiang Jin (Zhengzhou University) | Fengshou Han (Zhengzhou University) | Yangdong Ye (Zhengzhou University),,,,,,,
Zero-Reference Low-Light Enhancement via Physical Quadruple Priors,,,,Wenjing Wang (Peking University) | Huan Yang (Microsoft) | Jianlong Fu (Microsoft) | Jiaying Liu (Peking University),,,,,,,
Boosting Adversarial Transferability by Block Shuffle and Rotation,"Adversarial examples mislead deep neural networks with imperceptible perturbations and have brought significant threats to deep learning. An important aspect is their transferability, which refers to their ability to deceive other models, thus enabling attacks in the black-box setting. Though various methods have been proposed to boost transferability, the performance still falls short compared with white-box attacks. In this work, we observe that existing input transformation based attacks, one of the mainstream transfer-based attacks, result in different attention heatmaps on various models, which might limit the transferability. We also find that breaking the intrinsic relation of the image can disrupt the attention heatmap of the original image. Based on this finding, we propose a novel input transformation based attack called block shuffle and rotation (BSR). Specifically, BSR splits the input image into several blocks, then randomly shuffles and rotates these blocks to construct a set of new images for gradient calculation. Empirical evaluations on the ImageNet dataset demonstrate that BSR could achieve significantly better transferability than the existing input transformation based methods under single-model and ensemble-model settings. Combining BSR with the current input transformation method can further improve the transferability, which significantly outperforms the state-of-the-art methods.",http://arxiv.org/abs/2308.10299v2,,Kunyu Wang (The Chinese University Of Hong Kong) | He Xuanran (TikTok) | Wenxuan Wang (The Chinese University Of Hong Kong) | Xiaosen Wang (Huazhong University Of Science And Technology),2023-08-20 15:38:40+00:00,,,,,,
Bayesian Exploration of Pre-trained Models for Low-shot Image Classification,,,,Yibo Miao (Shanghai Jiao Tong University) | Yu Lei (Shanghai Jiao Tong University) | Feng Zhou (Renmin University Of China) | Zhijie Deng (Shanghai Jiao Tong University),,,,,,,
HashPoint: Accelerated Point Searching and Sampling for Neural Rendering,,,,Jiahao Ma (None) | Miaomiao Liu (Australian National University) | David Ahmedt-Aristizabal (CSIRO) | Chuong Nguyen (None),,,,,,,
Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening,,,,Yule Duan (University Of Electronic Science And Technology Of China) | Xiao Wu (University Of Electronic Science And Technology Of China) | Haoyu Deng (University Of Electronic Science And Technology Of China) | Liang-Jian Deng (University Of Electronic Science And Technology Of China),,,,,,,
SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers,"Unsupervised object-centric learning aims to decompose scenes into interpretable object entities, termed slots. Slot-based auto-encoders stand out as a prominent method for this task. Within them, crucial aspects include guiding the encoder to generate object-specific slots and ensuring the decoder utilizes them during reconstruction. This work introduces two novel techniques, (i) an attention-based self-training approach, which distills superior slot-based attention masks from the decoder to the encoder, enhancing object segmentation, and (ii) an innovative patch-order permutation strategy for autoregressive transformers that strengthens the role of slot vectors in reconstruction. The effectiveness of these strategies is showcased experimentally. The combined approach significantly surpasses prior slot-based autoencoder methods in unsupervised object segmentation, especially with complex real-world images. We provide the implementation code at https://github.com/gkakogeorgiou/spot .",http://arxiv.org/abs/2312.00648v1,,"Ioannis Kakogeorgiou (National Technical University Of Athens) | Spyros Gidaris (Valeo.Ai) | Konstantinos Karantzalos (IMIS - ""Athena"" Research Center) | Nikos Komodakis (University Of Crete)",2023-12-01 15:20:58+00:00,,,,,,
DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement,,,,Hao Wu (University Of Science And Technology Of China) | Huabin Liu (Shanghai Jiao Tong University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Xiao Sun (Shanghai Artificial Intelligence Laboratory),,,,,,,
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning,"Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples with high precision. Extensive experiments demonstrate the superiority of CroSel, which consistently outperforms previous state-of-the-art methods on benchmark datasets. Additionally, our method achieves over 90\% accuracy and quantity for selecting true labels on CIFAR-type datasets under various settings.",http://arxiv.org/abs/2303.10365v2,,Shiyu Tian (Chongqing University) | Hongxin Wei (Southern University Of Science And Technology) | Yiqun Wang (Chongqing University) | Lei Feng (Nanyang Technological University),2023-03-18 08:48:16+00:00,,,,,,
Visual Geometry Grounded Deep Structure From Motion,"Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.",http://arxiv.org/abs/2312.04563v1,,Jianyuan Wang (Oxford VGG) | Nikita Karaev (University Of Oxford) | Christian Rupprecht (University Of Oxford) | David Novotny (Facebook),2023-12-07 18:59:52+00:00,,,,,,
ESR-NeRF: Emissive Source Reconstruction Using LDR Multi-view Images,,,,Jinseo Jeong (Seoul National University) | Junseo Koo (Seoul National University) | Qimeng Zhang (Korea University) | Gunhee Kim (Seoul National University),,,,,,,
3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation,"In this work we develop 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This synergistic approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture a variety of shapes within different semantic regions. Project page: https://threedle.github.io/3d-paintbrush",http://arxiv.org/abs/2311.09571v1,,Dale Decatur (University Of Chicago) | Itai Lang (University Of Chicago & Tel Aviv University) | Kfir Aberman (Google) | Rana Hanocka (University Of Chicago),2023-11-16 05:13:44+00:00,,,,,,
Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It,"Data leakage is a critical issue when training and evaluating any method based on supervised learning. The state-of-the-art methods for online mapping are based on supervised learning and are trained predominantly using two datasets: nuScenes and Argoverse 2. These datasets revisit the same geographic locations across training, validation, and test sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2 validation and test samples are located less than $5$ m from a training sample. This allows methods to localize within a memorized implicit map during testing and leads to inflated performance numbers being reported. To reveal the true performance in unseen environments, we introduce geographical splits of the data. Experimental results show significantly lower performance numbers, for some methods dropping with more than $45$ mAP, when retraining and reevaluating existing online mapping models with the proposed split. Additionally, a reassessment of prior design choices reveals diverging conclusions from those based on the original split. Notably, the impact of the lifting method and the support from auxiliary tasks (e.g., depth supervision) on performance appears less substantial or follows a different trajectory than previously perceived. Geographical splits can be found https://github.com/LiljaAdam/geographical-splits",http://arxiv.org/abs/2312.06420v1,,Adam Lilja (None) | Junsheng Fu (Zenseact) | Erik Stenborg (Chalmers University) | Lars Hammarstrand (Chalmers University Of Technology),2023-12-11 14:43:23+00:00,,,,,,
EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting,"The portrait matting task aims to extract an alpha matte with complete semantics and finely-detailed contours. In comparison to CNN-based approaches, transformers with self-attention module have a better capacity to capture long-range dependencies and low-frequency semantic information of a portrait. However, the recent research shows that self-attention mechanism struggles with modeling high-frequency contour information and capturing fine contour details, which can lead to bias while predicting the portrait's contours. To deal with this issue, we propose EFormer to enhance the model's attention towards both of the low-frequency semantic and high-frequency contour features. For the high-frequency contours, our research demonstrates that cross-attention module between different resolutions can guide our model to allocate attention appropriately to these contour regions. Supported on this, we can successfully extract the high-frequency detail information around the portrait's contours, which are previously ignored by self-attention. Based on cross-attention module, we further build a semantic and contour detector (SCD) to accurately capture both of the low-frequency semantic and high-frequency contour features. And we design contour-edge extraction branch and semantic extraction branch to extract refined high-frequency contour features and complete low-frequency semantic information, respectively. Finally, we fuse the two kinds of features and leverage segmentation head to generate a predicted portrait matte. Experiments on VideoMatte240K (JPEG SD Format) and Adobe Image Matting (AIM) datasets demonstrate that EFormer outperforms previous portrait matte methods.",http://arxiv.org/abs/2308.12831v2,,Zitao Wang (None) | Qiguang Miao (Xidian University) | Yue Xi (Xi'an University Of Electronic Science And Technology) | Peipei Zhao (Xi'an University Of Electronic Science And Technology),2023-08-24 14:45:03+00:00,,,,,,
TexTile: A Differentiable Metric for Texture Tileability,,,,Carlos Rodriguez-Pardo (Universidad Rey Juan Carlos) | Dan Casas (Universidad Rey Juan Carlos) | Elena Garces (Universidad Rey Juan Carlos) | Jorge Lopez-Moreno (Universidad Rey Juan Carlos),,,,,,,
PEEKABOO: Interactive Video Generation via Masked-Diffusion,"Recently there has been a lot of progress in text-to-video generation, with state-of-the-art models being capable of generating high quality, realistic videos. However, these models lack the capability for users to interactively control and generate videos, which can potentially unlock new areas of application. As a first step towards this goal, we tackle the problem of endowing diffusion-based video generation models with interactive spatio-temporal control over their output. To this end, we take inspiration from the recent advances in segmentation literature to propose a novel spatio-temporal masked attention module - Peekaboo. This module is a training-free, no-inference-overhead addition to off-the-shelf video generation models which enables spatio-temporal control. We also propose an evaluation benchmark for the interactive video generation task. Through extensive qualitative and quantitative evaluation, we establish that Peekaboo enables control video generation and even obtains a gain of upto 3.8x in mIoU over baseline models.",http://arxiv.org/abs/2312.07509v1,,Yash Jain (Microsoft) | Anshul Nasery (University Of Washington) | Vibhav Vineet (Microsoft) | Harkirat Behl (Microsoft),2023-12-12 18:43:05+00:00,,,,,,
Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering,,,,Zhaohe Liao (Shanghai Jiao Tong University) | Jiangtong Li (Shanghai Jiao Tong University) | Li Niu (None) | Liqing Zhang (Shanghai Jiao Tong University),,,,,,,
MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion,"We introduce Multi-view Ancestral Sampling (MAS), a method for 3D motion generation, using 2D diffusion models that were trained on motions obtained from in-the-wild videos. As such, MAS opens opportunities to exciting and diverse fields of motion previously under-explored as 3D data is scarce and hard to collect. MAS works by simultaneously denoising multiple 2D motion sequences representing different views of the same 3D motion. It ensures consistency across all views at each diffusion step by combining the individual generations into a unified 3D sequence, and projecting it back to the original views. We demonstrate MAS on 2D pose data acquired from videos depicting professional basketball maneuvers, rhythmic gymnastic performances featuring a ball apparatus, and horse races. In each of these domains, 3D motion capture is arduous, and yet, MAS generates diverse and realistic 3D sequences. Unlike the Score Distillation approach, which optimizes each sample by repeatedly applying small fixes, our method uses a sampling process that was constructed for the diffusion framework. As we demonstrate, MAS avoids common issues such as out-of-domain sampling and mode-collapse. https://guytevet.github.io/mas-page/",http://arxiv.org/abs/2310.14729v2,,"Roy Kapon (Tel Aviv University) | Guy Tevet (None) | Daniel Cohen-Or (Google) | Amit H. Bermano (Tel Aviv University, Technion)",2023-10-23 09:05:18+00:00,,,,,,
AdvScale: Exploring the Limits of Adversarial Training at Scale,,,,"Zeyu Wang (University Of California, Santa Cruz) | Xianhang Li (University Of California, Santa Cruz) | Hongru Zhu (None) | Cihang Xie (University Of California, Santa Cruz)",,,,,,,
Overload: Latency Attacks on Object Detection,,,,"Erh-Chung Chen (National Tsing Hua University) | Pin-Yu Chen (None) | I-Hsin Chung (IBM Research) | Che-Rung Lee (Department Of Computer Science, National Tsing Hua University)",,,,,,,
Tri-Modal Motion Retrieval by Learning a Joint Embedding Space,"Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.",http://arxiv.org/abs/2403.00691v1,,Kangning Yin (ShanghaiTech University) | Shihao Zou (University Of Alberta) | Yuxuan Ge (ShanghaiTech University) | Zheng Tian (ShanghaiTech University),2024-03-01 17:23:30+00:00,,,,,,
Supervised Anomaly Detection for Complex Industrial Images,,,,Aimira Baitieva (Valeo) | David Hurych (Valeo.Ai) | Victor Besnier (Valeo.Ai) | Olivier BERNARD (Valeo),,,,,,,
The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing,,,,Denis Bobkov (Higher School Of Economics) | Vadim Titov (ARTIFICIAL INTELLIGENCE RESEARCH INSTITUTE) | Aibek Alanov (Artificial Intelligence Research Institute) | Dmitry Vetrov (Constructor University),,,,,,,
Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization,"Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.",http://arxiv.org/abs/2402.18447v1,,Deng Li (Tianjin University) | Aming Wu (Xidian University) | Yaowei Wang (Pengcheng Laboratory) | Yahong Han (Tianjin University),2024-02-28 16:16:51+00:00,,,,,,
In-Context Matting,,,,He Guo (None) | Zixuan Ye (None) | Zhiguo Cao (None) | Hao Lu (Huazhong University Of Science And Technology),,,,,,,
ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks,,,,Kai Han (Huawei Noah's Ark Lab) | Yunhe Wang (Huawei Noah's Ark Lab) | Jianyuan Guo (University Of Sydney) | Enhua Wu (University Of Macau),,,,,,,
Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning,,,,Joshua C. Zhao (Purdue University) | Ahaan Dabholkar (Purdue University) | Atul Sharma (Purdue University) | Saurabh Bagchi (KeyByte LLC),,,,,,,
Adversarial Score Distillation: When score distillation meets GAN,"Existing score distillation methods are sensitive to classifier-free guidance (CFG) scale: manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our WGAN paradigm, we extend ASD to the image editing task, which achieves competitive results. The project page and code are at https://github.com/2y7c3/ASD.",http://arxiv.org/abs/2312.00739v1,,Min Wei (Beijing University Of Posts And Telecommunications) | Jingkai Zhou (Alibaba DAMO Academy) | Junyao Sun (South China University Of Technology) | Xuesong Zhang (Beijing University Of Posts And Telecommunications),2023-12-01 17:20:47+00:00,,,,,,
Mask4Align: Aligned Entity Prompting with Color Masks for Multi-Entity Localization Problem,,,,Haoquan Zhang (South China University Of Technology) | Ronggang Huang (South China University Of Technology) | Yi Xie (South China University Of Technology) | Huaidong Zhang (South China University Of Technology),,,,,,,
Unsupervised Feature Learning with Emergent Data-Driven Prototypicality,"Given an image set without any labels, our goal is to train a model that maps each image to a point in a feature space such that, not only proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is according to the dataset.   Our key insight is to perform unsupervised feature learning in hyperbolic instead of Euclidean space, where the distance between points still reflect image similarity, and yet we gain additional capacity for representing prototypicality with the location of the point: The closer it is to the origin, the more prototypical it is. The latter property is simply emergent from optimizing the usual metric learning objective: The image similar to many training instances is best placed at the center of corresponding points in Euclidean space, but closer to the origin in hyperbolic space.   We propose an unsupervised feature learning algorithm in Hyperbolic space with sphere pACKing. HACK first generates uniformly packed particles in the Poincar\'e ball of hyperbolic space and then assigns each image uniquely to each particle. Images after congealing are regarded more typical of the dataset it belongs to. With our feature mapper simply trained to spread out training instances in hyperbolic space, we observe that images move closer to the origin with congealing, validating our idea of unsupervised prototypicality discovery. We demonstrate that our data-driven prototypicality provides an easy and superior unsupervised instance selection to reduce sample complexity, increase model generalization with atypical instances and robustness with typical ones.",http://arxiv.org/abs/2307.01421v1,,Yunhui Guo (The University Of Texas At Dallas) | Youren Zhang (University Of Michigan - Ann Arbor) | Yubei Chen (New York University) | Stella X. Yu (None),2023-07-04 01:26:26+00:00,,,,,,
3DiffTection: 3D Object Detection with Geometry-aware Diffusion Features,"We present 3DiffTection, a state-of-the-art method for 3D object detection from single images, leveraging features from a 3D-aware diffusion model. Annotating large-scale image data for 3D detection is resource-intensive and time-consuming. Recently, pretrained large image diffusion models have become prominent as effective feature extractors for 2D perception tasks. However, these features are initially trained on paired text and image data, which are not optimized for 3D tasks, and often exhibit a domain gap when applied to the target data. Our approach bridges these gaps through two specialized tuning strategies: geometric and semantic. For geometric tuning, we fine-tune a diffusion model to perform novel view synthesis conditioned on a single image, by introducing a novel epipolar warp operator. This task meets two essential criteria: the necessity for 3D awareness and reliance solely on posed image data, which are readily available (e.g., from videos) and does not require manual annotation. For semantic refinement, we further train the model on target data with detection supervision. Both tuning phases employ ControlNet to preserve the integrity of the original feature capabilities. In the final step, we harness these enhanced capabilities to conduct a test-time prediction ensemble across multiple virtual viewpoints. Through our methodology, we obtain 3D-aware features that are tailored for 3D detection and excel in identifying cross-view point correspondences. Consequently, our model emerges as a powerful 3D detector, substantially surpassing previous benchmarks, e.g., Cube-RCNN, a precedent in single-view 3D detection by 9.43\% in AP3D on the Omni3D-ARkitscene dataset. Furthermore, 3DiffTection showcases robust data efficiency and generalization to cross-domain data.",http://arxiv.org/abs/2311.04391v1,,"Chenfeng Xu (University Of California Berkeley) | Huan Ling (Nvidia, University Of Toronto) | Sanja Fidler (Department Of Computer Science, University Of Toronto) | Or Litany (NVIDIA / Technion)",2023-11-07 23:46:41+00:00,,,,,,
Frozen Feature Augmentation for Few-Shot Image Classification,"Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called 'frozen features', leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed 'frozen feature augmentation (FroFA)', covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.",http://arxiv.org/abs/2403.10519v1,,Andreas B??r (None) | Neil Houlsby (Google) | Mostafa Dehghani (Google DeepMind) | Manoj Kumar (Google Deepmind),2024-03-15 17:59:40+00:00,,,,,,
"1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness","The robustness of neural networks against input perturbations with bounded magnitude represents a serious concern in the deployment of deep learning models in safety-critical systems. Recently, the scientific community has focused on enhancing certifiable robustness guarantees by crafting 1-Lipschitz neural networks that leverage Lipschitz bounded dense and convolutional layers. Although different methods have been proposed in the literature to achieve this goal, understanding the performance of such methods is not straightforward, since different metrics can be relevant (e.g., training time, memory usage, accuracy, certifiable robustness) for different applications. For this reason, this work provides a thorough theoretical and empirical comparison between methods by evaluating them in terms of memory usage, speed, and certifiable robust accuracy. The paper also provides some guidelines and recommendations to support the user in selecting the methods that work best depending on the available resources. We provide code at https://github.com/berndprach/1LipschitzLayersCompared.",http://arxiv.org/abs/2311.16833v1,,Bernd Prach (ISTA) | Fabio Brau (Scuola Superiore Sant'Anna Pisa) | Giorgio Buttazzo (Scuola Superiore Sant'Anna Pisa) | Christoph Lampert (Institute Of Science And Technology Austria),2023-11-28 14:50:50+00:00,,,,,,
PoNQ: a Neural QEM-based Mesh Representation,,,,"Nissim Maruani (Inria) | Maks Ovsjanikov (Ecole Polytechnique, France) | Pierre Alliez (INRIA) | Mathieu Desbrun (INRIA)",,,,,,,
Adversarial Distillation Based on Slack Matching and Attribution Region Alignment,,,,"Shenglin Yin (None) | Zhen Xiao (Peking University) | Mingxuan Song (Peking University) | Jieyi Long (Theta Labs,)",,,,,,,
Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment,,,,Zheren Fu (University Of Science And Technology Of China) | Lei Zhang (University Of Science And Technology Of China) | Hou Xia (University Of Science And Technology Of China) | Zhendong Mao (None),,,,,,,
Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation,,,,"Dong Lao (University Of California, Los Angeles) | Congli Wang (University Of California, Berkeley) | Alex Wong (Yale University) | Stefano Soatto (UCLA)",,,,,,,
Unleashing Network Potentials for Semantic Scene Completion,"Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods.",http://arxiv.org/abs/2403.07560v2,,Fengyun Wang (None) | Qianru Sun (None) | Dong Zhang (The Hong Kong University Of Science And Technology) | Jinhui Tang (Nanjing University Of Science And Technology),2024-03-12 11:48:49+00:00,,,,,,
Towards Robust 3D Pose Transfer with Adversarial Learning,,,,Haoyu Chen (University Of Oulu) | Hao Tang (ETH Zurich And CMU) | Ehsan Adeli (Stanford University) | Guoying Zhao (None),,,,,,,
Building Vision-Language Models on Solid Foundations with Masked Distillation,,,,Sepehr Sameni (None) | Kushal Kafle (Adobe Systems) | Hao Tan (Adobe Systems) | Simon Jenni (Adobe Systems),,,,,,,
AM-RADIO: Agglomerative Models - Reduce All Domains Into One,,,,Mike Ranzinger (NVIDIA Research) | Greg Heinrich (NVIDIA) | Jan Kautz (NVIDIA) | Pavlo Molchanov (NVIDIA),,,,,,,
Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification,,,,Chao Yi (Nanjing University) | Lu Ren (Nanjing University) | De-Chuan Zhan (Nanjing University) | Han-Jia Ye (Nanjing University),,,,,,,
Wavelet-based Fourier Information Interaction with Frequency Diffusion Adjustment for Underwater Image Restoration,"Underwater images are subject to intricate and diverse degradation, inevitably affecting the effectiveness of underwater visual tasks. However, most approaches primarily operate in the raw pixel space of images, which limits the exploration of the frequency characteristics of underwater images, leading to an inadequate utilization of deep models' representational capabilities in producing high-quality images. In this paper, we introduce a novel Underwater Image Enhancement (UIE) framework, named WF-Diff, designed to fully leverage the characteristics of frequency domain information and diffusion models. WF-Diff consists of two detachable networks: Wavelet-based Fourier information interaction network (WFI2-net) and Frequency Residual Diffusion Adjustment Module (FRDAM). With our full exploration of the frequency domain information, WFI2-net aims to achieve preliminary enhancement of frequency information in the wavelet space. Our proposed FRDAM can further refine the high- and low-frequency information of the initial enhanced images, which can be viewed as a plug-and-play universal module to adjust the detail of the underwater images. With the above techniques, our algorithm can show SOTA performance on real-world underwater image datasets, and achieves competitive performance in visual quality.",http://arxiv.org/abs/2311.16845v1,,Chen Zhao (None) | Weiling Cai (Nanjing Normal University) | Chenyu Dong (Nanjing Normal University) | Chengwei Hu (Nanjing Normal University),2023-11-28 14:58:32+00:00,,,,,,
Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning,,,,Da-Wei Zhou (Nanjing University) | Hai-Long Sun (Nanjing University) | Han-Jia Ye (Nanjing University) | De-Chuan Zhan (Nanjing University),,,,,,,
Point-VOS: Pointing Up Video Object Segmentation,"Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task. We will make our code and annotations available at https://pointvos.github.io.",http://arxiv.org/abs/2402.05917v1,,Sabarinath Mahadevan (RWTH Aachen University) | Idil Esen Zulfikar (RWTH Aachen University) | Paul Voigtlaender (None) | Bastian Leibe (RWTH Aachen University),2024-02-08 18:52:23+00:00,,,,,,
EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation,"Semantic segmentation has innately relied on extensive pixel-level labeled annotated data, leading to the emergence of unsupervised methodologies. Among them, leveraging self-supervised Vision Transformers for unsupervised semantic segmentation (USS) has been making steady progress with expressive deep features. Yet, for semantically segmenting images with complex objects, a predominant challenge remains: the lack of explicit object-level semantic encoding in patch-level features. This technical limitation often leads to inadequate segmentation of complex objects with diverse structures. To address this gap, we present a novel approach, EAGLE, which emphasizes object-centric representation learning for unsupervised semantic segmentation. Specifically, we introduce EiCue, a spectral technique providing semantic and structural cues through an eigenbasis derived from the semantic similarity matrix of deep image features and color affinity from an image. Further, by incorporating our object-centric contrastive loss with EiCue, we guide our model to learn object-level representations with intra- and inter-image object-feature consistency, thereby enhancing semantic accuracy. Extensive experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art USS results of EAGLE with accurate and consistent semantic segmentation across complex scenes.",http://arxiv.org/abs/2403.01482v1,,Chanyoung Kim (Yonsei University) | Woojung Han (Yonsei University) | Dayun Ju (Yonsei University) | Seong Jae Hwang (Yonsei University),2024-03-03 11:24:16+00:00,,,,,,
Compositional Chain-of-Thought Prompting for Large Multimodal Models,"The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.",http://arxiv.org/abs/2311.17076v1,,"Chancharik Mitra (University Of California, Berkeley) | Brandon Huang (University Of California, Berkeley) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Roei Herzig (Tel Aviv University)",2023-11-27 22:23:27+00:00,,,,,,
"Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",,,,"Qin Liu (Department Of Computer Science, University Of North Carolina, Chapel Hill) | Jaemin Cho (UNC Chapel Hill) | Mohit Bansal (University Of North Carolina At Chapel Hill) | Marc Niethammer (The University Of North Carolina At Chapel Hill)",,,,,,,
Systematic comparison of semi-supervised and self-supervised learning for medical image classification,"In many medical image classification problems, labeled data is scarce while unlabeled data is more available. Semi-supervised learning and self-supervised learning are two different research directions that can improve accuracy by learning from extra unlabeled data. Recent methods from both directions have reported significant gains on traditional benchmarks. Yet past benchmarks do not focus on medical tasks and rarely compare self- and semi- methods together on equal footing. Furthermore, past benchmarks often handle hyperparameter tuning suboptimally. First, they may not tune hyperparameters at all, leading to underfitting. Second, when tuning does occur, it often unrealistically uses a labeled validation set much larger than the train set. Both cases make previously published rankings of methods difficult to translate to practical settings. This study contributes a systematic evaluation of self- and semi- methods with a unified experimental protocol intended to guide a practitioner with scarce overall labeled data and a limited compute budget. We answer two key questions: Can hyperparameter tuning be effective with realistic-sized validation sets? If so, when all methods are tuned well, which self- or semi-supervised methods reach the best accuracy? Our study compares 13 representative semi- and self-supervised methods to strong labeled-set-only baselines on 4 medical datasets. From 20000+ total GPU hours of computation, we provide valuable best practices to resource-constrained, results-focused practitioners.",http://arxiv.org/abs/2307.08919v2,,Zhe Huang (Tufts University) | Ruijie Jiang (Tufts University) | Shuchin Aeron (Tufts University) | Michael C. Hughes (Tufts University),2023-07-18 01:31:47+00:00,,,,,,
Navigating Beyond Dropout: An Intriguing Solution towards Generalizable Image Super-Resolution,"Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics. Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven benchmark datasets including both synthetic and real-world scenarios.",http://arxiv.org/abs/2402.18929v2,,Hongjun Wang (None) | Jiyuan Chen (Hong Kong Polytechnic University) | Yinqiang Zheng (None) | Tieyong Zeng (The Chinese University Of Hong Kong),2024-02-29 07:44:31+00:00,,,,,,
OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation,,,,Xiongwei Wu (Singapore Management University) | Sicheng Yu (Bytedance) | Ee-Peng Lim (Singapore Management University) | Chong Wah Ngo (Singapore Management University),,,,,,,
Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning,,,,Zhengwei Fang (Beijing Jiao Tong University) | Rui Wang (Beijing Jiao Tong University) | Tao Huang (Beijing Jiao Tong University) | Liping Jing (Beijing Jiao Tong University),,,,,,,
UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs,"Text-to-image diffusion models have demonstrated remarkable capabilities in transforming textual prompts into coherent images, yet the computational cost of their inference remains a persistent challenge. To address this issue, we present UFOGen, a novel generative model designed for ultra-fast, one-step text-to-image synthesis. In contrast to conventional approaches that focus on improving samplers or employing distillation techniques for diffusion models, UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN objective. Leveraging a newly introduced diffusion-GAN objective and initialization with pre-trained diffusion models, UFOGen excels in efficiently generating high-quality images conditioned on textual descriptions in a single step. Beyond traditional text-to-image generation, UFOGen showcases versatility in applications. Notably, UFOGen stands among the pioneering models enabling one-step text-to-image generation and diverse downstream tasks, presenting a significant advancement in the landscape of efficient generative models.",http://arxiv.org/abs/2311.09257v5,,Yanwu Xu (Boston University) | Yang Zhao (Google) | Zhisheng Xiao (Google) | Tingbo Hou (Google Research),2023-11-14 23:07:50+00:00,,,,,,
Contextual Augmented Global Contrast for Multimodal Intent Recognition,,,,Kaili Sun (None) | Zhiwen Xie (Central China Normal University) | Mang Ye (Wuhan University) | Huyin Zhang (Wuhan University),,,,,,,
StrokeFaceNeRF: Stroke-based Facial Appearance Editing in Neural Radiance Field,,,,Xiao-Juan Li (University Of The Chinese Academy Of Sciences) | Dingxi Zhang (University Of Chinese Academy Of Science) | Shu-Yu Chen (Chinese Academy Of Sciences) | Feng-Lin Liu (Chinese Academy Of Sciences),,,,,,,
Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces,,,,"Jiahong Wang (None) | Yinwei DU (Department Of Computer Science, ETHZ - ETH Zurich) | Stelian Coros (ETHZ - ETH Zurich) | Bernhard Thomaszewski (Swiss Federal Institute Of Technology)",,,,,,,
Revisiting Adversarial Training under Long-Tailed Distributions,"Deep neural networks are vulnerable to adversarial attacks, often leading to erroneous outputs. Adversarial training has been recognized as one of the most effective methods to counter such attacks. However, existing adversarial training techniques have predominantly been tested on balanced datasets, whereas real-world data often exhibit a long-tailed distribution, casting doubt on the efficacy of these methods in practical scenarios.   In this paper, we delve into adversarial training under long-tailed distributions. Through an analysis of the previous work ""RoBal"", we discover that utilizing Balanced Softmax Loss alone can achieve performance comparable to the complete RoBal approach while significantly reducing training overheads. Additionally, we reveal that, similar to uniform distributions, adversarial training under long-tailed distributions also suffers from robust overfitting. To address this, we explore data augmentation as a solution and unexpectedly discover that, unlike results obtained with balanced data, data augmentation not only effectively alleviates robust overfitting but also significantly improves robustness. We further investigate the reasons behind the improvement of robustness through data augmentation and identify that it is attributable to the increased diversity of examples. Extensive experiments further corroborate that data augmentation alone can significantly improve robustness. Finally, building on these findings, we demonstrate that compared to RoBal, the combination of BSL and data augmentation leads to a +6.66% improvement in model robustness under AutoAttack on CIFAR-10-LT. Our code is available at https://github.com/NISPLab/AT-BSL .",http://arxiv.org/abs/2403.10073v1,,Xinli Yue (Wuhan University) | Ningping Mou (Wuhan University) | Qian Wang (Wuhan University) | Lingchen Zhao (Wuhan University),2024-03-15 07:29:41+00:00,,,,,,
Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation,,,,Ruicong Liu (The University Of Tokyo) | Takehiko Ohkawa (The University Of Tokyo) | Mingfang Zhang (None) | Yoichi Sato (University Of Tokyo),,,,,,,
Mind Artist: Creating Artistic Snapshots with Human Thought,,,,Jiaxuan Chen (Zhejiang University) | Yu Qi (Zhejiang University) | Yueming Wang (Zhejiang University) | Gang Pan (Zhejiang University),,,,,,,
MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation,"Deep neural networks have shown exemplary performance on semantic scene understanding tasks on source domains, but due to the absence of style diversity during training, enhancing performance on unseen target domains using only single source domain data remains a challenging task. Generation of simulated data is a feasible alternative to retrieving large style-diverse real-world datasets as it is a cumbersome and budget-intensive process. However, the large domain-specific inconsistencies between simulated and real-world data pose a significant generalization challenge in semantic segmentation. In this work, to alleviate this problem, we propose a novel MultiResolution Feature Perturbation (MRFP) technique to randomize domain-specific fine-grained features and perturb style of coarse features. Our experimental results on various urban-scene segmentation datasets clearly indicate that, along with the perturbation of style-information, perturbation of fine-feature components is paramount to learn domain invariant robust feature maps for semantic segmentation models. MRFP is a simple and computationally efficient, transferable module with no additional learnable parameters or objective functions, that helps state-of-the-art deep neural networks to learn robust domain invariant features for simulation-to-real semantic segmentation.",http://arxiv.org/abs/2311.18331v1,,"Sumanth Udupa (None) | Prajwal Gurunath (Indian Institute Of Science) | Aniruddh Sikdar (Indian Institute Of Science) | Suresh Sundaram (Indian Institute Of Science, Indian Institute Of Science, Bangalore)",2023-11-30 08:02:49+00:00,,,,,,
Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness,,,,"Sibo Wang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Jie Zhang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Zheng Yuan (Institute Of Computing Technology, Chinese Academy Of Sciences) | Shiguang Shan (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
PolarMatte: Fully Computational Ground-Truth-Quality Alpha Matte Extraction for Images and Video using Polarized Screen Matting,,,,Kenji Enomoto (Adobe Systems) | TJ Rhodes (Adobe Research) | Brian Price (Adobe Research) | Gavin Miller (Adobe),,,,,,,
FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer,,,,Dongyeong Hwang (Korea Advanced Institute Of Science & Technology) | Hyunju Kim (Korea Advanced Institute Of Science & Technology) | Sunwoo Kim (Korea Advanced Institute Of Science & Technology) | Kijung Shin (Korea Advanced Institute Of Science And Technology),,,,,,,
PBWR: Parametric Building Wireframe Reconstruction from Aerial LiDAR Point Clouds,"In this paper, we present an end-to-end 3D building wireframe reconstruction method to regress edges directly from aerial LiDAR point clouds.Our method, named Parametric Building Wireframe Reconstruction (PBWR), takes aerial LiDAR point clouds and initial edge entities as input, and fully uses self-attention mechanism of transformers to regress edge parameters without any intermediate steps such as corner prediction. We propose an edge non-maximum suppression (E-NMS) module based on edge similarityto remove redundant edges. Additionally, a dedicated edge loss function is utilized to guide the PBWR in regressing edges parameters, where simple use of edge distance loss isn't suitable. In our experiments, we demonstrate state-of-the-art results on the Building3D dataset, achieving an improvement of approximately 36% in entry-level dataset edge accuracy and around 42% improvement in the Tallinn dataset.",http://arxiv.org/abs/2311.12062v1,,Shangfeng Huang (University Of Calgary) | Ruisheng Wang (University Of Calgary) | Bo Guo (Guangdong University Of Technology) | Hongxin Yang (University Of Calgary),2023-11-18 19:41:37+00:00,,,,,,
Honeybee: Locality-enhanced Projector for Multimodal LLM,"In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, our proposed MLLM, Honeybee, remarkably outperforms previous state-of-the-art methods across various benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly higher efficiency. Code and models are available at https://github.com/kakaobrain/honeybee.",http://arxiv.org/abs/2312.06742v1,,Junbum Cha (Kakao Brain) | Woo-Young Kang (Kakaobrain) | Jonghwan Mun (KakaoBrain) | Byungseok Roh (Kakao Brain),2023-12-11 18:59:06+00:00,,,,,,
SleepVST: Sleep Staging from Near-Infrared Video using Transformers,,,,Jonathan F. Carter (University Of Oxford) | Joao Jorge (Oxehealth) | Oliver Gibson (Oxehealth Limited) | Lionel Tarassenko (University Of Oxford),,,,,,,
Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments,"Research into dynamic 3D scene understanding has primarily focused on short-term change tracking from dense observations, while little attention has been paid to long-term changes with sparse observations. We address this gap with MoRE, a novel approach for multi-object relocalization and reconstruction in evolving environments. We view these environments as ""living scenes"" and consider the problem of transforming scans taken at different points in time into a 3D reconstruction of the object instances, whose accuracy and completeness increase over time. At the core of our method lies an SE(3)-equivariant representation in a single encoder-decoder network, trained on synthetic data. This representation enables us to seamlessly tackle instance matching, registration, and reconstruction. We also introduce a joint optimization algorithm that facilitates the accumulation of point clouds originating from the same instance across multiple scans taken at different points in time. We validate our method on synthetic and real-world data and demonstrate state-of-the-art performance in both end-to-end performance and individual subtasks.",http://arxiv.org/abs/2312.09138v1,,Liyuan Zhu (Stanford University) | Shengyu Huang (None) | Konrad Schindler (ETH Zurich) | Iro Armeni (Stanford University),2023-12-14 17:09:57+00:00,,,,,,
Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition,"Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries from Africa, and +4.6 on the hardest classes. We further show competitive performance vs. few-shot target training, and provide insights into how descriptive knowledge captures geographical differences.",http://arxiv.org/abs/2401.01482v1,,Kyle Buettner (None) | Sina Malakouti (University Of Pittsburgh) | Xiang Li (University Of Pittsburgh) | Adriana Kovashka (University Of Pittsburgh),2024-01-03 01:11:16+00:00,,,,,,
EVCap: Retrieval-Augmented Image Captioning with External Visual--Name Memory for Open-World Comprehension,"Large language models (LLMs)-based image captioning has the capability of describing objects not explicitly observed in training data; yet novel objects occur frequently, necessitating the requirement of sustaining up-to-date object knowledge for open-world comprehension. Instead of relying on large amounts of data and scaling up network parameters, we introduce a highly effective retrieval-augmented image captioning method that prompts LLMs with object names retrieved from External Visual--name memory (EVCap). We build ever-changing object knowledge memory using objects' visuals and names, enabling us to (i) update the memory at a minimal cost and (ii) effortlessly augment LLMs with retrieved object names utilizing a lightweight and fast-to-train model. Our model, which was trained only on the COCO dataset, can be adapted to out-domain data without additional fine-tuning or retraining. Our comprehensive experiments conducted on various benchmarks and synthetic commonsense-violating data demonstrate that EVCap, comprising solely 3.97M trainable parameters, exhibits superior performance compared to other methods of equivalent model size scale. Notably, it achieves competitive performance against specialist SOTAs with an enormous number of parameters. Our code is available at https://jiaxuan-li.github.io/EVCap.",http://arxiv.org/abs/2311.15879v1,,Jiaxuan Li (The University Of Tokyo) | Duc Minh Vo (The University Of Tokyo) | Akihiro Sugimoto (NII) | Hideki Nakayama (The University Of Tokyo),2023-11-27 14:51:37+00:00,,,,,,
CLIB-FIQA: Face Image Quality Assessment with Confidence Calibration,,,,Fu-Zhao Ou (City University Of Hong Kong) | Chongyi Li (None) | Shiqi Wang (City University Of Hong Kong) | Sam Kwong (Lingnan University),,,,,,,
Polos: Multimodal Metric Learning from Human Feedback for Image Captioning,"Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.",http://arxiv.org/abs/2402.18091v1,,Yuiga Wada (Keio University) | Kanta Kaneda (Keio University) | Daichi Saito (Keio University) | Komei Sugiura (Keio University),2024-02-28 06:24:39+00:00,,,,,,
Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery,,,,"Yuqi Zhang (The Chinese University Of Hong Kong, Shenzhen) | Guanying Chen (The Chinese University Of Hong Kong, Shenzhen) | Jiaxing Chen (Sun Yat-Sen University) | Shuguang Cui (The Chinese University Of Hong Kong, Shenzhen)",,,,,,,
HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields,"Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available at https://github.com/amathislab/HOISDF",http://arxiv.org/abs/2402.17062v1,,Haozhe Qi (EPFL - Switzerland) | Chen Zhao (EPFL) | Mathieu Salzmann (EPFL) | Alexander Mathis (None),2024-02-26 22:48:37+00:00,,,,,,
Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models,"The quality of the prompts provided to text-to-image diffusion models determines how faithful the generated content is to the user's intent, often requiring `prompt engineering'. To harness visual concepts from target images without prompt engineering, current approaches largely rely on embedding inversion by optimizing and then mapping them to pseudo-tokens. However, working with such high-dimensional vector representations is challenging because they lack semantics and interpretability, and only allow simple vector operations when using them. Instead, this work focuses on inverting the diffusion model to obtain interpretable language prompts directly. The challenge of doing this lies in the fact that the resulting optimization problem is fundamentally discrete and the space of prompts is exponentially large; this makes using standard optimization techniques, such as stochastic gradient descent, difficult. To this end, we utilize a delayed projection scheme to optimize for prompts representative of the vocabulary space in the model. Further, we leverage the findings that different timesteps of the diffusion process cater to different levels of detail in an image. The later, noisy, timesteps of the forward diffusion process correspond to the semantic information, and therefore, prompt inversion in this range provides tokens representative of the image semantics. We show that our approach can identify semantically interpretable and meaningful prompts for a target image which can be used to synthesize diverse images with similar content. We further illustrate the application of the optimized prompts in evolutionary image generation and concept removal.",http://arxiv.org/abs/2312.12416v1,,Shweta Mahajan (University Of British Columbia) | Tanzila Rahman (University Of British Columbia) | Kwang Moo Yi (University Of British Columbia) | Leonid Sigal (University Of British Columbia),2023-12-19 18:47:30+00:00,,,,,,
MoST: Motion Style Transformer between Diverse Action Contents,"While existing motion style transfer methods are effective between two motions with identical content, their performance significantly diminishes when transferring style between motions with different contents. This challenge lies in the lack of clear separation between content and style of a motion. To tackle this challenge, we propose a novel motion style transformer that effectively disentangles style from content and generates a plausible motion with transferred style from a source motion. Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion style transformer with 'part-attentive style modulator across body parts' and 'Siamese encoders that encode style and content features separately'; (2) style disentanglement loss. Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need for heuristic post-processing. Codes are available at https://github.com/Boeun-Kim/MoST.",http://arxiv.org/abs/2403.06225v1,,Boeun Kim (Seoul National University) | Jungho Kim (KETI) | Hyung Jin Chang (None) | Jin Young Choi (Seoul National University),2024-03-10 14:11:25+00:00,,,,,,
Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models,"Knowledge distillation methods have recently shown to be a promising direction to speedup the synthesis of large-scale diffusion models by requiring only a few inference steps. While several powerful distillation methods were recently proposed, the overall quality of student samples is typically lower compared to the teacher ones, which hinders their practical usage. In this work, we investigate the relative quality of samples produced by the teacher text-to-image diffusion model and its distilled student version. As our main empirical finding, we discover that a noticeable portion of student samples exhibit superior fidelity compared to the teacher ones, despite the ``approximate'' nature of the student. Based on this finding, we propose an adaptive collaboration between student and teacher diffusion models for effective text-to-image synthesis. Specifically, the distilled model produces the initial sample, and then an oracle decides whether it needs further improvements with a slow teacher model. Extensive experiments demonstrate that the designed pipeline surpasses state-of-the-art text-to-image alternatives for various inference budgets in terms of human preference. Furthermore, the proposed approach can be naturally used in popular applications such as text-guided image editing and controllable generation.",http://arxiv.org/abs/2312.10835v2,,Nikita Starodubcev (Yandex) | Dmitry Baranchuk (Higher School Of Economics) | Artem Fedorov (Moscow Institute Of Physics And Technology) | Artem Babenko (Yandex),2023-12-17 22:40:38+00:00,,,,,,
Minimal Perspective Autocalibration,,,,Andrea Porfiri Dal Cin (Polytechnic Institute Of Milan) | Timothy Duff (University Of Washington) | Luca Magri (Polytechnic Institute Of Milan) | Tomas Pajdla (CIIRC - Czech Technical University In Prague),,,,,,,
Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing,"White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene's actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB benchmarks, and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.",http://arxiv.org/abs/2402.18277v1,,Dongyoung Kim (None) | Jinwoo Kim (Yonsei University) | Junsang Yu (Samsung) | Seon Joo Kim (Yonsei University),2024-02-28 12:15:29+00:00,,,,,,
"Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing",,,,Boqiang Zhang (None) | Hongtao Xie (University Of Science And Technology Of China) | Zuan Gao (University Of Science And Technology Of China) | Yuxin Wang (University Of Science And Technology Of China),,,,,,,
Z??? : Zero-shot   S ???   tyle   T ???   ransfer via   A ???   ttention   R ???   earrangement ,,,,"Yingying Deng (None) | Xiangyu He (Meituan) | Fan Tang (Institute Of Computing Technology, CAS) | Weiming Dong (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
"Flexible Biometrics Recognition: Bridging the Multimodality Gap through Attention, Alignment and Prompt Tuning",,,,Leslie Ching Ow Tiong (Samsung Electronics) | Dick Sigmund (AIDOT) | Chen-Hui Chan (Korea Institute Of Science And Technology) | Andrew Beng Jin Teoh (None),,,,,,,
Part-aware Unified Representation of Language and Skeleton for Zero-shot Action Recognition,,,,Anqi Zhu (None) | Qiuhong Ke (Monash University) | Mingming Gong (University Of Melbourne) | James Bailey (The University Of Melbourne),,,,,,,
Improving Transferable Targeted Adversarial Attacks with Model Self-Enhancement,,,,Han Wu (Sun Yat-Sen University) | Guanyan Ou (Sun Yat-Sen University) | Weibin Wu (SUN YAT-SEN UNIVERSITY) | Zibin Zheng (Sun Yat-Sen University),,,,,,,
Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding,"Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language-embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language-embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.",http://arxiv.org/abs/2311.18482v1,,Jin-Chuan Shi (Beihang University) | Miao Wang (Beihang University) | Haobin Duan (Beihang University) | Shaohua Guan (None),2023-11-30 11:50:07+00:00,,,,,,
Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach,,,,"Mir Hossain Hossain (None) | Mennatullah Siam (York University) | Leonid Sigal (University Of British Columbia) | Jim Little (University Of British Columbia, Canada)",,,,,,,
InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning,"Recent advances in personalized image generation allow a pre-trained text-to-image model to learn a new concept from a set of images. However, existing personalization approaches usually require heavy test-time finetuning for each concept, which is time-consuming and difficult to scale. We propose InstantBooth, a novel approach built upon pre-trained text-to-image models that enables instant text-guided image personalization without any test-time finetuning. We achieve this with several major components. First, we learn the general concept of the input images by converting them to a textual token with a learnable image encoder. Second, to keep the fine details of the identity, we learn rich visual feature representation by introducing a few adapter layers to the pre-trained model. We train our components only on text-image pairs without using paired images of the same concept. Compared to test-time finetuning-based methods like DreamBooth and Textual-Inversion, our model can generate competitive results on unseen concepts concerning language-image alignment, image fidelity, and identity preservation while being 100 times faster.",http://arxiv.org/abs/2304.03411v1,,Jing Shi (Adobe Systems) | Wei Xiong (Adobe Systems) | Zhe Lin (Adobe Research) | HyunJoon Jung (Adobe Systems),2023-04-06 23:26:38+00:00,,,,,,
"Separating the ""Chirp"" from the ""Chat"": Self-supervised Visual Grounding of Sound and Language",,,,Mark Hamilton (Massachusetts Institute Of Technology) | Andrew Zisserman (University Of Oxford) | John Hershey (Google) | William Freeman (MIT And Google),,,,,,,
PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection,"Recent temporal LiDAR-based 3D object detectors achieve promising performance based on the two-stage proposal-based approach. They generate 3D box candidates from the first-stage dense detector, followed by different temporal aggregation methods. However, these approaches require per-frame objects or whole point clouds, posing challenges related to memory bank utilization. Moreover, point clouds and trajectory features are combined solely based on concatenation, which may neglect effective interactions between them. In this paper, we propose a point-trajectory transformer with long short-term memory for efficient temporal 3D object detection. To this end, we only utilize point clouds of current-frame objects and their historical trajectories as input to minimize the memory bank storage requirement. Furthermore, we introduce modules to encode trajectory features, focusing on long short-term and future-aware perspectives, and then effectively aggregate them with point cloud features. We conduct extensive experiments on the large-scale Waymo dataset to demonstrate that our approach performs well against state-of-the-art methods. Code and models will be made publicly available at https://github.com/kuanchihhuang/PTT.",http://arxiv.org/abs/2312.08371v1,,"Kuan-Chih Huang (University Of California, Merced) | Weijie Lyu (University Of California, Merced) | Ming-Hsuan Yang (University Of California At Merced) | Yi-Hsuan Tsai (Google)",2023-12-13 18:59:13+00:00,,,,,,
FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion,,,,George Cazenavette (Massachusetts Institute Of Technology) | Avneesh Sud (Google) | Thomas Leung (Google Inc) | Ben Usman (Google Research),,,,,,,
Accept the Modality Gap: An Exploration in the Hyperbolic Space,,,,Sameera Ramasinghe (Amazon) | Violetta Shevchenko (Amazon) | Gil Avraham (Amazon) | Thalaiyasingam Ajanthan (Amazon),,,,,,,
Instance-aware Contrastive Learning for Occluded Human Mesh Reconstruction,,,,Mi-Gyeong Gwon (Konkuk University) | Gi-Mun Um (Electronics And Telecommucations Research Institute) | Won-Sik Cheong (Electronics And Telecommunications Research Institute) | Wonjun Kim (Konkuk University),,,,,,,
Data Valuation and Detections in Federated Learning,"Federated Learning (FL) enables collaborative model training while preserving the privacy of raw data. A challenge in this framework is the fair and efficient valuation of data, which is crucial for incentivizing clients to contribute high-quality data in the FL task. In scenarios involving numerous data clients within FL, it is often the case that only a subset of clients and datasets are pertinent to a specific learning task, while others might have either a negative or negligible impact on the model training process. This paper introduces a novel privacy-preserving method for evaluating client contributions and selecting relevant datasets without a pre-specified training algorithm in an FL task. Our proposed approach FedBary, utilizes Wasserstein distance within the federated context, offering a new solution for data valuation in the FL framework. This method ensures transparent data valuation and efficient computation of the Wasserstein barycenter and reduces the dependence on validation datasets. Through extensive empirical experiments and theoretical analyses, we demonstrate the potential of this data valuation method as a promising avenue for FL research.",http://arxiv.org/abs/2311.05304v2,,Wenqian Li (National University Of Singapore) | Shuran Fu (National University Of Singapore) | Fengrui Zhang (Rutgers University) | Yan Pang (National University Of Singapore),2023-11-09 12:01:32+00:00,,,,,,
Making Visual Sense of Oracle Bones for You and Me,,,,Runqi Qiao (Beijing University Of Posts And Telecommunications) | LAN YANG (Beijing University Of Posts And Telecommunications) | Kaiyue Pang (SketchX AI) | Honggang Zhang (Beijing University Of Posts And Telecommunications),,,,,,,
Simple Semantic-Aided Few-Shot Learning,,,,Hai Zhang (Sichuan University) | Junzhe Xu (None) | Shanlin Jiang (The University Of Texas At Dallas) | ZHENAN HE (None),,,,,,,
Total Selfie: Generating Full-Body Selfies,"We present a method to generate full-body selfies -- photos that you take of yourself, but capturing your whole body as if someone else took the photo of you from a few feet away. Our approach takes as input a pre-captured video of your body, a target pose photo, and a selfie + background pair for each location. We introduce a novel diffusion-based approach to combine all of this information into high quality, well-composed photos of you with the desired pose and background.",http://arxiv.org/abs/2308.14740v1,,Bowei Chen (University Of Washington) | Brian Curless (University Of Washington) | Ira Kemelmacher-Shlizerman (University Of Washington) | Steve Seitz (University Of Washington),2023-08-28 17:41:14+00:00,,,,,,
DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback,"We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.",http://arxiv.org/abs/2311.10081v1,,"Yangyi Chen (School Of Computer Science, University Of Illinois At Urbana-Champaign) | Karan Sikka (SRI International) | Michael Cogswell (SRI International) | Heng Ji (University Of Illinois, Urbana-Champaign) | Ajay Divakaran (SRI International)",2023-11-16 18:37:29+00:00,,,,,,
Unsupervised Blind Image Deblurring Based on Self-Enhancement,,,,Lufei Chen (Sichuan University) | Xiangpeng Tian (SiChuan University) | Shuhua Xiong (Sichuan University) | Yinjie Lei (Sichuan University) | Chao Ren (Sichuan University),,,,,,,
SLICE: Stabilized LIME for Consistent Explanations for Image Classification,,,,"Revoti Prasad Bora (Norwegian University Of Science And Technology) | Kiran Raja (Norwegian University Of Science And Technology) | Philipp Terh??rst (Paderborn University, Germany) | Raymond Veldhuis (University Of Twente) | Raghavendra Ramachandra (Norwegian University Of Science And Technology (NTNU))",,,,,,,
Language-only Training of Zero-shot Composed Image Retrieval,,,,Geonmo Gu (NAVER) | Sanghyuk Chun (NAVER AI Lab) | Wonjae Kim (NAVER) | Yoohoon Kang (NAVER) | Sangdoo Yun (NAVER),,,,,,,
Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing,,,,ChangHee Yang (LG Electornic) | Chan Hee Kang (Sogang University) | Kyeongbo Kong (Pusan National University) | Hanni Oh (Sogang University) | Suk-Ju Kang (Sogang University),,,,,,,
ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object,,,,Chenshuang Zhang (Korea Advanced Institute Of Science And Technology (KAIST)) | Fei Pan (University Of Michigan - Ann Arbor) | Junmo Kim (Korea Advanced Institute Of Science And Technology) | In So Kweon (Korea Advanced Institute Of Science And Technology) | Chengzhi Mao (Columbia University),,,,,,,
FC-GNN: Recovering Reliable and Accurate Correspondences from Interferences,,,,Haobo Xu (None) | Jun Zhou (Shanghai Jiao Tong University) | Hua Yang (Shanghai Jiao Tong University) | Renjie Pan (Shanghai Jiao Tong University) | Cunyan Li (Shanghai Jiao Tong University),,,,,,,
SVDTree: Semantic Voxel Diffusion for Single Image Tree Reconstruction,,,,"Yuan Li (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Zhihao Liu (The University Of Tokyo) | Bedrich Benes (Purdue University) | Xiaopeng Zhang (Institute Of Automation, Chinese Academy Of Sciences) | Jianwei Guo (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
Named Entity Driven Zero-Shot Image Manipulation,,,,Zhida Feng (Wuhan University Of Science And Technology) | Li Chen (Wuhan University Of Science And Technology) | Jing Tian (National University Of Singapore) | Jiaxiang Liu (Baidu) | Shikun Feng (Baidu),,,,,,,
Boosting Image Restoration via Priors from Pre-trained Models,"Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising.",http://arxiv.org/abs/2403.06793v1,,Xiaogang Xu (Zhejiang Lab) | Shu Kong (None) | Tao Hu (National University Of Singapore) | Zhe Liu (Zhejiang Lab) | Hujun Bao (Zhejiang University),2024-03-11 15:11:57+00:00,,,,,,
DiffForensics: Leveraging Diffusion Prior to Image Forgery Detection and Localization,,,,Zeqin Yu (Sun Yat-Sen University) | Jiangqun Ni (Sun Yat-Sen University) | Yuzhen Lin (Shenzhen University) | Haoyi Deng (Shenzhen University) | Bin Li (Shenzhen University),,,,,,,
Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation,"Diffusion-based models have gained significant popularity for text-to-image generation due to their exceptional image-generation capabilities. A risk with these models is the potential generation of inappropriate content, such as biased or harmful images. However, the underlying reasons for generating such undesired content from the perspective of the diffusion model's internal representation remain unclear. Previous work interprets vectors in an interpretable latent space of diffusion models as semantic concepts. However, existing approaches cannot discover directions for arbitrary concepts, such as those related to inappropriate concepts. In this work, we propose a novel self-supervised approach to find interpretable latent directions for a given concept. With the discovered vectors, we further propose a simple approach to mitigate inappropriate generation. Extensive experiments have been conducted to verify the effectiveness of our mitigation approach, namely, for fair generation, safe generation, and responsible text-enhancing generation.",http://arxiv.org/abs/2311.17216v1,,Hang Li (University Of Munich) | Chengzhi Shen (Technische Universit??t M??nchen) | Philip H.S. Torr (University Of Oxford) | Volker Tresp (Ludwig-Maximilians-Universit??t M??nchen) | Jindong Gu (University Of Oxford & Google Research),2023-11-28 20:40:45+00:00,,,,,,
Depth-Aware Concealed Crop Detection in Dense Agricultural Scenes,,,,Liqiong Wang (China Three Gorges University) | Jinyu Yang (University Of Birmingham) | Yanfu Zhang (College Of William And Mary) | Fangyi Wang (China Three Gorges University) | Feng Zheng (Southern University Of Science And Technology),,,,,,,
DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model,"This paper endeavors to advance the precision of snapshot compressive imaging (SCI) reconstruction for multispectral image (MSI). To achieve this, we integrate the advantageous attributes of established SCI techniques and an image generative model, propose a novel structured zero-shot diffusion model, dubbed DiffSCI. DiffSCI leverages the structural insights from the deep prior and optimization-based methodologies, complemented by the generative capabilities offered by the contemporary denoising diffusion model. Specifically, firstly, we employ a pre-trained diffusion model, which has been trained on a substantial corpus of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration allows for the successful completion of SCI reconstruction, especially in the case that current methods struggle to address effectively. Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, thus enabling seamless adaptation of the RGB diffusion model to MSIs. Thirdly, an accelerated algorithm is implemented to expedite the resolution of the data subproblem. This augmentation not only accelerates the convergence rate but also elevates the quality of the reconstruction process. We present extensive testing to show that DiffSCI exhibits discernible performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets. Our code will be available.",http://arxiv.org/abs/2311.11417v1,,Zhenghao Pan (Harbin Institute Of Technology (Shenzhen)) | Haijin Zeng (IMEC & Universiteit Gent) | Jiezhang Cao (ETH Zurich) | Kai Zhang (None) | Yongyong Chen (Harbin Institute Of Technology (Shenzhen)),2023-11-19 20:27:14+00:00,,,,,,
C2 KD: Bridging the Modality Gap for Cross-Modal Knowledge Distillation ,,,,"Fushuo Huo (Hong Kong Polytechnic University) | Wenchao Xu (The Hong Kong Polytechnic University) | Jingcai Guo (The Hong Kong Polytechnic University) | Haozhao Wang (Huazhong University Of Science And Technology) | Song Guo (Department Of Computer Science And Engineering, Hong Kong University Of Science And Technology)",,,,,,,
Video Harmonization with Triplet Spatio-Temporal Variation Patterns,,,,"Zonghui Guo (None) | XinYu Han (Ocean University Of China) | Jie Zhang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Shiguang Shan (Institute Of Computing Technology, Chinese Academy Of Sciences) | Haiyong Zheng (Ocean University Of China)",,,,,,,
Scene-adaptive and Region-aware Multi-modal Prompt for Open Vocabulary Object Detection,,,,Xiaowei Zhao (None) | Xianglong Liu (BUAA) | Duorui Wang (Beijing University Of Aeronautics And Astronautics) | Yajun Gao (Beihang University) | Zhide Liu (Beijing University Of Aeronautics And Astronautics),,,,,,,
Neighbor Relations Matter in Video Scene Detection,,,,Jiawei Tan (Chongqing University) | Hongxing Wang (Chongqing University) | Jiaxin Li (Chongqing University) | Zhilong Ou (Chongqing University) | Zhangbin Qian (Chongqing University),,,,,,,
DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly,"Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble",http://arxiv.org/abs/2402.19302v1,,"Gianluca Scarpellini (Universit?? Degli Studi Di Genova, Istituto Italiano Di Tecnologia) | Stefano Fiorini (Istituto Italiano Di Tecnologia) | Francesco Giuliari (Istituto Italiano Di Tecnologia) | Pietro Morerio (Istituto Italiano Di Tecnologia) | Alessio Del Bue (Istituto Italiano Di Tecnologia (IIT))",2024-02-29 16:09:12+00:00,,,,,,
DAP: A Dynamic Adversarial Patch for Evading Person Detectors,"Patch-based adversarial attacks were proven to compromise the robustness and reliability of computer vision systems. However, their conspicuous and easily detectable nature challenge their practicality in real-world setting. To address this, recent work has proposed using Generative Adversarial Networks (GANs) to generate naturalistic patches that may not attract human attention. However, such approaches suffer from a limited latent space making it challenging to produce a patch that is efficient, stealthy, and robust to multiple real-world transformations. This paper introduces a novel approach that produces a Dynamic Adversarial Patch (DAP) designed to overcome these limitations. DAP maintains a naturalistic appearance while optimizing attack efficiency and robustness to real-world transformations. The approach involves redefining the optimization problem and introducing a novel objective function that incorporates a similarity metric to guide the patch's creation. Unlike GAN-based techniques, the DAP directly modifies pixel values within the patch, providing increased flexibility and adaptability to multiple transformations. Furthermore, most clothing-based physical attacks assume static objects and ignore the possible transformations caused by non-rigid deformation due to changes in a person's pose. To address this limitation, a 'Creases Transformation' (CT) block is introduced, enhancing the patch's resilience to a variety of real-world distortions. Experimental results demonstrate that the proposed approach outperforms state-of-the-art attacks, achieving a success rate of up to 82.28% in the digital world when targeting the YOLOv7 detector and 65% in the physical world when targeting YOLOv3tiny detector deployed in edge-based smart cameras.",http://arxiv.org/abs/2305.11618v2,,"Amira Guesmi (New York University, Abu Dhabi) | Ruitian Ding (New York University) | Muhammad Abdullah Hanif (New York University, Abu Dhabi) | Ihsen Alouani (The Queen's University Belfast) | Muhammad Shafique (New York University Abu Dhabi)",2023-05-19 11:52:42+00:00,,,,,,
Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization,"The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information. NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance. However, these models are found to be vulnerable to adversarial attacks, which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores. In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the adversarial robustness of NR-IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\ell_1$ norm of the model's gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on NR-IQA models. Our study offers valuable insights into the adversarial robustness of NR-IQA models and provides a foundation for future research in this area.",http://arxiv.org/abs/2403.11397v1,,"Yujia Liu (School Of Computer Science, Peking University, Beijing, China) | Chenxi Yang (Peking University) | Dingquan Li (Peng Cheng Laboratory) | Jianhao Ding (Peking University) | Tingting Jiang (Peking University)",2024-03-18 01:11:53+00:00,,,,,,
PRDP: Proximal Reward Difference Prediction\for Large-Scale Reward Finetuning of Diffusion Models,"Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.",http://arxiv.org/abs/2402.08714v1,,Fei Deng (Google) | Qifei Wang (Google) | Wei Wei (Google) | Tingbo Hou (Google Research) | Matthias Grundmann (Google),2024-02-13 18:58:16+00:00,,,,,,
Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation,,,,Ba Ngo (Chonnam National University) | Nhat-Tuong Do-Tran (National Yang Ming Chiao Tung University) | Tuan-Ngoc Nguyen (FPT Telecom) | Hae-Gon Jeon (GIST) | Tae Jong Choi (Chonnam National University),,,,,,,
Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation,"Controllable generation of 3D assets is important for many practical applications like content creation in movies, games and engineering, as well as in AR/VR. Recently, diffusion models have shown remarkable results in generation quality of 3D objects. However, none of the existing models enable disentangled generation to control the shape and appearance separately. For the first time, we present a suitable representation for 3D diffusion models to enable such disentanglement by introducing a hybrid point cloud and neural radiance field approach. We model a diffusion process over point positions jointly with a high-dimensional feature space for a local density and radiance decoder. While the point positions represent the coarse shape of the object, the point features allow modeling the geometry and appearance details. This disentanglement enables us to sample both independently and therefore to control both separately. Our approach sets a new state of the art in generation compared to previous disentanglement-capable methods by reduced FID scores of 30-90% and is on-par with other non disentanglement-capable state-of-the art methods.",http://arxiv.org/abs/2312.14124v1,,"Philipp Schr??ppel (University Of Freiburg, Germany) | Christopher Wewer (Saarland Informatics Campus, Max-Planck Institute) | Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) | Eddy Ilg (None) | Thomas Brox (University Of Freiburg)",2023-12-21 18:46:27+00:00,,,,,,
PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution,"Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance. Built on this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO. For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements.",http://arxiv.org/abs/2403.07589v2,,"Honghao Chen (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xiangxiang Chu (MeiTuan) | Renyongjian (University Of The Chinese Academy Of Sciences) | Xin Zhao (University Of Science And Technology Beijing) | Kaiqi Huang (, Institute Of Automation, Chinese Academy Of Science)",2024-03-12 12:19:05+00:00,,,,,,
Unbiased Estimator for Distorted Conic in Camera Calibration,"In the literature, points and conics have been major features for camera geometric calibration. Although conics are more informative features than points, the loss of the conic property under distortion has critically limited the utility of conic features in camera calibration. Many existing approaches addressed conic-based calibration by ignoring distortion or introducing 3D spherical targets to circumvent this limitation. In this paper, we present a novel formulation for conic-based calibration using moments. Our derivation is based on the mathematical finding that the first moment can be estimated without bias even under distortion. This allows us to track moment changes during projection and distortion, ensuring the preservation of the first moment of the distorted conic. With an unbiased estimator, the circular patterns can be accurately detected at the sub-pixel level and can now be fully exploited for an entire calibration pipeline, resulting in significantly improved calibration. The entire code is readily available from https://github.com/ChaehyeonSong/discocal.",http://arxiv.org/abs/2403.04583v2,,Chaehyeon Song (Seoul National University) | Jaeho Shin (Seoul National University) | Myung-Hwan Jeon (Seoul National University) | Jongwoo Lim (Seoul National University) | Ayoung Kim (None),2024-03-07 15:29:11+00:00,,,,,,
LayoutFormer: Hierarchical Text Detection Towards Scene Text Understanding,,,,Min Liang (University Of Science And Technology Beijing) | Jia-Wei Ma (University Of Science And Technology Beijing) | Xiaobin Zhu (University Of Science And Technology Beijing) | Jingyan Qin (University Of Science And Technology Beijing) | Xu-Cheng Yin (University Of Science And Technology Beijing),,,,,,,
Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos,"The gigapixel scale of whole slide images (WSIs) poses a challenge for histopathology multi-modal chatbots, requiring a global WSI analysis for diagnosis, compounding evidence from different WSI patches. Current visual instruction datasets, generated through large language models, focus on creating question/answer pairs for individual image patches, which may lack diagnostic capacity on their own in histopathology, further complicated by the absence of spatial grounding in histopathology image captions. To bridge this gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131 histopathology-specific instruction question/answer pairs, that is collected by leveraging educational histopathology videos from YouTube, which provides spatial localization of captions by automatically extracting narrators' cursor movements. In addition, we provide contextual reasoning by extracting diagnosis and supporting facts from the entire video content to guide the extrapolative reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can reason beyond the given single image patch, enabling diagnostic reasoning and the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a comprehensive evaluation dataset created from 985 images and 1283 human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using public histopathology datasets, where Quilt-LLaVA significantly outperforms SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set VQA. Our code, data, and model are publicly available at quilt-llava.github.io.",http://arxiv.org/abs/2312.04746v1,,Mehmet Saygin Seyfioglu (University Of Washington) | Wisdom Ikezogwo (University Of Washington) | Fatemeh Ghezloo (University Of Washington) | Ranjay Krishna (University Of Washington) | Linda Shapiro (UW Reality Lab University Of Washington),2023-12-07 23:16:37+00:00,,,,,,
ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions,"Although Vision Transformer (ViT) has achieved significant success in computer vision, it does not perform well in dense prediction tasks due to the lack of inner-patch information interaction and the limited diversity of feature scale. Most existing studies are devoted to designing vision-specific transformers to solve the above problems, which introduce additional pre-training costs. Therefore, we present a plain, pre-training-free, and feature-enhanced ViT backbone with Convolutional Multi-scale feature interaction, named ViT-CoMer, which facilitates bidirectional interaction between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has the following advantages: (1) We inject spatial pyramid multi-receptive field convolutional features into the ViT architecture, which effectively alleviates the problems of limited local information interaction and single-feature representation in ViT. (2) We propose a simple and efficient CNN-Transformer bidirectional fusion interaction module that performs multi-scale fusion across hierarchical features, which is beneficial for handling dense prediction tasks. (3) We evaluate the performance of ViT-CoMer across various dense prediction tasks, different frameworks, and multiple advanced pre-training. Notably, our ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and 62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art methods. We hope ViT-CoMer can serve as a new backbone for dense prediction tasks to facilitate future research. The code will be released at https://github.com/Traffic-X/ViT-CoMer.",http://arxiv.org/abs/2403.07392v2,,Chunlong Xia (Baidu) | Xinliang Wang (Baidu) | Feng Lv (Baidu) | Xin Hao (Beijing Institute Of Technology) | Yifeng Shi (Baidu),2024-03-12 07:59:41+00:00,,,,,,
Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation,,,,Luca Barsellotti (University Of Modena And Reggio Emilia) | Roberto Amoroso (University Of Modena And Reggio Emilia) | Marcella Cornia (University Of Modena And Reggio Emilia) | Lorenzo Baraldi (Universit?? Degli Studi Di Modena E Reggio Emilia) | Rita Cucchiara (Universit?? Di Modena E Reggio Emilia),,,,,,,
ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering,"Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.",http://arxiv.org/abs/2312.05941v1,,"Haokai Pang (ETH Zurich) | Heming Zhu (Max Planck Institute For Informatics, Saarland Informatics Campus) | Adam Kortylewski (University Of Freiburg & MPI-INF) | Christian Theobalt (MPI Informatik) | Marc Habermann (Saarland Informatics Campus, Max-Planck Institute)",2023-12-10 17:07:37+00:00,,,,,,
Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias,"Domain adaptation is a critical task in machine learning that aims to improve model performance on a target domain by leveraging knowledge from a related source domain. In this work, we introduce Universal Semi-Supervised Domain Adaptation (UniSSDA), a practical yet challenging setting where the target domain is partially labeled, and the source and target label space may not strictly match. UniSSDA is at the intersection of Universal Domain Adaptation (UniDA) and Semi-Supervised Domain Adaptation (SSDA): the UniDA setting does not allow for fine-grained categorization of target private classes not represented in the source domain, while SSDA focuses on the restricted closed-set setting where source and target label spaces match exactly. Existing UniDA and SSDA methods are susceptible to common-class bias in UniSSDA settings, where models overfit to data distributions of classes common to both domains at the expense of private classes. We propose a new prior-guided pseudo-label refinement strategy to reduce the reinforcement of common-class bias due to pseudo-labeling, a common label propagation strategy in domain adaptation. We demonstrate the effectiveness of the proposed strategy on benchmark datasets Office-Home, DomainNet, and VisDA. The proposed strategy attains the best performance across UniSSDA adaptation settings and establishes a new baseline for UniSSDA.",http://arxiv.org/abs/2403.11234v1,,"Wenyu Zhang (Institute For Infocomm Research, A*STAR) | Qingmu Liu (National University Of Singapore) | Felix Ong (National University Of Singapore) | Mohamed Ragab (Institute For Infocomm Research , A*STAR) | Chuan-Sheng Foo (Centre For Frontier AI Research, A*STAR)",2024-03-17 14:43:47+00:00,,,,,,
Time-Efficient Light-Field Acquisition Using Coded Aperture and Events,"We propose a computational imaging method for time-efficient light-field acquisition that combines a coded aperture with an event-based camera. Different from the conventional coded-aperture imaging method, our method applies a sequence of coding patterns during a single exposure for an image frame. The parallax information, which is related to the differences in coding patterns, is recorded as events. The image frame and events, all of which are measured in a single exposure, are jointly used to computationally reconstruct a light field. We also designed an algorithm pipeline for our method that is end-to-end trainable on the basis of deep optics and compatible with real camera hardware. We experimentally showed that our method can achieve more accurate reconstruction than several other imaging methods with a single exposure. We also developed a hardware prototype with the potential to complete the measurement on the camera within 22 msec and demonstrated that light fields from real 3-D scenes can be obtained with convincing visual quality. Our software and supplementary video are available from our project website.",http://arxiv.org/abs/2403.07244v1,,Shuji Habuchi (Nagoya University) | Keita Takahashi (Nagoya University) | Chihiro Tsutake (Nagoya University) | Toshiaki Fujii (Nagoya University) | Hajime Nagahara (Osaka University),2024-03-12 02:04:17+00:00,,,,,,
Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI,,,,"Sean I. Young (Harvard Medical School / MIT) | Ya??l Balbastre (Massachusetts General Hospital, Harvard Medical School) | Bruce Fischl (Massachusetts General Hospital, Harvard University) | Polina Golland (Massachusetts Institute Of Technology) | Juan Iglesias (Harvard University)",,,,,,,
Patch2Self2: Self-supervised Denoising on Coresets via Matrix Sketching,,,,Shreyas Fadnavis (Johnson And Johnson) | Agniva Chowdhury (Oak Ridge National Laboratory) | Joshua Batson (Anthropic) | Petros Drineas (Purdue University) | Eleftherios Garyfallidis (Indiana University),,,,,,,
C3: High-performance and low-complexity neural compression from a single image or video,"Most neural compression models are trained on large datasets of images or videos in order to generalize to unseen data. Such generalization typically requires large and expressive architectures with a high decoding complexity. Here we introduce C3, a neural compression method with strong rate-distortion (RD) performance that instead overfits a small model to each image or video separately. The resulting decoding complexity of C3 can be an order of magnitude lower than neural baselines with similar RD performance. C3 builds on COOL-CHIC (Ladune et al.) and makes several simple and effective improvements for images. We further develop new methodology to apply C3 to videos. On the CLIC2020 image benchmark, we match the RD performance of VTM, the reference implementation of the H.266 codec, with less than 3k MACs/pixel for decoding. On the UVG video benchmark, we match the RD performance of the Video Compression Transformer (Mentzer et al.), a well-established neural video codec, with less than 5k MACs/pixel for decoding.",http://arxiv.org/abs/2312.02753v1,,Hyunjik Kim (DeepMind) | Matthias Bauer (Google DeepMind) | Lucas Theis (Google) | Jonathan Richard Schwarz (Harvard University) | Emilien Dupont (Google DeepMind),2023-12-05 13:28:59+00:00,,,,,,
Collaborating Foundation models for Domain Generalized Semantic Segmentation,"Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under varying weather conditions, notably outperforming prior methods by 5.6% and 6.7% on averaged miou, respectively. The code is available at : https://github.com/yasserben/CLOUDS",http://arxiv.org/abs/2312.09788v1,,"Mohammed-Yasser BENIGMIM (Telecom Paris) | Subhankar Roy (University Of Aberdeen) | Slim Essid (T??l??com Paris) | Vicky Kalogeiton (Ecole Polytechnique, IP Paris) | St??phane Lathuili??re (T??l??com ParisTech)",2023-12-15 13:43:24+00:00,,,,,,
Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing,"Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable Diffusion often contain object attribution information that can result in editing failures. In contrast, self-attention maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and self-attention maps in diffusion models. Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies self-attention maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.",http://arxiv.org/abs/2403.03431v1,,Bingyan Liu (South China University Of Technology) | Chengyu Wang (Alibaba Group) | Tingfeng Cao (South China University Of Technology) | Kui Jia (South China University Of Technology) | Jun Huang (University Of Science And Technology Of China),2024-03-06 03:32:56+00:00,,,,,,
"CAMixerSR: Only Details Need More ""Attention""","To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution. We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.",http://arxiv.org/abs/2402.19289v2,,Yan Wang (Nankai University) | Yi Liu (ByteDance) | Shijie Zhao (ByteDance) | Junlin Li (ByteDance) | Li Zhang (Bytedance),2024-02-29 15:52:59+00:00,,,,,,
Holistic Features are almost Sufficient for Text-to-Video Retrieval,,,,Kaibin Tian (None) | Ruixiang Zhao (None) | Zijie Xin (Sichuan University) | Bangxiang Lan (Renmin University Of China) | Xirong Li (Renmin University Of China),,,,,,,
Domain-Rectifying Adapter for Cross-domain Few-Shot Segmentation,,,,"?????? ??? (Harbin Institute Of Technology) | Qi Fan (The Hong Kong University Of Science And Technology) | Wenjie Pei (Harbin Institute Of Technology) | Guangming Lu (Harbin Institute Of Technology, Shenzhen) | Fanglin Chen (Harbin Institute Of Technology (Shenzhen))",,,,,,,
Action Scene Graphs for Long-Form Understanding of Egocentric Videos,"We present Egocentric Action Scene Graphs (EASGs), a new representation for long-form understanding of egocentric videos. EASGs extend standard manually-annotated representations of egocentric videos, such as verb-noun action labels, by providing a temporally evolving graph-based description of the actions performed by the camera wearer, including interacted objects, their relationships, and how actions unfold in time. Through a novel annotation procedure, we extend the Ego4D dataset by adding manually labeled Egocentric Action Scene Graphs offering a rich set of annotations designed for long-from egocentric video understanding. We hence define the EASG generation task and provide a baseline approach, establishing preliminary benchmarks. Experiments on two downstream tasks, egocentric action anticipation and egocentric activity summarization, highlight the effectiveness of EASGs for long-form egocentric video understanding. We will release the dataset and the code to replicate experiments and annotations.",http://arxiv.org/abs/2312.03391v1,,"Ivan Rodin (University Of Catania) | Antonino Furnari (University Of Catania) | Kyle Min (Intel Labs) | Subarna Tripathi (Intel Corporation) | Giovanni Maria Farinella (University Of Catania, Italy)",2023-12-06 10:01:43+00:00,,,,,,
Affine Equivariant Networks Based on Differential Invariants,,,,"Yikang Li (Peking University) | Yeqing Qiu (The Chinese Univeristy Of Hong Kong, Shenzhen) | Yuxuan Chen (Peking University) | Lingshen He (Peking University) | Zhouchen Lin (Peking University)",,,,,,,
ToNNO: Tomographic Reconstruction of a Neural Network??s Output for Weakly Supervised Segmentation of 3D Medical Images,,,,Marius Schmidt-Mengin (None) | Alexis Benichoux (INRIA) | Shibeshih Belachew (Therapanacea) | Nikos Komodakis (University Of Crete) | Nikos Paragios (Ecole Centrale De Paris),,,,,,,
Producing and Leveraging Online Map Uncertainty in Trajectory Prediction,,,,Xunjiang Gu (University Of Toronto) | Guanyu Song (University Of Toronto) | Igor Gilitschenski (University Of Toronto) | Marco Pavone (NVIDIA) | Boris Ivanovic (NVIDIA),,,,,,,
Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval,,,,Zhen-Duo Chen (Shandong University) | Li-Jun Zhao (Shandong University) | Zi-Chao Zhang (Shandong University) | Xin Luo (Shandong University) | Xin-Shun Xu (Shandong University),,,,,,,
"What, How, and When Should Object Detectors Update in Continually Changing Test Domains?","It is a well-known fact that the performance of deep learning models deteriorates when they encounter a distribution shift at test time. Test-time adaptation (TTA) algorithms have been proposed to adapt the model online while inferring test data. However, existing research predominantly focuses on classification tasks through the optimization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it challenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to update it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Furthermore, we present a practical and straightforward class-wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by determining when the model is sufficiently adapted or when additional adaptation is needed due to changes in the test distribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9\%p and 7.9\%p in mAP for COCO $\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher.",http://arxiv.org/abs/2312.08875v1,,Jayeon Yoo (Seoul National University) | Dongkwan Lee (Seoul National University) | Inseop Chung (Seoul National University) | Donghyun Kim (MIT-IBM Watson AI Lab) | Nojun Kwak (Seoul National University),2023-12-12 07:13:08+00:00,,,,,,
Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching,"Geometric knowledge has been shown to be beneficial for the stereo matching task. However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked. To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge. ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding. Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships. This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities. Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models.",http://arxiv.org/abs/2402.19270v2,,"Rui Gong (Nanyang Technological University) | Weide Liu (Harvard University) | ZAIWANG GU (None) | Xulei Yang (Institute For Infocomm Research (I2R), A*STAR) | Jun Cheng (Institute For Infocomm Research, A*STAR)",2024-02-29 15:39:40+00:00,,,,,,
A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning,"Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, improving deblur performance for labeled and unlabeled data. Extensive experiments and downstream task validation show the framework achieves state-of-the-art performance across multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.",http://arxiv.org/abs/2403.02611v1,,Yuelin Zhang (The Chinese University Of Hong Kong) | Pengyu Zheng (The Chinese University Of Hong Kong) | Wanquan Yan (The Chinese University Of Hong Kong) | Chengyu Fang (Tsinghua University) | Shing Shin Cheng (The Chinese University Of Hong Kong),2024-03-05 02:59:35+00:00,,,,,,
ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification,,,,"Jiangbo Shi (Xi'an Jiao Tong University) | Chen Li (Xi'an Jiao Tong University) | Tieliang Gong (Xi'an Jiao Tong University) | Yefeng Zheng (None) | Huazhu Fu (Institute Of High Performance Computing, Singapore, A*STAR)",,,,,,,
A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling,"Point cloud upsampling (PCU) enriches the representation of raw point clouds, significantly improving the performance in downstream tasks such as classification and reconstruction. Most of the existing point cloud upsampling methods focus on sparse point cloud feature extraction and upsampling module design. In a different way, we dive deeper into directly modelling the gradient of data distribution from dense point clouds. In this paper, we proposed a conditional denoising diffusion probability model (DDPM) for point cloud upsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a condition, and iteratively learns the transformation relationship between the dense point cloud and the noise. Simultaneously, PUDM aligns with a dual mapping paradigm to further improve the discernment of point features. In this context, PUDM enables learning complex geometry details in the ground truth through the dominant features, while avoiding an additional upsampling module design. Furthermore, to generate high-quality arbitrary-scale point clouds during inference, PUDM exploits the prior knowledge of the scale between sparse point clouds and dense point clouds during training by parameterizing a rate factor. Moreover, PUDM exhibits strong noise robustness in experimental results. In the quantitative and qualitative evaluations on PU1K and PUGAN, PUDM significantly outperformed existing methods in terms of Chamfer Distance (CD) and Hausdorff Distance (HD), achieving state of the art (SOTA) performance.",http://arxiv.org/abs/2312.02719v1,,Qu Wentao (Nanjing University Of Science And Technology) | Yuantian Shao (Nanjing University Of Science And Technology) | Lingwu Meng (Nanjing University Of Science And Technology) | Xiaoshui Huang (Shanghai AI Laboratory) | Liang Xiao (Nanjing University Of Science And Technology),2023-12-03 12:41:41+00:00,,,,,,
Generating Handwritten Mathematical Expressions From Symbol Graphs: An End-to-End Pipeline,,,,"Yu Chen (Beijing Waiyan Online Digital Technology Co., Ltd) | Fei Gao (Hangzhou Institute Of Technology, Xidian University) | YanguangZhang (Hangzhou Dianzi University) | Maoying Qiao (University Of Technology Sydney) | Nannan Wang (Xidian University)",,,,,,,
Snapshot Lidar: Fourier embedding of phasors for single-image depth reconstruction,,,,"Sarah Friday (Dartmouth College) | Yunzi Shi (Dartmouth College) | Yaswanth Kumar Cherivirala (Univ. Of Michigan/NVIDIA) | Vishwanath Saragadam (University Of California, Riverside) | Adithya Pediredla (Dartmouth College)",,,,,,,
Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence,,,,Ripon Saha (Arizona State University) | Dehao Qin (Clemson University) | Nianyi Li (None) | Jinwei Ye (None) | Suren Jayasuriya (Arizona State University),,,,,,,
"Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks",,,,Bowen Deng (Computer Vision Laboratory University Of Nottingham) | Siyang Song (University Of Leicester) | Andrew French (University Of Nottingham) | Denis Schluppeck (University Of Nottingham) | Michael Pound (University Of Nottingham),,,,,,,
Single-View Refractive Index Tomography with Neural Fields,"Refractive Index Tomography is the inverse problem of reconstructing the continuously-varying 3D refractive index in a scene using 2D projected image measurements. Although a purely refractive field is not directly visible, it bends light rays as they travel through space, thus providing a signal for reconstruction. The effects of such fields appear in many scientific computer vision settings, ranging from refraction due to transparent cells in microscopy to the lensing of distant galaxies caused by dark matter in astrophysics. Reconstructing these fields is particularly difficult due to the complex nonlinear effects of the refractive field on observed images. Furthermore, while standard 3D reconstruction and tomography settings typically have access to observations of the scene from many viewpoints, many refractive index tomography problem settings only have access to images observed from a single viewpoint. We introduce a method that leverages prior knowledge of light sources scattered throughout the refractive medium to help disambiguate the single-view refractive index tomography problem. We differentiably trace curved rays through a neural field representation of the refractive field, and optimize its parameters to best reproduce the observed image. We demonstrate the efficacy of our approach by reconstructing simulated refractive fields, analyze the effects of light source distribution on the recovered field, and test our method on a simulated dark matter mapping problem where we successfully recover the 3D refractive field caused by a realistic dark matter distribution.",http://arxiv.org/abs/2309.04437v2,,Brandon Zhao (California Institute Of Technology) | Aviad Levis (California Institute Of Technology) | Liam Connor (California Institute Of Technology) | Pratul P. Srinivasan (Google Research) | Katherine Bouman (California Institute Of Technology),2023-09-08 17:01:34+00:00,,,,,,
TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding,"This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.",http://arxiv.org/abs/2312.02051v1,,Shuhuai Ren (None) | Linli Yao (Peking University) | Shicheng Li (Peking University) | Xu Sun (Peking University) | Lu Hou (Huawei Technologies Ltd.),2023-12-04 17:09:52+00:00,,,,,,
InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models,"Large-scale text-to-image (T2I) diffusion models have showcased incredible capabilities in generating coherent images based on textual descriptions, enabling vast applications in content generation. While recent advancements have introduced control over factors such as object localization, posture, and image contours, a crucial gap remains in our ability to control the interactions between objects in the generated content. Well-controlling interactions in generated images could yield meaningful applications, such as creating realistic scenes with interacting characters. In this work, we study the problems of conditioning T2I diffusion models with Human-Object Interaction (HOI) information, consisting of a triplet label (person, action, object) and corresponding bounding boxes. We propose a pluggable interaction control model, called InteractDiffusion that extends existing pre-trained T2I diffusion models to enable them being better conditioned on interactions. Specifically, we tokenize the HOI information and learn their relationships via interaction embeddings. A conditioning self-attention layer is trained to map HOI tokens to visual tokens, thereby conditioning the visual tokens better in existing T2I diffusion models. Our model attains the ability to control the interaction and location on existing T2I diffusion models, which outperforms existing baselines by a large margin in HOI detection score, as well as fidelity in FID and KID. Project page: https://jiuntian.github.io/interactdiffusion.",http://arxiv.org/abs/2312.05849v2,,Jiun Tian Hoe (Nanyang Technological University) | Xudong Jiang (Nanyang Technological University) | Chee Seng Chan (Universiti Malaya) | Yap-Peng Tan (Nanyang Technological University) | Weipeng Hu (Nanyang Technological University),2023-12-10 10:35:16+00:00,,,,,,
APISR: Anime Production Inspired Real-World Anime Super-Resolution,"While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we introduce the balanced twin perceptual loss combining both anime and photorealistic high-level features to mitigate unwanted color artifacts and increase visual clarity. We evaluate our method through extensive experiments on the public benchmark, showing our method outperforms state-of-the-art approaches by a large margin.",http://arxiv.org/abs/2403.01598v1,,Boyang Wang (University Of Michigan - Ann Arbor) | Fengyu Yang (Yale University) | Xihang Yu (University Of Michigan - Ann Arbor) | Chao Zhang (Zhejiang University) | Hanbin Zhao (Zhejiang University),2024-03-03 19:52:43+00:00,,,,,,
MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection,,,,Jakub Micorek (Technische Universit??t Graz) | Horst Possegger (Graz University Of Technology) | Dominik Narnhofer (Technische Universit??t Graz) | Horst Bischof (Graz University Of Technology) | Mateusz Kozinski (Technische Universit??t Graz),,,,,,,
Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation,,,,Fahimeh HosseiniNoohdani (Sharif University Of Technology) | Parsa Hosseini (Sharif University Of Technology) | Aryan Yazdan Parast (Sharif University Of Technology) | Hamidreza Araghi (Sharif University Of Technology) | Mahdieh Baghshah (Sharif University Of Technology),,,,,,,
The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding,"Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenarios, where object classes are defined in free-text formats during inference. In this paper, we aim to probe the state-of-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained properties of objects and their parts. To this end, we introduce an evaluation protocol based on dynamic vocabulary generation to test whether models detect, discern, and assign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing different properties like color, pattern, and material. We further enhance our investigation by evaluating several state-of-the-art open-vocabulary object detectors using the proposed protocol and find that most existing solutions, which shine in standard open-vocabulary benchmarks, struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. Data and code are available at https://github.com/lorebianchi98/FG-OVD.",http://arxiv.org/abs/2311.17518v1,,Lorenzo Bianchi (CNR-ISTI) | Fabio Carrara (CNR-ISTI) | Nicola Messina (Institute Of Information Science And Technologies - National Research Council (ISTI-CNR)) | Claudio Gennaro (CNR) | Fabrizio Falchi (CNR),2023-11-29 10:40:52+00:00,,,,,,
Space-time Diffusion Features for Zero-shot Text-driven Motion Transfer,"We present a new method for text-driven motion transfer - synthesizing a video that complies with an input text prompt describing the target objects and scene while maintaining an input video's motion and scene layout. Prior methods are confined to transferring motion across two subjects within the same or closely related object categories and are applicable for limited domains (e.g., humans). In this work, we consider a significantly more challenging setting in which the target and source objects differ drastically in shape and fine-grained motion characteristics (e.g., translating a jumping dog into a dolphin). To this end, we leverage a pre-trained and fixed text-to-video diffusion model, which provides us with generative and motion priors. The pillar of our method is a new space-time feature loss derived directly from the model. This loss guides the generation process to preserve the overall motion of the input video while complying with the target object in terms of shape and fine-grained motion traits.",http://arxiv.org/abs/2311.17009v2,,Rafail Fridman (Weizmann Institute Of Science) | Danah Yatim (Weizmann Institute Of Science) | Omer Bar-Tal (Weizmann Institute Of Science) | Yoni Kasten (NVIDIA Research) | Tali Dekel (Weizmann Institute Of Science),2023-11-28 18:03:27+00:00,,,,,,
Purified and Unified Steganographic Network,"Steganography is the art of hiding secret data into the cover media for covert communication. In recent years, more and more deep neural network (DNN)-based steganographic schemes are proposed to train steganographic networks for secret embedding and recovery, which are shown to be promising. Compared with the handcrafted steganographic tools, steganographic networks tend to be large in size. It raises concerns on how to imperceptibly and effectively transmit these networks to the sender and receiver to facilitate the covert communication. To address this issue, we propose in this paper a Purified and Unified Steganographic Network (PUSNet). It performs an ordinary machine learning task in a purified network, which could be triggered into steganographic networks for secret embedding or recovery using different keys. We formulate the construction of the PUSNet into a sparse weight filling problem to flexibly switch between the purified and steganographic networks. We further instantiate our PUSNet as an image denoising network with two steganographic networks concealed for secret image embedding and recovery. Comprehensive experiments demonstrate that our PUSNet achieves good performance on secret image embedding, secret image recovery, and image denoising in a single architecture. It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network. Code is available at \url{https://github.com/albblgb/PUSNet}",http://arxiv.org/abs/2402.17210v1,,GuoBiao Li (Fudan University) | Sheng Li (Fudan University) | Zicong Luo (Fudan University) | Zhenxing Qian (Fudan University) | Xinpeng Zhang (Fudan University),2024-02-27 05:04:00+00:00,,,,,,
Convolutional Prompting meets Language Models for Continual Learning,,,,ANURAG Roy (IIT Kharagpur) | Riddhiman Moulick (Indian Institute Of Technology Kharagpur) | Vinay Verma Verma (None) | Saptarshi Ghosh (Indian Institute Of Technology Kharagpur) | Abir Das (Indian Institute Of Technology Kharagpur),,,,,,,
TutteNet: Injective 3D Deformations by Composition of 2D Mesh Deformations,,,,"Bo Sun (University Of Texas, Austin) | Thibault Groueix (Adobe Systems) | Chen Song (University Of Texas At Austin) | Qixing Huang (University Of Texas At Austin) | Noam Aigerman (Universit?? De Montr??al)",,,,,,,
Dr.Hair: Reconstructing Scalp-Connected Hair Strands without Pre-training via Differentiable Rendering of Line Segments,,,,Yusuke Takimoto (Huawei Technologies Japan K.K.) | Hikari Takehara (Huawei Technologies Japan K.K.) | Hiroyuki Sato (Huawei Technologies Japan K.K.) | Zihao Zhu (Keio University) | Bo Zheng (Huawei Technologies Japan),,,,,,,
SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing,"Image diffusion models have been utilized in various tasks, such as text-to-image generation and controllable image synthesis. Recent research has introduced tuning methods that make subtle adjustments to the original models, yielding promising results in specific adaptations of foundational generative diffusion models. Rather than modifying the main backbone of the diffusion model, we delve into the role of skip connection in U-Net and reveal that hierarchical features aggregating long-distance information across encoder and decoder make a significant impact on the content and quality of image generation. Based on the observation, we propose an efficient generative tuning framework, dubbed SCEdit, which integrates and edits Skip Connection using a lightweight tuning module named SC-Tuner. Furthermore, the proposed framework allows for straightforward extension to controllable image synthesis by injecting different conditions with Controllable SC-Tuner, simplifying and unifying the network design for multi-condition inputs. Our SCEdit substantially reduces training parameters, memory usage, and computational expense due to its lightweight tuners, with backward propagation only passing to the decoder blocks. Extensive experiments conducted on text-to-image generation and controllable image synthesis tasks demonstrate the superiority of our method in terms of efficiency and performance. Project page: \url{https://scedit.github.io/}",http://arxiv.org/abs/2312.11392v1,,"Zeyinzi Jiang (Alibaba Group) | Chaojie Mao (Alibaba Group) | Yulin Pan (Alibaba Group, China) | Zhen Han (Alibaba Group) | Jingfeng Zhang (Alibaba Group)",2023-12-18 17:54:14+00:00,,,,,,
Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching,"Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.",http://arxiv.org/abs/2402.17372v1,,Matteo Bastico (Mines Paris - PSL) | Etienne Decenci??re (Mines Paris) | Laurent Cort?? (Mines ParisTech) | Yannick TILLIER (Mines Paris - PSL) | David Ryckelynck (Mines Paris PSL University),2024-02-27 10:10:12+00:00,,,,,,
MedBN: Robust Test Time Adaptation against Malicious Test Samples,,,,Hyejin Park (Pohang University Of Science And Technology (POSTECH)) | Jeongyeon Hwang (Pohang University Of Science And Technology) | Sunung Mun (Pohang University Of Science And Technology) | Sangdon Park (POSTECH) | Jungseul Ok (POSTECH),,,,,,,
EGTR: Extracting Graph from Transformer for Scene Graph Generation,,,,Jinbae Im (NAVER Cloud) | Jeongyeon Nam (Naver Cloud) | Nokyung Park (Korea University) | Hyungmin Lee (NAVER) | Seunghyun Park (NAVER Cloud),,,,,,,
DemoFusion: Democratising High-Resolution Image Generation With No $,"High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as ""previews"", facilitating rapid prompt iteration.",http://arxiv.org/abs/2311.16973v2,,Ruoyi DU (Beijing University Of Posts And Telecommunications) | Dongliang Chang (Tsinghua University) | Timothy Hospedales (None) | Yi-Zhe Song (None) | Zhanyu Ma (Beijing University Of Post And Telecommunication),2023-11-24 00:16:00+00:00,,,,,,
On Exact Inversion of DPM-Solvers,,,,Seongmin Hong (Seoul National University) | Kyeonghyun Lee (Seoul National University) | Suh Yoon Jeon (Seoul National University) | Hyewon Bae (Seoul National University) | Se Young Chun (Seoul National University),,,,,,,
VideoCon: Robust Video-Language Alignment via Contrast Captions,"Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations. Our code and data are available at https://github.com/Hritikbansal/videocon.",http://arxiv.org/abs/2311.10111v1,,"Hritik Bansal (University Of California, Los Angeles) | Yonatan Bitton (Google) | Idan Szpektor (Google) | Kai-Wei Chang (University Of California, Los Angeles) | Aditya Grover (University Of California, Los Angeles)",2023-11-15 19:51:57+00:00,,,,,,
FedSOL: Stabilized Orthogonal Learning in Federated Learning,"Federated Learning(FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when clients have heterogeneous data distributions. This data heterogeneity causes the model to forget the global knowledge acquired from previously sampled clients after being trained on local datasets. Although the introduction of proximal objectives in local updates helps to preserve global knowledge, it can also hinder local learning by interfering with local objectives. To address this problem, we propose a novel method, Federated Stabilized Orthogonal Learning(FedSOL), which adopts an orthogonal learning strategy to balance the two conflicting objectives. FedSOL is designed to identify gradients of local objectives that are inherently orthogonal to directions affecting the proximal objective. Specifically, FedSOL targets parameter regions where learning on the local objective is minimally influenced by proximal weight perturbations. Our experiments demonstrate that FedSOL consistently achieves state-of-the-art performance across various scenarios.",http://arxiv.org/abs/2308.12532v5,,Gihun Lee (KAIST AI) | Minchan Jeong (Korea Advanced Institute Of Science And Technology) | SangMook Kim (KAIST) | Jaehoon Oh (Samsung Advanced Institute Of Technology) | Se-Young Yun (KAIST),2023-08-24 03:43:02+00:00,,,,,,
DIOD: Self-Distillation Meets Object Discovery,,,,Sandra Kara (CEA) | Hejer AMMAR (CEA) | Julien Denize (CEA) | Florian Chabot (CEA) | Quoc Cuong PHAM (CEA),,,,,,,
PolarRec: Radio Interferometric Data Reconstruction with Polar Coordinate Representation,"In radio astronomy, visibility data, which are measurements of wave signals from radio telescopes, are transformed into images for observation of distant celestial objects. However, these resultant images usually contain both real sources and artifacts, due to signal sparsity and other factors. One way to obtain cleaner images is to reconstruct samples into dense forms before imaging. Unfortunately, existing reconstruction methods often miss some components of visibility in frequency domain, so blurred object edges and persistent artifacts remain in the images. Furthermore, the computation overhead is high on irregular visibility samples due to the data skew. To address these problems, we propose PolarRec, a transformer-encoder-conditioned reconstruction pipeline with visibility samples converted into the polar coordinate representation. This representation matches the way in which radio telescopes observe a celestial area as the Earth rotates. As a result, visibility samples distribute in the polar system more uniformly than in the Cartesian space. Therefore, we propose to use radial distance in the loss function, to help reconstruct complete visibility effectively. Also, we group visibility samples by their polar angles and propose a group-based encoding scheme to improve the efficiency. Our experiments demonstrate that PolarRec markedly improves imaging results by faithfully reconstructing all frequency components in the visibility domain while significantly reducing the computation cost in visibility data encoding. We believe this high-quality and high-efficiency imaging of PolarRec will better facilitate astronomers to conduct their research.",http://arxiv.org/abs/2308.14610v2,,Ruoqi Wang (The Hong Kong University Of Science And Technology (Guangzhou)) | Zhuoyang Chen (The Hong Kong University Of Science And Technology (Guangzhou)) | Jiayi Zhu (Hong Kong University Of Science And Technology (Guangzhou)) | Qiong Luo (Hong Kong University Of Science And Technology) | Feng Wang (Guangzhou University),2023-08-28 14:26:15+00:00,,,,,,
Self-supervised debiasing using low rank regularization,"Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attributes. This biased encoder is then used to discover and upweight bias-conflicting samples in a downstream task, serving as a boosting to effectively debias the main model. Remarkably, the proposed debiasing framework significantly improves the generalization performance of self-supervised learning baselines and, in some cases, even outperforms state-of-the-art supervised debiasing approaches.",http://arxiv.org/abs/2210.05248v2,,Geon Yeong Park (Korea Advanced Institute Of Science And Technology) | Chanyong Jung (Korea Advanced Institute Of Science And Technology) | Sangmin Lee (Korea Advanced Institute Of Science & Technology) | Jong Chul Ye (Korea Advanced Institute Of Science And Technology) | Sang Wan Lee (Korea Advanced Institute Of Science & Technology),2022-10-11 08:26:19+00:00,,,,,,
SURE: SUrvey REcipes for building reliable and robust deep networks,"In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques--spanning model regularization, classifier and optimization--substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the benchmark of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new benchmark for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \url{https://yutingli0606.github.io/SURE/}.",http://arxiv.org/abs/2403.00543v1,,"Yuting Li (China Three Gorges University) | Yingyi Chen (Department Of Electrical Engineering, KU Leuven, Belgium) | Xuanlong Yu (Universit?? Paris-Saclay) | Dexiong Chen (Max Planck Institute Of Biochemistry) | Xi Shen (Tencent AI Lab)",2024-03-01 13:58:19+00:00,,,,,,
MuseChat: A Conversational Music Recommendation System for Videos,"Music recommendation for videos attracts growing interest in multi-modal research. However, existing systems focus primarily on content compatibility, often ignoring the users' preferences. Their inability to interact with users for further refinements or to provide explanations leads to a less satisfying experience. We address these issues with MuseChat, a first-of-its-kind dialogue-based recommendation system that personalizes music suggestions for videos. Our system consists of two key functionalities with associated modules: recommendation and reasoning. The recommendation module takes a video along with optional information including previous suggested music and user's preference as inputs and retrieves an appropriate music matching the context. The reasoning module, equipped with the power of Large Language Model (Vicuna-7B) and extended to multi-modal inputs, is able to provide reasonable explanation for the recommended music. To evaluate the effectiveness of MuseChat, we build a large-scale dataset, conversational music recommendation for videos, that simulates a two-turn interaction between a user and a recommender based on accurate music track information. Experiment results show that MuseChat achieves significant improvements over existing video-based music retrieval methods as well as offers strong interpretability and interactability.",http://arxiv.org/abs/2310.06282v4,,Zhikang Dong (State University Of New York At Stony Brook) | Bin Chen (Bytedance) | Xiulong Liu (University Of Washington) | Pawel Polak (State University Of New York At Stony Brook) | Peng Zhang (Bytedance),2023-10-10 03:32:33+00:00,,,,,,
iToF-flow-based High Frame Rate Depth Imaging,,,,Yu Meng (Nanjing University) | Zhou Xue (Li Auto) | Xu Chang (Bytedance Inc) | Xuemei Hu (Nanjing University) | Tao Yue (Nanjing University),,,,,,,
Exact Fusion via Feature Distribution Matching for Few-shot Image Generation,,,,Yingbo Zhou (East China Normal University) | Yutong Ye (None) | Pengyu Zhang (East China Normal University) | Xian Wei (Chinese Academy Of Sciences) | Mingsong Chen (East China Normal University),,,,,,,
An Empirical Study of Scaling Law for OCR,"The laws of model size, data volume, computation and model performance have been extensively studied in the field of Natural Language Processing (NLP). However, the scaling laws in Optical Character Recognition (OCR) have not yet been investigated. To address this, we conducted comprehensive studies that involved examining the correlation between performance and the scale of models, data volume and computation in the field of text recognition.Conclusively, the study demonstrates smooth power laws between performance and model size, as well as training data volume, when other influencing factors are held constant. Additionally, we have constructed a large-scale dataset called REBU-Syn, which comprises 6 million real samples and 18 million synthetic samples. Based on our scaling law and new dataset, we have successfully trained a scene text recognition model, achieving a new state-ofthe-art on 6 common test benchmarks with a top-1 average accuracy of 97.42%. The models and dataset are publicly available at https://github.com/large-ocr-model/large-ocr-model.github.io.",http://arxiv.org/abs/2401.00028v3,,Miao Rang (Huawei Noah's Ark Lab) | Zhenni Bi (Huawei Noah Ark Lab) | Chuanjian Liu (Huawei Technologies Ltd.) | Yunhe Wang (Huawei Noah's Ark Lab) | Kai Han (Huawei Noah's Ark Lab),2023-12-29 03:08:57+00:00,,,,,,
Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs,,,,"Kanchana Ranasinghe (None) | Satya Narayan Shukla (Meta AI) | Omid Poursaeed (Meta AI) | Michael Ryoo (Stony Brook University) | Tsung-Yu Lin (Department Of Computer Science, University Of Massachusetts, Amherst)",,,,,,,
Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models,"Denoising probabilistic diffusion models have shown breakthrough performance that can generate more photo-realistic images or human-level illustrations than the prior models such as GANs. This high image-generation capability has stimulated the creation of many downstream applications in various areas. However, we find that this technology is indeed a double-edged sword: We identify a new type of attack, called the Natural Denoising Diffusion (NDD) attack based on the finding that state-of-the-art deep neural network (DNN) models still hold their prediction even if we intentionally remove their robust features, which are essential to the human visual system (HVS), by text prompts. The NDD attack can generate low-cost, model-agnostic, and transferrable adversarial attacks by exploiting the natural attack capability in diffusion models. Motivated by the finding, we construct a large-scale dataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically evaluate the risk of the natural attack capability of diffusion models with state-of-the-art text-to-image diffusion models. We evaluate the natural attack capability by answering 6 research questions. Through a user study to confirm the validity of the NDD attack, we find that the NDD attack can achieve an 88% detection rate while being stealthy to 93% of human subjects. We also find that the non-robust features embedded by diffusion models contribute to the natural attack capability. To confirm the model-agnostic and transferrable attack capability, we perform the NDD attack against an AD vehicle and find that 73% of the physically printed attacks can be detected as a stop sign. We hope that our study and dataset can help our community to be aware of the risk of diffusion models and facilitate further research toward robust DNN models.",http://arxiv.org/abs/2308.15692v1,,"Takami Sato (None) | Justin Yue (University Of California, Irvine) | Nanze Chen (University Of Cambridge) | Ningfei Wang (University Of California, Irvine) | Alfred Chen (University Of California, Irvine)",2023-08-30 01:21:11+00:00,,,,,,
DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos,"This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that fine-tuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers.",http://arxiv.org/abs/2312.09523v1,,Arjun Balasingam (Massachusetts Institute Of Technology) | Joseph Chandler (Massachusetts Institute Of Technology) | Chenning Li (None) | Zhoutong Zhang (Adobe Systems) | Hari Balakrishnan (Massachusetts Institute Of Technology),2023-12-15 04:06:52+00:00,,,,,,
LLSS: Low-Latency Neural Stereo Streaming,,,,"Qiqi Hou (Qualcomm Inc, QualComm) | Farzad Farhadzadeh (Qualcomm Inc, QualComm) | Amir Said (Qualcomm Inc, QualComm) | Guillaume Sautiere (Qualcomm Inc, QualComm) | Hoang Le (Qualcomm AI Research)",,,,,,,
Label-Efficient Group Robustness via Out-of-Distribution Concept Curation,,,,Yiwei Yang (University Of Washington) | Anthony Liu (University Of Michigan) | Robert Wolfe (University Of Washington) | Aylin Caliskan (University Of Washington) | Bill Howe (University Of Washington),,,,,,,
Solving Masked Jigsaw Puzzles with Diffusion Transformers,,,,Jinyang Liu (Northeastern University) | Wondmgezahu Teshome (Northeastern University) | Sandesh Ghimire (QualComm) | Mario Sznaier (Northeastern University) | Octavia Camps (Northeastern University),,,,,,,
Improving Subject-Driven Image Synthesis with Context-Agnostic Guidance,,,,Kelvin C.K. Chan (Google) | Yang Zhao (Google) | Xuhui Jia (Google) | Ming-Hsuan Yang (University Of California At Merced) | Huisheng Wang (Google),,,,,,,
Relightable Gaussian Avatars,"The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with spatially all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.",http://arxiv.org/abs/2312.03704v1,,Shunsuke Saito (Reality Labs Research) | Gabriel Schwartz (Meta) | Tomas Simon (Meta) | Junxuan Li (Meta Reality Labs) | Giljoo Nam (Meta),2023-12-06 18:59:58+00:00,,,,,,
TIM: A Time Interval Machine for Audio-Visual Video Understand,,,,Jacob Chalk (None) | Jaesung Huh (University Of Oxford) | Evangelos Kazakos (Czech Technical University Of Prague) | Andrew Zisserman (University Of Oxford) | Dima Damen (None),,,,,,,
Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation,,,,Tianshui Chen (Guangdong University Of Technology) | Jianman Lin (Guangdong University Of Technology) | Zhijing Yang (Guangdong University Of Technology) | Chunmei Qing (South China University Of Technology) | Liang Lin (Sun Yat-Sen University),,,,,,,
A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint,,,,Xiaofeng Cong (Southeast University) | Jie Gui (Southeast University) | Jing Zhang (The University Of Sydney) | Junming Hou (Southeast University) | Hao Shen (Hefei University Of Technology),,,,,,,
Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models,"Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.",http://arxiv.org/abs/2312.13763v2,,"Huan Ling (Nvidia, University Of Toronto) | Seung Wook Kim (NVIDIA) | Antonio Torralba (MIT) | Sanja Fidler (Department Of Computer Science, University Of Toronto) | Karsten Kreis (NVIDIA)",2023-12-21 11:41:02+00:00,,,,,,
ADFactory: An Effective Framework for Generalizing Optical Flow with Nerf,"A significant challenge facing current optical flow methods is the difficulty in generalizing them well to the real world. This is mainly due to the high cost of hand-crafted datasets, and existing self-supervised methods are limited by indirect loss and occlusions, resulting in fuzzy outcomes. To address this challenge, we introduce a novel optical flow training framework: automatic data factory (ADF). ADF only requires RGB images as input to effectively train the optical flow network on the target data domain. Specifically, we use advanced Nerf technology to reconstruct scenes from photo groups collected by a monocular camera, and then calculate optical flow labels between camera pose pairs based on the rendering results. To eliminate erroneous labels caused by defects in the scene reconstructed by Nerf, we screened the generated labels from multiple aspects, such as optical flow matching accuracy, radiation field confidence, and depth consistency. The filtered labels can be directly used for network supervision. Experimentally, the generalization ability of ADF on KITTI surpasses existing self-supervised optical flow and monocular scene flow algorithms. In addition, ADF achieves impressive results in real-world zero-point generalization evaluations and surpasses most supervised methods.",http://arxiv.org/abs/2311.04246v2,,Han Ling (Nanjing University Of Science And Technology) | Quansen Sun (Nanjing University Of Science And Technology) | Yinghui Sun (Nanjing University Of Science And Technology) | Xian Xu (Southeast Community College Area) | Xingfeng Li (Nanjing University Of Science And Technology),2023-11-07 05:21:45+00:00,,,,,,
HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions,,,,Hao Xu (None) | Li Haipeng (None) | Yinqiao Wang (The Chinese University Of Hong Kong) | Shuaicheng Liu (None) | Chi-Wing Fu (The Chinese University Of Hong Kong),,,,,,,
MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant,"Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical multi-modal for generation. MedM2G is the first medical generative model that unifies medical generation tasks of text-to-image, image-to-text, and unified generation of medical modalities (CT, MRI, X-ray). It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works.",http://arxiv.org/abs/2403.04290v1,,Chenlu Zhan (None) | Gaoang Wang (Zhejiang University) | Yu LIN (Zhejiang University) | Hongwei Wang (Zhejiang University) | Jian Wu (Zhejiang University),2024-03-07 07:39:00+00:00,,,,,,
DePT: Decoupled Prompt Tuning,"This work breaks through the Base-New Tradeoff (BNT)dilemma in prompt tuning, i.e., the better the tuned model generalizes to the base (or target) task, the worse it generalizes to new tasks, and vice versa. Specifically, through an in-depth analysis of the learned features of the base and new tasks, we observe that the BNT stems from a channel bias issue, i.e., the vast majority of feature channels are occupied by base-specific knowledge, resulting in the collapse of taskshared knowledge important to new tasks. To address this, we propose the Decoupled Prompt Tuning (DePT) framework, which decouples base-specific knowledge from feature channels into an isolated feature space during prompt tuning, so as to maximally preserve task-shared knowledge in the original feature space for achieving better zero-shot generalization on new tasks. Importantly, our DePT is orthogonal to existing prompt tuning methods, hence it can improve all of them. Extensive experiments on 11 datasets show the strong flexibility and effectiveness of DePT. Our code and pretrained models are available at https://github.com/Koorye/DePT.",http://arxiv.org/abs/2309.07439v1,,"Ji Zhang (University Of Electronic Science And Technology Of China) | Shihan Wu (University Of Electronic Science And Technology Of China) | Lianli Gao (University Of Electronic Science And Technology Of China, Tsinghua University) | Heng Tao Shen (University Of Electronic Science And Technology Of China) | Jingkuan Song (University Of Electronic Science And Technology Of China,)",2023-09-14 05:45:40+00:00,,,,,,
Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning,,,,Rui Zhao (Xi'an Jiao Tong University) | Bin Shi (Xi'an Jiao Tong University) | Jianfei Ruan (Xi'an Jiao Tong University) | Tianze Pan (Xi'an Jiao Tong University) | Bo Dong (Xi'an Jiao Tong University),,,,,,,
Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment,,,,Muhammad Sohail Danish (Mohamed Bin Zayed University Of Artificial Intelligence) | Muhammad Haris Khan (None) | Muhammad Akhtar Munir (None) | M. Saquib Sarfraz (Karlsruhe Institute Of Technology / Mercedes-Benz) | Mohsen Ali (Information Technology University),,,,,,,
Learning Without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels,"Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual information. Besides, facing the spatial mismatch of training data, a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.",http://arxiv.org/abs/2403.02746v2,,Zhuohong Li (None) | Wei He (Wuhan University) | Jiepan Li (None) | Fangxiao Lu (Wuhan University) | Hongyan Zhang (China University Of Geosciences Wuhan),2024-03-05 08:02:00+00:00,,,,,,
Multiple View Geometry Transformers for 3D Human Pose Estimation,,,,Ziwei Liao (University Of Toronto) | Jialiang Zhu (Southeast University) | Chunyu Wang (Microsoft) | Han Hu (Microsft Research Asia) | Steven L. Waslander (University Of Toronto),,,,,,,
Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning,"Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve long-range flattening of the minima in the loss landscape. This approach considers representations that are differently normalized as minima in the loss landscape and flattens the high-loss region in the middle by randomly sampling interpolated representations. We implement this method as a new normalization layer that replaces the original one in both CNNs and ViTs. This layer is simple and lightweight, introducing only a minimal number of additional parameters. Experimental results on 8 datasets demonstrate that our approach outperforms state-of-the-art methods in terms of average accuracy. Moreover, our method achieves performance improvements of up to 9\% compared to the current best approaches on individual datasets. Our code will be released.",http://arxiv.org/abs/2403.00567v1,,Yixiong Zou (Huazhong University Of Science And Technology) | Yicong Liu (Huazhong University Of Science And Technology) | Yiman Hu (Huazhong University Of Science And Technology) | Yuhua Li (Huazhong University Of Science And Technology) | Ruixuan Li (Huazhong University Of Science And Technology),2024-03-01 14:44:41+00:00,,,,,,
Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding,"Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. However, existing methods encounter challenges in effectively handling both image and video understanding, particularly with limited visual tokens. In this work, we introduce Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation. Specifically, we employ a set of dynamic visual tokens to uniformly represent images and videos. This representation framework empowers the model to efficiently utilize a limited number of visual tokens to simultaneously capture the spatial details necessary for images and the comprehensive temporal relationship required for videos. Moreover, we leverage a multi-scale representation, enabling the model to perceive both high-level semantic concepts and low-level visual details. Notably, Chat-UniVi is trained on a mixed dataset containing both images and videos, allowing direct application to tasks involving both mediums without requiring any modifications. Extensive experimental results demonstrate that Chat-UniVi, as a unified model, consistently outperforms even existing methods exclusively designed for either images or videos.",http://arxiv.org/abs/2311.08046v1,,"Peng Jin (Peking University) | Ryuichi Takanobu (MiHoYo) | Cai Zhang (Nanrui Group Co., Ltd) | Xiaochun Cao (SUN YAT-SEN UNIVERSITY) | Li Yuan (Peking University)",2023-11-14 10:11:36+00:00,,,,,,
CLiC: Concept Learning in Context,"This paper addresses the challenge of learning a local visual pattern of an object from one image, and generating images depicting objects with that pattern. Learning a localized concept and placing it on an object in a target image is a nontrivial task, as the objects may have different orientations and shapes. Our approach builds upon recent advancements in visual concept learning. It involves acquiring a visual concept (e.g., an ornament) from a source image and subsequently applying it to an object (e.g., a chair) in a target image. Our key idea is to perform in-context concept learning, acquiring the local visual concept within the broader context of the objects they belong to. To localize the concept learning, we employ soft masks that contain both the concept within the mask and the surrounding image area. We demonstrate our approach through object generation within an image, showcasing plausible embedding of in-context learned concepts. We also introduce methods for directing acquired concepts to specific locations within target images, employing cross-attention mechanisms, and establishing correspondences between source and target objects. The effectiveness of our method is demonstrated through quantitative and qualitative experiments, along with comparisons against baseline techniques.",http://arxiv.org/abs/2311.17083v1,,Mehdi Safaee (SFU GrUVi Lab) | Aryan Mikaeili (Simon Fraser University) | Or Patashnik (Tel Aviv University) | Daniel Cohen-Or (Google) | Ali Mahdavi Amiri (Simon Fraser University),2023-11-28 01:33:18+00:00,,,,,,
Few-shot Learner Parameterization by Diffusion Time-steps,"Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif.",http://arxiv.org/abs/2403.02649v1,,Zhongqi Yue (Nanyang Technological University) | Pan Zhou (Sea Group) | Richang Hong (Hefei University Of Technology) | Hanwang Zhang (Nanyang Technological University) | Qianru Sun (None),2024-03-05 04:38:13+00:00,,,,,,
The STVchrono Dataset: Towards Continuous Change Recognition in Time,,,,"Yanjun Sun (Keio University) | Yue Qiu (AIST, National Institute Of Advanced Industrial Science And Technology) | Mariia Khan (Edith Cowan University) | Fumiya Matsuzawa (AIST, University Of Tsukuba) | Kenji Iwata (AIST, National Institute Of Advanced Industrial Science And Technology)",,,,,,,
"SPIN: Simultaneous Perception, Interaction and Navigation",,,,"Shagun Uppal (Carnegie Mellon University) | Ananye Agarwal (Carnegie Mellon University) | Haoyu Xiong (CMU, Carnegie Mellon University) | Kenneth Shaw (Carnegie Mellon University) | Deepak Pathak (Carnegie Mellon University)",,,,,,,
SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation,"Open-vocabulary semantic segmentation strives to distinguish pixels into different semantic groups from an open set of categories. Most existing methods explore utilizing pre-trained vision-language models, in which the key is to adopt the image-level model for pixel-level segmentation task. In this paper, we propose a simple encoder-decoder, named SED, for open-vocabulary semantic segmentation, which comprises a hierarchical encoder-based cost map generation and a gradual fusion decoder with category early rejection. The hierarchical encoder-based cost map generation employs hierarchical backbone, instead of plain transformer, to predict pixel-level image-text cost map. Compared to plain transformer, hierarchical backbone better captures local spatial information and has linear computational complexity with respect to input size. Our gradual fusion decoder employs a top-down structure to combine cost map and the feature maps of different backbone levels for segmentation. To accelerate inference speed, we introduce a category early rejection scheme in the decoder that rejects many no-existing categories at the early layer of decoder, resulting in at most 4.7 times acceleration without accuracy degradation. Experiments are performed on multiple open-vocabulary semantic segmentation datasets, which demonstrates the efficacy of our SED method. When using ConvNeXt-B, our SED method achieves mIoU score of 31.6\% on ADE20K with 150 categories at 82 millisecond ($ms$) per image on a single A6000. We will release it at \url{https://github.com/xb534/SED.git}.",http://arxiv.org/abs/2311.15537v2,,Xie Bin (None) | Jiale Cao (Tianjin University) | Jin Xie (Chongqing University) | Fahad Shahbaz Khan (MBZUAI; Link??ping University) | Yanwei Pang (Tianjin University),2023-11-27 05:00:38+00:00,,,,,,
Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network,,,,Yong Shu (Shanghai University) | Liquan Shen (Shanghai University) | Xiangyu Hu (Shanghai University) | Mengyao Li (Shanghai University) | Zihao Zhou (Shanghai University),,,,,,,
DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation,"Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets. Code is available at https://github.com/Wu0409/DuPL.",http://arxiv.org/abs/2403.11184v1,,Yuanchen Wu (None) | Xichen Ye (Shanghai University) | KequanYang (Shanghai University) | Jide Li (None) | Xiaoqiang Li (Shanghai University),2024-03-17 12:14:34+00:00,,,,,,
Segment Every Out-of-Distribution Object,,,,"Wenjie Zhao (Univeristy Of Texas At Dallas) | Jia Li (None) | Xin Dong (Harvard University) | Yu Xiang (University Of Texas, Dallas) | Yunhui Guo (The University Of Texas At Dallas)",,,,,,,
A Physics-informed Low-rank Deep Neural Network for Blind and Universal Lens Aberration Correction,,,,"Jin Gong (Tsinghua University) | Runzhao Yang (Department Of Automation, Tsinghua University) | Weihang Zhang (Tsinghua University) | Jinli Suo (Tsinghua University) | Qionghai Dai (Tsinghua University)",,,,,,,
Efficient Stitchable Task Adaptation,"The paradigm of pre-training and fine-tuning has laid the foundation for deploying deep learning models. However, most fine-tuning methods are designed to meet a specific resource budget. Recently, considering diverse deployment scenarios with various resource budgets, stitchable neural network (SN-Net) is introduced to quickly obtain numerous new networks (stitches) from the pre-trained models (anchors) in a model family via model stitching. Although promising, SN-Net confronts new challenges when adapting it to new target domains, including huge memory and storage requirements and a long and sub-optimal multistage adaptation process. In this work, we present a novel framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce a palette of fine-tuned models that adhere to diverse resource constraints. Specifically, we first tailor parameter-efficient fine-tuning to share low-rank updates among the stitches while maintaining independent bias terms. In this way, we largely reduce fine-tuning memory burdens and mitigate the interference among stitches that arises in task adaptation. Furthermore, we streamline a simple yet effective one-stage deployment pipeline, which estimates the important stitches to deploy with training-time gradient statistics. By assigning higher sampling probabilities to important stitches, we also get a boosted Pareto frontier. Extensive experiments on 25 downstream visual recognition tasks demonstrate that our ESTA is capable of generating stitches with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net adaptation by remarkable margins with significantly lower training time and fewer trainable parameters. Furthermore, we demonstrate the flexibility and scalability of our ESTA framework by stitching LLMs from LLaMA family, obtaining chatbot stitches of assorted sizes.",http://arxiv.org/abs/2311.17352v1,,Haoyu He (Monash University) | Zizheng Pan (None) | Jing Liu (None) | Jianfei Cai (Monash University) | Bohan Zhuang (Monash University),2023-11-29 04:31:35+00:00,,,,,,
Small Scale Data-Free Knowledge Distillation,,,,He Liu (None) | Yikai Wang (Tsinghua University) | Huaping Liu (Tsinghua University) | Fuchun Sun (Tsinghua University) | Anbang Yao (Intel),,,,,,,
Text-image Alignment for Diffusion-based Perception,,,,Neehar Kondapaneni (California Institute Of Technology) | Markus Marks (None) | Manuel Knott (ETHZ - ETH Zurich) | Rog??rio Guimar??es (California Institute Of Technology) | Pietro Perona (California Institute Of Technology),,,,,,,
Towards Variable and Coordinated Holistic Co-Speech Motion Generation,,,,Yifei Liu (South China University Of Technology) | Qiong Cao (JD Explore Academy) | Yandong Wen (Max Planck Institute For Intelligent Systems) | Huaiguang Jiang (South China University Of Technology) | Changxing Ding (South China University Of Technology),,,,,,,
ToonerGAN: Reinforcing GANs for Obfuscating Automated Facial Indexing,,,,"Kartik Thakral (Indian Institute Of Technology Jodhpur) | Shashikant Prasad (Indian Institute Of Technology, Jodhpur, Dhirubhai Ambani Institute Of Information And Communication Technology) | Stuti Aswani (Indian Institute Of Technology, Jodhpur) | Mayank Vatsa (IIT Jodhpur) | Richa Singh (IIT Jodhpur)",,,,,,,
"The More You See in 2D, the More You Perceive in 3D",,,,"Xinyang Han (None) | Zelin Gao (None) | Angjoo Kanazawa (UC Berkeley) | Shubham Goel (Avataar) | Yossi Gandelsman (University Of California, Berkeley)",,,,,,,
Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection,"Deepfake detection faces a critical generalization hurdle, with performance deteriorating when there is a mismatch between the distributions of training and testing data. A broadly received explanation is the tendency of these detectors to be overfitted to forgery-specific artifacts, rather than learning features that are widely applicable across various forgeries. To address this issue, we propose a simple yet effective detector called LSDA (\underline{L}atent \underline{S}pace \underline{D}ata \underline{A}ugmentation), which is based on a heuristic idea: representations with a wider variety of forgeries should be able to learn a more generalizable decision boundary, thereby mitigating the overfitting of method-specific features (see Figure. 1). Following this idea, we propose to enlarge the forgery space by constructing and simulating variations within and across forgery features in the latent space. This approach encompasses the acquisition of enriched, domain-specific features and the facilitation of smoother transitions between different forgery types, effectively bridging domain gaps. Our approach culminates in refining a binary classifier that leverages the distilled knowledge from the enhanced features, striving for a generalizable deepfake detector. Comprehensive experiments show that our proposed method is surprisingly effective and transcends state-of-the-art detectors across several widely used benchmarks.",http://arxiv.org/abs/2311.11278v1,,"Zhiyuan Yan (Tencent YouTu Lab) | Yuhao Luo (The Chinese University Of Hong Kong, Shenzhen) | Siwei Lyu (State University Of New York, Buffalo) | Qingshan Liu (Nanjing University Of Posts And Telecommunications) | Baoyuan Wu (The Chinese University Of Hong Kong, Shenzhen)",2023-11-19 09:41:10+00:00,,,,,,
PFStorer: Personalized Face Restoration and Super-Resolution,"Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs. The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context. In this paper, we explore the potential of personalized face restoration with diffusion models. In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details. By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest. To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed. With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization. Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach. We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method's ability to generate fine-grained details with faithful restoration. In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes.",http://arxiv.org/abs/2403.08436v1,,Tuomas Varanka (University Of Oulu) | Tapani Toivonen (Huawei Technologies Ltd.) | Soumya Tripathy (Huawei Technologies Ltd. Finland) | Guoying Zhao (None) | Erman Acar (Huawei Technologies),2024-03-13 11:39:30+00:00,,,,,,
DeIl: Direct and Inverse CLIP for Open-World Few-Shot Learning,,,,Shuai Shao (Zhejiang Lab) | Yu Bai (China University Of Petroleum???East China???) | Yan WANG (Beihang University) | Bao-Di Liu (China University Of Petroleum (East China)) | Yicong Zhou (University Of Macau),,,,,,,
Discriminability-Driven Channel Selection for Out-of-Distribution Detection,,,,Yue Yuan (Shandong University) | Rundong He (Shandong University) | Yicong Dong (Shandong University) | Zhongyi Han (Shandong University) | Yilong Yin (Shandong University),,,,,,,
CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor,"Existing open-vocabulary image segmentation methods require a fine-tuning step on mask annotations and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions when there are text queries referring to non-existing concepts in the image. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a VLM with frozen weights. Thus, our model retains the VLM's broad vocabulary space and strengthens its segmentation capability. Experimental results show that our method outperforms not only the training-free counterparts, but also those fine-tuned with millions of additional data samples, and sets new state-of-the-art records for both zero-shot semantic and referring image segmentation tasks. Specifically, we improve the current record by 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.",http://arxiv.org/abs/2312.07661v2,,Shuyang Sun (University Of Oxford) | Runjia Li (University Of Oxford) | Philip H.S. Torr (University Of Oxford) | Xiuye Gu (None) | Siyang Li (Google),2023-12-12 19:00:04+00:00,,,,,,
Tuning Stable Rank Shrinkage: Aiming at the Overlooked Structural Risk in Fine-tuning,,,,Sicong Shen (Beihang University) | Yang Zhou (Beihang University) | Bingzheng Wei (Xiaomi Corporation) | Eric Chang (Massachusetts Institute Of Technology) | Yan Xu (Beijing University Of Aeronautics And Astronautics),,,,,,,
PostureHMR: Posture Transformation for 3D Human Mesh Recovery,,,,Yupei Song (None) | Xiao WU (Southwest Jiao Tong University) | Zhaoquan Yuan (None) | Jian-Jun Qiao (Southwest Jiao Tong University) | Qiang Peng (Southwest Jiao Tong University),,,,,,,
GART: Gaussian Articulated Template Models,"We introduce Gaussian Articulated Template Model GART, an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos. GART utilizes a mixture of moving 3D Gaussians to explicitly approximate a deformable subject's geometry and appearance. It takes advantage of a categorical template model prior (SMPL, SMAL, etc.) with learnable forward skinning while further generalizing to more complex non-rigid deformations with novel latent bones. GART can be reconstructed via differentiable rendering from monocular videos in seconds or minutes and rendered in novel poses faster than 150fps.",http://arxiv.org/abs/2311.16099v1,,"Jiahui Lei (University Of Pennsylvania) | Yufu Wang (University Of Pennsylvania) | Georgios Pavlakos (University Of Texas At Austin) | Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) | Kostas Daniilidis (University Of Pennsylvania)",2023-11-27 18:59:30+00:00,,,,,,
Spatio-Temporal Turbulence Mitigation: A Translational Perspective,"Recovering images distorted by atmospheric turbulence is a challenging inverse problem due to the stochastic nature of turbulence. Although numerous turbulence mitigation (TM) algorithms have been proposed, their efficiency and generalization to real-world dynamic scenarios remain severely limited. Building upon the intuitions of classical TM algorithms, we present the Deep Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major challenges when transitioning from classical to deep learning approaches. By carefully integrating the merits of classical multi-frame TM methods into a deep network structure, we demonstrate that DATUM can efficiently perform long-range temporal aggregation using a recurrent fashion, while deformable attention and temporal-channel attention seamlessly facilitate pixel registration and lucky imaging. With additional supervision, tilt and blur degradation can be jointly mitigated. These inductive biases empower DATUM to significantly outperform existing methods while delivering a tenfold increase in processing speed. A large-scale training dataset, ATSyn, is presented as a co-invention to enable generalization in real turbulence. Our code and datasets will be available at \href{https://xg416.github.io/DATUM}{\textcolor{pink}{https://xg416.github.io/DATUM}}",http://arxiv.org/abs/2401.04244v1,,"Xingguang Zhang (Purdue University) | Nicholas M Chimitt (None) | Yiheng Chi (Purdue University) | Zhiyuan Mao (Samsung Research America) | Stanley H. Chan (Purdue University, USA)",2024-01-08 21:35:05+00:00,,,,,,
SPU-PMD: Self-Supervised Point Cloud Upsampling via Progressive Mesh Deformation,,,,Yanzhe Liu (Dalian Maritime University) | Rong Chen (Dalian Maritime University) | Yushi Li (Xi'an Jiaotong-Liverpool University) | Yixi Li (Dalian Martime University) | Xuehou Tan (Tokai University),,,,,,,
Towards Learning a Generalist Model for Embodied Navigation,"Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide range of capabilities required by embodied navigation. We conduct extensive experiments to evaluate the performance and generalizability of our model. The experimental results demonstrate that our unified model achieves state-of-the-art performance on CVDN, SOON, and ScanQA. Specifically, it surpasses the previous stats-of-the-art method by a significant margin of 29% in goal progress on CVDN. Moreover, our model also demonstrates strong generalizability and presents impressive results on unseen tasks, e.g., embodied question answering and 3D captioning.",http://arxiv.org/abs/2312.02010v2,,"Duo Zheng (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Shijia Huang (The Chinese University Of Hong Kong) | Lin Zhao (Beijing Institute Of Technology) | Yiwu Zhong (University Of Wisconsin, Madison) | Liwei Wang (CUHK)",2023-12-04 16:32:51+00:00,,,,,,
GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs,"As pretrained text-to-image diffusion models become increasingly powerful, recent efforts have been made to distill knowledge from these text-to-image pretrained models for optimizing a text-guided 3D model. Most of the existing methods generate a holistic 3D model from a plain text input. This can be problematic when the text describes a complex scene with multiple objects, because the vectorized text embeddings are inherently unable to capture a complex description with multiple entities and relationships. Holistic 3D modeling of the entire scene further prevents accurate grounding of text entities and concepts. To address this limitation, we propose GraphDreamer, a novel framework to generate compositional 3D scenes from scene graphs, where objects are represented as nodes and their interactions as edges. By exploiting node and edge information in scene graphs, our method makes better use of the pretrained text-to-image diffusion model and is able to fully disentangle different objects without image-level supervision. To facilitate modeling of object-wise relationships, we use signed distance fields as representation and impose a constraint to avoid inter-penetration of objects. To avoid manual scene graph creation, we design a text prompt for ChatGPT to generate scene graphs based on text inputs. We conduct both qualitative and quantitative experiments to validate the effectiveness of GraphDreamer in generating high-fidelity compositional 3D scenes with disentangled object entities.",http://arxiv.org/abs/2312.00093v1,,"Gege Gao (ETH Zurich) | Weiyang Liu (University Of Cambridge) | Anpei Chen (Department Of Computer Science, ETHZ - ETH Zurich) | Andreas Geiger (University Of T??bingen) | Bernhard Sch??lkopf (ELLIS Institute)",2023-11-30 18:59:58+00:00,,,,,,
PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models,"Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space. This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance.",http://arxiv.org/abs/2312.13964v1,,Yiming Zhang (None) | Zhening Xing (Shanghai AI Laboratory) | Yanhong Zeng (None) | Youqing Fang (Anhui University) | Kai Chen (Shanghai AI Laboratory),2023-12-21 15:51:12+00:00,,,,,,
HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D,"Recent progress in single-image 3D generation highlights the importance of multi-view coherency, leveraging 3D priors from large-scale diffusion models pretrained on Internet-scale images. However, the aspect of novel-view diversity remains underexplored within the research landscape due to the ambiguity in converting a 2D image into 3D content, where numerous potential shapes can emerge. Here, we aim to address this research gap by simultaneously addressing both consistency and diversity. Yet, striking a balance between these two aspects poses a considerable challenge due to their inherent trade-offs. This work introduces HarmonyView, a simple yet effective diffusion sampling technique adept at decomposing two intricate aspects in single-image 3D generation: consistency and diversity. This approach paves the way for a more nuanced exploration of the two critical dimensions within the sampling process. Moreover, we propose a new evaluation metric based on CLIP image and text encoders to comprehensively assess the diversity of the generated views, which closely aligns with human evaluators' judgments. In experiments, HarmonyView achieves a harmonious balance, demonstrating a win-win scenario in both consistency and diversity.",http://arxiv.org/abs/2312.15980v1,,Sangmin Woo (Korea Advanced Institute Of Science & Technology) | Byeongjun Park (None) | Hyojun Go (Twelvelabs) | Jin-Young Kim (Yonsei University) | Changick Kim (Korea Advanced Institute Of Science And Technology),2023-12-26 10:15:28+00:00,,,,,,
Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation,"As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to navigate to a specified object depicted by a goal image in an unexplored environment.   The main challenge of this task lies in identifying the target object from different viewpoints while rejecting similar distractors.   Existing ImageGoal Navigation methods usually adopt the simple Exploration-Exploitation framework and ignore the identification of specific instance during navigation.   In this work, we propose to imitate the human behaviour of ``getting closer to confirm"" when distinguishing objects from a distance.   Specifically, we design a new modular navigation framework named Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level image goal navigation.   Our method allows for active switching among the exploration, verification, and exploitation actions, thereby facilitating the agent in making reasonable decisions under different situations.   On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our method surpasses previous state-of-the-art work, with a classical segmentation model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success). Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.",http://arxiv.org/abs/2402.17587v1,,"Xiaohan Lei (None) | Min Wang (Institute Of Artificial Intelligence, Hefei Comprehensive National Science Center) | Wengang Zhou (University Of Science And Technology Of China) | Li Li (University Of Science And Technology Of China) | Houqiang Li (University Of Science And Technology Of China)",2024-02-25 07:59:10+00:00,,,,,,
SFOD: Spiking Fusion Object Detector,,,,"Yimeng Fan (School Of Microelectronics, Tianjin University) | Wei Zhang (None) | Changsong Liu (Tianjin University) | Mingyang Li (Tianjin University) | Wenrui Lu (Tianjin University)",,,,,,,
Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching,"The lightweight ""local-match-global"" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various ""local-match-global"" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective ""generalized matching"" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance across both small-scale and large-scale datasets. Specifically, G-VBSM achieves a performance of 38.7% on CIFAR-100 with 128-width ConvNet, 47.6% on Tiny-ImageNet with ResNet18, and 31.4% on the full 224x224 ImageNet-1k with ResNet18, under images per class (IPC) 10, 50, and 10, respectively. These results surpass all SOTA methods by margins of 3.9%, 6.5%, and 10.1%, respectively.",http://arxiv.org/abs/2311.17950v3,,"Shitong Shao (Southeast University) | Zeyuan Yin (Mohamed Bin Zayed University Of Artificial Intelligence) | Muxin Zhou (Mohamed Bin Zayed University Of Artificial Intelligence) | Xindong Zhang (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Zhiqiang Shen (Mohamed Bin Zayed University Of Artificial Intelligence)",2023-11-29 06:25:59+00:00,,,,,,
AAMDM: Accelerated Auto-regressive Motion Diffusion Model,"Interactive motion synthesis is essential in creating immersive experiences in entertainment applications, such as video games and virtual reality. However, generating animations that are both high-quality and contextually responsive remains a challenge. Traditional techniques in the game industry can produce high-fidelity animations but suffer from high computational costs and poor scalability. Trained neural network models alleviate the memory and speed issues, yet fall short on generating diverse motions. Diffusion models offer diverse motion synthesis with low memory usage, but require expensive reverse diffusion processes. This paper introduces the Accelerated Auto-regressive Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to achieve quality, diversity, and efficiency all together. AAMDM integrates Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a lower-dimensional embedded space rather than the full-dimensional pose space, which reduces the training complexity as well as further improves the performance. We show that AAMDM outperforms existing methods in motion quality, diversity, and runtime efficiency, through comprehensive quantitative analyses and visual comparisons. We also demonstrate the effectiveness of each algorithmic component through ablation studies.",http://arxiv.org/abs/2401.06146v1,,Tianyu Li (Georgia Institute Of Technology) | Calvin Zhuhan Qiao (University Of British Columbia) | Ren Guanqiao (Beijing University Of Aeronautics And Astronautics) | KangKang Yin (Simon Fraser University) | Sehoon Ha (Georgia Institute Of Technology),2023-12-02 23:52:21+00:00,,,,,,
Compact 3D Gaussian Representation for Radiance Field,,,,Joo Chan Lee (Sungkyunkwan University) | Daniel Rho (Korea Telecom Research) | Xiangyu Sun (None) | Jong Hwan Ko (Sungkyunkwan University (SKKU)) | Eunbyung Park (SKKU),,,,,,,
ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting,"The long-tailed distribution problem in medical image analysis reflects a high prevalence of common conditions and a low prevalence of rare ones, which poses a significant challenge in developing a unified model capable of identifying rare or novel tumor categories not encountered during training. In this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT) based on query-disentangling and self-prompting to segment unseen tumor categories beyond the training set. ZePT disentangles the object queries into two subsets and trains them in two stages. Initially, it learns a set of fundamental queries for organ segmentation through an object-aware feature grouping strategy, which gathers organ-level visual features. Subsequently, it refines the other set of advanced queries that focus on the auto-generated visual prompts for unseen tumor segmentation. Moreover, we introduce query-knowledge alignment at the feature level to enhance each query's discriminative representation and generalizability. Extensive experiments on various tumor segmentation tasks demonstrate the performance superiority of ZePT, which surpasses the previous counterparts and evidence the promising ability for zero-shot tumor segmentation in real-world settings. Codes will be made publicly available.",http://arxiv.org/abs/2312.04964v1,,Yankai Jiang (Shanghai Artificial Intelligence Laboratory) | Zhongzhen Huang (None) | Rongzhao Zhang (Shanghai Artificial Intelligence Laboratory) | Xiaofan Zhang (Shanghai Jiao Tong University) | Shaoting Zhang (University Of North Carolina At Charlotte),2023-12-07 12:09:56+00:00,,,,,,
Structure-Aware Sparse-View X-ray 3D Reconstruction,"X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data will be released at https://github.com/caiyuanhao1998/SAX-NeRF",http://arxiv.org/abs/2311.10959v2,,Yuanhao Cai (Johns Hopkins University) | Jiahao Wang (Johns Hopkins University) | Alan L. Yuille (Johns Hopkins University) | Zongwei Zhou (Johns Hopkins University) | Angtian Wang (Johns Hopkins University),2023-11-18 03:39:02+00:00,,,,,,
A Linear N-Point Solver for Line and Motion Estimation with Event Cameras,,,,Ling Gao (ShanghaiTech University) | Daniel Gehrig (None) | Hang Su (None) | Davide Scaramuzza (University Of Zurich) | Laurent Kneip (ShanghaiTech University),,,,,,,
DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion,,,,"Tom Van Wouwe (Stanford University) | Seunghwan Lee (Stanford University) | Antoine Falisse (Stanford University) | Scott Delp (Stanford University) | Karen Liu (Computer Science Department, Stanford University)",,,,,,,
Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking,"We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based prior enables more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent vector sets instead of using a global latent. This novel 4D representation allows us to learn local surface shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporal-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid the computational overhead, we design an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against the state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations, notably achieving a 19% improvement in Intersection over Union (IoU) compared to CaDex for reconstructing unseen individuals from sparse point clouds on the DeformingThings4D-Animals dataset. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.",http://arxiv.org/abs/2401.06614v1,,Wei Cao (Technische Universit??t M??nchen) | Chang Luo (Technical University Of Munich) | Biao Zhang (KAUST) | Matthias Nie??ner (Technical University Of Munich) | Jiapeng Tang (Technische Universit??t M??nchen),2024-01-12 15:05:08+00:00,,,,,,
SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection,,,,Mingxuan Liu (University Of Trento) | Tyler Hayes (Naver Labs Europe) | Elisa Ricci (University Of Trento) | Gabriela Csurka (None) | Riccardo Volpi (Naver Labs Europe),,,,,,,
D4 M: Dataset Distillation via Disentangled Diffusion Model ,,,,"Duo Su (University Of Chinese Academy Of Sciences) | Junjie Hou (None) | Weizhi Gao (North Carolina State University) | Yingjie Tian (, Chinese Academy Of Sciences) | Bowen Tang (Huawei Technologies Ltd.)",,,,,,,
Draw Step by Step Like Human: Reconstructing CAD Construction Sequences from Point Clouds via Multimodal Diffusion.,,,,Weijian Ma (Fudan University) | Shuaiqi Chen (Fudan University) | Yunzhong Lou (Fudan University) | Xueyang Li (Fudan University) | Xiangdong Zhou (Fudan University),,,,,,,
RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features,"The inherent noisy and sparse characteristics of radar data pose challenges in finding effective representations for 3D object detection. In this paper, we propose RadarDistill, a novel knowledge distillation (KD) method, which can improve the representation of radar data by leveraging LiDAR data. RadarDistill successfully transfers desirable characteristics of LiDAR features into radar features using three key components: Cross-Modality Alignment (CMA), Activation-based Feature Distillation (AFD), and Proposal-based Feature Distillation (PFD). CMA enhances the density of radar features through multiple layers of dilation operations, effectively addressing the challenges of inefficient knowledge transfer from LiDAR to radar. AFD is designed to transfer knowledge from significant areas of the LiDAR features, specifically those regions where activation intensity exceeds a predetermined threshold. PFD guides the radar network to mimic LiDAR network features in the object proposals for accurately detected results while moderating features for misdetected proposals like false positives. Our comparative analyses conducted on the nuScenes datasets demonstrate that RadarDistill achieves state-of-the-art (SOTA) performance for radar-only object detection task, recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantly improves the performance of the camera-radar fusion model.",http://arxiv.org/abs/2403.05061v1,,Geonho Bang (Hanyang University) | Kwangjin Choi (Hanyang University) | Jisong Kim (Hanyang University) | Dongsuk Kum (Korea Advanced Institute Of Science And Technology) | JUN CHOI CHOI (None),2024-03-08 05:15:48+00:00,,,,,,
PerceptionGPT: Effectively Fusing Visual Perception into LLM,"The integration of visual inputs with large language models (LLMs) has led to remarkable advancements in multi-modal capabilities, giving rise to visual large language models (VLLMs). However, effectively harnessing VLLMs for intricate visual perception tasks remains a challenge. In this paper, we present a novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding. Our proposed method treats the token embedding of the LLM as the carrier of spatial information, then leverage lightweight visual task encoders and decoders to perform visual perception tasks (e.g., detection, segmentation). Our approach significantly alleviates the training difficulty suffered by previous approaches that formulate the visual outputs as discrete tokens, and enables achieving superior performance with fewer trainable parameters, less training data and shorted training time. Moreover, as only one token embedding is required to decode the visual outputs, the resulting sequence length during inference is significantly reduced. Consequently, our approach enables accurate and flexible representations, seamless integration of visual perception tasks, and efficient handling of a multiple of visual outputs. We validate the effectiveness and efficiency of our approach through extensive experiments. The results demonstrate significant improvements over previous methods with much fewer trainable parameters and GPU hours, which facilitates future research in enabling LLMs with visual perception abilities.",http://arxiv.org/abs/2311.06612v1,,"Renjie Pi (None) | Lewei Yao (Harbin Institute Of Technology) | Jiahui Gao (The University Of Hong Kong) | Jipeng Zhang (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology) | Tong Zhang (UIUC)",2023-11-11 16:59:20+00:00,,,,,,
PAD: Patch-Agnostic Defense against Adversarial Patch Attacks,,,,"Lihua Jing (University Of The Chinese Academy Of Sciences) | Rui Wang (Institute Of Information Engineering) | Wenqi Ren (Sun Yat-Sen University) | Xin Dong (University Of The Chinese Academy Of Sciences) | Cong Zou (Institute Of Information Engineering, CAS)",,,,,,,
MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark,,,,Sanghyun Woo (New York University) | Kwanyong Park ( Electronics And Telecommunication Research Institute) | Inkyu Shin (Korea Advanced Institute Of Science & Technology) | Myungchul Kim (Korea Advanced Institute Of Science & Technology) | In So Kweon (Korea Advanced Institute Of Science And Technology),,,,,,,
Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households,,,,"Zhihao Cao (Tsinghua University) | ZiDong Wang (Department Of Automation, Tsinghua University) | Siwen Xie (Peking University) | Anji Liu (University Of California, Los Angeles) | Lifeng Fan (Beijing Institute Of General Artificial Intelligence)",,,,,,,
Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization,,,,Khiem Le (VinUniversity) | Tuan Long Ho (VinUniversity) | Cuong Do (None) | Danh Le-Phuoc (TU Berlin) | KOK SENG WONG (VinUniversity),,,,,,,
Benchmarking Segmentation Models with Mask-Preserved Attribute Editing,"When deploying segmentation models in practice, it is critical to evaluate their behaviors in varied and complex scenes. Different from the previous evaluation paradigms only in consideration of global attribute variations (e.g. adverse weather), we investigate both local and global attribute variations for robustness evaluation. To achieve this, we construct a mask-preserved attribute editing pipeline to edit visual attributes of real images with precise control of structural information. Therefore, the original segmentation labels can be reused for the edited images. Using our pipeline, we construct a benchmark covering both object and image attributes (e.g. color, material, pattern, style). We evaluate a broad variety of semantic segmentation models, spanning from conventional close-set models to recent open-vocabulary large models on their robustness to different types of variations. We find that both local and global attribute variations affect segmentation performances, and the sensitivity of models diverges across different variation types. We argue that local attributes have the same importance as global attributes, and should be considered in the robustness evaluation of segmentation models. Code: https://github.com/PRIS-CV/Pascal-EA.",http://arxiv.org/abs/2403.01231v2,,Zijin Yin (None) | Kongming Liang (Beijing University Of Posts And Telecommunications) | Bing Li (None) | Zhanyu Ma (Beijing University Of Post And Telecommunication) | Jun Guo (Beijing University Of Posts And Telecommunications),2024-03-02 15:20:09+00:00,,,,,,
Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training,"Adversarial training is often formulated as a min-max problem, however, concentrating only on the worst adversarial examples causes alternating repetitive confusion of the model, i.e., previously defended or correctly classified samples are not defensible or accurately classifiable in subsequent adversarial training. We characterize such non-ignorable samples as ""hiders"", which reveal the hidden high-risk regions within the secure area obtained through adversarial training and prevent the model from finding the real worst cases. We demand the model to prevent hiders when defending against adversarial examples for improving accuracy and robustness simultaneously. By rethinking and redefining the min-max optimization problem for adversarial training, we propose a generalized adversarial training algorithm called Hider-Focused Adversarial Training (HFAT). HFAT introduces the iterative evolution optimization strategy to simplify the optimization problem and employs an auxiliary model to reveal hiders, effectively combining the optimization directions of standard adversarial training and prevention hiders. Furthermore, we introduce an adaptive weighting mechanism that facilitates the model in adaptively adjusting its focus between adversarial examples and hiders during different training periods. We demonstrate the effectiveness of our method based on extensive experiments, and ensure that HFAT can provide higher robustness and accuracy.",http://arxiv.org/abs/2312.07067v1,,Qian Li (Dalian University Of Technology) | Yuxiao Hu (None) | Yinpeng Dong (Tsinghua University) | Dongxiao Zhang (Eastern Institute For Advanced Study) | Yuntian Chen (Eastern Institute For Advanced Study),2023-12-12 08:41:18+00:00,,,,,,
Towards High-fidelity Artistic Image Vectorization via Texture-Encapsulated Shape Parameterization,,,,Ye Chen (Shanghai Jiao Tong University) | Bingbing Ni (Shanghai Jiao Tong University) | Jinfan Liu (Tongji University) | Xiaoyang Huang (Shanghai Jiao Tong University) | Xuanhong Chen (Shanghai Jiao Tong University),,,,,,,
Regularized Parameter Uncertainty for Improving Generalization in Reinforcement Learning,,,,"Pehuen Moure (ETH Zurich) | Longbiao Cheng (Insititute Of Neuroinformatics, University Of Zurich And ETH Zurich) | Joachim Ott (Swiss Federal Institute Of Technology) | Zuowen Wang (Institute Of Neuroinformatics, University Of Zurich And ETH Zurich) | Shih-Chii Liu (University Of Zurich And ETH Zurich)",,,,,,,
Molecular Data Programming: Towards Molecule Pseudo-labeling with Systematic Weak Supervision,,,,Xin Juan (None) | Kaixiong Zhou (Rice University) | Ninghao Liu (University Of Georgia) | Tianlong Chen (Massachusetts Institute Of Technology) | Xin Wang (Jilin University),,,,,,,
Would Deep Generative Models Amplify Bias in Future Models?,,,,Tianwei Chen (None) | Yusuke Hirota (Osaka University) | Mayu Otani (None) | Noa Garcia (Osaka University) | Yuta Nakashima (Osaka University),,,,,,,
HUGS: Human Gaussian Splatting,"Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs",http://arxiv.org/abs/2311.17910v1,,"Muhammed Kocabas (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Jen-Hao Rick Chang (Apple) | James Gabriel (Apple) | Oncel Tuzel (Apple) | Anurag Ranjan (Apple)",2023-11-29 18:56:32+00:00,,,,,,
"TurboSL: Dense, Accurate and Fast 3D by Neural Inverse Structured Light",,,,Parsa Mirdehghan (University Of Toronto) | Maxx Wu (University Of Toronto) | Wenzheng Chen (University Of Toronto) | David B. Lindell (University Of Toronto) | Kiriakos Kutulakos (University Of Toronto),,,,,,,
Tumor Micro-environment Interactions Guided Graph Learning for Survival Analysis of Human Cancers from Whole-slide Pathological Images.,,,,WEI SHAO (Nanjing University Of Aeronautics And Astronautics) | YangYang Shi (Nanjing University Of Aeronautics And Astronautics) | Daoqiang Zhang (Nanjing University Of Aeronautics And Astronautics) | Junjie Zhou (Nanjing University Of Aeronautics And Astronautics) | Peng Wan (Nanjing University Of Aeronautics And Astronautics),,,,,,,
Pushing the limits of 3D Object Detection with X-Ray Teachers,,,,Alexander Gambashidze (AIRI) | Aleksandr Dadukin (Higher School Of Economics) | Maksim Golyadkin (AIRI) | Maria Razzhivina (Higher School Of Economics) | Ilya Makarov (Moscow State Institute Of Steel And Alloys),,,,,,,
Language Models as Black-Box Optimizers for Vision-Language Models,,,,Shihong Liu (Carnegie Mellon University) | Samuel Yu (Carnegie Mellon University) | Zhiqiu Lin (Carnegie Mellon University) | Deepak Pathak (Carnegie Mellon University) | Deva Ramanan (Carnegie Mellon University),,,,,,,
Learning Transferable Negative Prompts for Out-of-Distribution Detection,,,,Tianqi Li (None) | Guansong Pang (Singapore Management University) | Wenjun Miao (None) | Xiao Bai (Beijing University Of Aeronautics And Astronautics) | Jin Zheng (Beijing University Of Aeronautics And Astronautics),,,,,,,
Authentic Hand Avatar from a Phone Scan via Universal Hand Model,,,,Gyeongsik Moon (None) | Weipeng Xu (Meta Reality Labs Research) | Rohan Joshi (Facebook) | Chenglei Wu (Meta) | Takaaki Shiratori (Meta Reality Labs Research),,,,,,,
Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model,,,,Tian Liang (None) | Jing Huang (Zhejiang University) | Ming Kong (None) | Luyuan Chen (Beijing Information Science And Technology University) | Qiang Zhu (Zhejiang University),,,,,,,
SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image,,,,Yunhao Li (Westlake University) | Xiaodong Wang (Zhejiang University) | Ping Wang (Zhejiang University) | Xin Yuan (Westlake University) | Peidong Liu (None),,,,,,,
"Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion","Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU. The project page is at \url{https://sites.google.com/view/diffseg/home}.",http://arxiv.org/abs/2308.12469v2,,Junjiao Tian (Georgia Institute Of Technology) | Lavisha Aggarwal (Amazon) | Andrea Colaco (Google) | Zsolt Kira (Georgia Institute Of Technology) | Mar Gonzalez-Franco (Google),2023-08-23 23:44:44+00:00,,,,,,
"Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction","We introduce multi-slice reasoning, a new notion for single-view 3D reconstruction which challenges the current and prevailing belief that multi-view synthesis is the most natural conduit between single-view and 3D. Our key observation is that object slicing is more advantageous than altering views to reveal occluded structures. Specifically, slicing is more occlusion-revealing since it can peel through any occluders without obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed to unveil all hidden object parts. We realize our idea by developing Slice3D, a novel method for single-view 3D reconstruction which first predicts multi-slice images from a single RGB image and then integrates the slices into a 3D model using a coordinate-based transformer network for signed distance prediction. The slice images can be regressed or generated, both through a U-Net based network. For the former, we inject a learnable slice indicator code to designate each decoded image into a spatial slice location, while the slice generator is a denoising diffusion model operating on the entirety of slice images stacked on the input channels. We conduct extensive evaluation against state-of-the-art alternatives to demonstrate superiority of our method, especially in recovering complex and severely occluded shape structures, amid ambiguities. All Slice3D results were produced by networks trained on a single Nvidia A40 GPU, with an inference time less than 20 seconds.",http://arxiv.org/abs/2312.02221v2,,"Yizhi Wang (Simon Fraser University) | Wallace Lira (Simon Fraser University) | Wenqi Wang (Computing Science, Simon Fraser University) | Ali Mahdavi Amiri (Simon Fraser University) | Hao Zhang (Simon Fraser University)",2023-12-03 18:36:40+00:00,,,,,,
ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting,"In recent years, text-image joint pre-training techniques have shown promising results in various tasks. However, in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and OCR-Text (referring to the text in images as OCR-Text to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called OCR-Text Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text prompt. With ODM, we achieve better alignment between text and OCR-Text and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in OCR tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at {https://github.com/PriNing/ODM}.",http://arxiv.org/abs/2403.00303v1,,Chen Duan (Meituan) | Pei Fu (Meituan) | Shan Guo (Meituan) | Qianyi Jiang (Meituan) | Xiaoming Wei (Meituan),2024-03-01 06:13:53+00:00,,,,,,
MACE: Mass Concept Erasure in Diffusion Models,"The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at https://github.com/Shilin-LU/MACE.",http://arxiv.org/abs/2403.06135v1,,"Shilin Lu (Nanyang Technological University) | Zilan Wang (Nanyang Technological University) | Leyang Li (Nanyang Technological University) | Yanzhu Liu (I2R, A*STAR) | Adams Wai-Kin Kong (Nanyang Technological University)",2024-03-10 08:50:56+00:00,,,,,,
XFeat: Accelerated Features for Lightweight Image Matching,,,,Guilherme Potje (Federal University Of Minas Gerais) | Felipe Cadar (Universidade Federal De Minas Gerais) | Andr?? Araujo (Google Research) | Renato Martins (Universit?? De Bourgogne) | Erickson R. Nascimento (Universidade Federal De Minas Gerais / Microsoft),,,,,,,
Day-Night Cross-domain Vehicle Re-identification,,,,Hongchao Li (Anhui Normal University) | Jingong Chen (Anhui Normal University) | AIHUA ZHENG (Anhui University) | Yong Wu (Anhui Normal University) | YongLong Luo (Anhui Normal University),,,,,,,
Structure-Guided Adversarial Training of Diffusion Models,,,,Ling Yang (Peking University) | Haotian Qian (Peking University) | Zhilong Zhang (Peking University) | Jingwei Liu (Peking University) | Bin CUI (Peking University),,,,,,,
FedUV: Uniformity and Variance for Heterogeneous Federated Learning,"Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting regardless of the local data distribution, thus offsetting proneness to bias while being flexible to the data. On extensive experiments in both label-shift and feature-shift settings, we verify that our method achieves highest performance by a large margin especially in highly non-IID cases in addition to being scalable to larger models and datasets.",http://arxiv.org/abs/2402.18372v2,,"Ha Min Son (University Of California, Davis) | Moon-Hyun Kim (Sungkyunkwan University) | Tai-Myoung Chung (Sung Kyun Kwan University) | Chao Huang (University Of California, Davis) | Xin Liu (University Of California, Davis)",2024-02-27 15:53:15+00:00,,,,,,
Insights from the Use of Previously Unseen Neural Architecture Search Datasets,,,,"Rob Geada (University Of Newcastle-Upon-Tyne) | David Towers (Newcastle University) | Matthew Forshaw (Newcastle University, UK) | Amir Atapour-Abarghouei (Durham University) | Stephen McGough (None)",,,,,,,
Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles,"We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines. Current AI-based generative models take advantage of powerful 2D priors to reconstruct 3D content in the form of point clouds, meshes, or volumetric functions. However, by using the 2D priors, they are intrinsically limited to only recovering the visual parts. Highly occluded hair structures can not be reconstructed with those methods, and they only model the ''outer shell'', which is not ready to be used in physics-based rendering or simulation pipelines. In contrast, we propose a first text-guided generative method that uses 3D hair strands as an underlying representation. Leveraging 2D visual question-answering (VQA) systems, we automatically annotate synthetic hair models that are generated from a small set of artist-created hairstyles. This allows us to train a latent diffusion model that operates in a common hairstyle UV space. In qualitative and quantitative studies, we demonstrate the capabilities of the proposed model and compare it to existing hairstyle generation approaches.",http://arxiv.org/abs/2312.11666v1,,"Vanessa Skliarova (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Egor Zakharov (Skolkovo Institute Of Science And Technology) | Otmar Hilliges (None) | Michael J. Black (University Of T??bingen) | Justus Thies (Max-Planck Institute For Intelligent Systems)",2023-12-18 19:19:32+00:00,,,,,,
Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting,,,,Haipeng Liu (Hefei University Of Technology) | Yang Wang (Hefei University Of Technology) | Biao Qian (None) | Meng Wang (Hefei University Of Technology) | Yong Rui (Lenovo),,,,,,,
Mip-Splatting: Alias-free 3D Gaussian Splatting,"Recently, 3D Gaussian Splatting has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency. However, strong artifacts can be observed when changing the sampling rate, \eg, by changing focal length or camera distance. We find that the source for this phenomenon can be attributed to the lack of 3D frequency constraints and the usage of a 2D dilation filter. To address this problem, we introduce a 3D smoothing filter which constrains the size of the 3D Gaussian primitives based on the maximal sampling frequency induced by the input views, eliminating high-frequency artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip filter, which simulates a 2D box filter, effectively mitigates aliasing and dilation issues. Our evaluation, including scenarios such a training on single-scale images and testing on multiple scales, validates the effectiveness of our approach.",http://arxiv.org/abs/2311.16493v1,,"Zehao Yu (None) | Anpei Chen (Department Of Computer Science, ETHZ - ETH Zurich) | Binbin Huang (ShanghaiTech University) | Torsten Sattler (Czech Technical University In Prague) | Andreas Geiger (University Of T??bingen)",2023-11-27 13:03:09+00:00,,,,,,
HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation,,,,Ce Zhang (Carnegie Mellon University) | Simon Stepputtis (Carnegie Mellon University) | Joseph Campbell (Carnegie Mellon University) | Katia Sycara (Carnegie Mellon University) | Yaqi Xie (CMU),,,,,,,
ProMark: Proactive Diffusion Watermarking for Causal Attribution,"Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as $2^{16}$ unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.",http://arxiv.org/abs/2403.09914v1,,Vishal Asnani (Michigan State University) | John Collomosse (University Of Surrey) | Tu Bui (Fujitsu Research And Development Center Co. Ltm.) | Xiaoming Liu (None) | Shruti Agarwal (None),2024-03-14 23:16:43+00:00,,,,,,
MoMask: Generative Masked Modeling of 3D Human Motions,"We introduce MoMask, a novel masked modeling framework for text-driven 3D human motion generation. In MoMask, a hierarchical quantization scheme is employed to represent human motion as multi-layer discrete motion tokens with high-fidelity details. Starting at the base layer, with a sequence of motion tokens obtained by vector quantization, the residual tokens of increasing orders are derived and stored at the subsequent layers of the hierarchy. This is consequently followed by two distinct bidirectional transformers. For the base-layer motion tokens, a Masked Transformer is designated to predict randomly masked motion tokens conditioned on text input at training stage. During generation (i.e. inference) stage, starting from an empty sequence, our Masked Transformer iteratively fills up the missing tokens; Subsequently, a Residual Transformer learns to progressively predict the next-layer tokens based on the results from current layer. Extensive experiments demonstrate that MoMask outperforms the state-of-art methods on the text-to-motion generation task, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset, and 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly applied in related tasks without further model fine-tuning, such as text-guided temporal inpainting.",http://arxiv.org/abs/2312.00063v1,,Chuan Guo (University Of Alberta) | Yuxuan Mu (University Of Alberta) | Muhammad Gohar Javed (University Of Alberta) | Sen Wang (HoYoverse) | Li Cheng (University Of Alberta),2023-11-29 19:04:10+00:00,,,,,,
BigGait: Learning Gait Representation You Want by Large Vision Models,"Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an unsupervised manner, drawing from design principles of established gait representation construction approaches. Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code will be available at https://github.com/ShiqiYu/OpenGait.",http://arxiv.org/abs/2402.19122v1,,Dingqiang Ye (None) | Chao Fan (None) | Jingzhe Ma (Southern University Of Science And Technology) | Xiaoming Liu (None) | Shiqi Yu (Southern University Of Science And Technology),2024-02-29 13:00:22+00:00,,,,,,
Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting,"Despite significant progress in the field, it is still challenging to create personalized visual representations that align closely with the desires and preferences of individual users. This process requires users to articulate their ideas in words that are both comprehensible to the models and accurately capture their vision, posing difficulties for many users. In this paper, we tackle this challenge by leveraging historical user interactions with the system to enhance user prompts. We propose a novel approach that involves rewriting user prompts based on a newly collected large-scale text-to-image dataset with over 300k prompts from 3115 users. Our rewriting model enhances the expressiveness and alignment of user prompts with their intended visual outputs. Experimental results demonstrate the superiority of our methods over baseline approaches, as evidenced in our new offline evaluation method and online tests. Our code and dataset are available at https://github.com/zzjchen/Tailored-Visions .",http://arxiv.org/abs/2310.08129v2,,Zijie Chen (Westlake University) | Lichao Zhang (Westlake University) | Fangsheng Weng (Xinchenai.Com/) | Lili Pan (University Of Electronic Science And Technology Of China) | ZHENZHONG Lan (None),2023-10-12 08:36:25+00:00,,,,,,
POPDG:Popular 3D Dance Generation with PopDanceSet,,,,ZhenYe Luo (Beijing Normal University) | Min Ren (Beijing Normal University) | Xuecai Hu (Beijing Normal University) | Yongzhen Huang (Beijing Normal University) | Li Yao (Beijing Normal University),,,,,,,
Is Conventional SNN Really Efficient? A Perspective from Network Quantization,"Spiking Neural Networks (SNNs) have been widely praised for their high energy efficiency and immense potential. However, comprehensive research that critically contrasts and correlates SNNs with quantized Artificial Neural Networks (ANNs) remains scant, often leading to skewed comparisons lacking fairness towards ANNs. This paper introduces a unified perspective, illustrating that the time steps in SNNs and quantized bit-widths of activation values present analogous representations. Building on this, we present a more pragmatic and rational approach to estimating the energy consumption of SNNs. Diverging from the conventional Synaptic Operations (SynOps), we champion the ""Bit Budget"" concept. This notion permits an intricate discourse on strategically allocating computational and storage resources between weights, activation values, and temporal steps under stringent hardware constraints. Guided by the Bit Budget paradigm, we discern that pivoting efforts towards spike patterns and weight quantization, rather than temporal attributes, elicits profound implications for model performance. Utilizing the Bit Budget for holistic design consideration of SNNs elevates model performance across diverse data types, encompassing static imagery and neuromorphic datasets. Our revelations bridge the theoretical chasm between SNNs and quantized ANNs and illuminate a pragmatic trajectory for future endeavors in energy-efficient neural computations.",http://arxiv.org/abs/2311.10802v1,,"Guobin Shen (None) | Dongcheng Zhao (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Tenglong Li (Institute Of Automation, Chinese Academy Of Sciences) | Jindong Li (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yi Zeng (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2023-11-17 09:48:22+00:00,,,,,,
Adaptive Multi-Modal Cross-Entropy Loss for Stereo Matching,,,,Peng Xu (Zhejiang University) | Zhiyu Xiang (None) | Chengyu Qiao (Zhejiang University) | Jingyun Fu (Zhejiang University) | Tianyu Pu (Zhejiang University),,,,,,,
DeconfuseTrack???Dealing with Confusion for Multi-Object Tracking,"Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.",http://arxiv.org/abs/2403.02767v1,,Cheng Huang (Huazhong University Of Science And Technology) | Shoudong Han (Huazhong University Of Science And Technology) | Mengyu He (Huazhong University Of Science And Technology) | Wenbo Zheng (Huazhong University Of Science And Technology) | Yuhao Wei (Huazhong University Of Science And Technology),2024-03-05 08:35:09+00:00,,,,,,
Mind The Edge: Refining Depth Edges in Sparsely-Supervised Monocular Depth Estimation,"Monocular Depth Estimation (MDE) is a fundamental problem in computer vision with numerous applications. Recently, LIDAR-supervised methods have achieved remarkable per-pixel depth accuracy in outdoor scenes. However, significant errors are typically found in the proximity of depth discontinuities, i.e., depth edges, which often hinder the performance of depth-dependent applications that are sensitive to such inaccuracies, e.g., novel view synthesis and augmented reality. Since direct supervision for the location of depth edges is typically unavailable in sparse LIDAR-based scenes, encouraging the MDE model to produce correct depth edges is not straightforward. To the best of our knowledge this paper is the first attempt to address the depth edges issue for LIDAR-supervised scenes. In this work we propose to learn to detect the location of depth edges from densely-supervised synthetic data, and use it to generate supervision for the depth edges in the MDE training. %Despite the 'domain gap' between synthetic and real data, we show that depth edges that are estimated directly are significantly more accurate than the ones that emerge indirectly from the MDE training. To quantitatively evaluate our approach, and due to the lack of depth edges ground truth in LIDAR-based scenes, we manually annotated subsets of the KITTI and the DDAD datasets with depth edges ground truth. We demonstrate significant gains in the accuracy of the depth edges with comparable per-pixel depth accuracy on several challenging datasets.",http://arxiv.org/abs/2212.05315v2,,Lior Talker (Samsung R&D Israel) | Aviad Cohen (Samsung) | Erez Yosef (Tel Aviv University) | Alexandra Dana (Samsung) | Michael Dinerstein (Samsung),2022-12-10 14:49:24+00:00,,,,,,
On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving,"End-to-end motion planning models equipped with deep neural networks have shown great potential for enabling full autonomous driving. However, the oversized neural networks render them impractical for deployment on resource-constrained systems, which unavoidably requires more computational time and resources during reference.To handle this, knowledge distillation offers a promising approach that compresses models by enabling a smaller student model to learn from a larger teacher model. Nevertheless, how to apply knowledge distillation to compress motion planners has not been explored so far. In this paper, we propose PlanKD, the first knowledge distillation framework tailored for compressing end-to-end motion planners. First, considering that driving scenes are inherently complex, often containing planning-irrelevant or even noisy information, transferring such information is not beneficial for the student planner. Thus, we design an information bottleneck based strategy to only distill planning-relevant information, rather than transfer all information indiscriminately. Second, different waypoints in an output planned trajectory may hold varying degrees of importance for motion planning, where a slight deviation in certain crucial waypoints might lead to a collision. Therefore, we devise a safety-aware waypoint-attentive distillation module that assigns adaptive weights to different waypoints based on the importance, to encourage the student to accurately mimic more crucial waypoints, thereby improving overall safety. Experiments demonstrate that our PlanKD can boost the performance of smaller planners by a large margin, and significantly reduce their reference time.",http://arxiv.org/abs/2403.01238v1,,Kaituo Feng (Beijing Institute Of Technology) | Changsheng Li (None) | Dongchun Ren (ALLRIDE.AI) | Ye Yuan (Beijing Institute Of Technology) | Guoren Wang (Beijing Institute Of Technology),2024-03-02 15:47:42+00:00,,,,,,
Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction,"Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, high-quality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object's deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects, maintaining 3D consistency across novel views. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.",http://arxiv.org/abs/2312.01196v1,,"Devikalyan Das (Max Planck Institute For Informatics) | Christopher Wewer (Saarland Informatics Campus, Max-Planck Institute) | Raza Yunus (Saarland Informatics Campus, Max-Planck Institute) | Eddy Ilg (None) | Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute)",2023-12-02 18:06:24+00:00,,,,,,
Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning,"Dataset pruning aims to construct a coreset capable of achieving performance comparable to the original, full dataset. Most existing dataset pruning methods rely on snapshot-based criteria to identify representative samples, often resulting in poor generalization across various pruning and cross-architecture scenarios. Recent studies have addressed this issue by expanding the scope of training dynamics considered, including factors such as forgetting event and probability change, typically using an averaging approach. However, these works struggle to integrate a broader range of training dynamics without overlooking well-generalized samples, which may not be sufficiently highlighted in an averaging manner. In this study, we propose a novel dataset pruning method termed as Temporal Dual-Depth Scoring (TDDS), to tackle this problem. TDDS utilizes a dual-depth strategy to achieve a balance between incorporating extensive training dynamics and identifying representative samples for dataset pruning. In the first depth, we estimate the series of each sample's individual contributions spanning the training progress, ensuring comprehensive integration of training dynamics. In the second depth, we focus on the variability of the sample-wise contributions identified in the first depth to highlight well-generalized samples. Extensive experiments conducted on CIFAR and ImageNet datasets verify the superiority of TDDS over previous SOTA methods. Specifically on CIFAR-100, our method achieves 54.51% accuracy with only 10% training data, surpassing random selection by 7.83% and other comparison methods by at least 12.69%.",http://arxiv.org/abs/2311.13613v2,,"Xin Zhang (Xidian University) | Jiawei Du (Centre For Frontier AI Research (CFAR), A*STAR, Singapore) | Weiying Xie (None) | Yunsong Li (None) | Joey Tianyi Zhou (National University Of Singapore )",2023-11-22 03:45:30+00:00,,,,,,
OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies,,,,"Lingdong Kong (National University Of Singapore) | Youquan Liu (Hochschule Bremerhaven) | Lai Xing Ng (Institute For Infocomm Research (I2R), A*STAR) | Benoit Cottereau (CNRS) | Wei Tsang Ooi (National University Of Singapore)",,,,,,,
CoGS: Controllable Gaussian Splatting,"Capturing and re-animating the 3D structure of articulated objects present significant barriers. On one hand, methods requiring extensively calibrated multi-view setups are prohibitively complex and resource-intensive, limiting their practical applicability. On the other hand, while single-camera Neural Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive training and rendering costs. 3D Gaussian Splatting would be a suitable alternative but for two reasons. Firstly, existing methods for 3D dynamic Gaussians require synchronized multi-view cameras, and secondly, the lack of controllability in dynamic scenarios. We present CoGS, a method for Controllable Gaussian Splatting, that enables the direct manipulation of scene elements, offering real-time control of dynamic scenes without the prerequisite of pre-computing control signals. We evaluated CoGS using both synthetic and real-world datasets that include dynamic objects that differ in degree of difficulty. In our evaluations, CoGS consistently outperformed existing dynamic and controllable neural representations in terms of visual fidelity.",http://arxiv.org/abs/2312.05664v1,,Heng Yu (Carnegie Mellon University) | Joel Julin (Carnegie Mellon University) | Zolt??n ??. Milacski (Carnegie Mellon University) | Koichiro Niinuma (Fujitsu Research Of America) | L??szl?? A. Jeni (Carnegie Mellon University),2023-12-09 20:06:29+00:00,,,,,,
Multimodal Sense-Informed Prediction of 3D Human Poses,,,,Zhenyu Lou (None) | Qiongjie Cui (Nanjing University Of Science And Technology) | Haofan Wang (Xiaohongshu) | Xu Tang (Shanghaitech University) | Hong Zhou (Zhejiang University),,,,,,,
Leveraging Predicate and Triplet Learning for Scene Graph Generation,,,,Jiankai Li (Beihang University) | Yunhong Wang (Beihang University) | Xiefan Guo (Beihang University) | Ruijie Yang (Beihang University) | Weixin Li (None),,,,,,,
GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos,"We address the task of generating temporally consistent and physically plausible images of actions and object state transformations. Given an input image and a text prompt describing the targeted transformation, our generated images preserve the environment and transform objects in the initial image. Our contributions are threefold. First, we leverage a large body of instructional videos and automatically mine a dataset of triplets of consecutive frames corresponding to initial object states, actions, and resulting object transformations. Second, equipped with this data, we develop and train a conditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a variety of objects and actions and show superior performance compared to existing methods. In particular, we introduce a quantitative evaluation where GenHowTo achieves 88% and 74% on seen and unseen interaction categories, respectively, outperforming prior work by a large margin.",http://arxiv.org/abs/2312.07322v1,,Tomas Soucek (Czech Technical University Of Prague) | Dima Damen (None) | Michael Wray (University Of Bristol) | Ivan Laptev (INRIA Paris) | Josef Sivic (Czech Technical University In Prague),2023-12-12 14:37:36+00:00,,,,,,
Towards a Perceptual Evaluation Framework for Lighting Estimation,"Progress in lighting estimation is tracked by computing existing image quality assessment (IQA) metrics on images from standard datasets. While this may appear to be a reasonable approach, we demonstrate that doing so does not correlate to human preference when the estimated lighting is used to relight a virtual scene into a real photograph. To study this, we design a controlled psychophysical experiment where human observers must choose their preference amongst rendered scenes lit using a set of lighting estimation algorithms selected from the recent literature, and use it to analyse how these algorithms perform according to human perception. Then, we demonstrate that none of the most popular IQA metrics from the literature, taken individually, correctly represent human perception. Finally, we show that by learning a combination of existing IQA metrics, we can more accurately represent human preference. This provides a new perceptual framework to help evaluate future lighting estimation algorithms.",http://arxiv.org/abs/2312.04334v2,,Justine Giroux (Universit?? Laval) | Mohammad Reza Karimi Dastjerdi (None) | Yannick Hold-Geoffroy (Adobe Research) | Javier Vazquez-Corral (Computer Vision Center / Autonomous University Of Barcelona) | Jean-Fran??ois Lalonde (Universit?? Laval),2023-12-07 14:51:12+00:00,,,,,,
OmniGlue: Generalizable Feature Matching with Foundation Model Guidance,,,,Hanwen Jiang (University Of Texas At Austin) | Arjun Karpur (Google Research) | Bingyi Cao (Google Research) | Qixing Huang (University Of Texas At Austin) | Andr?? Araujo (Google Research),,,,,,,
Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement,,,,Xiuquan Hou (Xi'an Jiao Tong University) | Meiqin Liu (None) | Senlin Zhang (Zhejiang University) | Ping Wei (None) | Badong Chen (Xi'an Jiao Tong University),,,,,,,
UnO: Unsupervised Occupancy Fields for Perception and Forecasting,,,,Ben Agro (Waabi) | Quinlan Sykora (Waabi) | Sergio Casas (Waabi) | Thomas Gilles (Waabi) | Raquel Urtasun (Waabi),,,,,,,
SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models,"Despite the success of diffusion-based customization methods on visual content creation, increasing concerns have been raised about such techniques from both privacy and political perspectives. To tackle this issue, several anti-customization methods have been proposed in very recent months, predominantly grounded in adversarial attacks. Unfortunately, most of these methods adopt straightforward designs, such as end-to-end optimization with a focus on adversarially maximizing the original training loss, thereby neglecting nuanced internal properties intrinsic to the diffusion model, and even leading to ineffective optimization in some diffusion time steps. In this paper, we strive to bridge this gap by undertaking a comprehensive exploration of these inherent properties, to boost the performance of current anti-customization approaches. Two aspects of properties are investigated: 1) We examine the relationship between time step selection and the model's perception in the frequency domain of images and find that lower time steps can give much more contributions to adversarial noises. This inspires us to propose an adaptive greedy search for optimal time steps that seamlessly integrates with existing anti-customization methods. 2) We scrutinize the roles of features at different layers during denoising and devise a sophisticated feature-based optimization framework for anti-customization. Experiments on facial benchmarks demonstrate that our approach significantly increases identity disruption, thereby enhancing user privacy and security.",http://arxiv.org/abs/2312.07865v1,,Feifei Wang (University Of Science And Technology Of China) | Zhentao Tan (Alibaba DAMO Academy; University Of Science And Technology Of China) | Tianyi Wei (None) | Yue Wu (Alibaba Group) | Qidong Huang (University Of Science And Technology Of China),2023-12-13 03:04:22+00:00,,,,,,
RoMa: Robust Dense Feature Matching,"Feature matching is an important computer vision task that involves estimating correspondences between two images of a 3D scene, and dense methods estimate all such correspondences. The aim is to learn a robust model, i.e., a model able to match under challenging real-world changes. In this work, we propose such a model, leveraging frozen pretrained features from the foundation model DINOv2. Although these features are significantly more robust than local features trained from scratch, they are inherently coarse. We therefore combine them with specialized ConvNet fine features, creating a precisely localizable feature pyramid. To further improve robustness, we propose a tailored transformer match decoder that predicts anchor probabilities, which enables it to express multimodality. Finally, we propose an improved loss formulation through regression-by-classification with subsequent robust regression. We conduct a comprehensive set of experiments that show that our method, RoMa, achieves significant gains, setting a new state-of-the-art. In particular, we achieve a 36% improvement on the extremely challenging WxBS benchmark. Code is provided at https://github.com/Parskatt/RoMa",http://arxiv.org/abs/2305.15404v2,,"Johan Edstedt (Computer Vision Laboratory, Link??ping University) | Qiyu Sun (East China University Of Science And Technology) | Georg B??kman (Chalmers University Of Technology) | M??rten Wadenb??ck (Link??ping University) | Michael Felsberg (Link??ping University)",2023-05-24 17:59:04+00:00,,,,,,
How to Configure Good In-Context Sequence for Visual Question Answering,"Inspired by the success of Large Language Models in dealing with new tasks via In-Context Learning (ICL) in NLP, researchers have also developed Large Vision-Language Models (LVLMs) with ICL capabilities. However, when implementing ICL using these LVLMs, researchers usually resort to the simplest way like random sampling to configure the in-context sequence, thus leading to sub-optimal results. To enhance the ICL performance, in this study, we use Visual Question Answering (VQA) as case study to explore diverse in-context configurations to find the powerful ones. Additionally, through observing the changes of the LVLM outputs by altering the in-context sequence, we gain insights into the inner properties of LVLMs, improving our understanding of them. Specifically, to explore in-context configurations, we design diverse retrieval methods and employ different strategies to manipulate the retrieved demonstrations. Through exhaustive experiments on three VQA datasets: VQAv2, VizWiz, and OK-VQA, we uncover three important inner properties of the applied LVLM and demonstrate which strategies can consistently improve the ICL VQA performance. Our code is provided in: https://github.com/GaryJiajia/OFv2_ICL_VQA.",http://arxiv.org/abs/2312.01571v1,,Li Li (Southeast University) | Jiawei Peng (Southeast University) | Huiyi Chen (Southeast University - Monash University Joint Graduate School (Suzhou)) | Chongyang Gao (Northwestern University) | Xu Yang (Southeast University),2023-12-04 02:03:23+00:00,,,,,,
Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images,,,,JungEun Kim (Korea Advanced Institute Of Science And Technology) | Hangyul Yoon (Korea Advanced Institute Of Science And Technology (KAIST)) | Geondo Park (Korea Advanced Institute Of Science And Technology) | Kyungsu Kim (Harvard Medical School And Massachusetts General Hospital) | Eunho Yang (Korea Advanced Institute Of Science & Technology),,,,,,,
ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing,,,,Zhongze Wang (East China University Of Science And Technology) | Haitao Zhao (East China University Of Science And Technology) | Jingchao Peng (East China University Of Science And Technology) | Lujian Yao (East China University Of Science And Technology) | Kaijie Zhao (None),,,,,,,
Towards Progressive Multi-Frequency Representation for Image Warping,,,,Jun Xiao (The Hong Kong Polytechnic University) | Zihang Lyu (The Hong Kong Polytechnic University) | Cong Zhang (Hong Kong Polytechnic University) | Yakun Ju (Nanyang Technological University) | Changjian Shui (Vector Institute) | Kin-Man Lam (The Hong Kong Polytechnic University),,,,,,,
UnionFormer: Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization,,,,"Shuaibo Li (None) | Wei Ma (None) | Jianwei Guo (Institute Of Automation, Chinese Academy Of Sciences) | Shibiao Xu (Beijing University Of Posts And Telecommunications) | Libenchong (Beijing University Of Technology) | Xiaopeng Zhang (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
"Low-power, Continuous Remote Behavioral Localization with Event Cameras","Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica during several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras' natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The low-power capabilities of the event camera allows to record three times longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/",http://arxiv.org/abs/2312.03799v1,,Friedhelm Hamann (TU Berlin) | Suman Ghosh (TU Berlin) | Ignacio Juarez Martinez (University Of Oxford) | Tom Hart (Oxford Brookes University) | Alex Kacelnik (University Of Oxford) | Guillermo Gallego (TU Berlin-ECDF-SCIoI),2023-12-06 14:58:03+00:00,,,,,,
UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory,"Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small portion of parameters, is an effective strategy for adapting pre-trained models to downstream domains. To further reduce the memory demand, recent PETL works focus on the more valuable memory-efficient characteristic. In this paper, we argue that the scalability, adaptability, and generalizability of state-of-the-art methods are hindered by structural dependency and pertinency on specific pre-trained backbones. To this end, we propose a new memory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate these weaknesses. Specifically, we facilitate the transfer process via a lightweight and learnable parallel network, which consists of: 1) A parallel interaction module that decouples the sequential connections and processes the intermediate activations detachedly from the pre-trained network. 2) A confidence aggregation module that learns optimal strategies adaptively for integrating cross-layer features. We evaluate UniPT with different backbones (e.g., T5, VSE$\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various vision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have validated that UniPT can not only dramatically reduce memory consumption and outperform the best competitor, but also achieve competitive performance over other plain PETL methods with lower training memory overhead. Our code is publicly available at: https://github.com/Paranioar/UniPT.",http://arxiv.org/abs/2308.14316v2,,Haiwen Diao (Dalian University Of Technology) | Bo Wan (KU Leuven) | Ying Zhang (Tencent) | Xu Jia (Dalian University Of Technology) | Huchuan Lu (Dalian University Of Technology) | Long Chen (HKUST),2023-08-28 05:38:43+00:00,,,,,,
Neural Visibility Field for Active Mapping,,,,Shangjie Xue (Georgia Institute Of Technology) | Jesse Dill (Georgia Institute Of Technology) | Pranay Mathur (Georgia Institute Of Technology) | Frank Dellaert (Georgia Tech) | Panagiotis Tsiotras (Georgia Institute Of Technology) | Danfei Xu (Georgia Institute Of Technology),,,,,,,
Initialization Matters for Adversarial Transfer Learning,"With the prevalence of the Pretraining-Finetuning paradigm in transfer learning, the robustness of downstream tasks has become a critical concern. In this work, we delve into adversarial robustness in transfer learning and reveal the critical role of initialization, including both the pretrained model and the linear head. First, we discover the necessity of an adversarially robust pretrained model. Specifically, we reveal that with a standard pretrained model, Parameter-Efficient Finetuning~(PEFT) methods either fail to be adversarially robust or continue to exhibit significantly degraded adversarial robustness on downstream tasks, even with adversarial training during finetuning. Leveraging a robust pretrained model, surprisingly, we observe that a simple linear probing can outperform full finetuning and other PEFT methods with random initialization on certain datasets. We further identify that linear probing excels in preserving robustness from the robust pretraining. Based on this, we propose Robust Linear Initialization~(RoLI) for adversarial finetuning, which initializes the linear head with the weights obtained by adversarial linear probing to maximally inherit the robustness from pretraining. Across five different image classification datasets, we demonstrate the effectiveness of RoLI and achieve new state-of-the-art results.",http://arxiv.org/abs/2312.05716v1,,"Andong Hua (University Of California, Santa Barbara) | Jindong Gu (University Of Oxford & Google Research) | Zhiyu Xue (University Of California, Santa Barbara) | Nicholas Carlini (None) | Eric Wong (University Of Pennsylvania) | Yao Qin (University Of California, Santa Barbara)",2023-12-10 00:51:05+00:00,,,,,,
DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation,"The realism of digital avatars is crucial in enabling telepresence applications with self-expression and customization. A key aspect of this realism originates from the physical accuracy of both a true-to-life body shape and clothing. While physical simulations can produce high-quality, realistic motions for clothed humans, they require precise estimation of body shape and high-quality garment assets with associated physical parameters for cloth simulations. However, manually creating these assets and calibrating their parameters is labor-intensive and requires specialized expertise. To address this gap, we propose DiffAvatar, a novel approach that performs body and garment co-optimization using differentiable simulation. By integrating physical simulation into the optimization loop and accounting for the complex nonlinear behavior of cloth and its intricate interaction with the body, our framework recovers body and garment geometry and extracts important material parameters in a physically plausible way. Our experiments demonstrate that our approach generates realistic clothing and body shape that can be easily used in downstream applications.",http://arxiv.org/abs/2311.12194v1,,Yifei Li (Massachusetts Institute Of Technology) | Hsiaoyu Chen (Meta) | Egor Larionov (Meta) | Nikolaos Sarafianos (Meta Reality Labs) | Wojciech Matusik (Massachusetts Institute Of Technology) | Tuur Stuyck (Meta),2023-11-20 21:20:37+00:00,,,,,,
360+ x : A Panoptic Multi-modal Scene Understanding Dataset ,,,,Hao Chen (ASML) | Yuqi Hou (University Of Birmingham) | Chenyuan Qu (University Of Birmingham) | Irene Testini (Cardiff University) | Xiaohan Hong (University Of Birmingham) | Jianbo Jiao (University Of Birmingham),,,,,,,
Seeing Motion During Nighttime with Event Camera,,,,Haoyue Liu (Huazhong University Of Science And Technology) | Shihan Peng (Huazhong University Of Science And Technology) | Lin Zhu (Beijing Institute Of Technology) | Yi Chang (Huazhong University Of Science And Technology) | Hanyu Zhou (Huazhong University Of Science And Technology) | Luxin Yan (Huazhong University Of Science And Technology),,,,,,,
Unmixing Diffusion for Self-Supervised Hyperspectral Image Denoising,,,,Haijin Zeng (IMEC & Universiteit Gent) | Jiezhang Cao (ETH Zurich) | Yongyong Chen (Harbin Institute Of Technology (Shenzhen)) | Kai Zhang (None) | Hiep Luong (Universiteit Gent - IMEC) | Wilfried Philips (Universiteit Gent),,,,,,,
Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning,,,,"Dipam Goswami (Computer Vision Center) | Albin Soutif (Computer Vision Center, Universitat Aut??noma De Barcelona) | Yuyang Liu (Shenyang Institute Of Automation, Chinese Academy Of Sciences/ University Of Chinese Academy Of Sciences) | Sandesh Kamath (Computer Vision Center, Universitat Aut??noma De Barcelona) | Bart??omiej Twardowski (Computer Vision Center / IDEAS NCBR) | Joost Van De Weijer (Computer Vision Center Barcelona)",,,,,,,
Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement,"Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple benchmark datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.",http://arxiv.org/abs/2403.09101v1,,Daiwei Yu (Hangzhou City University) | Zhuorong Li (HangZhou City University) | Lina Wei (Hangzhou City University ) | Canghong Jin (Hangzhou City University) | Yun Zhang (Hangzhou City University) | Sixian Chan (The College Of Computer Science And Technology At Zhejiang University Of Technology),2024-03-14 04:48:31+00:00,,,,,,
CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation,"Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditions in a few steps. Our conditional-task learning and distillation approach outperforms previous distillation methods, achieving a new state-of-the-art in producing high-quality images with very few steps (e.g., 1-4) across multiple tasks, including super-resolution, text-guided image editing, and depth-to-image generation.",http://arxiv.org/abs/2310.01407v2,,Kangfu Mei (Johns Hopkins University) | Mauricio Delbracio (None) | Hossein Talebi (Google Research) | Zhengzhong Tu (University Of Texas At Austin) | Vishal M. Patel (Johns Hopkins University) | Peyman Milanfar (Peyman Milanfar),2023-10-02 17:59:18+00:00,,,,,,
CLIP-Driven Open-Vocabulary 3D Scene Graph Generation via Cross-Modality Contrastive Learning,,,,Lianggangxu Chen (East China Normal University) | Xuejiao Wang (East China Normal University) | Jiale Lu (East China Normal University) | Shaohui Lin (East China Normal University) | Changbo Wang (East China Normal University) | Gaoqi He (East China Normal University),,,,,,,
An Aggregation-Free Federated Learning for Tackling Data Heterogeneity,,,,"Yuan Wang (Institute Of High Performance Computing, Singapore, A*STAR) | Huazhu Fu (Institute Of High Performance Computing, Singapore, A*STAR) | Renuga Kanagavelu (Institute Of High Performance Computing, Singapore, A*STAR) | Qingsong Wei (Agency For Science, Technology And Research (A*STAR)) | Yong Liu (Institute Of High Performance Computing, Singapore, A*STAR) | Rick Goh (Institute Of High Performance Computing, Singapore, A*STAR)",,,,,,,
Towards Efficient Replay in Federated Incremental Learning,"In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.",http://arxiv.org/abs/2403.05890v1,,Yichen Li (Huazhong University Of Science And Technology) | Qunwei Li (Ant Group) | Haozhao Wang (Huazhong University Of Science And Technology) | Ruixuan Li (Huazhong University Of Science And Technology) | Wenliang Zhong (Ant Group) | Guannan Zhang (Tongji University),2024-03-09 12:04:56+00:00,,,,,,
Dual-consistency Model Inversion for Non-exemplar Class Incremental Learning,,,,"Zihuan Qiu (University Of Electronic Science And Technology Of China) | Yi Xu (Dalian University Of Technology) | Fanman Meng (University Of Electronic Science And Technology Of China) | Hongliang Li (University Of Electronic Science And Technology Of China, Tsinghua University) | Linfeng Xu (University Of Electronic Science And Technology Of China) | Qingbo Wu (University Of Electronic Science And Technology Of China)",,,,,,,
FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning,,,,"Rishub Tamirisa (AI@UIUC) | Chulin Xie (University Of Illinois, Urbana Champaign) | Wenxuan Bao (University Of Illinois Urbana Champaign) | Andy Zhou (Lapis Labs) | Ron Arel (Lapis Lapis, UIUC) | Aviv Shamsian (Bar-Ilan University)",,,,,,,
Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology,"Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.",http://arxiv.org/abs/2402.17228v1,,"Wenhao Tang (Chongqing University) | Fengtao ZHOU (Department Of Computer Science And Engineering, Hong Kong University Of Science And Technology) | Sheng Huang (Chongqing University) | Xiang Zhu (Chongqing University) | Yi Zhang (Chongqing University) | Bo Liu (Rutgers University)",2024-02-27 05:42:38+00:00,,,,,,
Dual-View Visual Contextualization for Web Navigation,"Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their ""dual views"" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We validate our method on the recently released Mind2Web dataset, which features diverse navigation domains and tasks on real-world websites. Our method consistently outperforms the baseline in all the scenarios, including cross-task, cross-website, and cross-domain ones.",http://arxiv.org/abs/2402.04476v1,,"Jihyung Kil (Ohio State University, Columbus) | Chan Hee Song (The Ohio State University) | Boyuan Zheng (Ohio State University, Columbus) | Xiang Deng (Google) | Yu Su (Ohio State University) | Wei-Lun Chao (Ohio State University)",2024-02-06 23:52:10+00:00,,,,,,
One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion Schedule Flaws and Enhancing Low-Frequency Controls,"It is well known that many open-released foundational diffusion models have difficulty in generating images that substantially depart from average brightness, despite such images being present in the training data. This is due to an inconsistency: while denoising starts from pure Gaussian noise during inference, the training noise schedule retains residual data even in the final timestep distribution, due to difficulties in numerical conditioning in mainstream formulation, leading to unintended bias during inference. To mitigate this issue, certain $\epsilon$-prediction models are combined with an ad-hoc offset-noise methodology. In parallel, some contemporary models have adopted zero-terminal SNR noise schedules together with $\mathbf{v}$-prediction, which necessitate major alterations to pre-trained models. However, such changes risk destabilizing a large multitude of community-driven applications anchored on these pre-trained models. In light of this, our investigation revisits the fundamental causes, leading to our proposal of an innovative and principled remedy, called One More Step (OMS). By integrating a compact network and incorporating an additional simple yet effective step during inference, OMS elevates image fidelity and harmonizes the dichotomy between training and inference, while preserving original model parameters. Once trained, various pre-trained diffusion models with the same latent domain can share the same OMS module.",http://arxiv.org/abs/2311.15744v1,,Minghui Hu (Nanyang Technological University) | Jianbin Zheng (South China University Of Technology) | Chuanxia Zheng (University Of Oxford) | Chaoyue Wang (JD Explore Academy) | Dacheng Tao (None) | Tat-Jen Cham (Nanyang Technological University),2023-11-27 12:02:42+00:00,,,,,,
Human Gaussian Splatting : Real-time Rendering of Animatable Avatars,"This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose the first animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. Our body is represented by a set of gaussian primitives in a canonical space which are deformed in a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (\OURS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method presents a PSNR 1.5dbB better than the state-of-the-art on THuman4 dataset while being able to render at 20fps or more.",http://arxiv.org/abs/2311.17113v1,,Arthur Moreau (Huawei Noah's Ark Lab) | Jifei Song (Huawei Technologies Ltd.) | Helisa Dhamo (None) | Richard Shaw (Huawei Technologies Ltd.) | Yiren Zhou (Huawei Technologies Ltd.) | Eduardo P??rez-Pellitero (Huawei Noah's Ark Lab (UK)),2023-11-28 12:05:41+00:00,,,,,,
Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation,"Attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in weakly supervised semantic segmentation (WSSS). In this work, we devise a 'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to 'separate' the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to 'conquer' the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted to guarantee the correctness of knowledge and further facilitate the discrepancy among co-occurring objects. We streamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence without external supervision. Extensive experiments are conducted, validating the efficiency of our method tackling co-occurrence and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code will be available at https://github.com/zwyang6/SeCo.git.",http://arxiv.org/abs/2402.18467v2,,Zhiwei Yang (Fudan University) | Kexue Fu (Qilu University Of Technology (Shandong Academy Of Sciences)) | Minghong Duan (Fudan University) | Linhao Qu (Fudan University) | Shuo Wang (Fudan University) | Zhijian Song (Fudan University),2024-02-28 16:43:27+00:00,,,,,,
BlockGCN: Redefine Topology Awareness for Skeleton-Based Action Recognition,,,,"Yuxuan Zhou (University Of Mannheim) | Xudong Yan (City University Of Macao) | Zhi-Qi Cheng (Carnegie Mellon University) | Yan Yan (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Qi Dai (Microsoft Research Asia) | Xian-Sheng Hua (Terminus Group)",,,,,,,
PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization,,,,Zining Chen (Beijing University Of Posts And Telecommunications) | Weiqiu Wang (Beijing University Of Posts And Telecommunications) | Zhicheng Zhao (Beijing University Of Posts And Telecommunications) | Fei Su (Beijing University Of Posts And Telecommunications) | Aidong Men (Beijing University Of Posts And Telecommunications) | Hongying Meng (None),,,,,,,
EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams,,,,"Christen Millerdurai (Max Planck Institute For Informatics) | Hiroyasu Akada (Max Planck Institute For Informatics) | Jian Wang (Max Planck Institute For Informatics) | Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) | Christian Theobalt (MPI Informatik) | Vladislav Golyanik (MPI For Informatics)",,,,,,,
Generalized Event Cameras,,,,"Varun Sundar (University Of Wisconsin, Madison) | Matthew Dutson (University Of Wisconsin, Madison) | Andrei Ardelean (NovoViz) | Claudio Bruschini (EPFL - EPF Lausanne) | Edoardo Charbon (EPFL - EPF Lausanne) | Mohit Gupta (Department Of Computer Sciences, University Of Wisconsin - Madison)",,,,,,,
DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction,"In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) with Kalman Filter motion prediction works well in pedestrian-dominant scenarios but falls short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much less sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with 63.4 and 76.2 in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.",http://arxiv.org/abs/2403.02075v1,,Weiyi Lv (Shanghai University) | Yuhang Huang (National University Of Defense Technology) | NING Zhang (PAII) | Ruei-Sung Lin (PAII Inc) | Mei Han (PAII) | Dan Zeng (Shanghai University),2024-03-04 14:21:51+00:00,,,,,,
A Simple Baseline for Efficient Hand Mesh Reconstruction,"3D hand pose estimation has found broad application in areas such as gesture recognition and human-machine interaction tasks. As performance improves, the complexity of the systems also increases, which can limit the comparative analysis and practical implementation of these methods. In this paper, we propose a simple yet effective baseline that not only surpasses state-of-the-art (SOTA) methods but also demonstrates computational efficiency. To establish this baseline, we abstract existing work into two components: a token generator and a mesh regressor, and then examine their core structures. A core structure, in this context, is one that fulfills intrinsic functions, brings about significant improvements, and achieves excellent performance without unnecessary complexities. Our proposed approach is decoupled from any modifications to the backbone, making it adaptable to any modern models. Our method outperforms existing solutions, achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset, our approach produced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb dataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for performance speed, our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36",http://arxiv.org/abs/2403.01813v1,,Zhishan Zhou (None) | Shihao Zhou (None) | Zhi Lv (None) | Minqiang Zou (None) | Yao Tang (None) | Jiajun Liang (None),2024-03-04 08:00:20+00:00,,,,,,
Learning Vision from Generative Models Rivals Learning Vision from Data,"We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data. We synthesize a large dataset of image captions using LLMs, then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning, treating images sharing the same caption as positive pairs. The resulting representations transfer well to many downstream tasks, competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR outperforms previous self-supervised methods by a significant margin, e.g., improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.",http://arxiv.org/abs/2312.17742v1,,Yonglong Tian (Google) | Lijie Fan (Massachusetts Institute Of Technology) | Kaifeng Chen (Google) | Dina Katabi (Massachusetts Institute Of Technology) | Dilip Krishnan (Google) | Phillip Isola (None),2023-12-28 18:59:55+00:00,,,,,,
VOODOO 3D: VOlumetric pOrtrait Disentanglement fOr Online 3D head reenactment,"We present a 3D-aware one-shot head reenactment method based on a fully volumetric neural disentanglement framework for source appearance and driver expressions. Our method is real-time and produces high-fidelity and view-consistent output, suitable for 3D teleconferencing systems based on holographic displays. Existing cutting-edge 3D-aware reenactment methods often use neural radiance fields or 3D meshes to produce view-consistent appearance encoding, but, at the same time, they rely on linear face models, such as 3DMM, to achieve its disentanglement with facial expressions. As a result, their reenactment results often exhibit identity leakage from the driver or have unnatural expressions. To address these problems, we propose a neural self-supervised disentanglement approach that lifts both the source image and driver video frame into a shared 3D volumetric representation based on tri-planes. This representation can then be freely manipulated with expression tri-planes extracted from the driving images and rendered from an arbitrary view using neural radiance fields. We achieve this disentanglement via self-supervised learning on a large in-the-wild video dataset. We further introduce a highly effective fine-tuning approach to improve the generalizability of the 3D lifting using the same real-world data. We demonstrate state-of-the-art performance on a wide range of datasets, and also showcase high-quality 3D-aware head reenactment on highly challenging and diverse subjects, including non-frontal head poses and complex expressions for both source and driver.",http://arxiv.org/abs/2312.04651v1,,Phong Tran (MBZUAI) | Egor Zakharov (Skolkovo Institute Of Science And Technology) | Long Nhat Ho (Mohamed Bin Zayed University Of Artificial Intelligence) | Anh Tran (VinAI Research) | Liwen Hu (Pinscreen) | Hao Li (Mohamed Bin Zayed University Of Artificial Intelligence),2023-12-07 19:19:57+00:00,,,,,,
Multi-Task Dense Prediction via Mixture of Low-Rank Experts,,,,"Yuqi Yang (Nankai University) | Peng-Tao Jiang (Vivo Mobile Communication (Hangzhou) Co., Ltd.) | Qibin Hou (Nankai University) | Hao Zhang (Vivo Mobile Communication ???Hangzhou???Co., Ltd) | Jinwei Chen (Vivo Mobile Communication Co., Ltd.) | Bo Li (Vivo Mobile Communication Co.,Ltd.)",,,,,,,
3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions,,,,Weijia Li (Sun Yat-Sen University) | Haote Yang (PJLab) | Zhenghao Hu (SUN YAT-SEN UNIVERSITY) | Juepeng Zheng (Sun Yat-Sen University) | Gui-Song Xia (Wuhan University) | Conghui He (None),,,,,,,
MMA-Diffusion: MultiModal Attack on Diffusion Models,"In recent years, Text-to-Image (T2I) models have seen remarkable advancements, gaining widespread adoption. However, this progress has inadvertently opened avenues for potential misuse, particularly in generating inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces MMA-Diffusion, a framework that presents a significant and realistic threat to the security of T2I models by effectively circumventing current defensive measures in both open-source models and commercial online services. Unlike previous approaches, MMA-Diffusion leverages both textual and visual modalities to bypass safeguards like prompt filters and post-hoc safety checkers, thus exposing and highlighting the vulnerabilities in existing defense mechanisms.",http://arxiv.org/abs/2311.17516v2,,"Yijun Yang (The Chinese University Of Hong Kong) | Ruiyuan Gao (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Xiaosen Wang (Huazhong University Of Science And Technology) | Tsung-Yi Ho (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Xu Nan (Institute Of Automation, Chinese Academy Of Sciences) | Qiang Xu (The Chinese University Of Hong Kong)",2023-11-29 10:39:53+00:00,,,,,,
Fine-grained Bipartite Concept Factorization for Clustering,,,,Chong Peng (None) | Pengfei Zhang (Qingdao University) | Yongyong Chen (Harbin Institute Of Technology (Shenzhen)) | Zhao Kang (University Of Electronic Science And Technology Of China) | Chenglizhao Chen (China University Of Petroleum) | Qiang Cheng (University Of Kentucky),,,,,,,
Visual Layout Composer: Image-Vector Dual Diffusion Model for Design Layout Generation,,,,Mohammad Amin Shabani (Simon Fraser University) | Zhaowen Wang (Adobe Research) | Difan Liu (Adobe Research) | Nanxuan Zhao (Adobe Research) | Jimei Yang (Adobe Research) | Yasutaka Furukawa (Simon Fraser University),,,,,,,
FairRAG: Fair Human Generation via Fair Retrieval Augmentation,,,,Robik Shrestha (Rochester Institute Of Technology) | Yang Zou (Amazon) | Qiuyu Chen (Amazon) | Zhiheng Li (Amazon AGI) | Yusheng Xie (Amazon) | Siqi Deng (Amazon),,,,,,,
Reconstructing Hands in 3D with Transformers,"We present an approach that can reconstruct hands in 3D from monocular input. Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work. The key to HaMeR's success lies in scaling up both the data used for training and the capacity of the deep network for hand reconstruction. For training data, we combine multiple datasets that contain 2D or 3D hand annotations. For the deep model, we use a large scale Vision Transformer architecture. Our final model consistently outperforms the previous baselines on popular 3D hand pose benchmarks. To further evaluate the effect of our design in non-controlled settings, we annotate existing in-the-wild datasets with 2D hand keypoint annotations. On this newly collected dataset of annotations, HInt, we demonstrate significant improvements over existing baselines. We make our code, data and models available on the project website: https://geopavlakos.github.io/hamer/.",http://arxiv.org/abs/2312.05251v1,,Georgios Pavlakos (University Of Texas At Austin) | Dandan Shan (None) | Ilija Radosavovic (None) | Angjoo Kanazawa (UC Berkeley) | David Fouhey (New York University) | Jitendra Malik (University Of California At Berkeley),2023-12-08 18:59:07+00:00,,,,,,
ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection,,,,Yichen Bai (None) | Zongbo Han (Tianjin University) | Bing Cao (Tianjin University) | Xiaoheng Jiang (Zhengzhou University) | Qinghua Hu (Tianjin University) | Changqing Zhang (Tianjin University),,,,,,,
Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion,"Sampling from the posterior distribution poses a major computational challenge in solving inverse problems using latent diffusion models. Common methods rely on Tweedie's first-order moments, which are known to induce a quality-limiting bias. Existing second-order approximations are impractical due to prohibitive computational costs, making standard reverse diffusion processes intractable for posterior sampling. This paper introduces Second-order Tweedie sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency comparable to first-order Tweedie with a tractable reverse process using second-order approximation. Our theoretical results reveal that the second-order approximation is lower bounded by our surrogate loss that only requires $O(1)$ compute using the trace of the Hessian, and by the lower bound we derive a new drift term to make the reverse process tractable. Our method surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural function evaluations, respectively, while notably enhancing sampling quality on FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to text-guided image editing and addresses residual distortions present from corrupted images in leading text-guided image editing methods. To our best knowledge, this is the first work to offer an efficient second-order approximation in solving inverse problems using latent diffusion and editing real-world images with corruptions.",http://arxiv.org/abs/2312.00852v1,,"Litu Rout (University Of Texas At Austin) | Yujia Chen (Google) | Abhishek Kumar (Google DeepMind) | Constantine Caramanis (University Of Texas, Austin) | Sanjay Shakkottai (University Of Texas, Austin) | Wen-Sheng Chu (Google Research)",2023-12-01 14:36:24+00:00,,,,,,
Resolution Limit of Single-Photon LIDAR,,,,"Stanley H. Chan (Purdue University, USA) | Hashan K Weerasooriya (Purdue University) | Weijian Zhang (Purdue University) | Pamela Abshire (University Of Maryland, College Park) | Istvan Gyongy (University Of Edinburgh) | Robert Henderson (University Of Edinburgh)",,,,,,,
EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars,,,,Nikita Drobyshev (Meta) | Antoni Bigata Casademunt (Imperial College London) | Konstantinos Vougioukas (Facebook) | Zoe Landgraf (Facebook) | Stavros Petridis (Facebook) | Maja Pantic (Facebook),,,,,,,
FedHCA2 : Towards Hetero-Client Federated Multi-Task Learning ,,,,Yuxiang Lu (Shanghai Jiao Tong University) | Suizhi Huang (Shanghai Jiao Tong University) | Yuwen Yang (Shanghai Jiao Tong University) | Shalayiding Sirejiding (None) | Yue Ding (Shanghai Jiao Tong University) | Hongtao Lu (Shanghai Jiao Tong University),,,,,,,
Preserving Fairness Generalization in Deepfake Detection,"Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at https://github.com/Purdue-M2/Fairness-Generalization",http://arxiv.org/abs/2402.17229v1,,Li Lin (None) | Xinan He (Nanchang University) | Yan Ju (State University Of New York At Buffalo) | Xin Wang (State University Of New York At Albany) | Feng Ding (Nanchang University) | Shu Hu (Purdue University),2024-02-27 05:47:33+00:00,,,,,,
Continuous Optical Zooming: A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World,,,,"Huiyuan Fu (Beijing University Of Posts And Telecommunications) | Fei Peng (Beijing University Of Posts And Telecommunications) | Xianwei Li (Beijing University Of Posts And Telecommunications) | Yejun Li (Beijing University Of Posts And Telecommunications) | Xin Wang (State University Of New York At Stony Brook) | Huadong Ma (Beijing University Of Post And Telecommunication, Tsinghua University)",,,,,,,
Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for Enhanced Human Pose Estimation with Sparse Inertial Sensors,"This paper introduces a novel human pose estimation approach using sparse inertial sensors, addressing the shortcomings of previous methods reliant on synthetic data. It leverages a diverse array of real inertial motion capture data from different skeleton formats to improve motion diversity and model generalization. This method features two innovative components: a pseudo-velocity regression model for dynamic motion capture with inertial sensors, and a part-based model dividing the body and sensor data into three regions, each focusing on their unique characteristics. The approach demonstrates superior performance over state-of-the-art models across five public datasets, notably reducing pose error by 19\% on the DIP-IMU dataset, thus representing a significant improvement in inertial sensor-based human pose estimation. Our codes are available at {\url{https://github.com/dx118/dynaip}}.",http://arxiv.org/abs/2312.02196v2,,Yu Zhang (Shanghai Jiao Tong University) | Songpengcheng Xia (None) | Lei Chu (University Of Southern California) | Jiarui Yang (Shanghai Jiao Tong University) | Qi Wu (Shanghai Jiao Tong University) | Ling Pei (Shanghai Jiao Tong Univeristy),2023-12-02 13:17:10+00:00,,,,,,
CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition,"Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the self-attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features. The code is released at https://github.com/Lu-Feng/CricaVPR.",http://arxiv.org/abs/2402.19231v1,,Feng Lu (Tsinghua University) | Xiangyuan Lan (Peng Cheng Laboratory) | Lijun Zhang (University Of Chinese Academy Of Sciences) | Dongmei Jiang (Peng Cheng Laboratory) | Yaowei Wang (Pengcheng Laboratory) | Chun Yuan (Tsinghua University),2024-02-29 15:05:11+00:00,,,,,,
Explaining CLIP's performance disparities on data from blind/low vision users,"Large multi-modal models (LMMs) hold the potential to usher in a new era of automated visual assistance for people who are blind or low vision (BLV). Yet, these models have not been systematically evaluated on data captured by BLV users. We address this by empirically assessing CLIP, a widely-used LMM likely to underpin many assistive technologies. Testing 25 CLIP variants in a zero-shot classification task, we find that their accuracy is 15 percentage points lower on average for images captured by BLV users than web-crawled images. This disparity stems from CLIP's sensitivities to 1) image content (e.g. not recognizing disability objects as well as other objects); 2) image quality (e.g. not being robust to lighting variation); and 3) text content (e.g. not recognizing objects described by tactile adjectives as well as visual ones). We delve deeper with a textual analysis of three common pre-training datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content is rarely mentioned. We then provide three examples that illustrate how the performance disparities extend to three downstream models underpinned by CLIP: OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5 images can mitigate CLIP's quality-of-service disparities for BLV users in some scenarios, which we discuss alongside a set of other possible mitigations.",http://arxiv.org/abs/2311.17315v2,,"Daniela Massiceti (Microsoft Research) | Camilla Longden (Microsoft Research, Cambridge) | Agnieszka S??owik (Microsoft) | Samuel Wills (World Bank) | Martin Grayson (Research, Microsoft) | Cecily Morrison (Microsoft Research)",2023-11-29 02:10:31+00:00,,,,,,
Stratified Avatar Generation from Sparse Observations,,,,Han Feng (Wuhan University) | Wenchao Ma (Pennsylvania State University) | Quankai Gao (University Of Southern California) | Xianwei Zheng (Wuhan University) | Nan Xue (Ant Group) | Huijuan Xu (Pennsylvania State University--University Park),,,,,,,
SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting,"We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.",http://arxiv.org/abs/2402.18848v1,,Hoon Kim (Beeble) | Minje Jang (Beeble) | Wonjun Yoon (Beeble) | Jisoo Lee (Beeble) | Donghyun Na (Beeble) | Sanghyun Woo (New York University),2024-02-29 04:52:04+00:00,,,,,,
NeuRAD: Neural Rendering for Autonomous Driving,"Neural radiance fields (NeRFs) have gained popularity in the autonomous driving (AD) community. Recent methods show NeRFs' potential for closed-loop simulation, enabling testing of AD systems, and as an advanced training data augmentation technique. However, existing methods often require long training times, dense semantic supervision, or lack generalizability. This, in turn, hinders the application of NeRFs for AD at scale. In this paper, we propose NeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our method features simple network design, extensive sensor modeling for both camera and lidar -- including rolling shutter, beam divergence and ray dropping -- and is applicable to multiple datasets out of the box. We verify its performance on five popular AD datasets, achieving state-of-the-art performance across the board. To encourage further development, we will openly release the NeuRAD source code. See https://github.com/georghess/NeuRAD .",http://arxiv.org/abs/2311.15260v2,,Adam Tonderski (Lund University) | Carl Lindstr??m (Chalmers University Of Technology) | Georg Hess (Chalmers University Of Technology) | William Ljungbergh (Link??ping University) | Lennart Svensson (Chalmers University Of Technology) | Christoffer Petersson (Zenseact),2023-11-26 10:27:22+00:00,,,,,,
Analyzing and Improving the Training Dynamics of Diffusion Models,"Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.",http://arxiv.org/abs/2312.02696v1,,Tero Karras (NVIDIA) | Miika Aittala (NVIDIA) | Jaakko Lehtinen (Aalto University & NVIDIA) | Janne Hellsten (NVIDIA) | Timo Aila (NVIDIA) | Samuli Laine (NVIDIA),2023-12-05 11:55:47+00:00,,,,,,
ViewFusion: Towards Multi-View Consistency via Interpolated Denoising,"Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.",http://arxiv.org/abs/2402.18842v1,,Xianghui Yang (University Of Sydney) | Gil Avraham (Amazon) | Yan Zuo (Amazon) | Sameera Ramasinghe (Amazon) | Loris Bazzani (Amazon) | Anton Van Den Hengel (University Of Adelaide),2024-02-29 04:21:38+00:00,,,,,,
Improving Training Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures,"Diffusion models, emerging as powerful deep generative tools, excel in various applications. They operate through a two-steps process: introducing noise into training samples and then employing a model to convert random noise into new samples (e.g., images). However, their remarkable generative performance is hindered by slow training and sampling. This is due to the necessity of tracking extensive forward and reverse diffusion trajectories, and employing a large model with numerous parameters across multiple timesteps (i.e., noise levels). To tackle these challenges, we present a multi-stage framework inspired by our empirical findings. These observations indicate the advantages of employing distinct parameters tailored to each timestep while retaining universal parameters shared across all time steps. Our approach involves segmenting the time interval into multiple stages where we employ custom multi-decoder U-net architecture that blends time-dependent models with a universally shared encoder. Our framework enables the efficient distribution of computational resources and mitigates inter-stage interference, which substantially improves training efficiency. Extensive numerical experiments affirm the effectiveness of our framework, showcasing significant training and sampling efficiency enhancements on three state-of-the-art diffusion models, including large-scale latent diffusion models. Furthermore, our ablation studies illustrate the impact of two important components in our framework: (i) a novel timestep clustering algorithm for stage division, and (ii) an innovative multi-decoder U-net architecture, seamlessly integrating universal and customized hyperparameters.",http://arxiv.org/abs/2312.09181v1,,Huijie Zhang (University Of Michigan - Ann Arbor) | Yifu Lu (University Of Michigan - Ann Arbor) | Ismail Alkhouri (Michigan State University; University Of Michigan) | Saiprasad Ravishankar (Michigan State University) | Dogyoon Song (University Of Michigan - Ann Arbor) | Qing Qu (University Of Michigan),2023-12-14 17:48:09+00:00,,,,,,
CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation,"Listening head generation aims to synthesize a non-verbal responsive listener head by modeling the correlation between the speaker and the listener in dynamic conversion.The applications of listener agent generation in virtual interaction have promoted many works achieving the diverse and fine-grained motion generation. However, they can only manipulate motions through simple emotional labels, but cannot freely control the listener's motions. Since listener agents should have human-like attributes (e.g. identity, personality) which can be freely customized by users, this limits their realism. In this paper, we propose a user-friendly framework called CustomListener to realize the free-form text prior guided listener generation. To achieve speaker-listener coordination, we design a Static to Dynamic Portrait module (SDP), which interacts with speaker information to transform static text into dynamic portrait token with completion rhythm and amplitude information. To achieve coherence between segments, we design a Past Guided Generation Module (PGG) to maintain the consistency of customized listener attributes through the motion prior, and utilize a diffusion-based structure conditioned on the portrait token and the motion prior to realize the controllable generation. To train and evaluate our model, we have constructed two text-annotated listening head datasets based on ViCo and RealTalk, which provide text-video paired labels. Extensive experiments have verified the effectiveness of our model.",http://arxiv.org/abs/2403.00274v1,,Xi Liu (University Of Electronic Science And Technology Of China) | Ying Guo (Meituan) | Cheng Zhen (Meituan) | Tong Li (Meituan) | Yingying Ao (Meituan) | Pengfei Yan (Meituan),2024-03-01 04:31:56+00:00,,,,,,
Permutation Equivariance of Transformers and Its Applications,"Revolutionizing the field of deep learning, Transformer-based models have achieved remarkable performance in many tasks. Recent research has recognized these models are robust to shuffling but are limited to inter-token permutation in the forward propagation. In this work, we propose our definition of permutation equivariance, a broader concept covering both inter- and intra- token permutation in the forward and backward propagation of neural networks. We rigorously proved that such permutation equivariance property can be satisfied on most vanilla Transformer-based models with almost no adaptation. We examine the property over a range of state-of-the-art models including ViT, Bert, GPT, and others, with experimental validations. Further, as a proof-of-concept, we explore how real-world applications including privacy-enhancing split learning, and model authorization, could exploit the permutation equivariance property, which implicates wider, intriguing application scenarios.",http://arxiv.org/abs/2304.07735v2,,Hengyuan Xu (None) | Liyao Xiang (Shanghai Jiao Tong University) | Hangyu Ye (Shanghai Jiao Tong University) | Dixi Yao (University Of Toronto) | Pengzhi Chu (Shanghai Jiao Tong University) | Baochun Li (University Of Toronto),2023-04-16 09:25:24+00:00,,,,,,
Move Anything with Layered Scene Diffusion,,,,Jiawei Ren (Nanyang Technological University) | Mengmeng Xu (Meta AI) | Jui-Chieh Wu (Meta) | Ziwei Liu (Nanyang Technological University) | Tao Xiang (University Of Surrey) | Antoine Toisoul (Meta),,,,,,,
One-Class Face Anti-spoofing via Spoof Cue Map-Guided Feature Learning,,,,"Pei-Kai Huang (Department Of Computer Science, National Tsing Hua University) | Cheng-Hsuan Chiang (National Tsinghua University) | Tzu-Hsien Chen (National Tsinghua University) | Jun-Xiong Chong (National Tsing Hua University) | Tyng-Luh Liu (IIS/Academia Sinica) | Chiou-Ting Hsu (National Tsing Hua University)",,,,,,,
Regressor-Segmenter Mutual Prompt Learning for Crowd Counting,"Crowd counting has achieved significant progress by training regressors to predict instance positions. In heavily crowded scenarios, however, regressors are challenged by uncontrollable annotation variance, which causes density map bias and context information inaccuracy. In this study, we propose mutual prompt learning (mPrompt), which leverages a regressor and a segmenter as guidance for each other, solving bias and inaccuracy caused by annotation variance while distinguishing foreground from background. In specific, mPrompt leverages point annotations to tune the segmenter and predict pseudo head masks in a way of point prompt learning. It then uses the predicted segmentation masks, which serve as spatial constraint, to rectify biased point annotations as context prompt learning. mPrompt defines a way of mutual information maximization from prompt learning, mitigating the impact of annotation variance while improving model accuracy. Experiments show that mPrompt significantly reduces the Mean Average Error (MAE), demonstrating the potential to be general framework for down-stream vision tasks.",http://arxiv.org/abs/2312.01711v3,,Mingyue Guo (University Of Chinese Academy Of Sciences) | Li Yuan (Peking University) | Zhaoyi Yan (PengCheng Laboratory) | Binghui Chen (Alibaba Group) | Yaowei Wang (Pengcheng Laboratory) | Qixiang Ye (University Of Chinese Academy Of Sciences),2023-12-04 07:53:59+00:00,,,,,,
Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples,"While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing over 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",http://arxiv.org/abs/2312.00825v1,,Phillip Howard (Intel Labs) | Avinash Madasu (None) | Tiep Le (Intel) | Gustavo Lujan-Moreno (Intel) | Anahita Bhiwandiwalla (Intel) | Vasudev Lal (None),2023-11-30 18:32:14+00:00,,,,,,
OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition,"Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.",http://arxiv.org/abs/2402.18786v1,,"Yuchen Pan (Harbin Institute Of Technology) | Junjun Jiang (Harbin Institute Of Technology) | Kui Jiang (Harbin Institute Of Technology) | Zhihao Wu (Harbin Institute Of Technology, Shenzhen) | Keyuan Yu (Harbin Institute Of Technology) | Xianming Liu (Harbin Institute Of Technology)",2024-02-29 01:20:29+00:00,,,,,,
Artist-Friendly Relightable and Animatable Neural Heads,"An increasingly common approach for creating photo-realistic digital avatars is through the use of volumetric neural fields. The original neural radiance field (NeRF) allowed for impressive novel view synthesis of static heads when trained on a set of multi-view images, and follow up methods showed that these neural representations can be extended to dynamic avatars. Recently, new variants also surpassed the usual drawback of baked-in illumination in neural representations, showing that static neural avatars can be relit in any environment. In this work we simultaneously tackle both the motion and illumination problem, proposing a new method for relightable and animatable neural heads. Our method builds on a proven dynamic avatar approach based on a mixture of volumetric primitives, combined with a recently-proposed lightweight hardware setup for relightable neural fields, and includes a novel architecture that allows relighting dynamic neural avatars performing unseen expressions in any environment, even with nearfield illumination and viewpoints.",http://arxiv.org/abs/2312.03420v1,,"Yingyan Xu (Department Of Computer Science, ETHZ - ETH Zurich) | Prashanth Chandran (None) | Sebastian Weiss (DisneyResearch|Studios) | Markus Gross (Disney Research, Disney) | Gaspard Zoss (Disney Research, Disney) | Derek Bradley (DisneyResearch|Studios)",2023-12-06 11:06:46+00:00,,,,,,
Not All Prompts Are Secure: A Switchable Backdoor Attack against Pre-trained Models,,,,"Sheng Yang (None) | Jiawang Bai (None) | Kuofeng Gao (Tsinghua University) | Yong Yang (Tencent Security) | Yiming Li (Zhejiang University) | Shu-Tao Xia (Shenzhen International Graduate School, Tsinghua University)",,,,,,,
Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation,"Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL.",http://arxiv.org/abs/2403.07630v1,,"Feilong Tang (Monash University) | Zhongxing Xu (Weill Cornell Medicine, Cornell University) | Zhaojun QU (Xi'an Jiaotong-Liverpool University) | Wei Feng (Monash University) | Xingjian Jiang (University Of Michigan - Ann Arbor) | Zongyuan Ge (Monash University)",2024-03-12 13:11:58+00:00,,,,,,
NeRF Director: Revisiting View Selection in Neural Volume Rendering,,,,Wenhui Xiao (Queensland University Of Technology) | Rodrigo Santa Cruz (CSIRO) | David Ahmedt-Aristizabal (CSIRO) | Olivier Salvado (CSIRO) | Clinton Fookes (Queensland University Of Technology) | Leo Lebrat (CSIRO / QUT),,,,,,,
Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models,,,,Jingyao Xu (Beijing Jiao Tong University) | Yuetong Lu (Beijing Jiao Tong University) | Yandong Li (Google Research) | Siyang Lu (Beijing Jiao Tong University) | Dongdong Wang (University Of Central Florida) | Xiang Wei (Beijing Jiaotong University),,,,,,,
Enhancing the Power of OOD Detection via Sample-Aware Model Selection,,,,Feng Xue (Shanghai Jiao Tong University) | Zi He (HuNan University) | Yuan Zhang (Beijing Normal University) | Chuanlong Xie (Beijing Normal University) | Zhenguo Li (Huawei) | Falong Tan (Hunan University),,,,,,,
Dynamic LiDAR Re-simulation using Compositional Neural Fields,"We introduce DyNFL, a novel neural field-based approach for high-fidelity re-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR measurements from dynamic environments, accompanied by bounding boxes of moving objects, to construct an editable neural field. This field, comprising separately reconstructed static backgrounds and dynamic objects, allows users to modify viewpoints, adjust object positions, and seamlessly add or remove objects in the re-simulated scene. A key innovation of our method is the neural field composition technique, which effectively integrates reconstructed neural assets from various scenes through a ray drop test, accounting for occlusions and transparent surfaces. Our evaluation with both synthetic and real-world environments demonstrates that \ShortName substantial improves dynamic scene simulation based on LiDAR scans, offering a combination of physical fidelity and flexible editing capabilities.",http://arxiv.org/abs/2312.05247v1,,"Hanfeng Wu (None) | Xingxing Zuo (Caltech) | Stefan Leutenegger (Department Of Informatics, Technische Universit??t M??nchen) | Or Litany (NVIDIA / Technion) | Konrad Schindler (ETH Zurich) | Shengyu Huang (None)",2023-12-08 18:55:24+00:00,,,,,,
Nearest Is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks,,,,Boheng Li (Wuhan University) | Yishuo Cai (Central South University) | Haowei Li (Wuhan University) | Feng Xue (ZJU-Hangzhou Global Scientific And Technological Innovation Center) | Zhifeng Li (Tencent) | Yiming Li (Zhejiang University),,,,,,,
BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning,,,,Hongwei Zheng (Meituan) | Linyuan Zhou (Meituan) | Han Li (Shanghai Jiao Tong University) | Jinming Su (Meituan) | Xiaoming Wei (Meituan) | Xu Xiaoming (Meituan),,,,,,,
Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding,"3D Visual Grounding (3DVG) aims at localizing 3D object based on textual descriptions. Conventional supervised methods for 3DVG often necessitate extensive annotations and a predefined vocabulary, which can be restrictive. To address this issue, we propose a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs). Our approach begins with a unique dialog-based method, engaging with LLMs to establish a foundational understanding of zero-shot 3DVG. Building on this, we design a visual program that consists of three types of modules, i.e., view-independent, view-dependent, and functional modules. These modules, specifically tailored for 3D scenarios, work collaboratively to perform complex reasoning and inference. Furthermore, we develop an innovative language-object correlation module to extend the scope of existing 3D object detectors into open-vocabulary scenarios. Extensive experiments demonstrate that our zero-shot approach can outperform some supervised baselines, marking a significant stride towards effective 3DVG.",http://arxiv.org/abs/2311.15383v1,,"Zhihao Yuan (The Chinese University Of Hong Kong, Shenzhen) | Jinke Ren (The Chinese University Of Hong Kong, Shenzhen) | Chun-Mei Feng (None) | Hengshuang Zhao (The University Of Hong Kong) | Shuguang Cui (The Chinese University Of Hong Kong, Shenzhen) | Zhen Li (The Chinese University Of Hong Kong, Shenzhen)",2023-11-26 19:01:14+00:00,,,,,,
RepKPU: Point Cloud Upsampling with Kernel Point Representation and KernelPoint-to-Displacement Generation,,,,Yi Rong (Nanjing University) | Haoran Zhou (Nanjing University) | Kang Xia (Nanjing University) | Cheng Mei (Nanjing University) | Jiahao Wang (None) | Tong Lu (Nanjing University),,,,,,,
UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion,,,,Junsheng Zhou (Tsinghua University) | Weiqi Zhang (Tsinghua University) | Baorui Ma (BAAI) | Kanle Shi (Kuaishou Technology) | Yu-Shen Liu (None) | Zhizhong Han (Wayne State University),,,,,,,
Segment Any Events via Weighted Adaptation of Pivotal Tokens,"In this paper, we delve into the nuanced challenge of tailoring the Segment Anything Models (SAMs) for integration with event data, with the overarching objective of attaining robust and universal object segmentation within the event-centric domain. One pivotal issue at the heart of this endeavor is the precise alignment and calibration of embeddings derived from event-centric data such that they harmoniously coincide with those originating from RGB imagery. Capitalizing on the vast repositories of datasets with paired events and RGB images, our proposition is to harness and extrapolate the profound knowledge encapsulated within the pre-trained SAM framework. As a cornerstone to achieving this, we introduce a multi-scale feature distillation methodology. This methodology rigorously optimizes the alignment of token embeddings originating from event data with their RGB image counterparts, thereby preserving and enhancing the robustness of the overall architecture. Considering the distinct significance that token embeddings from intermediate layers hold for higher-level embeddings, our strategy is centered on accurately calibrating the pivotal token embeddings. This targeted calibration is aimed at effectively managing the discrepancies in high-level embeddings originating from both the event and image domains. Extensive experiments on different datasets demonstrate the effectiveness of the proposed distillation method. Code in http://github.com/happychenpipi/EventSAM.",http://arxiv.org/abs/2312.16222v1,,Zhiwen Chen (Xidian University) | Zhiyu Zhu (City University Of Hong Kong) | Yifan Zhang (City University Of Hong Kong) | Junhui Hou (City University Of Hong Kong) | Guangming Shi (Xidian University) | Jinjian Wu (Xidian University),2023-12-24 12:47:08+00:00,,,,,,
PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF,"We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalon.github.io/pienerf/.",http://arxiv.org/abs/2311.13099v1,,"Yutao Feng (Zhejiang University) | Yintong Shang (University Of Utah) | Xuan Li (None) | Tianjia Shao (Zhejiang University) | Chenfanfu Jiang (University Of California, Los Angeles) | Yin Yang (University Of Utah)",2023-11-22 01:58:26+00:00,,,,,,
LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding,,,,"Chuwei Luo (DAMO Academy, Alibaba Group) | Yufan Shen (Zhejiang University) | Zhaoqing Zhu (Alibaba Group) | Qi Zheng (Alibaba Group) | Zhi Yu (Zhejiang University) | Cong Yao (Alibaba DAMO Academy)",,,,,,,
CMA: A Chromaticity Map Adapter for Robust Detection of Screen-Recapture Document Images,,,,Changsheng Chen (Shenzhen University) | Liangwei Lin (Shenzhen University) | Yongqi Chen (Shenzhen University) | Bin Li (Shenzhen University) | Jishen Zeng (Alibaba Group) | Jiwu Huang (Shenzhen University),,,,,,,
CoDi-2: Interleaved and In-Context Any-to-Any Generation,"We present CoDi-2, a versatile and interactive Multimodal Large Language Model (MLLM) that can follow complex multimodal interleaved instructions, conduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any input-output modality paradigm. By aligning modalities with language for both encoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not only understand complex modality-interleaved instructions and in-context examples, but also autoregressively generate grounded and coherent multimodal outputs in the continuous feature space. To train CoDi-2, we build a large-scale generation dataset encompassing in-context multimodal instructions across text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot capabilities for multimodal generation, such as in-context learning, reasoning, and compositionality of any-to-any modality generation through multi-round interactive conversation. CoDi-2 surpasses previous domain-specific models on tasks such as subject-driven image generation, vision transformation, and audio editing. CoDi-2 signifies a substantial breakthrough in developing a comprehensive multimodal foundation model adept at interpreting in-context language-vision-audio interleaved instructions and producing multimodal outputs.",http://arxiv.org/abs/2311.18775v1,,"Zineng Tang (University Of North Carolina, Chapel Hill) | Ziyi Yang (Microsoft) | MAHMOUD KHADEMI (Microsoft) | Yang Liu (Microsoft) | Chenguang Zhu (Zoom) | Mohit Bansal (University Of North Carolina At Chapel Hill)",2023-11-30 18:21:25+00:00,,,,,,
Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization,,,,Insoo Kim (Korea Advanced Institute Of Science And Technology) | Jae Seok Choi (Samsung Advanced Institute Of Technology (SAIT)) | Geonseok Seo (Samsung) | Kinam Kwon (Samsung) | Jinwoo Shin (Korea Advanced Institute Of Science And Technology) | Hyong-Euk Lee (Samsung Advanced Institute Of Technology),,,,,,,
Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences,"Customizing robotic behaviors to be aligned with diverse human preferences is an underexplored challenge in the field of embodied AI. In this paper, we present Promptable Behaviors, a novel framework that facilitates efficient personalization of robotic agents to diverse human preferences in complex environments. We use multi-objective reinforcement learning to train a single policy adaptable to a broad spectrum of preferences. We introduce three distinct methods to infer human preferences by leveraging different types of interactions: (1) human demonstrations, (2) preference feedback on trajectory comparisons, and (3) language instructions. We evaluate the proposed method in personalized object-goal navigation and flee navigation tasks in ProcTHOR and RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human preferences in various scenarios. Project page: https://promptable-behaviors.github.io",http://arxiv.org/abs/2312.09337v1,,Minyoung Hwang (Seoul National University) | Luca Weihs (Allen Institute For Artificial Intelligence) | Chanwoo Park (Massachusetts Institute Of Technology) | Kimin Lee (KAIST) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence) | Kiana Ehsani (Allen Institute For Artificial Intelligence),2023-12-14 21:00:56+00:00,,,,,,
HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models,"Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks. Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability. To alleviate these issues, this paper proposes an unsupervised HSI restoration framework with pre-trained diffusion model (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix. Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved diffusion model where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled. The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\times$ acceleration for denoising) with little performance decrease. Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting. The code is available at https://github.com/LiPang/HIRDiff.",http://arxiv.org/abs/2402.15865v1,,Li Pang (Xi'an Jiao Tong University) | Xiangyu Rui (Xi'an Jiao Tong University) | Long Cui (Xi'an Jiao Tong University) | Hongzhong Wang (Xi'an Jiao Tong University) | Deyu Meng (None) | Xiangyong Cao (Xi'an Jiao Tong University),2024-02-24 17:15:05+00:00,,,,,,
ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments,,,,Jingyu Zhang (Fudan University) | Kun Yang (Fudan University) | Yilei Wang (Fudan University) | Hanqi Wang (Fudan University) | Peng Sun (Duke Kunshan University) | Liang Song (Fudan University),,,,,,,
Cross Initialization for Personalized Text-to-Image Generation,"Recently, there has been a surge in face personalization techniques, benefiting from the advanced capabilities of pretrained text-to-image diffusion models. Among these, a notable method is Textual Inversion, which generates personalized images by inverting given images into textual embeddings. However, methods based on Textual Inversion still struggle with balancing the trade-off between reconstruction quality and editability. In this study, we examine this issue through the lens of initialization. Upon closely examining traditional initialization methods, we identified a significant disparity between the initial and learned embeddings in terms of both scale and orientation. The scale of the learned embedding can be up to 100 times greater than that of the initial embedding. Such a significant change in the embedding could increase the risk of overfitting, thereby compromising the editability. Driven by this observation, we introduce a novel initialization method, termed Cross Initialization, that significantly narrows the gap between the initial and learned embeddings. This method not only improves both reconstruction and editability but also reduces the optimization steps from 5000 to 320. Furthermore, we apply a regularization term to keep the learned embedding close to the initial embedding. We show that when combined with Cross Initialization, this regularization term can effectively improve editability. We provide comprehensive empirical evidence to demonstrate the superior performance of our method compared to the baseline methods. Notably, in our experiments, Cross Initialization is the only method that successfully edits an individual's facial expression. Additionally, a fast version of our method allows for capturing an input image in roughly 26 seconds, while surpassing the baseline methods in terms of both reconstruction and editability. Code will be made publicly available.",http://arxiv.org/abs/2312.15905v1,,"Lianyu Pang (None) | Jian Yin (None) | Haoran Xie (Lingnan University) | Qiping Wang (East China Normal University) | Qing Li (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) | Xudong Mao (None)",2023-12-26 06:49:53+00:00,,,,,,
A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization,,,,Hongwei Ren (Hong Kong University Of Science And Technology) | Jiadong Zhu (The Hong Kong University Of Science And Technology (Guangzhou)) | Yue Zhou (Hong Kong University Of Science And Technology) | Haotian FU (Hong Kong University Of Science And Technology) | Yulong Huang (Central South University) | Bojun Cheng (Hong Kong University Of Science And Technology),,,,,,,
Synergistic Global-space Camera and Human Reconstruction from Videos,,,,Yizhou Zhao (Carnegie Mellon University) | Tuanfeng Y. Wang (None) | Bhiksha Raj (Carnegie Mellon University) | Min Xu (Carnegie Mellon University) | Jimei Yang (Adobe Research) | Chun-Hao P. Huang (Adobe Systems),,,,,,,
TEA: Test-time Energy Adaptation,"Test-time adaptation (TTA) aims to improve model generalizability when test data diverges from training distribution, offering the distinct advantage of not requiring access to training data and processes, especially valuable in the context of large pre-trained models. However, current TTA methods fail to address the fundamental issue: covariate shift, i.e., the decreased generalizability can be attributed to the model's reliance on the marginal distribution of the training data, which may impair model calibration and introduce confirmation bias. To address this, we propose a novel energy-based perspective, enhancing the model's perception of target data distributions without requiring access to training data or processes. Building on this perspective, we introduce $\textbf{T}$est-time $\textbf{E}$nergy $\textbf{A}$daptation ($\textbf{TEA}$), which transforms the trained classifier into an energy-based model and aligns the model's distribution with the test data's, enhancing its ability to perceive test distributions and thus improving overall generalizability. Extensive experiments across multiple tasks, benchmarks and architectures demonstrate TEA's superior generalization performance against state-of-the-art methods. Further in-depth analyses reveal that TEA can equip the model with a comprehensive perception of test distribution, ultimately paving the way toward improved generalization and calibration.",http://arxiv.org/abs/2311.14402v2,,"Yige Yuan (None) | Bingbing Xu (Institute Of Computing Technology, Chinese Academy Of Sciences) | Liang Hou (Kuaishou Technology) | Fei Sun (Institute Of Computing Technology, Chinese Academy Of Sciences) | Huawei Shen (Institute Of Computing Technology, Chinese Academy Of Sciences) | Xueqi Cheng (, Chinese Academy Of Sciences)",2023-11-24 10:49:49+00:00,,,,,,
Multimodal autoregressive learning for time-aligned and contextual modalities,"One of the main challenges of multimodal learning is the need to combine heterogeneous modalities (e.g., video, audio, text). For example, video and audio are obtained at much higher rates than text and are roughly aligned in time. They are often not synchronized with text, which comes as a global context, e.g., a title, or a description. Furthermore, video and audio inputs are of much larger volumes, and grow as the video length increases, which naturally requires more compute dedicated to these modalities and makes modeling of long-range dependencies harder.   We here decouple the multimodal modeling, dividing it into separate, focused autoregressive models, processing the inputs according to the characteristics of the modalities. We propose a multimodal model, called Mirasol3B, consisting of an autoregressive component for the time-synchronized modalities (audio and video), and an autoregressive component for the context modalities which are not necessarily aligned in time but are still sequential. To address the long-sequences of the video-audio inputs, we propose to further partition the video and audio sequences in consecutive snippets and autoregressively process their representations. To that end, we propose a Combiner mechanism, which models the audio-video information jointly within a timeframe. The Combiner learns to extract audio and video features from raw spatio-temporal signals, and then learns to fuse these features producing compact but expressive representations per snippet.   Our approach achieves the state-of-the-art on well established multimodal benchmarks, outperforming much larger models. It effectively addresses the high computational demand of media inputs by both learning compact representations, controlling the sequence length of the audio-video feature representations, and modeling their dependencies in time.",http://arxiv.org/abs/2311.05698v2,,AJ Piergiovanni (Google) | Isaac Noble (Google) | Dahun Kim (Google) | Michael Ryoo (Stony Brook University) | Victor Gomes (Google) | Anelia Angelova (Google),2023-11-09 19:15:12+00:00,,,,,,
A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions,"Curation methods for massive vision-language datasets trade off between dataset size and quality. However, even the highest quality of available curated captions are far too short to capture the rich visual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 8012 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each. With precise and reliable captions associated with specific parts of an image, we can evaluate vision-language models' (VLMs) understanding of image content with a novel task that matches each caption with its corresponding subcrop. As current models are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which each caption length is limited. We show that modern techniques that make progress on standard benchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set. By releasing the first human annotated dense image captioning dataset, we hope to enable the development of new benchmarks or fine-tuning recipes for the next generation of VLMs to come.",http://arxiv.org/abs/2312.08578v1,,Jack Urbanek (Facebook) | Florian Bordes (Meta AI) | Pietro Astolfi (Meta AI) | Mary Williamson (Meta AI (FAIR)) | Vasu Sharma (Meta AI/ CMU) | Adriana Romero-Soriano (Meta),2023-12-14 00:42:23+00:00,,,,,,
ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization,"Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence"": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.",http://arxiv.org/abs/2401.08937v1,,Weiyao Wang (Facebook) | Pierre Gleize (Polytech Nice Sophia) | Hao Tang (Meta Platforms) | Xingyu Chen (Facebook) | Kevin Liang (FAIR At Meta) | Matt Feiszli (Meta AI),2024-01-17 03:18:02+00:00,,,,,,
Tyche: Stochastic in Context Learning for Universal Medical Image Segmentation,"Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.",http://arxiv.org/abs/2401.13650v1,,Marianne Rakic (Massachusetts Institute Of Technology) | Hallee Wong (MIT) | Jose Javier Gonzalez Ortiz (DataBricks) | Beth Cimini (Broad Institute) | John Guttag (Massachusetts Institute Of Technology) | Adrian V. Dalca (Harvard University),2024-01-24 18:35:55+00:00,,,,,,
Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations,"Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any group annotation. To this end, we systematically study the existence of spurious correlation on CLIP and CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP. In view of them, we advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels. Extensive experiments and in-depth visualizations on several benchmarks validate the effectiveness of our proposals, largely reducing reliance and significantly boosting the model generalization.",http://arxiv.org/abs/2403.07241v1,,Chenyu You (Yale University) | Yifei Min (Yale University) | Weicheng Dai (Yale University) | Jasjeet Sekhon (Yale University) | Lawrence Staib (Yale University) | James Duncan (Yale University),2024-03-12 01:47:17+00:00,,,,,,
The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes,"Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models.   In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches.   We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF.",http://arxiv.org/abs/2402.08922v1,,Myeongseob Ko (Virginia Polytechnic Institute And State University) | Feiyang Kang (Virginia Polytechnic Institute And State University) | Weiyan Shi (Stanford University) | Ming Jin (Virginia Tech) | Zhou Yu (Columbia University) | Ruoxi Jia (Virginia Tech),2024-02-14 03:43:05+00:00,,,,,,
Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection,,,,Suyeon Kim (Pohang University Of Science And Technology) | Dongha Lee (Yonsei University) | SeongKu Kang (University Of Illinois Urbana-Champaign) | Sukang Chae (Pohang University Of Science And Technology) | Sanghwan Jang (POSTECH) | Hwanjo Yu (POSTECH),,,,,,,
3D Face Tracking from 2D Video through Iterative Dense UV to Image Flow,,,,"Felix Taubner (LG Electronics) | Prashant Raina (LG Electronics) | Mathieu Tuli (LG Electronics Canada Incorporated, TAIL) | Eu Wern Teh (LG Corporation) | Chul Lee (Department Of Computer Science, University Of Toronto) | Jinmiao Huang (Meta)",,,,,,,
Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition,"Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an ""adapt then align"" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel ""Align before Adapt"" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, particularly when facing unfamiliar or unseen categories. ALT achieves competitive performance and superior generalizability while requiring significantly low computational costs. In fully supervised scenarios, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. In 2-shot experiments, ALT outperforms the previous state-of-the-art by 7.1% and 9.2% on HMDB-51 and UCF-101, respectively.",http://arxiv.org/abs/2311.15619v1,,Yifei Chen (Huawei) | Dapeng Chen (Huawei Technologies Ltd.) | Ruijin Liu (Xi'an Jiao Tong University) | Sai Zhou (Huawei Technologies Ltd.) | Wenyuan Xue (Huawei Technologies Ltd.) | Wei Peng (Huawei Technologies Ltd.),2023-11-27 08:32:28+00:00,,,,,,
Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based 3D Semantic Scene Completion,,,,Yujie Xue (HNU) | Ruihui Li (Hunan University) | F AnWu (Wuhan University) | Zhuo Tang (Hunan University) | Kenli Li (Hunan University) | Duan Mingxing (Hunan University),,,,,,,
AUEditNet: Dual-Branch Facial Action Unit Intensity Manipulation with Implicit Disentanglement,,,,"Shiwei Jin (None) | Zhen Wang (Qualcomm Technologies,) | Lei Wang (Qualcomm) | Peng Liu (Qualcomm Inc, QualComm) | Ning Bi (QualComm) | Truong Nguyen (University Of California, San Diego)",,,,,,,
Shadows Don??t Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now,,,,Ayush Sarkar (Department Of Computer Science At University Of Illinois Urbana-Champaign) | Hanlin Mai (University Of Illinois Urbana Champaign) | Amitabh Mahapatra (University Of Illinois Urbana-Champaign) | David Forsyth (University Of Illinois At Urbana-Champaign) | Svetlana Lazebnik (University Of Illinois At Urbana-Champaign) | Anand Bhattad (None),,,,,,,
SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes,,,,"Alexandros Delitzas (ETH Zurich) | Ay??a Takmaz (None) | Federico Tombari (Google, TUM) | Robert Sumner (Massachusetts Institute Of Technology) | Marc Pollefeys (ETH Zurich / Microsoft) | Francis Engelmann (Department Of Computer Science, ETHZ - ETH Zurich)",,,,,,,
NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild,,,,Weining Ren (ETHz) | Zihan Zhu (ETHZ - ETH Zurich) | Boyang Sun (ETH Zurich) | Jiaqi Chen (ETHZ - ETH Zurich) | Marc Pollefeys (ETH Zurich / Microsoft) | Songyou Peng (ETH Zurich & MPI T??bingen),,,,,,,
Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling,"Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the interference of distributional latent bias. Then, we incorporate a suite of critical geometric properties to impose proper constraints on the layout of constructed embedding space, which in turn minimizes the uncontrollable risk for unknown class learning and structuring. Furthermore, a spectral graph-theoretic method is devised to estimate the number of potential novel classes. It inherits two intriguing merits compared to existent approaches, namely high computational efficiency and flexibility for taxonomy-adaptive estimation. Extensive experiments across various biomedical scenarios substantiate the effectiveness and general applicability of our method.",http://arxiv.org/abs/2403.01053v2,,Jianan Fan (University Of Sydney) | Dongnan Liu (University Of Sydney) | Hang Chang (Lawrence Berkeley National Lab) | Heng Huang (University Of Pittsburgh) | Mei Chen (None) | Weidong Cai (The University Of Sydney),2024-03-02 00:56:05+00:00,,,,,,
BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP,"Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.",http://arxiv.org/abs/2311.16194v1,,"Jiawang Bai (None) | Kuofeng Gao (Tsinghua University) | Shaobo Min (University Of Science And Technology Of China) | Shu-Tao Xia (Shenzhen International Graduate School, Tsinghua University) | Zhifeng Li (Tencent) | Wei Liu (Tencent AI Lab)",2023-11-26 14:24:13+00:00,,,,,,
Inlier Confidence Calibration for Point Cloud Registration,,,,Yongzhe Yuan (Xidian University) | Yue Wu (Xidian University) | Xiaolong Fan (Xidian University) | Maoguo Gong (Xidian University) | Qiguang Miao (Xidian University) | Wenping Ma (Xidian University),,,,,,,
IReNe: Instant Recoloring of Neural Radiance Fields,,,,Alessio Mazzucchelli (Arquimea Research Center) | Adrian Garcia-Garcia (Arquimea Research Center) | Elena Garces (Universidad Rey Juan Carlos) | Fernando Rivas-Manzaneque (None) | Francesc Moreno-Noguer (Universidad Polit??cnica De Cataluna) | Adrian Penate-Sanchez (Universidad De Las Palmas De Gran Canaria),,,,,,,
Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID,,,,Wentao Tan (South China University Of Technology) | Changxing Ding (South China University Of Technology) | Jiayu Jiang (South China University Of Technology) | Fei Wang (South China University Of Technology) | Yibing Zhan (JD Explore Academy) | Dapeng Tao (Yunnan University),,,,,,,
Fully Exploiting Every Real Sample: Super-Pixel Sample Gradient Model Stealing,,,,Yunlong Zhao (None) | Xiaoheng Deng (Central South University) | Yijing Liu (Zhejiang University) | Xinjun Pei (None) | Jiazhi Xia (Central South University) | Wei Chen (State Key Laboratory Of CAD&CG),,,,,,,
Bi-level Learning of Task-Specific Decoders for Joint Registration and One-Shot Segmentation,,,,Xin Fan (Dalian University Of Technology) | Wang (Dalian University Of Technology) | Jiaxin Gao (Dalian University Of Technology) | Jia Wang (Dalian University Of Technology) | Zhongxuan Luo (Dalian University Of Technology) | Risheng Liu (Dalian University Of Technology),,,,,,,
H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration,,,,MORTEZA GHAHREMANI (Technische Universit??t M??nchen) | Mohammad Khateri (University Of Eastern Finland) | Bailiang Jian (Technische Universit??t M??nchen) | Benedikt Wiestler (Technical University Munich) | Ehsan Adeli (Stanford University) | Christian Wachinger (Technische Universit??t M??nchen),,,,,,,
Training-free Pretrained Model Merging,"Recently, model merging techniques have surfaced as a solution to combine multiple single-talent models into a single multi-talent model. However, previous endeavors in this field have either necessitated additional training or fine-tuning processes, or require that the models possess the same pre-trained initialization. In this work, we identify a common drawback in prior works w.r.t. the inconsistency of unit similarity in the weight space and the activation space. To address this inconsistency, we propose an innovative model merging framework, coined as merging under dual-space constraints (MuDSC). Specifically, instead of solely maximizing the objective of a single space, we advocate for the exploration of permutation matrices situated in a region with a unified high similarity in the dual space, achieved through the linear combination of activation and weight similarity matrices. In order to enhance usability, we have also incorporated adaptations for group structure, including Multi-Head Attention and Group Normalization. Comprehensive experimental comparisons demonstrate that MuDSC can significantly boost the performance of merged models with various task combinations and architectures. Furthermore, the visualization of the merged model within the multi-task loss landscape reveals that MuDSC enables the merged model to reside in the overlapping segment, featuring a unified lower loss for each task. Our code is publicly available at https://github.com/zju-vipa/training_free_model_merging.",http://arxiv.org/abs/2403.01753v3,,Zhengqi Xu (Zhejiang University) | Ke Yuan (Zhejiang University) | Huiqiong Wang (Zhejiang University) | Yong Wang (State Grid Shandong Electronic Power Company) | Mingli Song (Zhejiang University) | Jie Song (Zhejiang University),2024-03-04 06:19:27+00:00,,,,,,
LEAD: Exploring Logit Space Evolution for Model Selection,,,,Zixuan Hu (None) | Xiaotong Li (Peking University) | SHIXIANG TANG (The Chinese University Of Hong Kong) | Jun Liu (None) | Yichun Hu (Peking University) | Ling-Yu Duan (Peking University),,,,,,,
Unleashing Channel Potential: Space-Frequency Selection Convolution for SAR Object Detection,,,,Ke Li (Xidian University) | Di Wang (Xidian University) | Zhangyuan Hu (Xidian University) | Wenxuan Zhu (Xidian University) | Shaofeng Li (None) | Quan Wang (Xidian University),,,,,,,
Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination,,,,"Yixin Zeng (None) | Zoubin Bi (State Key Laboratory Of CAD&CG, Zhejiang Univerisity) | Yin Mingrui (Zhejiang University) | Xiang Feng (Zhejiang University) | Kun Zhou (Zhejiang University) | Hongzhi Wu (Zhejiang University)",,,,,,,
Image Neural Field Diffusion Models,,,,"Yinbo Chen (University Of California, San Diego) | Oliver Wang (Adobe Research) | Richard Zhang (Adobe Systems) | Eli Shechtman (Adobe) | Xiaolong Wang (UCSD) | Micha??l Gharbi (Massachusetts Institute Of Technology)",,,,,,,
MeaCap: Memory-Augmented Zero-shot Image Captioning,"Zero-shot image captioning (IC) without well-paired image-text data can be divided into two categories, training-free and text-only-training. Generally, these two types of methods realize zero-shot IC by integrating pretrained vision-language models like CLIP for image-text similarity evaluation and a pre-trained language model (LM) for caption generation. The main difference between them is whether using a textual corpus to train the LM. Though achieving attractive performance w.r.t. some metrics, existing methods often exhibit some common drawbacks. Training-free methods tend to produce hallucinations, while text-only-training often lose generalization capability. To move forward, in this paper, we propose a novel Memory-Augmented zero-shot image Captioning framework (MeaCap). Specifically, equipped with a textual memory, we introduce a retrieve-then-filter module to get key concepts that are highly related to the image. By deploying our proposed memory-augmented visual-related fusion score in a keywords-to-sentence LM, MeaCap can generate concept-centered captions that keep high consistency with the image with fewer hallucinations and more world-knowledge. The framework of MeaCap achieves the state-of-the-art performance on a series of zero-shot IC settings. Our code is available at https://github.com/joeyz0z/MeaCap.",http://arxiv.org/abs/2403.03715v1,,"Zequn Zeng (None) | Yan Xie (None) | Hao Zhang (Xidian University, Xi'an, China) | Chiyu Chen (Xi'an University Of Electronic Science And Technology) | Zhengjue Wang (Xidian University) | Bo Chen (Xidian University)",2024-03-06 14:00:31+00:00,,,,,,
MuGE: Multiple Granularity Edge Detection,,,,"Caixia Zhou (None) | Yaping Huang (Beijing Jiao Tong University) | Mengyang Pu (North China Electric Power University) | Qingji Guan (Beijing Jiao Tong University) | Ruoxi Deng (Wenzhou University) | Haibin Ling (State University Of New York, Stony Brook)",,,,,,,
Resource-Efficient Transformer Pruning for Finetuning of Large Models,,,,"Fatih Ilhan (Georgia Institute Of Technology) | Gong Su (IBM, International Business Machines) | Selim Tekin (College Of Computing, Georgia Institute Of Technology) | Tiansheng Huang (Georgia Institute Of Technology) | Sihao Hu (Georgia Institute Of Technology) | Ling Liu (Georgia Institute Of Technology)",,,,,,,
BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning,"The recent progress in Large Language Models (LLM) has spurred various advancements in image-language conversation agents, while how to build a proficient video-based dialogue system is still under exploration. Considering the extensive scale of LLM and visual backbone, minimal GPU memory is left for facilitating effective temporal modeling, which is crucial for comprehending and providing feedback on videos. To this end, we propose Branching Temporal Adapter (BT-Adapter), a novel method for extending image-language pretrained models into the video domain. Specifically, BT-Adapter serves as a plug-and-use temporal modeling branch alongside the pretrained visual encoder, which is tuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can be seamlessly integrated into all image conversation models using this version of CLIP, enabling video conversations without the need for video instructions. Besides, we develop a unique asymmetric token masking strategy inside the branch with tailor-made training tasks for BT-Adapter, facilitating faster convergence and better results. Thanks to BT-Adapter, we are able to empower existing multimodal dialogue models with strong video understanding capabilities without incurring excessive GPU costs. Without bells and whistles, BT-Adapter achieves (1) state-of-the-art zero-shot results on various video tasks using thousands of fewer GPU hours. (2) better performance than current video chatbots without any video instruction tuning. (3) state-of-the-art results of video chatting using video instruction tuning, outperforming previous SOTAs by a large margin.",http://arxiv.org/abs/2309.15785v1,,"Ruyang Liu (Peking University) | Chen Li (Tencent ARC Lab) | Yixiao Ge (Tencent) | Thomas H. Li (AIIT, Peking University) | Ying Shan (Tencent) | Ge Li (Peking University Shenzhen Graduate School)",2023-09-27 16:58:35+00:00,,,,,,
Rethinking the Evaluation Protocol of Domain Generalization,"Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is necessary to ensure that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the potential risks of test data information leakage in two aspects of the current protocol: pretraining on ImageNet and oracle model selection. We propose that training from scratch and using multiple test domains would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the modified protocol and introduce a new leaderboard to encourage future research in domain generalization with a fairer comparison.",http://arxiv.org/abs/2305.15253v1,,Han Yu (Tsinghua University) | Xingxuan Zhang (Tsinghua University) | Renzhe Xu (Tsinghua University) | Jiashuo Liu (Tsinghua University) | Yue He (Tsinghua University) | Peng Cui (Tsinghua University),2023-05-24 15:36:46+00:00,,,,,,
Eclipse: Disambiguating Illumination and Materials using Unintended Shadows,"Decomposing an object's appearance into representations of its materials and the surrounding illumination is difficult, even when the object's 3D shape is known beforehand. This problem is especially challenging for diffuse objects: it is ill-conditioned because diffuse materials severely blur incoming light, and it is ill-posed because diffuse materials under high-frequency lighting can be indistinguishable from shiny materials under low-frequency lighting. We show that it is possible to recover precise materials and illumination -- even from diffuse objects -- by exploiting unintended shadows, like the ones cast onto an object by the photographer who moves around it. These shadows are a nuisance in most previous inverse rendering pipelines, but here we exploit them as signals that improve conditioning and help resolve material-lighting ambiguities. We present a method based on differentiable Monte Carlo ray tracing that uses images of an object to jointly recover its spatially-varying materials, the surrounding illumination environment, and the shapes of the unseen light occluders who inadvertently cast shadows upon it.",http://arxiv.org/abs/2305.16321v3,,Dor Verbin (None) | Ben Mildenhall (Google) | Peter Hedman (Google) | Jonathan T. Barron (Google) | Todd Zickler (Harvard University) | Pratul P. Srinivasan (Google Research),2023-05-25 17:59:52+00:00,,,,,,
M&M VTO: Multi-Garment Virtual Try-On and Editing,,,,"Luyang Zhu (Department Of Computer Science, University Of Washington) | Yingwei Li (Google) | Nan Liu (Google) | Hao Peng (Google) | Dawei Yang (Google) | Ira Kemelmacher-Shlizerman (University Of Washington)",,,,,,,
DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization,"The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.",http://arxiv.org/abs/2402.09812v1,,Jisu Nam (Korea University) | Heesu Kim (NAVER) | DongJae Lee (KAIST) | Siyoon Jin (Korea University) | Seungryong Kim (Korea University) | Seunggyu Chang (NAVER Cloud),2024-02-15 09:21:16+00:00,,,,,,
DS-NeRV: Implicit Neural Video Representation with Decomposed Static and Dynamic Codes,,,,Hao Yan (Tianjin University) | Zhihui Ke (Tianjin University) | Xiaobo Zhou (Tianjin University) | Tie Qiu (Tianjin University) | Xidong Shi (Tianjin University) | DaDong Jiang (Tianjin University),,,,,,,
Towards Transferable Targeted 3D Adversarial Attack in the Physical World,"Compared with transferable untargeted attacks, transferable targeted adversarial attacks could specify the misclassification categories of adversarial samples, posing a greater threat to security-critical tasks. In the meanwhile, 3D adversarial samples, due to their potential of multi-view robustness, can more comprehensively identify weaknesses in existing deep learning systems, possessing great application value. However, the field of transferable targeted 3D adversarial attacks remains vacant. The goal of this work is to develop a more effective technique that could generate transferable targeted 3D adversarial examples, filling the gap in this field. To achieve this goal, we design a novel framework named TT3D that could rapidly reconstruct from few multi-view images into Transferable Targeted 3D textured meshes. While existing mesh-based texture optimization methods compute gradients in the high-dimensional mesh space and easily fall into local optima, leading to unsatisfactory transferability and distinct distortions, TT3D innovatively performs dual optimization towards both feature grid and Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which significantly enhances black-box transferability while enjoying naturalness. Experimental results show that TT3D not only exhibits superior cross-model transferability but also maintains considerable adaptability across different renders and vision tasks. More importantly, we produce 3D adversarial examples with 3D printing techniques in the real world and verify their robust performance under various scenarios.",http://arxiv.org/abs/2312.09558v2,,Yao Huang (Beihang University) | Yinpeng Dong (Tsinghua University) | Shouwei Ruan (None) | Xiao Yang (Tsinghua University) | Hang Su (Tsinghua University) | Xingxing Wei (None),2023-12-15 06:33:14+00:00,,,,,,
Exploring the Transferability of Visual Prompting for Multimodal Large Language Models,,,,Yichi Zhang (Tsinghua University) | Yinpeng Dong (Tsinghua University) | Siyuan Zhang (None) | Tianzan Min (Tsinghua University) | Hang Su (Tsinghua University) | Jun Zhu (Tsinghua University),,,,,,,
3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos,"Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\""ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.",http://arxiv.org/abs/2403.01444v2,,Jiakai Sun (Zhejiang University) | Han Jiao (Zhejiang University) | Guangyuan Li (Zhejiang University) | Zhanjie Zhang (Zhejiang University) | Lei Zhao (Zhejiang University) | Wei Xing (Zhejiang University),2024-03-03 08:42:40+00:00,,,,,,
"Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation","Diffusion models have recently achieved remarkable progress in generating realistic images. However, challenges remain in accurately understanding and synthesizing the layout requirements in the textual prompts. To align the generated image with layout instructions, we present a training-free layout calibration system SimM that intervenes in the generative process on the fly during inference time. Specifically, following a ""check-locate-rectify"" pipeline, the system first analyses the prompt to generate the target layout and compares it with the intermediate outputs to automatically detect errors. Then, by moving the located activations and making intra- and inter-map adjustments, the rectification process can be performed with negligible computational overhead. To evaluate SimM over a range of layout requirements, we present a benchmark SimMBench that compensates for the lack of superlative spatial relations in existing datasets. And both quantitative and qualitative results demonstrate the effectiveness of the proposed SimM in calibrating the layout inconsistencies. Our project page is at https://simm-t2i.github.io/SimM.",http://arxiv.org/abs/2311.15773v2,,Biao Gong (Alibaba Group) | Siteng Huang (Zhejiang University & Westlake University) | Yutong Feng (Alibaba Group) | Shiwei Zhang (Alibaba Group) | Yuyuan Li (Zhejiang University) | Yu Liu (Alibaba Group),2023-11-27 12:48:33+00:00,,,,,,
Seeing the Unseen: Visual Common Sense for Semantic Placement,"Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding what is not present. Specifically, given an image (e.g. of a living room) and name of an object (""cushion""), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Semantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), and AR devices (automatically rendering an object in the user's space). Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context from web, and then remove that object from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. Using this, we collect a novel dataset, with ${\sim}1.3$M images across $9$ object categories, and train a SP prediction model called CLIP-UNet. CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors on real-world and simulated images. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored $43.7\%$ and $31.3\%$ times when comparing against the $4$ SP baselines on real and simulated images. In addition, we demonstrate leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments.",http://arxiv.org/abs/2401.07770v1,,Ram Ramrakhya (None) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence) | Dhruv Batra (FAIR (Meta) And Georgia Tech) | Zsolt Kira (Georgia Institute Of Technology) | Kuo-Hao Zeng (Allen Institute For Artificial Intelligence) | Luca Weihs (Allen Institute For Artificial Intelligence),2024-01-15 15:28:30+00:00,,,,,,
Contextrast: Contextual Contrastive Learning for Semantic Segmentation,,,,Changki Sung (Korea Advanced Institute Of Science & Technology) | Wanhee Kim (Korea Advanced Institute Of Science & Technology) | Jungho An (Korea Advanced Institute Of Science & Technology) | WooJu Lee (KAIST) | Hyungtae Lim (KAIST) | Hyun Myung (KAIST),,,,,,,
HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion,"Generating a 3D human model from a single reference image is challenging because it requires inferring textures and geometries in invisible views while maintaining consistency with the reference image. Previous methods utilizing 3D generative models are limited by the availability of 3D training data. Optimization-based methods that lift text-to-image diffusion models to 3D generation often fail to preserve the texture details of the reference image, resulting in inconsistent appearances in different views. In this paper, we propose HumanRef, a 3D human generation framework from a single-view input. To ensure the generated 3D model is photorealistic and consistent with the input image, HumanRef introduces a novel method called reference-guided score distillation sampling (Ref-SDS), which effectively incorporates image guidance into the generation process. Furthermore, we introduce region-aware attention to Ref-SDS, ensuring accurate correspondence between different body regions. Experimental results demonstrate that HumanRef outperforms state-of-the-art methods in generating 3D clothed humans with fine geometry, photorealistic textures, and view-consistent appearances.",http://arxiv.org/abs/2311.16961v1,,Jingbo Zhang (City University Of Hong Kong) | Xiaoyu Li (Tencent AI Lab) | Qi Zhang (Tencent AI Lab) | Yan-Pei Cao (Tencent ARC Lab) | Ying Shan (Tencent) | Jing Liao (City University Of Hong Kong),2023-11-28 17:06:28+00:00,,,,,,
Learning Visual Prompt for Gait Recognition,,,,Kang Ma (Beijing Institute Of Technology) | Ying Fu (None) | Chunshui Cao (Watrix Technology) | Saihui Hou (Beijing Normal University) | Yongzhen Huang (Beijing Normal University) | Dezhi Zheng (None),,,,,,,
Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving,,,,Junhao Zheng (Xi'an Jiao Tong University) | Chenhao Lin (Xi'an Jiao Tong University) | Jiahao Sun (Xi'an Jiao Tong University) | Zhengyu Zhao (Xi'an Jiao Tong University) | Qian Li (Xi'an Jiao Tong University) | Chao Shen (Xi??an Jiao Tong University),,,,,,,
Discovering and Mitigating Visual Biases through Keyword Explanation,,,,"Younghyun Kim (KAIST) | Sangwoo Mo (None) | Minkyu Kim (KRAFTON,) | Kyungmin Lee (Korea Advanced Institute Of Science & Technology) | Jaeho Lee (POSTECH) | Jinwoo Shin (Korea Advanced Institute Of Science And Technology)",,,,,,,
What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models,"Counterfactual reasoning, a fundamental aspect of human cognition, involves contemplating alternatives to established facts or past events, significantly enhancing our abilities in planning and decision-making. In light of the advancements in current multi-modal large language models, we explore their effectiveness in counterfactual reasoning. To facilitate this investigation, we introduce a novel dataset, C-VQA, specifically designed to test the counterfactual reasoning capabilities of modern multi-modal large language models. This dataset is constructed by infusing original questions with counterfactual presuppositions, spanning various types such as numerical and boolean queries. It encompasses a mix of real and synthetic data, representing a wide range of difficulty levels. Our thorough evaluations of contemporary vision-language models using this dataset have revealed substantial performance drops, with some models showing up to a 40% decrease, highlighting a significant gap between current models and human-like vision reasoning capabilities. We hope our dataset will serve as a vital benchmark for evaluating the counterfactual reasoning capabilities of models. Code and dataset are publicly available at https://bzhao.me/C-VQA/.",http://arxiv.org/abs/2310.06627v3,,"Letian Zhang (Tongji University) | Xiaotong Zhai (University Of Warwick) | Zhongkai Zhao (National University Of Singapore) | Yongshuo Zong (School Of Informatics, University Of Edinburgh) | Xin Wen (The University Of Hong Kong) | Bingchen Zhao (University Of Edinburgh)",2023-10-10 13:45:59+00:00,,,,,,
Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution,,,,Guangyuan Li (Zhejiang University) | Chen Rao (Zhejiang University) | Juncheng Mo (Zhejiang University) | Zhanjie Zhang (Zhejiang University) | Wei Xing (Zhejiang University) | Lei Zhao (Zhejiang University),,,,,,,
Novel Class Discovery for Ultra-Fine-Grained Visual Categorization,,,,Yu Liu (None) | Yaqi Cai (Dalian University Of Technology) | Qi Jia (Dalian University Of Technology) | Binglin Qiu (Dalian University Of Technology) | Weimin Wang (Dalian University Of Techonoly) | Nan Pu (University Of Trento),,,,,,,
SignGraph: A Sign Sequence is Worth Graphs of Nodes,,,,Shiwei Gan (None) | Yafeng Yin (Nanjing University) | Zhiwei Jiang (Nanjing University) | Hongkai Wen (University Of Warwick) | Lei Xie (Nanjing University) | Sanglu Lu (Nanjing University),,,,,,,
FreeDrag: Feature Dragging for Reliable Point-based Image Editing,"To serve the intricate and varied demands of image editing, precise and flexible manipulation in image content is indispensable. Recently, Drag-based editing methods have gained impressive performance. However, these methods predominantly center on point dragging, resulting in two noteworthy drawbacks, namely ""miss tracking"", where difficulties arise in accurately tracking the predetermined handle points, and ""ambiguous tracking"", where tracked points are potentially positioned in wrong regions that closely resemble the handle points. To address the above issues, we propose FreeDrag, a feature dragging methodology designed to free the burden on point tracking. The FreeDrag incorporates two key designs, i.e., template feature via adaptive updating and line search with backtracking, the former improves the stability against drastic content change by elaborately controls feature updating scale after each dragging, while the latter alleviates the misguidance from similar points by actively restricting the search area in a line. These two technologies together contribute to a more stable semantic dragging with higher efficiency. Comprehensive experimental results substantiate that our approach significantly outperforms pre-existing methodologies, offering reliable point-based editing even in various complex scenarios.",http://arxiv.org/abs/2307.04684v3,,Pengyang Ling (University Of Science And Technology Of China) | Lin Chen (University Of Science And Technology Of China) | Pan Zhang (Shanghai Artificial Intelligence Laboratory) | Huaian Chen (University Of Science And Technology Of China) | Yi Jin (University Of Science And Technology Of China) | Jinjin Zheng (University Of Science And Technology Of China),2023-07-10 16:37:46+00:00,,,,,,
A Unified Approach for Text- and Image-guided 4D Scene Generation,"Large-scale diffusion generative models are greatly simplifying image, video and 3D asset creation from user-provided text prompts and images. However, the challenging problem of text-to-4D dynamic 3D scene generation with diffusion guidance remains largely unexplored. We propose Dream-in-4D, which features a novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D diffusion guidance to effectively learn a high-quality static 3D asset in the first stage; (2) a deformable neural radiance field that explicitly disentangles the learned static asset from its deformation, preserving quality during motion learning; and (3) a multi-resolution feature grid for the deformation field with a displacement total variation loss to effectively learn motion with video diffusion guidance in the second stage. Through a user preference study, we demonstrate that our approach significantly advances image and motion quality, 3D consistency and text fidelity for text-to-4D generation compared to baseline approaches. Thanks to its motion-disentangled representation, Dream-in-4D can also be easily adapted for controllable generation where appearance is defined by one or multiple images, without the need to modify the motion learning stage. Thus, our method offers, for the first time, a unified approach for text-to-4D, image-to-4D and personalized 4D generation tasks.",http://arxiv.org/abs/2311.16854v2,,"Yufeng Zheng (ETH Zurich, MPI-IS) | Xueting Li (NVIDIA) | Koki Nagano (None) | Sifei Liu (NVIDIA) | Otmar Hilliges (None) | Shalini De Mello (NVIDIA Research)",2023-11-28 15:03:53+00:00,,,,,,
Video Recognition in Portrait Mode,"The creation of new datasets often presents new challenges for video recognition and can inspire novel ideas while addressing these challenges. While existing datasets mainly comprise landscape mode videos, our paper seeks to introduce portrait mode videos to the research community and highlight the unique challenges associated with this video format. With the growing popularity of smartphones and social media applications, recognizing portrait mode videos is becoming increasingly important. To this end, we have developed the first dataset dedicated to portrait mode video recognition, namely PortraitMode-400. The taxonomy of PortraitMode-400 was constructed in a data-driven manner, comprising 400 fine-grained categories, and rigorous quality assurance was implemented to ensure the accuracy of human annotations. In addition to the new dataset, we conducted a comprehensive analysis of the impact of video format (portrait mode versus landscape mode) on recognition accuracy and spatial bias due to the different formats. Furthermore, we designed extensive experiments to explore key aspects of portrait mode video recognition, including the choice of data augmentation, evaluation procedure, the importance of temporal information, and the role of audio modality. Building on the insights from our experimental results and the introduction of PortraitMode-400, our paper aims to inspire further research efforts in this emerging research area.",http://arxiv.org/abs/2312.13746v1,,Mingfei Han (University Of Technology Sydney) | Linjie Yang (ByteDance) | Xiaojie Jin (ByteDance/TikTok) | Jiashi Feng (ByteDance) | Xiaojun Chang (University Of Technology Sydney) | Heng Wang (Bytedance),2023-12-21 11:30:02+00:00,,,,,,
View-Category Interactive Sharing Transformer for Incomplete Multi-View Multi-Label Learning,,,,Shilong Ou (Beijing University Of Posts And Telecommunications) | Zhe Xue (Beijing University Of Posts And Telecommunications) | Yawen Li (Beijing University Of Posts And Telecommunications) | Meiyu Liang (Beijing University Of Posts And Telecommunications) | Yuanqiang Cai (Beijing University Of Posts And Telecommunications) | Junjiang Wu (None),,,,,,,
EasyDrag: Efficient Point-based Manipulation on Diffusion Models,,,,"Xingzhong Hou (University Of Chinese Academy Of Sciences) | Boxiao Liu (Sensetime Research) | Yi Zhang (The Chinese University Of Hong Kong) | Jihao Liu (The Chinese University Of Hong Kong) | Yu Liu (The Chinese University Of Hong Kong) | Haihang You (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
CosmicMan: A Text-to-Image Foundation Model for Humans,,,,Shikai Li (Shanghai AI Lab) | Jianglin Fu (None) | Kaiyuan Liu (None) | Wentao Wang (Shanghai AI Laboratory) | Kwan-Yee Lin (The Chinese University Of Hong Kong) | Wayne Wu (None),,,,,,,
Towards Generalizable Multi-Object Tracking,,,,"Zheng Qin (None) | Le Wang (Xi'an Jiao Tong University) | Sanping Zhou (Xi'an Jiao Tong University) | Panpan Fu (Xi'an Jiao Tong University) | Gang Hua (Wormpex AI Research) | Wei Tang (University Of Illinois, Chicago)",,,,,,,
Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction,"Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world datasets. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering.",http://arxiv.org/abs/2309.13101v2,,"Ziyi Yang (None) | Xinyu Gao (Zhejiang University) | Wen Zhou (University Of Science And Technology Of China) | Shaohui Jiao (Bytedance) | Yuqing Zhang (Zhejiang University) | Xiaogang Jin (State Key Lab Of CAD&CG, Zhejiang University)",2023-09-22 16:04:02+00:00,,,,,,
Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection,,,,"Yajing Liu (Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Shijun Zhou (Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Xiyao Liu (Shenyang Institute Of Automation Chinese Academy Of Sciences ) | Chunhui Hao (None) | Baojie Fan (Nanjing University Of Posts And Telecommunications) | Jiandong Tian (The Shenyang Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model,"Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash the potential of large foundation models in novel scenarios with limited training data. In the computer vision community, PEFT has shown effectiveness in image classification, but little research has studied its ability for image segmentation. Fine-tuning segmentation models usually require a heavier adjustment of parameters to align the proper projection directions in the parameter space for new scenarios. This raises a challenge to existing PEFT algorithms, as they often inject a limited number of individual parameters into each block, which prevents substantial adjustment of the projection direction of the parameter space due to the limitation of Hidden Markov Chain along blocks. In this paper, we equip PEFT with a cross-block orchestration mechanism to enable the adaptation of the Segment Anything Model (SAM) to various downstream scenarios. We introduce a novel inter-block communication module, which integrates a learnable relation matrix to facilitate communication among different coefficient sets of each PEFT block's parameter space. Moreover, we propose an intra-block enhancement module, which introduces a linear projection head whose weights are generated from a hyper-complex layer, further enhancing the impact of the adjustment of projection directions on the entire parameter space. Extensive experiments on diverse benchmarks demonstrate that our proposed approach consistently improves the segmentation performance significantly on novel scenarios with only around 1K additional parameters.",http://arxiv.org/abs/2311.17112v1,,Zelin Peng (None) | Zhengqin Xu (Shanghai Jiao Tong University) | Zhilin Zeng (Shanghai Jiao Tong University) | Lingxi Xie (Huawei Technologies Ltd.) | Qi Tian (Huawei Technologies Ltd.) | Wei Shen (None),2023-11-28 11:23:34+00:00,,,,,,
DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans,,,,Akash Sengupta (University Of Cambridge) | Thiemo Alldieck (Google) | NIKOS KOLOTOUROS (None) | Enric Corona (Google) | Andrei Zanfir (Google) | Cristian Sminchisescu (Lund University),,,,,,,
X3 : Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies ,"We present $\mathcal{X}^3$ (pronounced XCube), a novel generative model for high-resolution sparse 3D voxel grids with arbitrary attributes. Our model can generate millions of voxels with a finest effective resolution of up to $1024^3$ in a feed-forward fashion without time-consuming test-time optimization. To achieve this, we employ a hierarchical voxel latent diffusion model which generates progressively higher resolution grids in a coarse-to-fine manner using a custom framework built on the highly efficient VDB data structure. Apart from generating high-resolution objects, we demonstrate the effectiveness of XCube on large outdoor scenes at scales of 100m$\times$100m with a voxel size as small as 10cm. We observe clear qualitative and quantitative improvements over past approaches. In addition to unconditional generation, we show that our model can be used to solve a variety of tasks such as user-guided editing, scene completion from a single scan, and text-to-3D. More results and details can be found at https://research.nvidia.com/labs/toronto-ai/xcube/.",http://arxiv.org/abs/2312.03806v1,,"Xuanchi Ren (University Of Toronto) | Jiahui Huang (None) | Xiaohui Zeng (Department Of Computer Science, University Of Toronto) | Ken Museth (NVIDIA) | Sanja Fidler (Department Of Computer Science, University Of Toronto) | Francis Williams (NVIDIA)",2023-12-06 16:23:26+00:00,,,,,,
Probabilistic Human Mesh Estimation with Hypothesis Scoring,,,,Yuan Xu (Peking University) | Xiaoxuan Ma (Peking University) | Jiajun Su (Peking University) | Wentao Zhu (None) | Yu Qiao (Shanghai Jiao Tong University) | Yizhou Wang (Peking University),,,,,,,
A Unified and Interpretable Emotion Representation and Expression Generation,,,,"Reni Paskaleva (INSAIT) | Mykyta Holubakha (INSAIT) | Andela Ilic (ETHZ - ETH Zurich) | Saman Motamed (INSAIT, Sofia University) | Luc Van Gool (ETH Zurich) | Danda Paudel (None)",,,,,,,
Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning,"In-context learning provides a new perspective for multi-task modeling for vision and NLP. Under this setting, the model can perceive tasks from prompts and accomplish them without any extra task-specific head predictions or model fine-tuning. However, Skeleton sequence modeling via in-context learning remains unexplored. Directly applying existing in-context models from other areas onto skeleton sequences fails due to the inter-frame and cross-task pose similarity that makes it outstandingly hard to perceive the task correctly from a subtle context. To address this challenge, we propose Skeleton-in-Context (SiC), an effective framework for in-context skeleton sequence modeling. Our SiC is able to handle multiple skeleton-based tasks simultaneously after a single training process and accomplish each task from context according to the given prompt. It can further generalize to new, unseen tasks according to customized prompts. To facilitate context perception, we additionally propose a task-unified prompt, which adaptively learns tasks of different natures, such as partial joint-level generation, sequence-level prediction, or 2D-to-3D motion prediction. We conduct extensive experiments to evaluate the effectiveness of our SiC on multiple tasks, including motion prediction, pose estimation, joint completion, and future pose estimation. We also evaluate its generalization capability on unseen tasks such as motion-in-between. These experiments show that our model achieves state-of-the-art multi-task performance and even outperforms single-task methods on certain tasks.",http://arxiv.org/abs/2312.03703v1,,"Xinshun Wang (Sun Yat-Sen University) | Zhongbin Fang (Sun Yat-Sen University) | Xia Li (Department Of Computer Science, ETH Zurich) | Xiangtai Li (Nanyang Technological University) | Chen Chen (None) | Mengyuan Liu (SUN YAT-SEN UNIVERSITY)",2023-12-06 18:59:44+00:00,,,,,,
Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training,"In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.",http://arxiv.org/abs/2312.02914v2,,Arun Reddy (Johns Hopkins University) | William Paul (None) | Corban Rivera (Johns Hopkins University Applied Physics Laboratory) | Ketul Shah (Johns Hopkins University) | Celso M. De Melo (University Of Southern California) | Rama Chellappa (Johns Hopkins University),2023-12-05 17:39:19+00:00,,,,,,
Directed Decentralized Collaboration for Personalized Federated Learning,,,,"Yingqi Liu (None) | Yifan Shi (Tsinghua University) | Qinglun Li (National University Of Defense Technology) | Baoyuan Wu (The Chinese University Of Hong Kong, Shenzhen) | Xueqian Wang (Tsinghua University) | Li Shen (JD Explore Academy)",,,,,,,
Scaling Laws of Synthetic Images for Model Training ... for Now,"Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images, potentially overcoming the difficulty of collecting curated data at scale. It is unclear, however, how these models behave at scale, as more synthetic data is added to the training set. In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models, for the training of supervised models: image classifiers with label supervision, and CLIP with language supervision. We identify several factors, including text prompts, classifier-free guidance scale, and types of text-to-image models, that significantly affect scaling behavior. After tuning these factors, we observe that synthetic images demonstrate a scaling trend similar to, but slightly less effective than, real images in CLIP training, while they significantly underperform in scaling when training supervised image classifiers. Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts, a limitation that significantly impairs the training of image classifiers. Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the evaluation dataset diverges significantly from the training data, indicating the out-of-distribution scenario, or (3) when synthetic data is used in conjunction with real images, as demonstrated in the training of CLIP models.",http://arxiv.org/abs/2312.04567v1,,Lijie Fan (Massachusetts Institute Of Technology) | Kaifeng Chen (Google) | Dilip Krishnan (Google) | Dina Katabi (Massachusetts Institute Of Technology) | Phillip Isola (None) | Yonglong Tian (Google),2023-12-07 18:59:59+00:00,,,,,,
Towards General Robustness Verification of MaxPool-based Convolutional Neural Networks via Tightening Linear Approximation,,,,Yuan Xiao (Nanjing University) | Shiqing Ma (University Of Massachusetts At Amherst) | Juan Zhai (University Of Massachusetts At Amherst) | Chunrong Fang (Nanjing University) | Jinyuan Jia (Pennsylvania State University) | Zhenyu Chen (Nanjing University),,,,,,,
DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction,"Audio-visual saliency prediction can draw support from diverse modality complements, but further performance enhancement is still challenged by customized architectures as well as task-specific loss functions. In recent studies, denoising diffusion models have shown more promising in unifying task frameworks owing to their inherent ability of generalization. Following this motivation, a novel Diffusion architecture for generalized audio-visual Saliency prediction (DiffSal) is proposed in this work, which formulates the prediction problem as a conditional generative task of the saliency map by utilizing input audio and video as the conditions. Based on the spatio-temporal audio-visual features, an extra network Saliency-UNet is designed to perform multi-modal attention modulation for progressive refinement of the ground-truth saliency map from the noisy map. Extensive experiments demonstrate that the proposed DiffSal can achieve excellent performance across six challenging audio-visual benchmarks, with an average relative improvement of 6.3\% over the previous state-of-the-art results by six metrics.",http://arxiv.org/abs/2403.01226v1,,"Junwen Xiong (Northwestern Polytechnical University) | Peng Zhang (Northwest Polytechnical University Xi'an) | Tao You (Northwest Polytechnical University Xi'an) | Chuanyue Li (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) | Wei Huang (Nanchang University) | Yufei Zha (Northwestern Polytechinical University)",2024-03-02 14:52:58+00:00,,,,,,
"Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization",,,,Mainak Singha (Indian Institute Of Technology Bombay) | Ankit Jha (Indian Institute Of Technology Bombay) | Shirsha Bose (Technische Universit??t M??nchen) | Ashwin Nair (Indian Institute Of Science Education And Research Thiruvananthapuram) | Moloud Abdar (Deakin University) | Biplab Banerjee (IIT Bombay),,,,,,,
Rethinking Human Motion Prediction with Symplectic Integral,,,,"Haipeng Chen (Jilin University) | Kedi L??Yu (None) | Zhenguang Liu (Zhejiang University) | Yifang Yin (I2R, A*STAR) | Xun Yang (University Of Science And Technology Of China) | Yingda Lyu (Jilin University)",,,,,,,
Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network,"Recently, 3D anomaly detection, a crucial problem involving fine-grained geometry discrimination, is getting more attention. However, the lack of abundant real 3D anomaly data limits the scalability of current models. To enable scalable anomaly data collection, we propose a 3D anomaly synthesis pipeline to adapt existing large-scale 3Dmodels for 3D anomaly detection. Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, basedon ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40 categories, which provides a rich and varied collection of data, enabling efficient training and enhancing adaptability to industrial scenarios. Meanwhile,to enable scalable representation learning for 3D anomaly localization, we propose a self-supervised method, i.e., Iterative Mask Reconstruction Network (IMRNet). During training, we propose a geometry-aware sample module to preserve potentially anomalous local regions during point cloud down-sampling. Then, we randomly mask out point patches and sent the visible patches to a transformer for reconstruction-based self-supervision. During testing, the point cloud repeatedly goes through the Mask Reconstruction Network, with each iteration's output becoming the next input. By merging and contrasting the final reconstructed point cloud with the initial input, our method successfully locates anomalies. Experiments show that IMRNet outperforms previous state-of-the-art methods, achieving 66.1% in I-AUC on Anomaly-ShapeNet dataset and 72.5% in I-AUC on Real3D-AD dataset. Our dataset will be released at https://github.com/Chopper-233/Anomaly-ShapeNet",http://arxiv.org/abs/2311.14897v3,,Wenqiao Li (ShanghaiTech University) | Xiaohao Xu (University Of Michigan - Ann Arbor) | Yao Gu (Shanghaitech University) | BoZhong Zheng (None) | Shenghua Gao (ShanghaiTech University) | Yingna Wu (ShanghaiTech University),2023-11-25 01:45:09+00:00,,,,,,
PairDETR : Joint Detection and Association of Human Bodies and Faces,,,,Ammar Ali (ITMO University) | Georgii Gaikov (MTS AI) | Denis Rybalchenko (VisionLabs) | Alexander Chigorin (VisionLabs MENA) | Ivan Laptev (INRIA Paris) | Sergey Zagoruyko (MTS AI),,,,,,,
ZERO-IG: Zero-Shot Illumination-Guided Joint Denoising and Adaptive Enhancement for Low-Light Images,,,,Yiqi Shi (Harbin Engineering University) | Duo Liu (Harbin Engineering University) | Liguo Zhang (Harbin Engineering University) | Ye Tian (Xidian University) | Xuezhi Xia (Harbin Engineering University) | Fuxiaojing (Harbin Engineering University),,,,,,,
Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation,,,,Yeonguk Yu (Gwangju Institute Of Science And Technology) | Sungho Shin (None) | Seunghyeok Back (Gwangju Institute Of Science And Technology) | Minhwan Ko (Gwangju Institute Of Science And Technology) | Sangjun Noh (Gwangju Institute Of Science And Technology) | Kyoobin Lee (None),,,,,,,
RepAn: Enhanced Annealing through Re-parameterization,,,,"Xiang Fei (School Of Informatics, Xiamen University) | Xiawu Zheng (Xiamen University) | Yan Wang (Samsara) | Fei Chao (Xiamen University) | Chenglin Wu (DeepWisdom) | Liujuan Cao (Xiamen University)",,,,,,,
Context-Aware Integration of Language and Visual References for Natural Language Tracking,,,,Yanyan Shao (None) | Shuting He (Nanyang Technological University) | Qi Ye (Zhejiang University) | Yuchao Feng (None) | Wenhan Luo (SUN YAT-SEN UNIVERSITY) | Jiming Chen (Zhejiang University),,,,,,,
PointBeV: A Sparse Approach for BeV Predictions,"Bird's-eye View (BeV) representations have emerged as the de-facto shared space in driving applications, offering a unified space for sensor data fusion and supporting various downstream tasks. However, conventional models use grids with fixed resolution and range and face computational inefficiencies due to the uniform allocation of resources across all cells. To address this, we propose PointBeV, a novel sparse BeV segmentation model operating on sparse BeV cells instead of dense grids. This approach offers precise control over memory usage, enabling the use of long temporal contexts and accommodating memory-constrained platforms. PointBeV employs an efficient two-pass strategy for training, enabling focused computation on regions of interest. At inference time, it can be used with various memory/performance trade-offs and flexibly adjusts to new specific use cases. PointBeV achieves state-of-the-art results on the nuScenes dataset for vehicle, pedestrian, and lane segmentation, showcasing superior performance in static and temporal settings despite being trained solely with sparse signals. We will release our code along with two new efficient modules used in the architecture: Sparse Feature Pulling, designed for the effective extraction of features from images to BeV, and Submanifold Attention, which enables efficient temporal modeling. Our code is available at https://github.com/valeoai/PointBeV.",http://arxiv.org/abs/2312.00703v1,,Loick Chambon (Valeo) | ??loi Zablocki (Valeo) | Micka??l Chen (Valeo) | Florent Bartoccioni (Valeo) | Patrick P??rez (None) | Matthieu Cord (None),2023-12-01 16:38:14+00:00,,,,,,
B??zier Everywhere All at Once: Learning Drivable Lanes as B??zier Graphs,,,,Hugh Blayney (DRISK.Ai) | Hanlin Tian (Imperial College London) | Hamish Scott (DRISK AI) | Nils Goldbeck (DRISK) | Chess Stetson (DRISK) | Panagiotis Angeloudis (Imperial College London),,,,,,,
SAI3D: Segment Any Instance in 3D Scenes,"Advancements in 3D instance segmentation have traditionally been tethered to the availability of annotated datasets, limiting their application to a narrow spectrum of object categories. Recent efforts have sought to harness vision-language models like CLIP for open-set semantic reasoning, yet these methods struggle to distinguish between objects of the same categories and rely on specific prompts that are not universally applicable. In this paper, we introduce SAI3D, a novel zero-shot 3D instance segmentation approach that synergistically leverages geometric priors and semantic cues derived from Segment Anything Model (SAM). Our method partitions a 3D scene into geometric primitives, which are then progressively merged into 3D instance segmentations that are consistent with the multi-view SAM masks. Moreover, we design a hierarchical region-growing algorithm with a dynamic thresholding mechanism, which largely improves the robustness of finegrained 3D scene parsing. Empirical evaluations on Scan-Net and the more challenging ScanNet++ datasets demonstrate the superiority of our approach. Notably, SAI3D outperforms existing open-vocabulary baselines and even surpasses fully-supervised methods in class-agnostic segmentation on ScanNet++.",http://arxiv.org/abs/2312.11557v1,,Yingda Yin (None) | Yuzheng Liu (Peking University) | Yang Xiao (Huawei Technologies Ltd.) | Daniel Cohen-Or (Google) | Jingwei Huang (Huawei Technologies Ltd.) | Baoquan Chen (Peking University),2023-12-17 09:05:47+00:00,,,,,,
De-Diffusion Makes Text a Strong Cross-Modal Interface,"We demonstrate text as a strong cross-modal interface. Rather than relying on deep embeddings to connect image and language as the interface representation, our approach represents an image as text, from which we enjoy the interpretability and flexibility inherent to natural language. We employ an autoencoder that uses a pre-trained text-to-image diffusion model for decoding. The encoder is trained to transform an input image into text, which is then fed into the fixed text-to-image diffusion decoder to reconstruct the original input -- a process we term De-Diffusion. Experiments validate both the precision and comprehensiveness of De-Diffusion text representing images, such that it can be readily ingested by off-the-shelf text-to-image tools and LLMs for diverse multi-modal tasks. For example, a single De-Diffusion model can generalize to provide transferable prompts for different text-to-image tools, and also achieves a new state of the art on open-ended vision-language tasks by simply prompting large language models with few-shot examples.",http://arxiv.org/abs/2311.00618v1,,Chen Wei (Johns Hopkins University) | Chenxi Liu (Waymo) | Siyuan Qiao (Google) | Zhishuai Zhang (Google) | Alan L. Yuille (Johns Hopkins University) | Jiahui Yu (Google Brain),2023-11-01 16:12:40+00:00,,,,,,
CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection,,,,"Jiayi Zhu (East China Normal University) | Qing Guo (Institute Of High Performance Computing, Singapore, A*STAR) | Felix Juefei Xu (None) | Yihao Huang (Nanyang Technological University) | Yang Liu (Nanyang Technological University) | Geguang Pu (East China Normal University)",,,,,,,
Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models,,,,Gihyun Kwon (Korea Advanced Institute Of Science & Technology) | Simon Jenni (Adobe Systems) | Ding Li (None) | Joon-Young Lee (Adobe Research) | Jong Chul Ye (Korea Advanced Institute Of Science And Technology) | Fabian Caba Heilbron (Adobe Research),,,,,,,
Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension,"Registration of point clouds collected from a pair of distant vehicles provides a comprehensive and accurate 3D view of the driving scenario, which is vital for driving safety related applications, yet existing literature suffers from the expensive pose label acquisition and the deficiency to generalize to new data distributions. In this paper, we propose EYOC, an unsupervised distant point cloud registration method that adapts to new point cloud distributions on the fly, requiring no global pose labels. The core idea of EYOC is to train a feature extractor in a progressive fashion, where in each round, the feature extractor, trained with near point cloud pairs, can label slightly farther point cloud pairs, enabling self-supervision on such far point cloud pairs. This process continues until the derived extractor can be used to register distant point clouds. Particularly, to enable high-fidelity correspondence label generation, we devise an effective spatial filtering scheme to select the most representative correspondences to register a point cloud pair, and then utilize the aligned point clouds to discover more correct correspondences. Experiments show that EYOC can achieve comparable performance with state-of-the-art supervised methods at a lower training cost. Moreover, it outwits supervised methods regarding generalization performance on new data distributions.",http://arxiv.org/abs/2403.03532v1,,"Quan Liu (Shanghai Jiao Tong University) | Hongzi Zhu (Shanghai Jiao Tong University) | Zhenxi Wang (Shanghai Jiao Tong University) | Yunsong Zhou (Shanghai Jiao Tong University) | Shan Chang (Donghua University, Shanghai) | Minyi Guo (Shanghai Jiao Tong University)",2024-03-06 08:18:02+00:00,,,,,,
VS: Reconstructing Clothed 3D Human from Single Image via Vertex Shift,,,,Leyuan Liu (Central China Normal University) | Yuhan Li (Central China Normal University) | Yunqi Gao (Central China Normal University) | Changxin Gao (Huazhong University Of Science And Technology) | Yuanyuan Liu (None) | Jingying Chen (Central China Normal University),,,,,,,
Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences,,,,Seungwook Kim (POSTECH) | Kejie Li (University Of Oxford) | Xueqing Deng (ByteDance Research) | Yichun Shi (ByteDance) | Minsu Cho (POSTECH) | Peng Wang (Bytedance US AILab),,,,,,,
Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition,,,,Zihan Wang (None) | Siyang Song (University Of Leicester) | Cheng Luo (Shenzhen University) | Songhe Deng (None) | Weicheng Xie (Shenzhen University) | Linlin Shen (None),,,,,,,
Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning,,,,Wei Zhang (None) | Chaoqun Wan (Alibaba Group) | Tongliang Liu (Mohamed Bin Zayed University Of Artificial Intelligence) | Xinmei Tian (University Of Science And Technology Of China) | Xu Shen (Alibaba Group) | Jieping Ye (Alibaba Group),,,,,,,
Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following,"Existing text-to-image (T2I) diffusion models usually struggle in interpreting complex prompts, especially those with quantity, object-attribute binding, and multi-subject descriptions. In this work, we introduce a semantic panel as the middleware in decoding texts to images, supporting the generator to better follow instructions. The panel is obtained through arranging the visual concepts parsed from the input text by the aid of large language models, and then injected into the denoising network as a detailed control signal to complement the text condition. To facilitate text-to-panel learning, we come up with a carefully designed semantic formatting protocol, accompanied by a fully-automatic data preparation pipeline. Thanks to such a design, our approach, which we call Ranni, manages to enhance a pre-trained T2I generator regarding its textual controllability. More importantly, the introduction of the generative middleware brings a more convenient form of interaction (i.e., directly adjusting the elements in the panel or using language instructions) and further allows users to finely customize their generation, based on which we develop a practical system and showcase its potential in continuous generation and chatting-based editing. Our project page is at https://ranni-t2i.github.io/Ranni.",http://arxiv.org/abs/2311.17002v2,,Yutong Feng (Alibaba Group) | Biao Gong (Alibaba Group) | Di Chen (Alibaba Group) | Yujun Shen (The Chinese University Of Hong Kong) | Yu Liu (Alibaba Group) | Jingren Zhou (Alibaba Group),2023-11-28 17:57:44+00:00,,,,,,
SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis,,,,Teng Hu (None) | Ran Yi (Shanghai Jiao Tong University) | Baihong Qian (Shanghai Jiao Tong University) | Jiangning Zhang (Tencent Youtu Lab) | Paul L. Rosin (Cardiff University) | Yu-Kun Lai (Cardiff University),,,,,,,
Semantic-aware SAM for Point-Prompted Instance Segmentation,"Single-point annotation in visual tasks, with the goal of minimizing labelling costs, is becoming increasingly prominent in research. Recently, visual foundation models, such as Segment Anything (SAM), have gained widespread usage due to their robust zero-shot capabilities and exceptional annotation performance. However, SAM's class-agnostic output and high confidence in local segmentation introduce 'semantic ambiguity', posing a challenge for precise category-specific segmentation. In this paper, we introduce a cost-effective category-specific segmenter using SAM. To tackle this challenge, we have devised a Semantic-Aware Instance Segmentation Network (SAPNet) that integrates Multiple Instance Learning (MIL) with matching capability and SAM with point prompts. SAPNet strategically selects the most representative mask proposals generated by SAM to supervise segmentation, with a specific focus on object category information. Moreover, we introduce the Point Distance Guidance and Box Mining Strategy to mitigate inherent challenges: 'group' and 'local' issues in weakly supervised segmentation. These strategies serve to further enhance the overall segmentation performance. The experimental results on Pascal VOC and COCO demonstrate the promising performance of our proposed SAPNet, emphasizing its semantic matching capabilities and its potential to advance point-prompted instance segmentation. The code will be made publicly available.",http://arxiv.org/abs/2312.15895v1,,Zhaoyang Wei (University Of The Chinese Academy Of Sciences) | Pengfei Chen (University Of The Chinese Academy Of Sciences) | Xuehui Yu (None) | Guorong Li (University Of Chinese Academy Of Sciences) | Jianbin Jiao (None) | Zhenjun Han (University Of The Chinese Academy Of Sciences),2023-12-26 05:56:44+00:00,,,,,,
Disentangled Prompt Representation for Domain Generalization,,,,De Cheng (Xidian University) | Zhipeng Xu (Xi'an University Of Electronic Science And Technology) | XINYANG JIANG (Microsoft Research) | Nannan Wang (Xidian University) | Dongsheng Li (Microsoft Research Asia) | Xinbo Gao (Chongqing University Of Post And Telecommunications),,,,,,,
A Pedestrian is Worth One Prompt: Towards Language Guidance Person Re-Identification,,,,"Zexian Yang (None) | Dayan Wu (Iie,Cas) | Chenming Wu (None) | Zheng Lin (Institute Of Information Engineering, Chinese Academy Of Sciences) | JingziGU (INSTATUTE OF INFORMATION ENGINEERING,CAS) | Weiping Wang (IIE)",,,,,,,
TULIP: Transformer for Upsampling of LiDAR Point Cloud,"LiDAR Upsampling is a challenging task for the perception systems of robots and autonomous vehicles, due to the sparse and irregular structure of large-scale scene contexts. Recent works propose to solve this problem by converting LiDAR data from 3D Euclidean space into an image super-resolution problem in 2D image space. Although their methods can generate high-resolution range images with fine-grained details, the resulting 3D point clouds often blur out details and predict invalid points. In this paper, we propose TULIP, a new method to reconstruct high-resolution LiDAR point clouds from low-resolution LiDAR input. We also follow a range image-based approach but specifically modify the patch and window geometries of a Swin-Transformer-based network to better fit the characteristics of range images. We conducted several experiments on three different public real-world and simulated datasets. TULIP outperforms state-of-the-art methods in all relevant metrics and generates robust and more realistic point clouds than prior works.",http://arxiv.org/abs/2312.06733v2,,Bin Yang (ETHZ - ETH Zurich) | Patrick Pfreundschuh (ETHZ - ETH Zurich) | Roland Siegwart (Swiss Federal Institute Of Technology) | Marco Hutter (ETHZ - ETH Zurich) | Peyman Moghadam (None) | Vaishakh Patil (ETHZ - ETH Zurich),2023-12-11 10:43:28+00:00,,,,,,
Incremental Residual Concept Bottleneck Model,,,,Chenming Shang (Tsinghua University) | Shiji Zhou (Tsinghua University) | Hengyuan Zhang (Tsinghua University) | Xinzhe Ni (Tsinghua University) | Yujiu Yang (Tsinghua University) | Yuwang Wang (Tsinghua University),,,,,,,
Density-Adaptive Model Based on Motif Matrix for Multi-Agent Trajectory Prediction,,,,Di Wen (None) | Haoran Xu (None) | Zhaocheng He (SUN YAT-SEN UNIVERSITY) | Zhe Wu (Pengcheng Laboratory) | Guang Tan (Sun Yat-Sen University) | Peixi Peng (Peking University),,,,,,,
Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling,,,,Xinhang Liu (HKUST) | Yu-Wing Tai (None) | Chi-Keung Tang (The Hong Kong University Of Science And Technology) | Pedro Miraldo (None) | Suhas Lohit (Mitsubishi Electric Research Labs) | Moitreya Chatterjee (Mitsubishi Electric Research Labs),,,,,,,
Towards Backward-Compatible Continual Learning of Image Compression,"This paper explores the possibility of extending the capability of pre-trained neural image compressors (e.g., adapting to new data or target bitrates) without breaking backward compatibility, the ability to decode bitstreams encoded by the original model. We refer to this problem as continual learning of image compression. Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility. To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue. We also design a new model architecture that enables more effective continual learning than existing baselines. Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning. The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better performance (compared to their pre-trained version) on new data and rates without compromising backward compatibility. Our code is available at https://gitlab.com/viper-purdue/continual-compression",http://arxiv.org/abs/2402.18862v1,,Zhihao Duan (Purdue University) | Ming Lu (Nanjing University) | Justin Yang (Purdue University) | Jiangpeng He (Purdue University) | Zhan Ma (Nanjing University) | Fengqing Zhu (Purdue University),2024-02-29 05:25:04+00:00,,,,,,
Leveraging Frame Affinity for sRGB-to-RAW Video De-rendering,,,,Chen Zhang (Sensetime) | Wencheng Han (University Of Macau) | Yang Zhou (Sensetime Group) | Jianbing Shen (University Of Macau) | Cheng-Zhong Xu (University Of Macau) | Wentao Liu (Sensetime),,,,,,,
Bootstrapping Autonomous Radars with Self-Supervised Learning,"The perception of autonomous vehicles using radars has attracted increased research interest due its ability to operate in fog and bad weather. However, training radar models is hindered by the cost and difficulty of annotating large-scale radar data. To overcome this bottleneck, we propose a self-supervised learning framework to leverage the large amount of unlabeled radar data to pre-train radar-only embeddings for self-driving perception tasks. The proposed method combines radar-to-radar and radar-to-vision contrastive losses to learn a general representation from unlabeled radar heatmaps paired with their corresponding camera images. When used for downstream object detection, we demonstrate that the proposed self-supervision framework can improve the accuracy of state-of-the-art supervised baselines by 5.8% in mAP.",http://arxiv.org/abs/2312.04519v2,,"Yiduo Hao (University Of Cambridge) | Sohrab Madani (UIUC) | Junfeng Guan (EPFL - EPF Lausanne) | Mohammed Alloulah (Nokia Bell Labs) | Saurabh Gupta (University Of Illinois, Urbana Champaign) | Haitham Al Hassanieh (University Of Illinois At Urbana-Champaign)",2023-12-07 18:38:39+00:00,,,,,,
Generative Quanta Color Imaging,,,,"Vishal Purohit (Purdue University) | Junjie Luo (Purdue University) | Yiheng Chi (Purdue University) | Qi Guo (Purdue University) | Stanley H. Chan (Purdue University, USA) | Qiang Qiu (Purdue University)",,,,,,,
Noisy-Correspondence Learning for Text-to-Image Person Re-identification,,,,Yang Qin (Sichuan University) | Yingke Chen (Northumbria University) | Dezhong Peng (Sichuan University) | Xi Peng (Sichuan University) | Joey Tianyi Zhou (National University Of Singapore ) | Peng Hu (Sichuan University),,,,,,,
Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization,,,,Zhi-Fan Wu (Alibaba DAMO Academy) | Chaojie Mao (Alibaba Group) | Xue Wang (Pennsylvania State University) | Jianwen Jiang (Alibaba DAMO Academy) | Yiliang Lv (Gientech AIL) | Rong Jin (Twitter),,,,,,,
BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning,"Studying backdoor attacks is valuable for model copyright protection and enhancing defenses. While existing backdoor attacks have successfully infected multimodal contrastive learning models such as CLIP, they can be easily countered by specialized backdoor defenses for MCL models. This paper reveals the threats in this practical scenario that backdoor attacks can remain effective even after defenses and introduces the \emph{\toolns} attack, which is resistant to backdoor detection and model fine-tuning defenses. To achieve this, we draw motivations from the perspective of the Bayesian rule and propose a dual-embedding guided framework for backdoor attacks. Specifically, we ensure that visual trigger patterns approximate the textual target semantics in the embedding space, making it challenging to detect the subtle parameter variations induced by backdoor learning on such natural trigger patterns. Additionally, we optimize the visual trigger patterns to align the poisoned samples with target vision features in order to hinder the backdoor unlearning through clean fine-tuning. Extensive experiments demonstrate that our attack significantly outperforms state-of-the-art baselines (+45.3% ASR) in the presence of SoTA backdoor defenses, rendering these mitigation and detection strategies virtually ineffective. Furthermore, our approach effectively attacks some more rigorous scenarios like downstream tasks. We believe that this paper raises awareness regarding the potential threats associated with the practical application of multimodal contrastive learning and encourages the development of more robust defense mechanisms.",http://arxiv.org/abs/2311.12075v3,,"Siyuan Liang (National University Of Singapore) | Mingli Zhu (The Chinese University Of Hong Kong(Shen Zhen)) | Aishan Liu (None) | Baoyuan Wu (The Chinese University Of Hong Kong, Shenzhen) | Xiaochun Cao (SUN YAT-SEN UNIVERSITY) | Ee-Chien Chang (National University Of Singapore)",2023-11-20 02:21:49+00:00,,,,,,
U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation,,,,"You Wu (Institute Of Computing Technology, CAS) | Kean Liu (University Of The Chinese Academy Of Sciences) | Xiaoyue Mi (None) | Fan Tang (Institute Of Computing Technology, CAS) | Juan Cao (Institute Of Computing Technology, Chinese Academy Of Sciences) | Jintao Li (Institute Of Computing Technology, Chinese Academy Of Sciences)",,,,,,,
High-Quality Spatiotemporal Predictive Learning with Coarse-to-Fine Iterative Decoding,,,,"Xuesong Nie (None) | Haoyuan Jin (Zhejiang University) | Yunfeng Yan (Zhejiang University) | Xi Chen (The University Of Hong Kong, University Of Hong Kong) | Zhihang Zhu (Zhejiang University) | Donglian Qi (Zhejiang University)",,,,,,,
DYSON: Dynamic Feature Space Self-Organization for Online Task-Free Class Incremental Learning,,,,Yuhang He (Xi'an Jiao Tong University) | YingJie Chen (Xi'an Jiao Tong University) | Yuhan Jin (Xi'an Jiao Tong University) | Songlin Dong (Xi'an Jiao Tong University) | Xing Wei (None) | Yihong Gong (Xi'an Jiao Tong University),,,,,,,
LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation,"Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we propose a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two issues by leveraging the LLM's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. To further engage the LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy. To validate the effectiveness of LLM4SGG, we conduct extensive experiments on Visual Genome and GQA datasets, showing significant improvements in both Recall@K and mean Recall@K compared to the state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is data-efficient, enabling effective model training with a small amount of training images.",http://arxiv.org/abs/2310.10404v5,,Kibum Kim (Korea Advanced Institute Of Science &Amp; Technology) | Kanghoon Yoon (Korea Advanced Institute Of Science & Technology) | Jaehyeong Jeon (Korea Advanced Institute Of Science And Technology) | Yeonjun In (Korea Advanced Institute Of Science & Technology) | Jinyoung Moon (ETRI) | Donghyun Kim (MIT-IBM Watson AI Lab) | Chanyoung Park (Korea Advanced Institute Of Science And Technology),2023-10-16 13:49:46+00:00,,,,,,
SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer,,,,"Rui Zhu (Chinese University Of Hong Kong (Shenzhen)) | Yingwei Pan (HiDream.Ai) | Yehao Li (JD AI Research) | Ting Yao (JD AI Research) | Zhenglong Sun (The Chinese University Of Hong Kong, Shenzhen) | Tao Mei (JD Explore Academy) | Chang-Wen Chen (The Hong Kong Polytechnic University)",,,,,,,
Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging,"Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in simulations and real-world experiments in a passive, snapshot, and compact manner.",http://arxiv.org/abs/2402.18102v1,,"Bhargav Ghanekar (Rice University) | Salman Siddique Khan (Rice University) | Pranav Sharma (IIT Madras, India) | Shreyas Singh (Indian Institute Of Technology, Madras) | Vivek Boominathan (Rice University) | Kaushik Mitra (Indian Institute Of Technology, Madras, Dhirubhai Ambani Institute Of Information And Communication Technology) | Ashok Veeraraghavan (William Marsh Rice University)",2024-02-28 06:45:47+00:00,,,,,,
DREAM: Diffusion Rectification and Estimation-Adaptive Models,"We present DREAM, a novel training framework representing Diffusion Rectification and Estimation-Adaptive Models, requiring minimal code changes (just three lines) yet significantly enhancing the alignment of training with sampling in diffusion models. DREAM features two components: diffusion rectification, which adjusts training to reflect the sampling process, and estimation adaptation, which balances perception against distortion. When applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff between minimizing distortion and preserving high image quality. Experiments demonstrate DREAM's superiority over standard diffusion-based SR methods, showing a $2$ to $3\times $ faster training convergence and a $10$ to $20\times$ reduction in necessary sampling steps to achieve comparable or superior results. We hope DREAM will inspire a rethinking of diffusion model training paradigms.",http://arxiv.org/abs/2312.00210v1,,"Jinxin Zhou (Ohio State University, Columbus) | Tianyu Ding (Microsoft) | Tianyi Chen (Microsoft) | Jiachen Jiang (Ohio State University, Columbus) | Ilya Zharkov (Microsoft) | Zhihui Zhu (Ohio State University, Columbus) | Luming Liang (Microsoft)",2023-11-30 21:44:39+00:00,,,,,,
MoCha-Stereo: Motif Channel Attention Network for Stereo Matching,,,,Ziyang Chen (Guizhou University) | Wei Long (Guizhou University) | He Yao (Guizhou University) | Yongjun Zhang (Guizhou University) | Bingshu Wang (Northwest Polytechnical University Xi'an) | Yongbin Qin (Guizhou University) | Jia Wu (Monash University),,,,,,,
Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution,,,,Qingping Zheng (Northwestern Polytechnical University) | Ling Zheng (Tsinghua-Fuzhou Institute For Data Technology) | Yuanfan Guo (Huawei Technologies Ltd.) | Ying Li (Northwestern Polytechnical University) | Songcen Xu (Huawei Noah's Ark Lab) | Jiankang Deng (Imperial College London & Huawei UKRD) | Hang Xu (Huawei Noah??S Ark Lab),,,,,,,
ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis,"In this work, we propose a method to address the challenge of rendering a 3D human from a single image in a free-view manner. Some existing approaches could achieve this by using generalizable pixel-aligned implicit fields to reconstruct a textured mesh of a human or by employing a 2D diffusion model as guidance with the Score Distillation Sampling (SDS) method, to lift the 2D image into 3D space. However, a generalizable implicit field often results in an over-smooth texture field, while the SDS method tends to lead to a texture-inconsistent novel view with the input image. In this paper, we introduce a texture-consistent back view synthesis module that could transfer the reference image content to the back view through depth and text-guided attention injection. Moreover, to alleviate the color distortion that occurs in the side region, we propose a visibility-aware patch consistency regularization for texture mapping and refinement combined with the synthesized back view texture. With the above techniques, we could achieve high-fidelity and texture-consistent human rendering from a single image. Experiments conducted on both real and synthetic data demonstrate the effectiveness of our method and show that our approach outperforms previous baseline methods.",http://arxiv.org/abs/2311.17123v1,,Xiangjun Gao ((HKUST) The Hong Kong University Of Science And Technology) | Xiaoyu Li (Tencent AI Lab) | Chaopeng Zhang (Tencent AI Lab) | Qi Zhang (Tencent AI Lab) | Yan-Pei Cao (Tencent ARC Lab) | Ying Shan (Tencent) | Long Quan (The Hong Kong University Of Science And Technology),2023-11-28 13:55:53+00:00,,,,,,
RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction,"This paper introduces a versatile paradigm for integrating multi-view reflectance and normal maps acquired through photometric stereo. Our approach employs a pixel-wise joint re-parameterization of reflectance and normal, considering them as a vector of radiances rendered under simulated, varying illumination. This re-parameterization enables the seamless integration of reflectance and normal maps as input data in neural volume rendering-based 3D reconstruction while preserving a single optimization objective. In contrast, recent multi-view photometric stereo (MVPS) methods depend on multiple, potentially conflicting objectives. Despite its apparent simplicity, our proposed approach outperforms state-of-the-art approaches in MVPS benchmarks across F-score, Chamfer distance, and mean angular error metrics. Notably, it significantly improves the detailed 3D reconstruction of areas with high curvature or low visibility.",http://arxiv.org/abs/2312.01215v1,,"Baptiste Brument (IRIT, University Of Toulouse, France) | Robin Bruneau (University Of Copenhagen) | Yvain Queau (CNRS) | Jean M??lou (IRIT) | Francois Lauze (Department Fo Computer Science, University Of Copenhagen) | Jean-Denis Durou (IRIT) | Lilian Calvet (OR-X, Balgrist Hospital, University Of Zurich)",2023-12-02 19:49:27+00:00,,,,,,
RoHM: Robust Human Motion Reconstruction via Diffusion,"We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code will be available at https://sanweiliti.github.io/ROHM/ROHM.html.",http://arxiv.org/abs/2401.08570v1,,Siwei Zhang (None) | Bharat Lal Bhatnagar (Eberhard-Karls-Universit??t T??bingen) | Yuanlu Xu (Meta Reality Labs Research) | Alexander Winkler (Meta) | Petr Kadlecek (Meta Reality Labs Research) | Siyu Tang (ETH Zurich) | Federica Bogo (Meta),2024-01-16 18:57:50+00:00,,,,,,
Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding,"Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",http://arxiv.org/abs/2311.16922v1,,"Sicong Leng (Nanyang Technological University) | Hang Zhang (Sichuan University) | Guanzheng Chen (SUN YAT-SEN UNIVERSITY) | Xin Li (Alibaba Group) | Shijian Lu (Nanyang Technological University) | Chunyan Miao (School Of Computer Science And Engineering, Nanyang Technological University) | Lidong Bing (Alibaba DAMO Academy)",2023-11-28 16:26:35+00:00,,,,,,
UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement,,,,"Yaofeng Xie (Ocean University Of China) | Lingwei Kong (Sanya Oceanographic Institution, Ocean University Of China) | Kai Chen (Sanya Oceanographic Institution, Ocean University Of China) | Zheng Ziqiang (Hong Kong University Of Science And Technology) | Xiao Yu (Sanya Oceanographic Institution, Ocean University Of China) | Zhibin Yu (Sanya Oceanographic Institution, Ocean University Of China) | Bing Zheng (Sanya Oceanographic Institution, Ocean University Of China)",,,,,,,
Breathing Life Into Sketches Using Text-to-Video Priors,"A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, ""breathing life into it""), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.",http://arxiv.org/abs/2311.13608v1,,"Rinon Gal (Tel Aviv University, NVIDIA) | Yael Vinker (Tel Aviv University) | Yuval Alaluf (Tel Aviv University) | Amit H. Bermano (Tel Aviv University, Technion) | Daniel Cohen-Or (Google) | Ariel Shamir (Reichman University) | Gal Chechik (NVIDIA, Bar-Ilan University)",2023-11-21 18:09:30+00:00,,,,,,
HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation,"Recent text-to-3D methods employing diffusion models have made significant advancements in 3D human generation. However, these approaches face challenges due to the limitations of text-to-image diffusion models, which lack an understanding of 3D structures. Consequently, these methods struggle to achieve high-quality human generation, resulting in smooth geometry and cartoon-like appearances. In this paper, we propose HumanNorm, a novel approach for high-quality and realistic 3D human generation. The main idea is to enhance the model's 2D perception of 3D geometry by learning a normal-adapted diffusion model and a normal-aligned diffusion model. The normal-adapted diffusion model can generate high-fidelity normal maps corresponding to user prompts with view-dependent and body-aware text. The normal-aligned diffusion model learns to generate color images aligned with the normal maps, thereby transforming physical geometry details into realistic appearance. Leveraging the proposed normal diffusion model, we devise a progressive geometry generation strategy and a multi-step Score Distillation Sampling (SDS) loss to enhance the performance of 3D human generation. Comprehensive experiments substantiate HumanNorm's ability to generate 3D humans with intricate geometry and realistic appearances. HumanNorm outperforms existing text-to-3D methods in both geometry and texture quality. The project page of HumanNorm is https://humannorm.github.io/.",http://arxiv.org/abs/2310.01406v2,,Xin Huang (Northwest Polytechnical University Xi'an) | Ruizhi Shao (Tsinghua University) | Qi Zhang (Northwest Polytechnical University Xi'an) | Hongwen Zhang (Beijing Normal University) | Ying Feng (Northwest Polytechnical University Xi'an) | Yebin Liu (Tsinghua University) | Qing Wang (Northwestern Polytechnical University),2023-10-02 17:59:17+00:00,,,,,,
OHTA: One-shot Hand Avatar via Data-driven Implicit Priors,"In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.",http://arxiv.org/abs/2402.18969v1,,Xiaozheng Zheng (ByteDance) | Chao Wen (ByteDance) | Zhuo Su (ByteDance) | Zeran Xu (Bytedance) | Zhaohu Li (ByteDance) | Yang Zhao (ByteDance) | Zhou Xue (Li Auto),2024-02-29 09:14:50+00:00,,,,,,
DART: Implicit Doppler Tomography for Radar Novel View Synthesis,"Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.",http://arxiv.org/abs/2403.03896v1,,"Tianshu Huang (Carnegie Mellon University) | John Miller (Carnegie Mellon University) | Akarsh Prabhakara (Carnegie Mellon University) | Tao Jin (CMU, Carnegie Mellon University) | Tarana Laroia (CMU, Carnegie Mellon University) | Zico Kolter (Carnegie Mellon University) | Anthony Rowe (Carnegie Mellon University)",2024-03-06 17:54:50+00:00,,,,,,
D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection,"Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain. However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain. Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain. The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at https://github.com/EdwardDo69/D3T .",http://arxiv.org/abs/2403.09359v1,,"Dinh Phat Do (Ajou University) | TAEHOON KIM (Ajou University) | JAEMIN NA (Tech. Innovation Group, KT) | Jiwon Kim (Hyundai Motor Company) | Keonho LEE (Hyundai Motor Company) | Kyunghwan Cho (Hyundai Motor Company) | Wonjun Hwang (Ajou University)",2024-03-14 13:05:43+00:00,,,,,,
Personalized Residuals for Concept-Driven Text-to-Image Generation,,,,Cusuh Ham (None) | Matthew Fisher (Adobe Research) | James Hays (Georgia Institute Of Technology) | Nicholas Kolkin (Adobe Systems) | Yuchen Liu (None) | Richard Zhang (Adobe Systems) | Tobias Hinz (Adobe Systems),,,,,,,
Do Vision and Language Encoders Represent the World Similarly?,"Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demonstrate the effectiveness of this on several downstream tasks including cross-lingual, cross-domain caption matching and image classification.",http://arxiv.org/abs/2401.05224v1,,"Mayug Maniparambil (ML Labs, Dublin City University) | Raiymbek Akshulakov (University Of California, Berkeley) | YASSER ABDELAZIZ DAHOU DJILALI (Technology Innovation Institute) | Mohamed El Amine Seddik (Technology Innovation Institute) | Sanath Narayan (Technology Innovation Institute) | Karttikeya Mangalam (University Of California Berkeley) | Noel O'Connor (Dublin City University)",2024-01-10 15:51:39+00:00,,,,,,
Can Vision-Language Models Think from a First-person Perspective?,,,,Sijie Cheng (None) | Zhicheng Guo (Tsinghua University) | Jingwen Wu (University Of Toronto) | Kechen Fang (Tsinghua University) | Peng Li (Tsinghua University) | Huaping Liu (Tsinghua University) | Yang Liu (Tsinghua University),,,,,,,
Robust Synthetic-to-Real Transfer for Stereo Matching,,,,Jiawei Zhang (Beijing University Of Aeronautics And Astronautics) | Jiahe Li (Beijing University Of Aeronautics And Astronautics) | Lei Huang (Beihang University) | Xiaohan Yu (Macquarie University) | Lin Gu (RIKEN / The University Of Tokyo) | Jin Zheng (Beijing University Of Aeronautics And Astronautics) | Xiao Bai (Beijing University Of Aeronautics And Astronautics),,,,,,,
Efficient Model Stealing Defense with Noise Transition Matrix,,,,Dong-Dong Wu (Southeast University) | Chilin Fu (Ant Group) | Weichang Wu (Alibaba Group) | Wenwen Xia (Shanghai Jiao Tong University) | Xiaolu Zhang (None) | JUN ZHOU (Ant Group) | Min-Ling Zhang (Southeast University),,,,,,,
PerAda: Parameter-Efficient Federated Learning Personalization with Generalization Guarantees,"Personalized Federated Learning (pFL) has emerged as a promising solution to tackle data heterogeneity across clients in FL. However, existing pFL methods either (1) introduce high communication and computation costs or (2) overfit to local data, which can be limited in scope, and are vulnerable to evolved test samples with natural shifts. In this paper, we propose PerAda, a parameter-efficient pFL framework that reduces communication and computational costs and exhibits superior generalization performance, especially under test-time distribution shifts. PerAda reduces the costs by leveraging the power of pretrained models and only updates and communicates a small number of additional parameters from adapters. PerAda has good generalization since it regularizes each client's personalized adapter with a global adapter, while the global adapter uses knowledge distillation to aggregate generalized information from all clients. Theoretically, we provide generalization bounds to explain why PerAda improves generalization, and we prove its convergence to stationary points under non-convex settings. Empirically, PerAda demonstrates competitive personalized performance (+4.85% on CheXpert) and enables better out-of-distribution generalization (+5.23% on CIFAR-10-C) on different datasets across natural and medical domains compared with baselines, while only updating 12.6% of parameters per model based on the adapter.",http://arxiv.org/abs/2302.06637v1,,"Chulin Xie (University Of Illinois, Urbana Champaign) | De-An Huang (NVIDIA) | Wenda Chu (California Institute Of Technology) | Daguang Xu (NVIDIA) | Chaowei Xiao (Arizona State University) | Bo Li (UIUC) | Anima Anandkumar (California Institute Of Technology)",2023-02-13 19:00:37+00:00,,,,,,
ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images,"Unlike color photography images, which are consistently encoded into RGB channels, biological images encompass various modalities, where the type of microscopy and the meaning of each channel varies with each experiment. Importantly, the number of channels can range from one to a dozen and their correlation is often comparatively much lower than RGB, as each of them brings specific information content. This aspect is largely overlooked by methods designed out of the bioimage field, and current solutions mostly focus on intra-channel spatial attention, often ignoring the relationship between channels, yet crucial in most biological applications. Importantly, the variable channel type and count prevent the projection of several experiments to a unified representation for large scale pre-training. In this study, we propose ChAda-ViT, a novel Channel Adaptive Vision Transformer architecture employing an Inter-Channel Attention mechanism on images with an arbitrary number, order and type of channels. We also introduce IDRCell100k, a bioimage dataset with a rich set of 79 experiments covering 7 microscope modalities, with a multitude of channel types, and channel counts varying from 1 to 10 per experiment. Our proposed architecture, trained in a self-supervised manner, outperforms existing approaches in several biologically relevant downstream tasks. Additionally, it can be used to bridge the gap for the first time between assays with different microscopes, channel numbers or types by embedding various image and experimental modalities into a unified biological image representation. The latter should facilitate interdisciplinary studies and pave the way for better adoption of deep learning in biological image-based analyses. Code and Data to be released soon.",http://arxiv.org/abs/2311.15264v1,,Nicolas Bourriez (Ecole Normale Sup??rieure De Paris) | Ihab Bendidi (Ecole Normale Superieure) | Cohen Ethan (Ecole Normale Sup??rieure De Paris) | Gabriel Watkinson (Ecole Normale Sup??rieure De Paris) | Maxime Sanchez (IBENS) | Guillaume Bollot (Synsight Company) | Auguste Genovesio (Ecole Normale Sup??rieure De Paris),2023-11-26 10:38:47+00:00,,,,,,
Accelerating Diffusion Sampling with Optimized Time Steps,"Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.",http://arxiv.org/abs/2402.17376v1,,"Shuchen Xue (Academy Of Mathematics And Systems Science, Chinese Academy Of Sciences) | Zhaoqiang Liu (University Of Electronic Science And Technology Of China) | Fei Chen (Huawei Noah's Ark Lab) | Shifeng Zhang (Huawei Technologies Ltd.) | Tianyang Hu (Huawei Noah's Ark Lab) | Enze Xie (Huawei Noah's Ark Lab) | Zhenguo Li (Huawei)",2024-02-27 10:13:30+00:00,,,,,,
PixelLM: Pixel Reasoning with Large Multimodal Model,"While large multimodal models (LMMs) have achieved remarkable progress, generating pixel-level masks for image reasoning tasks involving multiple open-world targets remains a challenge. To bridge this gap, we introduce PixelLM, an effective and efficient LMM for pixel-level reasoning and understanding. Central to PixelLM is a novel, lightweight pixel decoder and a comprehensive segmentation codebook. The decoder efficiently produces masks from the hidden embeddings of the codebook tokens, which encode detailed target-relevant information. With this design, PixelLM harmonizes with the structure of popular LMMs and avoids the need for additional costly segmentation models. Furthermore, we propose a target refinement loss to enhance the model's ability to differentiate between multiple targets, leading to substantially improved mask quality. To advance research in this area, we construct MUSE, a high-quality multi-target reasoning segmentation benchmark. PixelLM excels across various pixel-level image reasoning and understanding tasks, outperforming well-established methods in multiple benchmarks, including MUSE, single- and multi-referring segmentation. Comprehensive ablations confirm the efficacy of each proposed component. All code, models, and datasets will be publicly available.",http://arxiv.org/abs/2312.02228v1,,Zhongwei Ren (Beijing Jiao Tong University) | Zhicheng Huang (University Of Science And Technology Beijing) | Yunchao Wei (Beijing Jiao Tong University) | Yao Zhao (Beijing Jiao Tong University) | Dongmei Fu (University Of Science And Technology Beijing) | Jiashi Feng (ByteDance) | Xiaojie Jin (ByteDance/TikTok),2023-12-04 03:05:59+00:00,,,,,,
YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection,"Out-of-distribution (OOD) detection has attracted a large amount of attention from the machine learning research community in recent years due to its importance in deployed systems. Most of the previous studies focused on the detection of OOD samples in the multi-class classification task. However, OOD detection in the multi-label classification task, a more common real-world use case, remains an underexplored domain. In this research, we propose YolOOD - a method that utilizes concepts from the object detection domain to perform OOD detection in the multi-label classification task. Object detection models have an inherent ability to distinguish between objects of interest (in-distribution) and irrelevant objects (e.g., OOD objects) in images that contain multiple objects belonging to different class categories. These abilities allow us to convert a regular object detection model into an image classifier with inherent OOD detection capabilities with just minor changes. We compare our approach to state-of-the-art OOD detection methods and demonstrate YolOOD's ability to outperform these methods on a comprehensive suite of in-distribution and OOD benchmark datasets.",http://arxiv.org/abs/2212.02081v2,,Alon Zolfi (Ben-Gurion University Of The Negev) | Guy AmiT (Ben-Gurion University Of The Negev) | Amit Baras (None) | Satoru Koda (Fujitsu Limited) | Ikuya Morikawa (Fujitsu Research) | Yuval Elovici (Ben Gurion University Of The Negev) | Asaf Shabtai (Ben-Gurion University Of The Negev),2022-12-05 07:52:08+00:00,,,,,,
How to Train Neural Field Representations: A Comprehensive Study and Benchmark,"Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields.   In this work, we propose $\verb|fit-a-nef|$, a JAX-based library that leverages parallelization to enable fast optimization of large-scale NeF datasets, resulting in a significant speed-up. With this library, we perform a comprehensive study that investigates the effects of different hyperparameters -- including initialization, network architecture, and optimization strategies -- on fitting NeFs for downstream tasks. Our study provides valuable insights on how to train NeFs and offers guidance for optimizing their effectiveness in downstream applications. Finally, based on the proposed library and our analysis, we propose Neural Field Arena, a benchmark consisting of neural field variants of popular vision datasets, including MNIST, CIFAR, variants of ImageNet, and ShapeNetv2. Our library and the Neural Field Arena will be open-sourced to introduce standardized benchmarking and promote further research on neural fields.",http://arxiv.org/abs/2312.10531v1,,Samuele Papa (University Of Amsterdam) | Riccardo Valperga (University Of Amsterdam) | David Knigge (University Of Amsterdam) | Miltiadis Kofinas (University Of Amsterdam) | Phillip Lippe (University Of Amsterdam) | Jan-Jakob Sonke (Netherlands Cancer Institute) | Efstratios Gavves (None),2023-12-16 20:10:23+00:00,,,,,,
Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework,,,,"Ziyao Huang (, Chinese Academy Of Sciences) | Fan Tang (Institute Of Computing Technology, CAS) | Yong Zhang (Tencent AI Lab) | Xiaodong Cun (Tencent AI Lab) | Juan Cao (Institute Of Computing Technology, Chinese Academy Of Sciences) | Jintao Li (Institute Of Computing Technology, Chinese Academy Of Sciences) | Tong-Yee Lee (National Cheng Kung University)",,,,,,,
Learning Triangular Distribution in Visual World,"Convolution neural network is successful in pervasive vision tasks, including label distribution learning, which usually takes the form of learning an injection from the non-linear visual features to the well-defined labels. However, how the discrepancy between features is mapped to the label discrepancy is ambient, and its correctness is not guaranteed.To address these problems, we study the mathematical connection between feature and its label, presenting a general and simple framework for label distribution learning. We propose a so-called Triangular Distribution Transform (TDT) to build an injective function between feature and label, guaranteeing that any symmetric feature discrepancy linearly reflects the difference between labels. The proposed TDT can be used as a plug-in in mainstream backbone networks to address different label distribution learning tasks. Experiments on Facial Age Recognition, Illumination Chromaticity Estimation, and Aesthetics assessment show that TDT achieves on-par or better results than the prior arts.",http://arxiv.org/abs/2311.18605v3,,Ping Chen (Great Wall Motor Company Limited) | Xingpeng Zhang (Southwest Petroleum University) | Chengtao Zhou (Microbt) | Dichao Fan (MicroBT) | Peng Tu (RuqiMobility) | Le Zhang (Shenzhen MicroBT Electronics Technology Corporation ) | Yanlin Qian (Tampere University),2023-11-30 15:02:13+00:00,,,,,,
Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain,"Existing quality enhancement methods for compressed images focus on aligning the enhancement domain with the raw domain to yield realistic images. However, these methods exhibit a pervasive enhancement bias towards the compression domain, inadvertently regarding it as more realistic than the raw domain. This bias makes enhanced images closely resemble their compressed counterparts, thus degrading their perceptual quality. In this paper, we propose a simple yet effective method to mitigate this bias and enhance the quality of compressed images. Our method employs a conditional discriminator with the compressed image as a key condition, and then incorporates a domain-divergence regularization to actively distance the enhancement domain from the compression domain. Through this dual strategy, our method enables the discrimination against the compression domain, and brings the enhancement domain closer to the raw domain. Comprehensive quality evaluations confirm the superiority of our method over other state-of-the-art methods without incurring inference overheads.",http://arxiv.org/abs/2402.17200v2,,"Qunliang Xing (Beihang University) | Mai Xu (Beihang University, Tsinghua University) | Shengxi Li (Beihang University) | Xin Deng (Beijing University Of Aeronautics And Astronautics) | Meisong Zheng (Alibaba Group) | Huaida Liu (Alibaba Group) | Ying Chen (Alibaba Group)",2024-02-27 04:37:04+00:00,,,,,,
E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and Bottom-Up Generator,,,,Wenjun Wu (None) | Lingling Zhang (Xi'an Jiao Tong University) | Jun Liu (Xi'an Jiao Tong University) | Xi Tang (Xi'an Jiao Tong University) | Yaxian Wang (Xi'an Jiao Tong University) | Shaowei Wang (Xi'an Jiao Tong University) | QianYing Wang (Lenovo Group),,,,,,,
Open3DIS: Open-vocabulary 3D Instance Segmentation with 2D Mask Guidance,"We introduce Open3DIS, a novel solution designed to tackle the problem of Open-Vocabulary Instance Segmentation within 3D scenes. Objects within 3D environments exhibit diverse shapes, scales, and colors, making precise instance-level identification a challenging task. Recent advancements in Open-Vocabulary scene understanding have made significant strides in this area by employing class-agnostic 3D instance proposal networks for object localization and learning queryable features for each 3D mask. While these methods produce high-quality instance proposals, they struggle with identifying small-scale and geometrically ambiguous objects. The key idea of our method is a new module that aggregates 2D instance masks across frames and maps them to geometrically coherent point cloud regions as high-quality object proposals addressing the above limitations. These are then combined with 3D class-agnostic instance proposals to include a wide range of objects in the real world. To validate our approach, we conducted experiments on three prominent datasets, including ScanNet200, S3DIS, and Replica, demonstrating significant performance gains in segmenting objects with diverse categories over the state-of-the-art approaches.",http://arxiv.org/abs/2312.10671v1,,Phuc Nguyen (VinAI Research) | Tuan Duc Ngo (UMass Amherst) | Evangelos Kalogerakis (UMass Amherst) | Chuang Gan (MIT-IBM Watson AI Lab) | Anh Tran (VinAI Research) | Cuong Pham (Posts & Telecommunications Institute Of Technology And VinAI Research) | Khoi Nguyen (VinAI Research),2023-12-17 10:07:03+00:00,,,,,,
CORES: Convolutional Response-based Score for Out-of-distribution Detection,,,,Keke Tang (Guangzhou University) | Chao Hou (Guangzhou University) | Weilong Peng (None) | Runnan Chen (None) | Peican Zhu (Northwest Polytechnical University Xi'an) | Wenping Wang (Texas A&M University - College Station) | Zhihong Tian (Guangzhou University),,,,,,,
LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking,"Deep learning models, particularly those based on transformers, often employ numerous stacked structures, which possess identical architectures and perform similar functions. While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications. In today's landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue. To mitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage. We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset. Experimental results demonstrate the effectiveness of our method, as even with a 70\% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or",http://arxiv.org/abs/2403.04303v1,,Jialin Li (Tencent) | Qiang Nie (Tencent Youtu Lab) | Weifu Fu (Tencent Youtu Lab) | Yuhuan Lin (Tencent Youtu Lab) | Guangpin Tao (Tencent YoutuLab) | Yong Liu (Tencent Youtu Lab) | Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University),2024-03-07 08:10:59+00:00,,,,,,
Language-driven Grasp Detection,,,,"An Vuong (FPT Software - AIC Lab) | Minh VU (Automation & Control Institute, TU Wien) | Baoru Huang (University College London, University Of London) | Nghia Nguyen (FPT Software) | Hieu Le (FPT Software AI Center) | Thieu Vo (Ton Duc Thang University) | Anh Nguyen (University Of Liverpool)",,,,,,,
Neural Implicit Morphing of Face Images,"Face morphing is a problem in computer graphics with numerous artistic and forensic applications. It is challenging due to variations in pose, lighting, gender, and ethnicity. This task consists of a warping for feature alignment and a blending for a seamless transition between the warped images. We propose to leverage coord-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping/blending of the images. During morphing inference, we need both direct and inverse transformations of the time-dependent warping. The first (second) is responsible for warping the target (source) image into the source (target) image. Our neural warping stores those maps in a single network dismissing the need for inverting them. The results of our experiments indicate that our method is competitive with both classical and generative models under the lens of image quality and face-morphing detectors. Aesthetically, the resulting images present a seamless blending of diverse faces not yet usual in the literature.",http://arxiv.org/abs/2308.13888v2,,"Guilherme Schardong (Institute Of Systems And Robotics, University Of Coimbra) | Tiago Novello (IMPA) | Hallison Paz (IMPA) | Iurii Medvedev (Institute Of Systems And Robotics, University Of Coimbra) | Vin??cius Silva (PUC-Rio) | Luiz Velho (IMPA) | Nuno Gon??alves (University Of Coimbra)",2023-08-26 14:12:19+00:00,,,,,,
SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model,,,,Zhengang Li (Northeastern University) | Yan Kang (None) | Yuchen Liu (None) | Difan Liu (Adobe Research) | Tobias Hinz (Adobe Systems) | Feng Liu (Adobe Systems) | Yanzhi Wang (Northeastern University),,,,,,,
CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification,"The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs, focusing on image-text alignment. However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports. To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology Zero-Shot Classification (CARZero). Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics. This representation is then linearly projected to form an image-text similarity matrix for cross-modality alignment. Additionally, recognizing the pivotal role of prompt selection in zero-shot learning, CARZero incorporates a Large Language Model-based prompt alignment strategy. This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual prompt design. Our approach is simple yet effective, demonstrating state-of-the-art performance in zero-shot classification on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases. This achievement is attributed to our new image-text alignment strategy, which effectively addresses the complex relationship between medical images and reports.",http://arxiv.org/abs/2402.17417v1,,"Haoran Lai (University Of Science And Technology Of China) | Qingsong Yao (University Of The Chinese Academy Of Sciences) | Zihang Jiang (University Of Science And Technology Of China) | Rongsheng Wang (University Of Science And Technology Of China) | Zhiyang He (Xunfei Healthcare Technology Co., Ltd.) | Xiaodong Tao (Xunfei Healthcare Co. Ltd) | S Kevin Zhou (University Of Science And Technology Of China)",2024-02-27 11:17:46+00:00,,,,,,
Targeted Representation Alignment for Open-World Semi-Supervised Learning,,,,"Ruixuan Xiao (Zhejiang University) | Lei Feng (Nanyang Technological University) | Kai Tang (Zhejiang University) | Junbo Zhao (Zhejiang University) | Yixuan Li (University Of Wisconsin Madison) | Gang Chen (College Of Computer Science And Technology, Zhejiang University) | Haobo Wang (Zhejiang University)",,,,,,,
FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition,"Recent approaches such as ControlNet offer users fine-grained spatial control over text-to-image (T2I) diffusion models. However, auxiliary modules have to be trained for each type of spatial condition, model architecture, and checkpoint, putting them at odds with the diverse intents and preferences a human designer would like to convey to the AI models during the content creation process. In this work, we present FreeControl, a training-free approach for controllable T2I generation that supports multiple conditions, architectures, and checkpoints simultaneously. FreeControl designs structure guidance to facilitate the structure alignment with a guidance image, and appearance guidance to enable the appearance sharing between images generated using the same seed. Extensive qualitative and quantitative experiments demonstrate the superior performance of FreeControl across a variety of pre-trained T2I models. In particular, FreeControl facilitates convenient training-free control over many different architectures and checkpoints, allows the challenging input conditions on which most of the existing training-free methods fail, and achieves competitive synthesis quality with training-based approaches.",http://arxiv.org/abs/2312.07536v1,,"SICHENG MO (University Of California, Los Angeles) | Fangzhou Mu (University Of Wisconsin-Madison) | Kuan Heng Lin (University Of California, Los Angeles) | Yanli Liu (Shein Technology LLC) | Bochen Guan (OPPO US Research Center) | Yin Li (University Of Wisconsin, Madison) | Bolei Zhou (University Of California, Los Angeles)",2023-12-12 18:59:14+00:00,,,,,,
Depth-aware Test-Time Training for Zero-shot Video Object Segmentation,,,,Weihuang Liu (University Of Macau) | Xi Shen (Tencent AI Lab) | Haolun Li (University Of Macau) | Xiuli Bi (Chongqing University Of Posts And Telecommunications) | Bo Liu (Chongqing University Of Posts And Telecommunications) | Chi-Man Pun (University Of Macau) | Xiaodong Cun (Tencent AI Lab),,,,,,,
A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing,"This paper presents a video inversion approach for zero-shot video editing, which aims to model the input video with low-rank representation during the inversion process. The existing video editing methods usually apply the typical 2D DDIM inversion or na\""ive spatial-temporal DDIM inversion before editing, which leverages time-varying representation for each frame to derive noisy latent. Unlike most existing approaches, we propose a Spatial-Temporal Expectation-Maximization (STEM) inversion, which formulates the dense video feature under an expectation-maximization manner and iteratively estimates a more compact basis set to represent the whole video. Each frame applies the fixed and global representation for inversion, which is more friendly for temporal consistency during reconstruction and editing. Extensive qualitative and quantitative experiments demonstrate that our STEM inversion can achieve consistent improvement on two state-of-the-art video editing methods.",http://arxiv.org/abs/2312.05856v1,,Li Maomao (The University Of HongKong) | Yu Li (International Digital Economy Academy) | Tianyu Yang (IDEA) | Yunfei Liu (International Digital Economy Academy (IDEA)) | Dongxu Yue (Peking University) | Zhihui Lin (Xverse) | Dong Xu (University Of Hong Kong),2023-12-10 11:20:18+00:00,,,,,,
Rotation-Agnostic Image Representation Learning for Digital Pathology,,,,Saghir Alfasly (Mayo Clinic) | Abubakr Shafique (Mayo Clinic) | Peyman Nejat (Mayo Clinic) | Jibran Khan (Luther College) | Areej Alsaafin (Mayo Clinic) | Ghazal Alabtah (Mayo Clinic) | Hamid Tizhoosh (None),,,,,,,
Desigen: A Pipeline for Controllable Design Template Generation,"Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create. In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background. Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements. To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process. Then conditioned on the background, we synthesize the layout with a Transformer-based autoregressive generator. To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds. We constructed a design dataset with more than 40k advertisement banners to verify our approach. Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers. More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides. The data and code are available at https://whaohan.github.io/desigen.",http://arxiv.org/abs/2403.09093v1,,"Haohan Weng (South China University Of Technology) | Danqing Huang (Microsoft) | YU QIAO (Central South University) | Hu Zheng (Keio University, Tokyo Institute Of Technology) | Chin-Yew Lin (Microsoft) | Tong Zhang (South China University Of Technology) | C. L. Philip Chen (South China University Of Technology)",2024-03-14 04:32:28+00:00,,,,,,
Towards Generalizable Tumor Synthesis,"Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.",http://arxiv.org/abs/2402.19470v1,,Qi Chen (University Of Science And Technology Of China) | Xiaoxi Chen (None) | Haorui Song (Johns Hopkins University) | Alan L. Yuille (Johns Hopkins University) | Zhiwei Xiong (None) | Chen Wei (Johns Hopkins University) | Zongwei Zhou (Johns Hopkins University),2024-02-29 18:57:39+00:00,,,,,,
NeISF: Neural Incident Stokes Field for Geometry and Material Estimation,"Multi-view inverse rendering is the problem of estimating the scene parameters such as shapes, materials, or illuminations from a sequence of images captured under different viewpoints. Many approaches, however, assume single light bounce and thus fail to recover challenging scenarios like inter-reflections. On the other hand, simply extending those methods to consider multi-bounced light requires more assumptions to alleviate the ambiguity. To address this problem, we propose Neural Incident Stokes Fields (NeISF), a multi-view inverse rendering framework that reduces ambiguities using polarization cues. The primary motivation for using polarization cues is that it is the accumulation of multi-bounced light, providing rich information about geometry and material. Based on this knowledge, the proposed incident Stokes field efficiently models the accumulated polarization effect with the aid of an original physically-based differentiable polarimetric renderer. Lastly, experimental results show that our method outperforms the existing works in synthetic and real scenarios.",http://arxiv.org/abs/2311.13187v2,,Chenhao Li (Osaka University) | Taishi Ono (Sony Semiconductor Solutions Europe) | Takeshi Uemori (Sony Semiconductor Solutions Corporation) | Hajime Mihara (Sony Semiconductor Solutions Corporation) | Alexander Gatto (Sony Semiconductor Solutions Europe) | Hajime Nagahara (Osaka University) | Yusuke Moriuchi (Sony Semiconductor Solutions Corporation),2023-11-22 06:28:30+00:00,,,,,,
FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models,,,,LIn Zhao (Infinigence) | Tianchen Zhao (Tsinghua University) | Zinan Lin (Microsoft Research) | Xuefei Ning (Tsinghua University) | Guohao Dai (Shanghai Jiao Tong University) | Huazhong Yang (Tsinghua University) | Yu Wang (Tsinghua University),,,,,,,
CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion,"Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",http://arxiv.org/abs/2403.11162v1,,Xiaoyu Wu (Shanghai Jiao Tong University) | Yang Hua (Queen's University Belfast) | Chumeng Liang (University Of Southern California) | Jiaru Zhang (Shanghai Jiao Tong University) | Hao Wang (Louisiana State University) | Tao Song (Shanghai Jiao Tong University) | Haibing Guan (Shanghai Jiao Tong University),2024-03-17 10:06:38+00:00,,,,,,
Discovering Syntactic Interaction Clues for Human-Object Interaction Detection,,,,"Jinguo Luo (None) | Weihong Ren (Harbin Institute Of Technology, Shenzhen) | Weibo Jiang (Harbin Institute Of Technology) | Xi'ai Chen (Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Qiang Wang (Shenyang University) | Zhi Han (Shenyang Institute Of Automation, Chinese Academy Of Sciences) | Honghai LIU (Harbin Institute Of Technology, Shenzhen)",,,,,,,
Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation,"A versatile medical image segmentation model applicable to imaging data collected with diverse equipment and protocols can facilitate model deployment and maintenance. However, building such a model typically requires a large, diverse, and fully annotated dataset, which is rarely available due to the labor-intensive and costly data curation. In this study, we develop a cost-efficient method by harnessing readily available data with partially or even sparsely annotated segmentation labels. We devise strategies for model self-disambiguation, prior knowledge incorporation, and imbalance mitigation to address challenges associated with inconsistently labeled data from various sources, including label ambiguity and imbalances across modalities, datasets, and segmentation labels. Experimental results on a multi-modal dataset compiled from eight different sources for abdominal organ segmentation have demonstrated our method's effectiveness and superior performance over alternative state-of-the-art methods, highlighting its potential for optimizing the use of existing annotated data and reducing the annotation efforts for new data to further enhance model capability.",http://arxiv.org/abs/2311.10696v1,,Xiaoyang Chen (University Of Pennsylvania) | Hao Zheng (University Of Pennsylvania) | Yuemeng LI (University Of Pennsylvania) | Yuncong Ma (University Of Pennsylvania) | Liang Ma (University Of Pennsylvania) | Hongming Li (University Of Pennsylvania) | Yong Fan (University Of Pennsylvania),2023-11-17 18:28:32+00:00,,,,,,
PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics,"We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, ""cage meshes,"" or any other geometry embedding, highlighting the principle of ""what you see is what you simulate (WS$^2$)."" Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. Our project page is at: https://xpandora.github.io/PhysGaussian/",http://arxiv.org/abs/2311.12198v2,,"Tianyi Xie (University Of California, Los Angeles) | Zeshun Zong (University Of California, Los Angeles) | Yuxing Qiu (UCLA & LightSpeed Studios) | Xuan Li (None) | Yutao Feng (Zhejiang University) | Yin Yang (University Of Utah) | Chenfanfu Jiang (University Of California, Los Angeles)",2023-11-20 21:34:52+00:00,,,,,,
Make Pixels Dance: High-Dynamic Video Generation,,,,Yan Zeng (ByteDance) | Guoqiang Wei (ByteDance) | Jiani Zheng (None) | Jiaxin Zou (ByteDance Ltd.) | Yang Wei (East China Normal University) | Yuchen Zhang ( ByteDance Research) | Hang Li (ByteDance Technology),,,,,,,
Rethinking Boundary Discontinuity Problem for Oriented Object Detection,"Oriented object detection has been developed rapidly in the past few years, where rotation equivariant is crucial for detectors to predict rotated bounding boxes. It is expected that the prediction can maintain the corresponding rotation when objects rotate, but severe mutational in angular prediction is sometimes observed when objects rotate near the boundary angle, which is well-known boundary discontinuity problem. The problem has been long believed to be caused by the sharp loss increase at the angular boundary during training, and widely used IoU-like loss generally deal with this problem by loss-smoothing. However, we experimentally find that even state-of-the-art IoU-like methods do not actually solve the problem. On further analysis, we find the essential cause of the problem lies at discontinuous angular ground-truth(box), not just discontinuous loss. There always exists an irreparable gap between continuous model ouput and discontinuous angular ground-truth, so angular prediction near the breakpoints becomes highly unstable, which cannot be eliminated just by loss-smoothing in IoU-like methods. To thoroughly solve this problem, we propose a simple and effective Angle Correct Module (ACM) based on polar coordinate decomposition. ACM can be easily plugged into the workflow of oriented object detectors to repair angular prediction. It converts the smooth value of the model output into sawtooth angular value, and then IoU-like loss can fully release their potential. Extensive experiments on multiple datasets show that whether Gaussian-based or SkewIoU methods are improved to the same performance of AP50 and AP75 with the enhancement of ACM.",http://arxiv.org/abs/2305.10061v1,,"Hang Xu (Hangzhou Dianzi University) | Xinyuan Liu (Institute Of Computing Technology, Chinese Academy Of Sciences) | Haonan Xu (ICT, Chinese Academy Of Sciences) | Yike Ma (, Chinese Academy Of Sciences) | Zunjie Zhu (Hangzhou Dianzi University) | Chenggang Yan (Hangzhou Dianzi University, Tsinghua University) | Feng Dai (ICT, Chinese Academy Of Sciences)",2023-05-17 09:04:22+00:00,,,,,,
LEDITS++: Limitless Image Editing using Text-to-Image Models,"Text-to-image diffusion models have recently received increasing interest for their astonishing ability to produce high-fidelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabilities to real image editing. However, existing image-to-image methods are often inefficient, imprecise, and of limited versatility. They either require time-consuming fine-tuning, deviate unnecessarily strongly from the input image, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEDITS++, an efficient yet versatile and precise textual image manipulation technique. LEDITS++'s novel inversion approach requires no tuning nor optimization and produces high-fidelity results with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architecture-agnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaustive evaluation. Our results demonstrate the capabilities of LEDITS++ and its improvements over previous methods. The project page is available at https://leditsplusplus-project.static.hf.space .",http://arxiv.org/abs/2311.16711v1,,"Manuel Brack (Technische Universit??t Darmstadt) | Felix Friedrich (TU Darmstadt, Hessian.AI) | Katharina Kornmeier (Align Technology) | Linoy Tsaban (Hugging Face) | Patrick Schramowski (TU Darmstadt) | Kristian Kersting (TU Darmstadt) | Apolin??rio Passos (Universidade De Bras??lia)",2023-11-28 11:45:35+00:00,,,,,,
HEAL-SWIN: A Vision Transformer On The Sphere,"High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentation and depth regression tasks on both synthetic and real automotive datasets. Our code is available at https://github.com/JanEGerken/HEAL-SWIN.",http://arxiv.org/abs/2307.07313v1,,Oscar Carlsson (Chalmers University Of Technology) | Jan E. Gerken (Chalmers University Of Technology) | Hampus Linander (Chalmers University Of Technology) | Heiner Spiess (Technische Universit??t Berlin) | Fredrik Ohlsson (Umea University) | Christoffer Petersson (Zenseact) | Daniel Persson (Chalmers University Of Technology),2023-07-14 12:46:59+00:00,,,,,,
GDA: Generalized Diffusion for Robust Test-time Adaptation,,,,Yun-Yun Tsai (Columbia University) | Fu-Chen Chen (Amazon Lab126) | Albert Chen (Amazon) | Junfeng Yang (Columbia University) | Che-Chun Su (Amazon) | Min Sun (None) | Cheng-Hao Kuo (Amazon),,,,,,,
PromptKD: Unsupervised Prompt Distillation for Vision-Language Models,"Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method.",http://arxiv.org/abs/2403.02781v4,,Zheng Li (Nankai University) | Xiang Li (Nankai University) | Xinyi Fu (Ant Group) | Xin Zhang (Nankai University) | Weiqiang Wang (University Of Southern California) | Shuo Chen (RIKEN) | Jian Yang (Nankai University),2024-03-05 08:53:30+00:00,,,,,,
Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis,,,,"Tianci Bi (Xi'an Jiao Tong University) | Xiaoyi Zhang (Research, Microsoft) | Zhizheng Zhang (Microsoft Research) | Wenxuan Xie (Microsoft Research Asia) | Cuiling Lan (Microsoft) | Yan Lu (Microsoft Research Asia) | Nanning Zheng (Xi'an Jiao Tong University)",,,,,,,
PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation,,,,Jinfeng Xu (Huazhong University Of Science And Technology) | Siyuan Yang (HUST) | Xianzhi Li (Huazhong University Of Science And Technology) | Yuan Tang (Huazhong University Of Science And Technology) | Yixue Hao (Huazhong University Of Science And Technology) | Long Hu (Huazhong University Of Science And Technology) | Min Chen (South China University Of Technology),,,,,,,
Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios,"Multi-view clustering (MVC) aims at exploring category structures among multi-view data in self-supervised manners. Multiple views provide more information than single views and thus existing MVC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical multi-view scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MVC method (namely MVCAN) to address this issue. Specifically, we propose a novel MVC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a two-level multi-view iterative optimization is designed to generate robust learning targets for refining individual views' representation learning. Theoretical analysis reveals that MVCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on extensive public datasets demonstrate that MVCAN outperforms state-of-the-art methods and is robust against the existence of noisy views.",http://arxiv.org/abs/2303.17245v3,,Jie Xu (University Of Electronic Science And Technology Of China) | Yazhou Ren (University Of Electronic Science And Technology Of China) | Xiaolong Wang (University Of Electronic Science And Technology Of China) | Lei Feng (Nanyang Technological University) | Zheng Zhang (Harbin Institute Of Technology) | Gang Niu (RIKEN) | Xiaofeng Zhu (University Of Electronic Science And Technology Of China),2023-03-30 09:22:17+00:00,,,,,,
SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective,"Tensor network (TN) representation is a powerful technique for computer vision and machine learning. TN structure search (TN-SS) aims to search for a customized structure to achieve a compact representation, which is a challenging NP-hard problem. Recent ""sampling-evaluation-based"" methods require sampling an extensive collection of structures and evaluating them one by one, resulting in prohibitively high computational costs. To address this issue, we propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN), which allows us to efficiently solve the TN-SS problem from a regularized modeling perspective, eliminating the repeated structure evaluations. To be specific, by inserting a diagonal factor for each edge of the fully-connected TN, SVDinsTN allows us to calculate TN cores and diagonal factors simultaneously, with the factor sparsity revealing a compact TN structure. In theory, we prove a convergence guarantee for the proposed method. Experimental results demonstrate that the proposed method achieves approximately 100 to 1000 times acceleration compared to the state-of-the-art TN-SS methods while maintaining a comparable representation ability.",http://arxiv.org/abs/2305.14912v4,,Yu-Bang Zheng (Southwest Jiao Tong University) | Xile Zhao (University Of Electronic Science And Technology Of China) | Junhua Zeng (RIKEN) | Chao Li (RIKEN) | Qibin Zhao (RIKEN) | Heng-Chao Li (Southwest Jiao Tong University) | Ting-Zhu Huang (University Of Electronic Science And Technology Of China),2023-05-24 09:02:01+00:00,,,,,,
A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition,"Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at https://github.com/dalision/ModalBiasAVSR",http://arxiv.org/abs/2403.04245v1,,Yusheng Dai (University Of Science And Technology Of China) | HangChen (University Of Science And Technology Of China) | Jun Du (University Of Science And Technology Of China) | Ruoyu Wang (University Of Science And Technology Of China) | Shihao Chen (University Of Science And Technology Of China) | Haotian Wang (University Of Science And Technology Of China) | Chin-Hui Lee (Georgia Institute Of Technology),2024-03-07 06:06:55+00:00,,,,,,
Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition,"Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space can be represented as the Cartesian product of single-source sub-spaces. We are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. We propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. Extensive experiments demonstrate that our semantically decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with ResNet50 backbone. https://github.com/lxa9867/QSD.",http://arxiv.org/abs/2310.00132v2,,"Xiang Li (Carnegie Mellon University) | Jinglu Wang (Microsoft Research Asia) | Xiaohao Xu (University Of Michigan - Ann Arbor) | Xiulian Peng (Microsoft Research Asia) | Rita Singh (School Of Computer Science, Carnegie Mellon University) | Yan Lu (Microsoft Research Asia) | Bhiksha Raj (Carnegie Mellon University)",2023-09-29 20:48:44+00:00,,,,,,
UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model,,,,"Shuai Yuan (Duke University, Meta, TikTok) | Lei Luo (Meta) | Zhuo Hui (Facebook) | Can Pu (Facebook) | Xiaoyu Xiang (Meta) | Rakesh Ranjan (None) | Denis Demandolx (Meta)",,,,,,,
A Closer Look at Audio-Visual Segmentation,,,,Yuanhong Chen (University Of Adelaide) | Yuyuan Liu (University Of Adelaide) | Hu Wang (The University Of Adelaide) | Fengbei Liu (Cornell University) | Chong Wang (University Of Adelaide) | Helen Frazer (BreastScreen Victoria) | Gustavo Carneiro (University Of Surrey),,,,,,,
IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection,,,,Junbo Yin (Beijing Institute Of Technology) | Wenguan Wang (Zhejiang University) | Runnan Chen (None) | Wei Li (Inceptio) | Ruigang Yang (Inceptio ) | Pascal Frossard (EPFL) | Jianbing Shen (University Of Macau),,,,,,,
Virtual Immunohistochemistry Staining for Histological Images Assisted by Weakly-supervised Learning,,,,"Jiahan Li (Harbin Institute Of Technology) | Jiuyang Dong (Harbin Institute Of Technology) | Shenjin Huang (None) | Xi Li (Department Of Gastroenterology, Shenzhen Hospital, Peking University) | Junjun Jiang (Harbin Institute Of Technology) | Xiaopeng Fan (Harbin Institute Of Technology) | Yongbing Zhang (Harbin Institute Of Technology)",,,,,,,
Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection,,,,"Heng Zhang (Gaoling School Of Artificial Intelligence, Renmin University Of China) | Qiuyu Zhao (JD) | Linyu Zheng (JD) | Hao Zeng (JD.Com) | Zhiwei Ge (JD) | Tianhao Li (JD) | Sulong Xu (JD)",,,,,,,
Outdoor Scene Extrapolation with Hierarchical Generative Cellular Automata,,,,"Dongsu Zhang (Seoul National University) | Francis Williams (NVIDIA) | ??an Goj??i?? (NVIDIA) | Karsten Kreis (NVIDIA) | Sanja Fidler (Department Of Computer Science, University Of Toronto) | Young Min Kim (Seoul National University) | Amlan Kar (NVIDIA)",,,,,,,
DIEM: Decomposition-Integration Enhancing Multimodal Insights,,,,Xinyi Jiang (None) | Guoming Wang (Zhejiang University) | Junhao Guo (Zhejiang University) | Juncheng Li (Zhejiang University) | Wenqiao Zhang (National University Of Singapore) | Rongxing Lu (University Of New Brunswick) | Siliang Tang (Zhejiang University),,,,,,,
Real-Time Simulated Avatar from Head-Mounted Sensors,,,,Zhengyi Luo (Carnegie Mellon University) | Jinkun Cao (Carnegie Mellon University) | Rawal Khirodkar (Meta) | Alexander Winkler (Meta) | Jing Huang (Facebook) | Kris Kitani (Carnegie Mellon University) | Weipeng Xu (Meta Reality Labs Research),,,,,,,
Splat-SLAM: Dense RGB-D SLAM via 3D Gaussian Splatting,,,,"Nikhil Keetha (Carnegie Mellon University) | Jay Karhade (Carnegie Mellon University) | Krishna Murthy Jatavallabhula (Massachusetts Institute Of Technology) | Gengshan Yang (Reality Labs Research, Meta) | Sebastian Scherer (None) | Deva Ramanan (Carnegie Mellon University) | Jonathon Luiten (RWTH Aachen University)",,,,,,,
Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Multimodal Humor Generation,"Chain-of-Thought (CoT) guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logical tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game but also boosts creative abilities in various tasks like cloud guessing game and divergent association task. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models will be released online. https://zhongshsh.github.io/CLoT/.",http://arxiv.org/abs/2312.02439v2,,Shanshan Zhong (SUN YAT-SEN UNIVERSITY) | Zhongzhan Huang (Sun Yat-Sen University) | Shanghua Gao (Harvard University) | Wushao Wen (SUN YAT-SEN UNIVERSITY) | Liang Lin (Sun Yat-Sen University) | Marinka Zitnik (Harvard University) | Pan Zhou (Sea Group),2023-12-05 02:41:57+00:00,,,,,,
Clockwork Diffusion: Efficient Generation With Model-Step Distillation,"This work aims to improve the efficiency of text-to-image diffusion models. While diffusion models use computationally expensive UNet-based denoising operations in every generation step, we identify that not all operations are equally relevant for the final output quality. In particular, we observe that UNet layers operating on high-res feature maps are relatively sensitive to small perturbations. In contrast, low-res feature maps influence the semantic layout of the final image and can often be perturbed with no noticeable change in the output. Based on this observation, we propose Clockwork Diffusion, a method that periodically reuses computation from preceding denoising steps to approximate low-res feature maps at one or more subsequent steps. For multiple baselines, and for both text-to-image generation and image editing, we demonstrate that Clockwork leads to comparable or improved perceptual scores with drastically reduced computational complexity. As an example, for Stable Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and CLIP change.",http://arxiv.org/abs/2312.08128v2,,"Amirhossein Habibian (Qualcomm AI Research) | Amir Ghodrati (QualComm AI Research) | Noor Fathima (Qualcomm Inc, QualComm) | Guillaume Sautiere (Qualcomm Inc, QualComm) | Risheek Garrepalli (Qualcomm Inc, QualComm) | Fatih Porikli (QualComm) | Jens Petersen (Qualcomm AI Research)",2023-12-13 13:30:27+00:00,,,,,,
Constrained Layout Design with Factor Graphs,,,,Mohammed Haroon Dupty (National University Of Singapore) | Yanfei Dong (PayPal) | Sicong Leng (Nanyang Technological University) | Guoji Fu (National University Of Singapore) | Yong Liang Goh (National University Of Singapore) | Wei Lu (Singapore University Of Technology And Design) | Wee Sun Lee (National University Of Singapore),,,,,,,
PaReNeRF: Toward Fast Large-scale Dynamic NeRF with Patch-based Reference,,,,Xiao Tang (None) | Min Yang (None) | Penghui Sun (Samsung R&D Institute) | Hui Li (Samsung R&D Institute China Xi??an (SRCX)) | Yuchao Dai (Northwestern Polytechnical University) | Feng Zhu (None) | Hojae Lee (None),,,,,,,
Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis,"Histopathological whole slide images (WSIs) classification has become a foundation task in medical microscopic imaging processing. Prevailing approaches involve learning WSIs as instance-bag representations, emphasizing significant instances but struggling to capture the interactions between instances. Additionally, conventional graph representation methods utilize explicit spatial positions to construct topological structures but restrict the flexible interaction capabilities between instances at arbitrary locations, particularly when spatially distant. In response, we propose a novel dynamic graph representation algorithm that conceptualizes WSIs as a form of the knowledge graph structure. Specifically, we dynamically construct neighbors and directed edge embeddings based on the head and tail relationships between instances. Then, we devise a knowledge-aware attention mechanism that can update the head node features by learning the joint attention score of each neighbor and edge. Finally, we obtain a graph-level embedding through the global pooling process of the updated head, serving as an implicit representation for the WSI classification. Our end-to-end graph representation learning approach has outperformed the state-of-the-art WSI analysis methods on three TCGA benchmark datasets and in-house test sets. Our code is available at https://github.com/WonderLandxD/WiKG.",http://arxiv.org/abs/2403.07719v1,,"Jiawen Li (Tsinghua University) | Yuxuan Chen (Tsinghua University) | Hongbo Chu (None) | Sun Qiehe (Tsinghua University) | Tian Guan (Graduate School At Shenzhen, Tsinghua University) | Anjia Han (SUN YAT-SEN UNIVERSITY) | Yonghong He (Tsinghua University)",2024-03-12 14:58:51+00:00,,,,,,
Domain Separation Graph Neural Networks for Saliency Object Ranking,,,,Zijian Wu (Nanjing University Of Science And Technology) | Jun Lu (Nanjing University Of Science And Technology) | Jing Han (Nanjing University Of Science And Technology) | Lianfa Bai (Nanjing University Of Science And Technology) | Yi Zhang (Nanjing University Of Science And Technology) | Zhuang Zhao (Nanjing University Of Science And Technology) | Siyang Song (University Of Leicester),,,,,,,
Multimodal Dataset Pruning using Image-Captioning Models,"Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highly-aligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP's pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset filtering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6\% and 1.7\% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a significant improvement of 2.7% and 4.5% on medium and large scale respectively.",http://arxiv.org/abs/2310.02110v2,,"Anas Mahmoud (University Of Toronto) | Mostafa Elhoushi (Meta, FAIR) | Amro Abbas (Meta) | Yu Yang (University Of California, Los Angeles) | Newsha Ardalani (Facebook) | Hugh Leather (Facebook) | Ari Morcos (Meta AI (FAIR))",2023-10-03 14:53:53+00:00,,,,,,
SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities,"Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/",http://arxiv.org/abs/2401.12168v1,,Boyuan Chen (MIT) | Zhuo Xu (Google Deepmind) | Sean Kirmani (Google DeepMind) | Brian Ichter (Google) | Dorsa Sadigh (Google) | Leonidas Guibas (Stanford University) | Fei Xia (Google),2024-01-22 18:01:01+00:00,,,,,,
Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection,,,,Xiaohong Zhang (None) | Huisheng Ye (Nanjing University) | Jingwen Li (Nanjing University) | Qinyu Tang (Nanjing University) | Yuanqi Li (Nanjing University) | Yanwen Guo (Nanjing University) | Jie Guo (Nanjing University),,,,,,,
TransLoc4D: Transformer-based 4D-Radar Place Recognition,,,,Guohao Peng (Nanyang Technological University) | Heshan Li (Nanyang Technological University) | Yangyang Zhao (Nanyang Technological University) | Jun Zhang (Nanyang Technological University) | Zhenyu Wu (Nanyang Technological University) | Pengyu Zheng (Chinese University Of Hong Kong) | Danwei Wang (Nanyang Technological University),,,,,,,
APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentation,,,,Weizhao He (None) | Yang Zhang (Shenzhen University) | Wei Zhuo (Shenzhen University) | Linlin Shen (None) | Jiaqi Yang (University Of Nottingham) | Songhe Deng (None) | Liang Sun (Shenzhen University),,,,,,,
3D Facial Expressions through Analysis-by-Neural-Synthesis,,,,"George Retsinas (None) | Panagiotis Filntisis (None) | Radek Danecek (Max Planck Institute For Intelligent Systems, Max-Planck Institute) | Victoria Abrevaya (None) | Anastasios Roussos (Foundation For Research And Technology - Hellas) | Timo Bolkart (Google) | Petros Maragos (National Technical University Of Athens)",,,,,,,
JointSQ: Joint Sparsification-Quantization for Distributed Learning,,,,Weiying Xie (None) | Haowei Li (None) | Ma Jitao (None) | Yunsong Li (None) | Jie Lei (Xi'an University Of Electronic Science And Technology) | Donglai Liu (Xi'an University Of Electronic Science And Technology) | Leyuan Fang (None),,,,,,,
Segmenting Whole Objects by Synthesizing Them,,,,Ege Ozguroglu (None) | Ruoshi Liu (Columbia University) | D??dac Sur??s (Columbia University) | Dian Chen (Toyota Research Institute) | Achal Dave (None) | Pavel Tokmakov (Toyota Research Institute) | Carl Vondrick (Columbia University),,,,,,,
LiDAR-based Person Re-identification,,,,"Wenxuan Guo (Tsinghua University) | Zhiyu Pan (Department Of Automation, Tsinghua University) | Yingping Liang (None) | Ziheng Xi (Tsinghua University) | Zhi Chen Zhong (Tsinghua University) | Jianjiang Feng (Tsinghua University) | Jie Zhou (None)",,,,,,,
CoG-DQA: Chain-of-Guiding Learning with Large Language Models for Diagram Question Answering,,,,Shaowei Wang (Xi'an Jiao Tong University) | Lingling Zhang (Xi'an Jiao Tong University) | Longji Zhu (Xi'an Jiao Tong University) | Tao Qin (Xi'an Jiao Tong University) | Kim-Hui Yap (Nanyang Technological University) | Xinyu Zhang (None) | Jun Liu (Xi'an Jiao Tong University),,,,,,,
Positive-Unlabeled Learning by Latent Group-Aware Meta Disambiguation,,,,"Lin Long (Zhejiang University) | Haobo Wang (Zhejiang University) | Zhijie Jiang (Zhejiang University) | Lei Feng (Nanyang Technological University) | Chang Yao (Zhejiang University) | Gang Chen (College Of Computer Science And Technology, Zhejiang University) | Junbo Zhao (Zhejiang University)",,,,,,,
JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation,,,,Yu Zeng (None) | Vishal M. Patel (Johns Hopkins University) | Haochen Wang (Toyota Technological Institute At Chicago) | Xun Huang (NVIDIA) | Ting-Chun Wang (NVIDIA) | Ming-Yu Liu (NVIDIA) | Yogesh Balaji (NVIDIA),,,,,,,
PEM: Prototype-based Efficient MaskFormer for Image Segmentation,"Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.",http://arxiv.org/abs/2402.19422v2,,Niccol?? Cavagnero (Polytechnic Institute Of Turin) | Gabriele Rosi (Polytechnic Institute Of Turin) | Claudia Cuttano (Polytechnic Institute Of Turin) | Francesca Pistilli (Polytechnic Institute Of Turin) | Marco Ciccone (Politecnico Di Torino) | Giuseppe Averta (Polytechnic Of Turin) | Fabio Cermelli (Politecnico Di Torino),2024-02-29 18:21:54+00:00,,,,,,
SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation,,,,"Junyan Ye (SUN YAT-SEN UNIVERSITY) | Qiyan Luo (None) | Jinhua Yu (Sun Yat-Sen University, School Of Geospatial Engineering And Science) | Huaping Zhong (SenseTime) | Zhimeng Zheng (Zhejiang University) | Conghui He (None) | Weijia Li (Sun Yat-Sen University)",,,,,,,
StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation,,,,Yining Shi (Tsinghua University) | Kun JIANG (Tsinghua University) | Ke Wang (Didi Research) | Jiusi Li (Tongji University) | Yunlong Wang (Tsinghua University) | Mengmeng Yang (None) | Diange Yang (Tsinghua University),,,,,,,
HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction,"Vectorized High-Definition (HD) map construction requires predictions of the category and point coordinates of map elements (e.g. road boundary, lane divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based on point-level representation learning for regressing accurate point coordinates. However, this pipeline has limitations in obtaining element-level information and handling element-level failures, e.g. erroneous element shape or entanglement between elements. To tackle the above issues, we propose a simple yet effective HybrId framework named HIMap to sufficiently learn and interact both point-level and element-level information. Concretely, we introduce a hybrid representation called HIQuery to represent all map elements, and propose a point-element interactor to interactively extract and encode the hybrid information of elements, e.g. point position and element shape, into the HIQuery. Additionally, we present a point-element consistency constraint to enhance the consistency between the point-level and element-level information. Finally, the output point-element integrated HIQuery can be directly converted into map elements' class, point coordinates, and mask. We conduct extensive experiments and consistently outperform previous methods on both nuScenes and Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.",http://arxiv.org/abs/2403.08639v1,,Yi ZHOU (Samsung Research China-Beijing(SRCB)) | Hui Zhang (Samsung Rearch China-Beijing(SRCB)) | Jiaqian Yu (None) | Yifan Yang (Samsung) | Sangil Jung (Samsung) | Seung-In Park (Samsung Advanced Institute Of Technology) | ByungIn Yoo (Samsung Advanced Institute Of Technology),2024-03-13 15:51:23+00:00,,,,,,
Multiplane Prior Guided Few-Shot Aerial Scene Rendering,,,,Zihan Gao (Xidian University) | Licheng Jiao (Xi'an University Of Electronic Science And Technology) | Lingling Li (Xidian University) | Xu Liu (Xidian University) | Fang Liu (Xidian University) | Puhua Chen (None) | Yuwei Guo (Xidian University),,,,,,,
CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update,"Leveraging large language models (LLMs) to integrate off-the-shelf tools (e.g., visual models and image processing functions) is a promising research direction to build powerful visual assistants for solving diverse visual tasks. However, the learning capability is rarely explored in existing methods, as they freeze the used tools after deployment, thereby limiting the generalization to new environments requiring specific knowledge. In this paper, we propose CLOVA, a Closed-LOop Visual Assistant to address this limitation, which encompasses inference, reflection, and learning phases in a closed-loop framework. During inference, LLMs generate programs and execute corresponding tools to accomplish given tasks. The reflection phase introduces a multimodal global-local reflection scheme to analyze whether and which tool needs to be updated based on environmental feedback. Lastly, the learning phase uses three flexible manners to collect training data in real-time and introduces a novel prompt tuning scheme to update the tools, enabling CLOVA to efficiently learn specific knowledge for new environments without human involvement. Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.",http://arxiv.org/abs/2312.10908v1,,"Zhi Gao (Peking University) | Yuntao Du. (Nanjing University) | Xintong Zhang (Beijing Institute For General Artificial Intelligence) | Xiaojian Ma (University Of California, Los Angeles) | Wenjuan Han (Beijing Jiao Tong University) | Song-Chun Zhu (UCLA) | Qing Li (Beijing Institute For General Artificial Intelligence (BIGAI))",2023-12-18 03:34:07+00:00,,,,,,
GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning,"Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.",http://arxiv.org/abs/2312.11461v1,,Ye Yuan (NVIDIA Research) | Xueting Li (NVIDIA) | Yangyi Huang (Zhejiang University) | Shalini De Mello (NVIDIA Research) | Koki Nagano (None) | Jan Kautz (NVIDIA) | Umar Iqbal (None),2023-12-18 18:59:12+00:00,,,,,,
X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization,,,,Anna Kukleva (MPII) | Fadime Sener (None) | Edoardo Remelli (EPFL - EPF Lausanne) | Bugra Tekin (Meta) | Eric Sauser (Meta) | Bernt Schiele (Max Planck Institute For Informatics) | Shugao Ma (Meta),,,,,,,
One-step Diffusion with Distribution Matching Distillation,"Diffusion models generate high-quality images but require dozens of forward passes. We introduce Distribution Matching Distillation (DMD), a procedure to transform a diffusion model into a one-step image generator with minimal impact on image quality. We enforce the one-step image generator match the diffusion model at distribution level, by minimizing an approximate KL divergence whose gradient can be expressed as the difference between 2 score functions, one of the target distribution and the other of the synthetic distribution being produced by our one-step generator. The score functions are parameterized as two diffusion models trained separately on each distribution. Combined with a simple regression loss matching the large-scale structure of the multi-step diffusion outputs, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.",http://arxiv.org/abs/2311.18828v3,,Tianwei Yin (Massachusetts Institute Of Technology) | Micha??l Gharbi (Massachusetts Institute Of Technology) | Richard Zhang (Adobe Systems) | Eli Shechtman (Adobe) | Fredo Durand (Massachusetts Institute Of Technology) | William Freeman (MIT And Google) | Taesung Park (Adobe Systems),2023-11-30 18:59:20+00:00,,,,,,
"Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges","Surveillance videos are an essential component of daily life with various critical applications, particularly in public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory semantic understanding, although they have obtained considerable performance. To address this issue, we propose a new research direction of surveillance video-and-language understanding, and construct the first multimodal surveillance video dataset. We manually annotate the real-world surveillance dataset UCF-Crime with fine-grained event content and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains 23,542 sentences, with an average length of 20 words, and its annotated videos are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four multimodal tasks on this newly created dataset, which serve as new baselines for surveillance video-and-language understanding. Through our experiments, we find that mainstream models used in previously publicly available datasets perform poorly on surveillance video, which demonstrates the new challenges in surveillance video-and-language understanding. To validate the effectiveness of our UCA, we conducted experiments on multimodal anomaly detection. The results demonstrate that our multimodal surveillance learning can improve the performance of conventional anomaly detection tasks. All the experiments highlight the necessity of constructing this dataset to advance surveillance AI. The link to our dataset is provided at: https://xuange923.github.io/Surveillance-Video-Understanding.",http://arxiv.org/abs/2309.13925v2,,"Tongtong Yuan (Beijing University Of Technology) | Xuange Zhang (Beijing University Of Technology) | Kun Liu (Beijing University Of Posts And Telecommunications) | Bo Liu (Beijing University Of Technology) | Chen Chen (None) | Jian Jin (China Academy Of Information And Communications Technology) | Zhenzhen Jiao (Beijing Teleinfo Technology, CAICT)",2023-09-25 07:46:56+00:00,,,,,,
Interactive Continual Learning: Fast and Slow Thinking,"Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods.",http://arxiv.org/abs/2403.02628v1,,Biqing Qi (Harbin Institute Of Technology & Tsinghua University & Frontis.AI) | Xinquan Chen (Harbin Institute Of Technology) | Junqi Gao (Harbin Institute Of Technology) | Dong Li (Harbin Institute Of Technology) | Jianxing Liu (Harbin Institute Of Technology) | Ligang Wu (Harbin Institute Of Technology) | Bowen Zhou (Tsinghua University),2024-03-05 03:37:28+00:00,,,,,,
Weakly Misalignment-free Adaptive Feature Alignment for UAVs-based Multimodal Object Detection,,,,Chen Chen (National University Of Defense Technology) | Jiahao Qi (None) | Xingyue Liu (National University Of Defense Technology) | Kangcheng Bin (National University Of Defense Technology) | Ruigang Fu (None) | Xikun Hu (None) | Ping Zhong (National University Of Defense Technology),,,,,,,
Vlogger: Make Your Dream A Vlog,"In this work, we present Vlogger, a generic AI system for generating a minute-level video blog (i.e., vlog) of user descriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog professionals, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. Moreover, we introduce a novel video diffusion model, ShowMaker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its capacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor. The code and model is all available at https://github.com/zhuangshaobin/Vlogger.",http://arxiv.org/abs/2401.09414v1,,"Shaobin Zhuang (Shanghai AI Laboratory) | Kunchang Li (SIAT, UCAS) | Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) | Yaohui Wang (Shanghai AI Laboratory) | Ziwei Liu (Nanyang Technological University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Yali Wang (SIAT, Chinese Academy Of Sciences)",2024-01-17 18:55:12+00:00,,,,,,
DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization,"Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed.",http://arxiv.org/abs/2403.06912v2,,"Jiahe Li (Beijing University Of Aeronautics And Astronautics) | Jiawei Zhang (Beijing University Of Aeronautics And Astronautics) | Xiao Bai (Beijing University Of Aeronautics And Astronautics) | Jin Zheng (Beijing University Of Aeronautics And Astronautics) | Xin Ning (Institute Of Semiconductors, Chinese Academy Of Sciences) | Jun Zhou (Griffith University) | Lin Gu (RIKEN / The University Of Tokyo)",2024-03-11 17:02:11+00:00,,,,,,
NeRF Analogies - Example-Based Visual Attribute Transfer for NeRFs,"A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines.",http://arxiv.org/abs/2402.08622v1,,"Michael Fischer (University College London) | Zhengqin Li (Facebook) | Thu Nguyen-Phuoc (Reality Labs Research, Meta) | Alja?? Bo??i?? (Facebook) | Zhao Dong (Meta RL Research) | Carl Marshall (Reality Labs Research) | Tobias Ritschel (University College London)",2024-02-13 17:47:42+00:00,,,,,,
AHIVE: Anatomy-aware Hierarchical Vision Encoding for Interactive Radiology Report Retrieval,,,,Sixing Yan (None) | William K. Cheung (Hong Kong Baptist University) | Ivor Tsang (A*STAR) | Wan Hang Keith Chiu (Queen Elizabeth Hospital) | Tong Terence (The Chinese University Of Hong Kong) | Ka Chun Cheung (NVIDIA) | Simon See (NVIDIA),,,,,,,
Diffusion-based Blind Text Image Super-Resolution,,,,Yuzhe Zhang (None) | Jiawei Zhang (Sensetime) | Hao Li (Beihang University) | Zhouxia Wang (Nanyang Technological University) | Luwei Hou (Beihang University) | Dongqing Zou (Sensetime Research) | Liheng Bian (Beijing Institute Of Technology),,,,,,,
PRS: Sharp Feature Prior for Resolution-Free Surface Remeshing,"Surface reconstruction with preservation of geometric features is a challenging computer vision task. Despite significant progress in implicit shape reconstruction, state-of-the-art mesh extraction methods often produce aliased, perceptually distorted surfaces and lack scalability to high-resolution 3D shapes. We present a data-driven approach for automatic feature detection and remeshing that requires only a coarse, aliased mesh as input and scales to arbitrary resolution reconstructions. We define and learn a collection of surface-based fields to (1) capture sharp geometric features in the shape with an implicit vertexwise model and (2) approximate improvements in normals alignment obtained by applying edge-flips with an edgewise model. To support scaling to arbitrary complexity shapes, we learn our fields using local triangulated patches, fusing estimates on complete surface meshes. Our feature remeshing algorithm integrates the learned fields as sharp feature priors and optimizes vertex placement and mesh connectivity for maximum expected surface improvement. On a challenging collection of high-resolution shape reconstructions in the ABC dataset, our algorithm improves over state-of-the-art by 26% normals F-score and 42% perceptual $\text{RMSE}_{\text{v}}$.",http://arxiv.org/abs/2311.18494v1,,Natalia Soboleva (Skolkovo Institute Of Science And Technology) | Olga Gorbunova (Skolkovo Institute Of Science And Technology) | Maria Ivanova (Skoltech) | Evgeny Burnaev (Skolkovo Institute Of Science And Technology) | Matthias Nie??ner (Technical University Of Munich) | Denis Zorin (New York University) | Alexey Artemov (Technische Universit??t M??nchen),2023-11-30 12:15:45+00:00,,,,,,
MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading,"Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: https://ubisoft-laforge.github.io/character/mosar/",http://arxiv.org/abs/2312.13091v2,,"Abdallah Dib (Ubisoft) | Luiz Gustavo Hafemann (Ubisoft La Forge) | Emeline Got (La Forge - Ubisoft ) | Trevor Anderson (Ubisoft) | Amin Fadaeinejad (None) | Rafael M. O. Cruz (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Marc-Andr?? Carbonneau (Ubisoft)",2023-12-20 15:12:53+00:00,,,,,,
UniHuman: A Unified Model For Editing Human Images in the Wild,"Human image editing includes tasks like changing a person's pose, their clothing, or editing the image according to a text prompt. However, prior work often tackles these tasks separately, overlooking the benefit of mutual reinforcement from learning them jointly. In this paper, we propose UniHuman, a unified model that addresses multiple facets of human image editing in real-world settings. To enhance the model's generation quality and generalization capacity, we leverage guidance from human visual encoders and introduce a lightweight pose-warping module that can exploit different pose representations, accommodating unseen textures and patterns. Furthermore, to bridge the disparity between existing human editing benchmarks with real-world data, we curated 400K high-quality human image-text pairs for training and collected 2K human images for out-of-domain testing, both encompassing diverse clothing styles, backgrounds, and age groups. Experiments on both in-domain and out-of-domain test sets demonstrate that UniHuman outperforms task-specific models by a significant margin. In user studies, UniHuman is preferred by the users in an average of 77% of cases.",http://arxiv.org/abs/2312.14985v1,,Nannan Li (Boston University) | Qing Liu (Adobe Systems) | Krishna Kumar Singh (Adobe Systems) | Yilin Wang (Adobe Systems) | Jianming Zhang (Adobe Systems) | Bryan A. Plummer (None) | Zhe Lin (Adobe Research),2023-12-22 05:00:30+00:00,,,,,,
Unsupervised Natural Light Uncalibrated Photometric Stereo with Spin Lights,,,,Zongrui Li (Nanyang Technological University) | Zhan Lu (Nanyang Technological University) | Haojie Yan (Zhejiang University) | Boxin Shi (Peking University) | Gang Pan (Zhejiang University) | Qian Zheng (Zhejiang University) | Xudong Jiang (Nanyang Technological University),,,,,,,
Plug-and-Play Diffusion Distillation,,,,Yi-Ting Hsiao (University Of Michigan - Ann Arbor) | Siavash Khodadadeh (Adobe Systems) | Kevin Duarte (Adobe Systems) | Wei-An Lin (Adobe Systems) | Hui Qu (Adobe) | Mingi Kwon (None) | Ratheesh Kalarot (Adobe Systems),,,,,,,
A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network,"Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.",http://arxiv.org/abs/2403.03739v1,,Ruichen Ma (University Of Electronic Science And Technology Of China) | Guanchao Qiao (University Of Electronic Science And Technology Of China) | Yian Liu (University Of Electronic Science And Technology Of China) | Liwei Meng (University Of Electronic Science And Technology Of China) | Ning Ning (University Of Electronic Science And Technology Of China) | Yang Liu (University Of Electronic Science And Technology Of China) | Shaogang Hu (University Of Electronic Science And Technology Of China),2024-03-06 14:28:49+00:00,,,,,,
Efficient Dataset Distillation via Minimax Diffusion,"Dataset distillation reduces the storage and computational consumption of training a network by generating a small surrogate dataset that encapsulates rich information of the original large-scale one. However, previous distillation methods heavily rely on the sample-wise iterative optimization scheme. As the images-per-class (IPC) setting or image resolution grows larger, the necessary computation will demand overwhelming time and resources. In this work, we intend to incorporate generative diffusion techniques for computing the surrogate dataset. Observing that key factors for constructing an effective surrogate dataset are representativeness and diversity, we design additional minimax criteria in the generative training to enhance these facets for the generated images of diffusion models. We present a theoretical model of the process as hierarchical diffusion control demonstrating the flexibility of the diffusion process to target these criteria without jeopardizing the faithfulness of the sample to the desired distribution. The proposed method achieves state-of-the-art validation performance while demanding much less computational resources. Under the 100-IPC setting on ImageWoof, our method requires less than one-twentieth the distillation time of previous methods, yet yields even better performance. Source code available in https://github.com/vimar-gu/MinimaxDiffusion.",http://arxiv.org/abs/2311.15529v1,,"Jianyang Gu (Zhejiang University) | Saeed Vahidian (Duke University) | Vyacheslav Kungurtsev (Czech Technical Univeresity In Prague, Czech Technical University Of Prague) | Haonan Wang (National University Of Singaore, National University Of Singapore) | Wei Jiang (Zhejiang University) | Yang You (National University Of Singapore) | Yiran Chen (Duke University)",2023-11-27 04:22:48+00:00,,,,,,
MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild,,,,Zeren Jiang (ETHZ - ETH Zurich) | Chen Guo (ETH Zurich) | Manuel Kaufmann (ETH Zurich) | Tianjian Jiang (None) | Julien Valentin (Microsoft) | Otmar Hilliges (None) | Jie Song (ETHZ - ETH Zurich),,,,,,,
The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective,"In recent years, the thriving development of research related to egocentric videos has provided a unique perspective for the study of conversational interactions, where both visual and audio signals play a crucial role. While most prior work focus on learning about behaviors that directly involve the camera wearer, we introduce the Ego-Exocentric Conversational Graph Prediction problem, marking the first attempt to infer exocentric conversational interactions from egocentric videos. We propose a unified multi-modal, multi-task framework -- Audio-Visual Conversational Attention (Av-CONV), for the joint prediction of conversation behaviors -- speaking and listening -- for both the camera wearer as well as all other social partners present in the egocentric video. Specifically, we customize the self-attention mechanism to model the representations across-time, across-subjects, and across-modalities. To validate our method, we conduct experiments on a challenging egocentric video dataset that includes first-person perspective, multi-speaker, and multi-conversation scenarios. Our results demonstrate the superior performance of our method compared to a series of baselines. We also present detailed ablation studies to assess the contribution of each component in our model. Project page: https://vjwq.github.io/AV-CONV/.",http://arxiv.org/abs/2312.12870v1,,Wenqi Jia (None) | Miao Liu (META AI) | Hao Jiang (Facebook) | Ishwarya Ananthabhotla (Meta Reality Labs Research) | James Rehg (None) | Vamsi Krishna Ithapu (Facebook Reality Labs) | Ruohan Gao (Stanford University),2023-12-20 09:34:22+00:00,,,,,,
Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline,"Tracking using bio-inspired event cameras has drawn more and more attention in recent years. Existing works either utilize aligned RGB and event data for accurate tracking or directly learn an event-based tracker. The first category needs more cost for inference and the second one may be easily influenced by noisy events or sparse spatial resolution. In this paper, we propose a novel hierarchical knowledge distillation framework that can fully utilize multi-modal / multi-view information during training to facilitate knowledge transfer, enabling us to achieve high-speed and low-latency visual tracking during testing by using only event signals. Specifically, a teacher Transformer-based multi-modal tracking framework is first trained by feeding the RGB frame and event stream simultaneously. Then, we design a new hierarchical knowledge distillation strategy which includes pairwise similarity, feature representation, and response maps-based knowledge distillation to guide the learning of the student Transformer network. Moreover, since existing event-based tracking datasets are all low-resolution ($346 \times 260$), we propose the first large-scale high-resolution ($1280 \times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. The dataset, evaluation toolkit, and source code are available on \url{https://github.com/Event-AHU/EventVOT_Benchmark}",http://arxiv.org/abs/2309.14611v1,,Xiao Wang (Anhui University) | Shiao Wang (None) | Chuanming Tang (University Of Chinese Academy Of Sciences) | Lin Zhu (Beijing Institute Of Technology) | Bo Jiang (Anhui University) | Yonghong Tian (Peking University) | Jin Tang (Anhui University),2023-09-26 01:42:26+00:00,,,,,,
MLP Can Be A Good Transformer Learner,,,,Sihao Lin (Royal Melbourne Institute Of Technology) | Pumeng Lyu (Shanghai AI Lab) | Dongrui Liu (None) | Tao Tang (SYSU) | Xiaodan Liang (Sun Yat-Sen University) | Andy Song (Royal Melbourne Institute Of Technology) | Xiaojun Chang (University Of Technology Sydney),,,,,,,
Benchmarking the Robustness of Document Layout Analysis Models,,,,Yufan Chen (None) | Jiaming Zhang (KIT) | Kunyu Peng (KIT) | Junwei Zheng (Karlsruhe Institute Of Technology) | Ruiping Liu (Karlsruher Institut F??r Technologie) | Philip H.S. Torr (University Of Oxford) | Rainer Stiefelhagen (Karlsruhe Institute Of Technology),,,,,,,
DiffusionLight: Light Probes for Free by Painting a Chrome Ball,"We present a simple yet effective technique to estimate lighting in a single input image. Current techniques rely heavily on HDR panorama datasets to train neural networks to regress an input with limited field-of-view to a full environment map. However, these approaches often struggle with real-world, uncontrolled settings due to the limited diversity and size of their datasets. To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image. Despite its simplicity, this task remains challenging: the diffusion models often insert incorrect or inconsistent objects and cannot readily generate images in HDR format. Our research uncovers a surprising relationship between the appearance of chrome balls and the initial diffusion noise map, which we utilize to consistently generate high-quality chrome balls. We further fine-tune an LDR difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure bracketing for HDR light estimation. Our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios.",http://arxiv.org/abs/2312.09168v2,,Pakkapon Phongthawee (Vidyasirimedhi Institute Of Science And Technology) | Worameth Chinchuthakun (Tokyo Institute Of Technology) | Nontaphat Sinsunthithet (Vidyasirimedhi Institute Of Science And Technology) | Varun Jampani (Google Research) | Amit Raj (Google ) | Pramook Khungurn (Cornell University) | Supasorn Suwajanakorn (Vidyasirimedhi Institute Of Science And Technology),2023-12-14 17:34:53+00:00,,,,,,
An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing,,,,"Feiran Hu (Nanjing University Of Science And Technology) | Chenlin Zhang (Moonshot AI, Ltd) | Jiangliang GUO (Www.Ainnovation.Com) | Xiu-Shen Wei (Nanjing University Of Science And Technology) | Lin Zhao (Nanjing University Of Science And Technology) | Anqi Xu (University Of Toronto) | Lingyan Gao (AInnovation Lab)",,,,,,,
Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization,,,,Ziying Xia (None) | Jian Cheng (University Of Electronic Science And Technology Of China) | Siyu Liu (None) | Yongxiang Hu (University Of Electronic Science And Technology Of China) | Shiguang Wang (None) | Zhang Yijie (University Of Electronic Science And Technology Of China) | Wanli Dang (University Of Electronic Science And Technology Of China),,,,,,,
Capturing Closely Interacted Two-Person Motions with Reaction Priors,,,,"Qi Fang (None) | Yinghui Fan (None) | Yanjun Li (None) | Junting Dong (None) | Dingwei Wu (NetEase,) | Weidong Zhang (Netease Games AI Lab) | Kang Chen (None)",,,,,,,
Revisiting Single Image Reflection Removal In the Wild,"This research focuses on the issue of single-image reflection removal (SIRR) in real-world conditions, examining it from two angles: the collection pipeline of real reflection pairs and the perception of real reflection locations. We devise an advanced reflection collection pipeline that is highly adaptable to a wide range of real-world reflection scenarios and incurs reduced costs in collecting large-scale aligned reflection pairs. In the process, we develop a large-scale, high-quality reflection dataset named Reflection Removal in the Wild (RRW). RRW contains over 14,950 high-resolution real-world reflection pairs, a dataset forty-five times larger than its predecessors. Regarding perception of reflection locations, we identify that numerous virtual reflection objects visible in reflection images are not present in the corresponding ground-truth images. This observation, drawn from the aligned pairs, leads us to conceive the Maximum Reflection Filter (MaxRF). The MaxRF could accurately and explicitly characterize reflection locations from pairs of images. Building upon this, we design a reflection location-aware cascaded framework, specifically tailored for SIRR. Powered by these innovative techniques, our solution achieves superior performance than current leading methods across multiple real-world benchmarks. Codes and datasets will be publicly available.",http://arxiv.org/abs/2311.17320v1,,"Yurui Zhu (University Of Science And Technology Of China) | Bo Li (Vivo Mobile Communication Co.,Ltd.) | Xueyang Fu (University Of Science And Technology Of China) | Peng-Tao Jiang (Vivo Mobile Communication (Hangzhou) Co., Ltd.) | Hao Zhang (Vivo Mobile Communication ???Hangzhou???Co., Ltd) | Qibin Sun (University Of Science And Technology Of China) | Zheng-Jun Zha (University Of Science And Technology Of China) | Jinwei Chen (Vivo Mobile Communication Co., Ltd.)",2023-11-29 02:31:10+00:00,,,,,,
ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks,,,,"Andrea Rosasco (Universit?? Degli Studi Di Genova, Istituto Italiano Di Tecnologia) | Stefano Berti (Universit?? Degli Studi Di Genova, Istituto Italiano Di Tecnologia) | Giulia Pasquale (Istituto Italiano Di Tecnologia) | Damiano Malafronte (Istituto Italiano Di Tecnologia) | Shogo Sato (Sony Interactive Entertainment) | Hiroyuki Segawa (Sony Interactive Entertainment) | Tetsugo Inada (Sony Interactive Entertainment) | Lorenzo Natale (Istituto Italiano Di Tecnologia)",,,,,,,
AlignMiF: Geometry-Aligned Multimodal Implicit Field for Enhanced LiDAR-Camera Joint Synthesis,"Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned multimodal implicit field with two proposed modules: Geometry-Aware Alignment (GAA) and Shared Geometry Initialization (SGI). These modules effectively align the coarse geometry across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).",http://arxiv.org/abs/2402.17483v1,,"Tao Tang (SYSU) | Guangrun Wang (University Of Oxford) | Yixing Lao (None) | Peng Chen (Alibaba Group) | Jie Liu (North China University Of Technology) | Liang Lin (SUN YAT-SEN UNIVERSITY, Tsinghua University) | Kaicheng Yu (Alibaba Group) | Xiaodan Liang (Sun Yat-Sen University)",2024-02-27 13:08:47+00:00,,,,,,
The Neglected Tails in Vision-Language Models,,,,Shubham Parashar (Texas A&M University - College Station) | Tian Liu (Texas A&M University - College Station) | Zhiqiu Lin (Carnegie Mellon University) | Xiangjue Dong (Texas A&Amp;M University - College Station) | Yanan Li (Zhejiang Lab) | James Caverlee (Texas A&M University) | Deva Ramanan (Carnegie Mellon University) | Shu Kong (None),,,,,,,
Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction,"State-of-the-art single-view 360-degree room layout reconstruction methods formulate the problem as a high-level 1D (per-column) regression task. On the other hand, traditional low-level 2D layout segmentation is simpler to learn and can represent occluded regions, but it requires complex post-processing for the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to render 1D layout depth regression from the 2D segmentation map in a differentiable and occlusion-aware way, marrying the merits of both sides. Specifically, our model predicts floor-plan density for the input equirectangular 360-degree image. Formulating the 2D layout representation as a density field enables us to employ `flattened' volume rendering to form 1D layout depth regression. In addition, we propose a novel 3D warping augmentation on layout to improve generalization. Finally, we re-implement recent room layout reconstruction methods into our codebase for benchmarking and explore modern backbones and training techniques to serve as the strong baseline. Our model significantly outperforms previous arts. The code will be made available upon publication.",http://arxiv.org/abs/2311.18695v1,,Cheng Sun (NVIDIA) | Wei-En Tai (National Tsinghua University) | Yu-Lin Shih (National Tsinghua University) | Kuan-Wei Chen (National Tsinghua University) | Yong-Jing Syu (National Tsinghua University) | Kent Selwyn The (National Tsinghua University) | Yu-Chiang Frank Wang (NVIDIA) | Hwann-Tzong Chen (National Tsing Hua University),2023-11-30 16:42:24+00:00,,,,,,
Cross-view and Cross-pose Completion for 3D Human Understanding,,,,Matthieu Armando (Naver Labs Europe) | Salma Galaaoui (Naver Labs Europe) | Fabien Baradel (NAVER LABS Europe) | Thomas Lucas (Naver Labs Europe) | Vincent Leroy (Naver Labs Europe) | Romain BR??GIER (None) | Philippe Weinzaepfel (Naver Labs Europe) | Gr??gory Rogez (Naver Labs Europe),,,,,,,
NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images,,,,Yufei Han (None) | Heng Guo (Beijing University Of Posts And Telecommunications) | Koki Fukai (Osaka University) | Hiroaki Santo (Osaka University) | Boxin Shi (Peking University) | Fumio Okura (Osaka University) | Zhanyu Ma (Beijing University Of Post And Telecommunication) | Yunpeng Jia (Beijing University Of Posts And Telecommunications),,,,,,,
Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction,"Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel ""Fine-grained Visual-Semantic Interaction"" (FiVE) framework for WSI classification. It is designed to enhance the model's generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments.",http://arxiv.org/abs/2402.19326v1,,"Hao Li (Xiamen University) | Ying Chen (Xiamen University) | Yifei Chen (Huawei) | Rongshan Yu (National University Of Singapore) | Wenxian Yang (Aginome Scientific) | Liansheng Wang (Xiamen University, Tsinghua University) | Bowen Ding (Shanghai Jiao Tong University) | Yuchen Han (Shanghai Jiao Tong University)",2024-02-29 16:29:53+00:00,,,,,,
Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation,"Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \url{https://hcplayercvpr2024.github.io}.",http://arxiv.org/abs/2403.05239v1,,Junyan Wang (University Of New South Wales) | Zhenhong Sun (University Of New South Wales) | Stewart Tan (Alibaba DAMO Academy) | Xuanbai Chen (Carnegie Mellon University) | Weihua Chen (Alibaba Group) | Li (None) | Cheng Zhang (Carnegie Mellon University) | Yang Song (University Of New South Wales),2024-03-08 11:59:32+00:00,,,,,,
MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception,"It is a long-lasting goal to design an embodied system that can solve long-horizon open-world tasks in human-like ways. However, existing approaches usually struggle with compound difficulties caused by the logic-aware decomposition and context-aware execution of these tasks. To this end, we introduce MP5, an open-ended multimodal embodied system built upon the challenging Minecraft simulator, which can decompose feasible sub-objectives, design sophisticated situation-aware plans, and perform embodied action control, with frequent communication with a goal-conditioned active perception scheme. Specifically, MP5 is developed on top of recent advances in Multimodal Large Language Models (MLLMs), and the system is modulated into functional modules that can be scheduled and collaborated to ultimately solve pre-defined context- and process-dependent tasks. Extensive experiments prove that MP5 can achieve a 22% success rate on difficult process-dependent tasks and a 91% success rate on tasks that heavily depend on the context. Moreover, MP5 exhibits a remarkable ability to address many open-ended tasks that are entirely novel.",http://arxiv.org/abs/2312.07472v3,,Yiran Qin (The Chinese University Of Hong Kong(Shenzhen)) | Enshen Zhou (Shanghai AI Laboratory) | Qichang Liu (None) | Zhenfei Yin (University Of Sydney) | Lu Sheng (Beihang University) | Ruimao Zhang (The Chinese University Of Hong Kong (Shenzhen)) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Jing Shao (Shanghai AI Laboratory),2023-12-12 17:55:45+00:00,,,,,,
Multi-View Attentive Contextualization for Multi-View 3D Object Detection,,,,Xianpeng Liu (North Carolina State University) | Ce Zheng (University Of Central Florida) | Ming Qian (None) | Nan Xue (Ant Group) | Chen Chen (None) | Zhebin Zhang (OPPO) | Chen Li (Innopeak Technology) | Tianfu Wu (None),,,,,,,
A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution,,,,Zhixiong Yang (National University Of Defense Technology) | Jingyuan Xia (National University Of Defense Technology) | Shengxi Li (Beihang University) | Xinghua Huang (National University Of Defense Technology) | Shuanghui Zhang (National University Of Defense Technology) | Zhen Liu (National University Of Defense Technology) | Yaowen Fu (National University Of Defense Technology) | Yongxiang Liu (National University Of Defense Technology),,,,,,,
Point Cloud Pre-training with Diffusion Models,,,,Xiao Zheng (None) | Xiaoshui Huang (Shanghai AI Laboratory) | Guofeng Mei (Fondazione Bruno Kessler) | Zhaoyang Lyu (Shanghai AI Laboratory) | Yuenan Hou (Shanghai AI Laboratory) | Wanli Ouyang (University Of Sydney) | Bo Dai (Shanghai AI Laboratory) | Yongshun Gong (Shandong University),,,,,,,
Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation,"In human-centric content generation, the pre-trained text-to-image models struggle to produce user-wanted portrait images, which retain the identity of individuals while exhibiting diverse expressions. This paper introduces our efforts towards personalized face generation. To this end, we propose a novel multi-modal face generation framework, capable of simultaneous identity-expression control and more fine-grained expression synthesis. Our expression control is so sophisticated that it can be specialized by the fine-grained emotional vocabulary. We devise a novel diffusion model that can undertake the task of simultaneously face swapping and reenactment. Due to the entanglement of identity and expression, it's nontrivial to separately and precisely control them in one framework, thus has not been explored yet. To overcome this, we propose several innovative designs in the conditional diffusion model, including balancing identity and expression encoder, improved midpoint sampling, and explicitly background conditioning. Extensive experiments have demonstrated the controllability and scalability of the proposed framework, in comparison with state-of-the-art text-to-image, face swapping, and face reenactment methods.",http://arxiv.org/abs/2401.01207v1,,"Renshuai Liu (Xiamen University) | Bowen Ma (NetEase,) | Wei Zhang (None) | Zhipeng Hu (Leihuo Game, NetEase) | Changjie Fan (Netease, Fuxi AI Lab) | Tangjie Lv (NetEase,) | Yu Ding (Fuxi AI Lab In Netease) | Xuan Cheng (Xiamen University)",2024-01-02 13:28:39+00:00,,,,,,
SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields,"In rapidly-evolving domains such as autonomous driving, the use of multiple sensors with different modalities is crucial to ensure high operational precision and stability. To correctly exploit the provided information by each sensor in a single common frame, it is essential for these sensors to be accurately calibrated. In this paper, we leverage the ability of Neural Radiance Fields (NeRF) to represent different sensors modalities in a common volumetric representation to achieve robust and accurate spatio-temporal sensor calibration. By designing a partitioning approach based on the visible part of the scene for each sensor, we formulate the calibration problem using only the overlapping areas. This strategy results in a more robust and accurate calibration that is less prone to failure. We demonstrate that our approach works on outdoor urban scenes by validating it on multiple established driving datasets. Results show that our method is able to get better accuracy and robustness compared to existing methods.",http://arxiv.org/abs/2311.15803v2,,"Quentin HERAU (Huawei/University Of Burgundy) | Nathan Piasco (Huawei Technologies Ltd.) | Moussab Bennehar (Huawei Noah's Ark Lab) | Luis Guiller,O Roldao Jimenez (Huawei Technologies Ltd.) | Dzmitry Tsishkou (Huawei Technologies Ltd.) | MigniotCyrille (University Of Burgundy) | Mod??lisation Information Syst??mes (Universit?? De Picardie Jules-Verne) | Cedric Demonceaux (Universit?? De Bourgogne)",2023-11-27 13:25:47+00:00,,,,,,
Boosting Neural Representations for Videos with a Conditional Decoder,"Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts multiple baseline INRs in the reconstruction quality and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs. Code is available at https://github.com/Xinjie-Q/Boosting-NeRV.",http://arxiv.org/abs/2402.18152v3,,XINJIE ZHANG (The Hong Kong University Of Science And Technology) | Ren Yang (Microsoft Research Asia) | Dailan He (The Chinese University Of Hong Kong) | Xingtong Ge (Beijing Institute Of Technology) | Tongda Xu (Tsinghua University) | Yan Wang (Tsinghua University) | Hongwei Qin (SenseTime Co.) | Jun Zhang (The Hong Kong University Of Science And Technology),2024-02-28 08:32:19+00:00,,,,,,
DIMAT: Decentralized Iterative Merging-And-Training for Deep Learning Models,,,,Nastaran Saadati (Iowa State University) | Minh Pham (New York University) | Nasla Saleem (Iowa State University) | Joshua R. Waite (Iowa State University) | Aditya Balu (Iowa State University) | Zhanhong Jiang (Iowa State University) | Chinmay Hegde (New York University) | Soumik Sarkar (Iowa State University),,,,,,,
Building Optimal Neural Architectures using Interpretable Knowledge,,,,Keith Mills (University Of Alberta) | Fred Han (Huawei Technologies Ltd.) | Mohammad Salameh (Huawei Technologies Canada Ltd.) | Shengyao Lu (University Of Alberta) | CHUNHUA ZHOU (Huawei Technologies Ltd.) | Jiao He (Huawei) | Fengyu Sun (Tongji University) | Di Niu (University Of Alberta),,,,,,,
PREGO: online mistake detection in PRocedural EGOcentric videos,,,,"Alessandro Flaborea (None) | Guido M. D'Amely Di Melendugno (University Of Roma ""La Sapienza"") | Leonardo Plini (Sapienza University Of Rome & INFN) | Luca Scofano (University Of Roma ""La Sapienza"") | Edoardo De Matteis (Sapienza University) | Antonino Furnari (University Of Catania) | Giovanni Maria Farinella (University Of Catania, Italy) | Fabio Galasso (None)",,,,,,,
A General and Efficient Training for Transformer via Token Expansion,,,,Wenxuan Huang (East China Normal University) | Yunhang Shen (Tencent) | Jiao Xie (Xiamen University) | Baochang Zhang (Beihang University) | Gaoqi He (East China Normal University) | Ke Li (Tencent) | Xing Sun (Tencent YouTu Lab) | Shaohui Lin (East China Normal University),,,,,,,
DiffPortrait3D: Single-Portrait Novel View Synthesis with 3D-Aware Diffusion,,,,"Yuming Gu (USC Institute For Creative Technologies, University Of Southern California) | Hongyi Xu (Bytedance) | You Xie (Bytedance) | Guoxian Song (Bytedance Inc) | Yichun Shi (ByteDance) | Di Chang (University Of Southern California) | Jing Yang (USC Institute For Creative Technologies) | Linjie Luo (ByteDance)",,,,,,,
Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving,,,,"JINLONG LI (Cleveland State University) | Baolu Li (Cleveland State University) | Zhengzhong Tu (University Of Texas At Austin) | XINYU LIU (Cleveland State University) | Qing Guo (Institute Of High Performance Computing, Singapore, A*STAR) | Felix Juefei Xu (None) | Runsheng Xu (University Of California, Los Angeles) | Hongkai Yu (Cleveland State University)",,,,,,,
MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation,,,,Petru-Daniel Tudosiu (Huawei) | Yongxin Yang (Queen Mary University Of London) | Shifeng Zhang (Huawei Technologies Ltd.) | Fei Chen (Huawei Noah's Ark Lab) | Steven McDonagh (University Of Edinburgh) | Gerasimos Lampouras (Huawei Technologies Ltd.) | Ignacio Iacobacci (Huawei Noah's Ark Lab) | Sarah Parisot (Huawei),,,,,,,
MeshPose: Unifying DensePose and 3D Body Mesh reconstruction,,,,Eric-Tuan Le (University College London) | Antonios Kakolyris (Snap) | Petros Koutras (Snap) | Himmy Tam (Snap) | Efstratios Skordos (Snap) | George Papandreou (Snap) | Riza Alp Guler (Snap) | Iasonas Kokkinos (Snap),,,,,,,
SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting,"We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.",http://arxiv.org/abs/2403.05087v1,,"Zhijing Shao (The Hong Kong University Of Science And Technology (Guangzhou)) | Wang Zhaolong (Tsinghua University) | Zhuang Li (Prometheus Vision Technology Co., Ltd.) | Duotun Wang (The Hong Kong University Of Science And Technology (Guangzhou)) | Xiangru Lin (None) | Yu Zhang (Prometheus Vision Technology Co., Ltd.) | Mingming Fan (Hong Kong University Of Science And Technology) | Zeyu Wang (The Hong Kong University Of Science And Technology (Guangzhou))",2024-03-08 06:28:09+00:00,,,,,,
Describing Differences in Image Sets with Natural Language,"How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.",http://arxiv.org/abs/2312.02974v1,,"Lisa Dunlap (University Of California, Berkeley) | Yuhui Zhang (Stanford University) | Xiaohan Wang (Stanford University) | Ruiqi Zhong (University Of California Berkeley) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Jacob Steinhardt (University Of California Berkeley) | Joseph Gonzalez (University Of California - Berkeley) | Serena Yeung (Stanford)",2023-12-05 18:59:16+00:00,,,,,,
A Vision Check-up for Language Models,,,,"Pratyusha Sharma (Massachusetts Institute Of Technology) | Tamar Rott Shaham (MIT) | Manel Baradad (Massachusetts Institute Of Technology) | Stephanie Fu (University Of California, Berkeley) | Adrian Rodriguez-Munoz (Massachusetts Institute Of Technology) | Shivam Duggal (Massachusetts Institute Of Technology) | Phillip Isola (None) | Antonio Torralba (MIT)",,,,,,,
TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing,,,,"Sherry X. Chen (University Of California, Santa Barbara) | Yaron Vaxman (Cloudinary) | Elad Ben Baruch (Cloudinary) | David Asulin (Cloudinary Ltd.) | Aviad Moreshet (Cloudinary) | Kuo-Chin Lien (Layer AI) | Misha Sra (University Of California, Santa Barbara) | Pradeep Sen (UC Santa Barbara)",,,,,,,
HMD-Poser: On-Device Realtime Human Motion Tracking from Scalable Sparse Observations,"It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users' choices for both high tracking accuracy and easy-to-wear. A lightweight temporal-spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the-art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser's on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available https://pico-ai-team.github.io/hmd-poser",http://arxiv.org/abs/2403.03561v1,,Peng Dai (Bytedance) | Yang Zhang (None) | Tao Liu (ByteDance) | ZhenFan (Bytedance) | Tianyuan Du (Bytedance) | Zhuo Su (ByteDance) | Xiaozheng Zheng (ByteDance) | Zeming Li (BYTEDANCE),2024-03-06 09:10:36+00:00,,,,,,
3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation,"Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 3D information of the current scene. Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency.",http://arxiv.org/abs/2403.09439v1,,"Songchun Zhang (Zhejiang University) | Yibo Zhang (Jilin University) | Quan Zheng (Institute Of Software, Chinese Academy Of Sciences) | Rui Ma (Jilin University) | Wei Hua (Zhejiang Lab) | Hujun Bao (Zhejiang University) | Weiwei Xu (Zhejiang University) | Changqing Zou (Zhejiang University)",2024-03-14 14:31:22+00:00,,,,,,
NC-TTT: A Noise Constrastive Approach for Test-Time Training,,,,"David OSOWIECHI (??cole De Technologie Sup??rieure, ETS Montreal) | Gustavo Vargas Hakim (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Mehrdad Noori (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Milad Cheraghalikhani (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Ali Bahri (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Moslem Yazdanpanah (??cole De Technologie Sup??rieure, Universit?? Du Qu??bec) | Ismail Ben Ayed (ETS Montreal) | Christian Desrosiers (??cole De Technologie Sup??rieure)",,,,,,,
"Sketch in VR, Make it Real: Rapid 3D Model Generation using VR 3D Sketching",,,,Tianrun Chen (Zhejiang University) | Chaotao Ding (Huzhou University) | Shangzhan Zhang (None) | Chunan Yu (Huzhou University) | Ying Zang (Huzhou University) | Zejian Li (Zhejiang University) | Sida Peng (None) | Lingyun Sun (Zhejiang University),,,,,,,
SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation,"Accurate representation in media is known to improve the well-being of the people who consume it. Generative image models trained on large web-crawled datasets such as LAION are known to produce images with harmful stereotypes and misrepresentations of cultures. We improve inclusive representation in generated images by (1) engaging with communities to collect a culturally representative dataset that we call the Cross-Cultural Understanding Benchmark (CCUB) and (2) proposing a novel Self-Contrastive Fine-Tuning (SCoFT) method that leverages the model's known biases to self-improve. SCoFT is designed to prevent overfitting on small datasets, encode only high-level information from the data, and shift the generated distribution away from misrepresentations encoded in a pretrained model. Our user study conducted on 51 participants from 5 different countries based on their self-selected national cultural affiliation shows that fine-tuning on CCUB consistently generates images with higher cultural relevance and fewer stereotypes when compared to the Stable Diffusion baseline, which is further improved with our SCoFT technique.",http://arxiv.org/abs/2401.08053v1,,"Zhixuan Liu (Carnegie Mellon University) | Peter Schaldenbrand (CMU, Carnegie Mellon University) | Beverley-Claire Okogwu (CMU, Carnegie Mellon University) | Wenxuan Peng (Nanyang Technological University) | Youngsik Yun (Dongguk University) | Andrew Hundt (Carnegie Mellon University) | Jihie Kim (Dongguk University) | Jean Oh (Carnegie Mellon University)",2024-01-16 02:10:13+00:00,,,,,,
Data-Efficient Multimodal Fusion on a Single GPU,"The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs. Additionally, we show how our method can be applied to convert pre-trained text-to-image generative models into audio-to-image ones. Code is available at: https://github.com/layer6ai-labs/fusemix.",http://arxiv.org/abs/2312.10144v2,,No??l Vouitsis (Layer 6 AI) | Zhaoyan Liu (Layer6 AI) | Satya Krishna Gorti (Layer6 AI) | Valentin Villecroze (Layer 6) | Jesse C. Cresswell (Layer 6 AI) | Guangwei Yu (Layer6 AI) | Gabriel Loaiza-Ganem (Layer 6 AI) | Maksims Volkovs (Layer6 AI),2023-12-15 19:00:07+00:00,,,,,,
MoDE: CLIP Data Experts via Clustering,,,,"Jiawei Ma (Columbia University) | Po-Yao Huang (Facebook) | Saining Xie (Facebook) | Shang-Wen Li (Facebook) | Luke Zettlemoyer (University Of Washington) | Shih-Fu Chang (Columbia University) | Wen-Tau Yih (Meta Platforms,) | Hu Xu (FAIR, Multimodal Foundation)",,,,,,,
Text-to-3D Generation with Bidirectional Diffusion using both 3D and 2D priors,"Most 3D generation research focuses on up-projecting 2D foundation models into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS) loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these methods often lead to geometric anomalies and multi-view inconsistency. Recently, researchers have attempted to improve the genuineness of 3D objects by directly training on 3D datasets, albeit at the cost of low-quality texture generation due to the limited texture diversity in 3D datasets. To harness the advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a unified framework that incorporates both a 3D and a 2D diffusion process, to preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a simple combination may yield inconsistent generation results, we further bridge them with novel bidirectional guidance. In addition, our method can be used as an initialization of optimization-based models to further improve the quality of 3D model and efficiency of optimization, reducing the generation process from 3.4 hours to 20 minutes. Experimental results have shown that our model achieves high-quality, diverse, and scalable 3D generation. Project website: https://bidiff.github.io/.",http://arxiv.org/abs/2312.04963v1,,"Lihe Ding (The Chinese University Of Hong Kong) | Shaocong Dong (Hong Kong University Of Science And Technology) | Zhanpeng Huang (SenseTime Research) | Zibin Wang (Sensetime Group Limited) | Yiyuan Zhang (The Chinese University Of Hong Kong) | Kaixiong Gong (None) | Dan Xu (Department Of Computer Science And Engineering, The Hong Kong University Of Science And Technology) | Tianfan Xue (The Chinese University Of Hong Kong)",2023-12-07 10:00:04+00:00,,,,,,
DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning,"Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.",http://arxiv.org/abs/2403.08506v1,,"Sikai Bai (The Hong Kong University Of Science And Technology) | Jie ZHANG (The Hong Kong Polytechnic University) | Song Guo (Department Of Computer Science And Engineering, Hong Kong University Of Science And Technology) | Shuaicheng Li (Sensetime Group Limited) | Jingcai Guo (The Hong Kong Polytechnic University) | Jun Hou (Sensetime) | Tao Han (Northwestern Polytechnical University) | Xiaocheng Lu (Northwestern Polytechnical University)",2024-03-11 15:58:15+00:00,,,,,,
Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models,"Solving complex visual tasks such as ""Who invented the musical instrument on the right?"" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.",http://arxiv.org/abs/2312.03052v1,,Yushi Hu (University Of Washington) | Otilia Stretcu (Google Research) | Chun-Ta Lu (Google Research) | Krishnamurthy Viswanathan (Google) | Kenji Hata (Google) | Enming Luo (Google) | Ranjay Krishna (University Of Washington) | Ariel Fuxman (Google),2023-12-05 18:58:37+00:00,,,,,,
HardMo:A Large-scale Hardcase Dataset for Motion Capture,,,,"Jiaqi Liao (None) | Chuanchen Luo (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yinuo Du (Beijing University Of Posts And Telecommunications) | Yuxi Wang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Xu-Cheng Yin (University Of Science And Technology Beijing) | Man Zhang (None) | Zhaoxiang Zhang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Junran Peng (Institute Of Automation, Chinese Academy Of Science)",,,,,,,
VideoMosaic: Connecting the Temporal Dots in Long Videos for LLMs,,,,Reuben Tan (Boston University) | Ximeng Sun (Boston University) | Ping Hu (University Of Electronic Science And Technology Of China) | Jui-Hsien Wang (Adobe Systems) | Hanieh Deilamsalehy (None) | Bryan A. Plummer (None) | Bryan Russell (Adobe Research) | Kate Saenko (Meta / Boston University),,,,,,,
Alchemist: Parametric Control of Material Properties with Diffusion Models,"We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images. Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties. Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials. Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes. We show the potential application of our model to material edited NeRFs.",http://arxiv.org/abs/2312.02970v1,,Prafull Sharma (Massachusetts Institute Of Technology) | Varun Jampani (Google Research) | Yuanzhen Li (Massachusetts Institute Of Technology) | Xuhui Jia (Google) | Dmitry Lagun (Google) | Fredo Durand (Massachusetts Institute Of Technology) | William Freeman (MIT And Google) | Mark Matthews (Google),2023-12-05 18:58:26+00:00,,,,,,
Validating Privacy-Preserving Face Recognition under a Minimum Assumption,,,,Hui Zhang (Anhui University) | Xingbo Dong (Anhui University) | YenLungLai (Anhui University) | Ying Zhou (Anhui University) | Xiaoyan ZHANG (Anhui University) | Xingguo Lv (Anhui University) | Zhe Jin (Anhui University) | Xuejun Li (Anhui University),,,,,,,
Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval,,,,Jiamian Wang (Rochester Institute Of Technology) | Guohao Sun (Rochester Institute Of Technology) | Pichao Wang (Amazon) | Dongfang Liu (Rochester Institute Of Technology) | Sohail Dianat (Rochester Institute Of Technology) | MAJID RABBANI (Rochester Institute Of Technology) | Raghuveer Rao (DEVCOM Army Research Laboratory) | ZHIQIANG TAO (Rochester Institute Of Technology),,,,,,,
Suppress and Balance: Towards Generalized Multi-Modal Face Anti-Spoofing,,,,Xun Lin (Beihang University) | Shuai Wang (Beihang University) | RIZHAO CAI (Nanyang Technological University) | Yizhong Liu (Beihang University) | Ying Fu (None) | Wenzhong Tang (Beihang University) | Zitong YU (Nanyang Technological University) | Alex C. Kot (Nanyang Technological University),,,,,,,
Universal Novelty Detection through Adaptive Contrastive Learning,,,,Hossein Mirzaei (Sharif University Of Technology) | Mojtaba Nafez (Sharif University Of Technology) | Mohammad Jafari (Sharif University Of Technology) | Mohammad Soltani (Sharif University Of Technology) | Mohammad Azizmalayeri (Amsterdam UMC) | Jafar Habibi (Sharif University Of Technology) | Mohammad Sabokrou (Okinawa Institute Of Science And Technology (OIST)) | Mohammad Rohban (Sharif University Of Technology),,,,,,,
Single-Model and Any-Modality for Video Object Tracking,,,,Zongwei Wu (Bayerische Julius-Maximilians-Universit??t W??rzburg) | Jilai Zheng (Shanghai Jiao Tong University) | Xiangxuan Ren (Shanghai Jiao Tong University) | Florin-Alexandru Vasluianu (Bayerische Julius-Maximilians-Universit??t W??rzburg) | Chao Ma (Shanghai Jiao Tong University) | Danda Paudel (None) | Luc Van Gool (ETH Zurich) | Radu Timofte (University Of W??rzburg),,,,,,,
A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning,,,,"Xiaoyang Xu (Wuhan University) | Mengda Yang (None) | Wenzhe Yi (Wuhan University) | Ziang Li (None) | Juan Wang (None) | Hongxin Hu (State University Of New York, Buffalo) | Yong ZHUANG (Wuhan University) | Yaxin Liu (Wuhan University)",,,,,,,
Going Beyond Multi-Task Dense Prediction with Synergy Embedding Models,,,,Huimin Huang (Zhejiang University) | Yawen Huang (None) | Lanfen Lin (Zhejiang University) | Ruofeng Tong (None) | Yen-Wei Chen (Ritsumeikan University) | Hao Zheng (Tencent) | Yuexiang Li (Tencent Jarvis Lab) | Yefeng Zheng (None),,,,,,,
MCNet: Rethinking the Core Ingredients for Accurate and Efficient Homography Estimation,,,,Haokai Zhu (Zhejiang University) | Si-Yuan Cao (Zhejiang University) | Jianxin Hu (Zhejiang University) | Sitong Zuo (Beijing University Of Posts And Telecommunications) | Beinan Yu (Zhejiang University) | Jiacheng Ying (Zhejiang University) | Junwei Li (Zhejiang University) | Hui-Liang Shen (None),,,,,,,
Rendering Every Pixel for High-Fidelity Geometry in 3D GANs,"3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly ""render every pixel"" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs.",http://arxiv.org/abs/2401.02411v1,,Alex Trevithick (None) | Matthew Chan (NVIDIA) | Towaki Takikawa (NVIDIA) | Umar Iqbal (None) | Shalini De Mello (NVIDIA Research) | Manmohan Chandraker (UC San Diego) | Ravi Ramamoorthi (None) | Koki Nagano (None),2024-01-04 18:50:38+00:00,,,,,,
RegionGPT: Towards Region Understanding Vision Language Model,"Vision language models (VLMs) have experienced rapid advancements through the integration of large language models (LLMs) with image-text pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model's versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, reasoning, object classification, and referring expressions comprehension.",http://arxiv.org/abs/2403.02330v1,,Qiushan Guo (The University Of Hong Kong) | Shalini De Mello (NVIDIA Research) | Danny Yin (NVIDIA) | Wonmin Byeon (NVIDIA) | Ka Chun Cheung (NVIDIA) | Yizhou Yu (The University Of Hong Kong) | Ping Luo (The University Of Hong Kong) | Sifei Liu (NVIDIA),2024-03-04 18:58:08+00:00,,,,,,
Learning for Transductive Threshold Calibration in Open-World Recognition,,,,"Qin ZHANG (Amazon) | DONGSHENG An (Amazon) | Tianjun Xiao (Amazon) | Tong He (Amazon Web Services) | Qingming Tang (Amazon, Alexa) | Ying Nian Wu (UCLA) | Joseph Tighe (Meta) | Yifan Xing (None)",,,,,,,
PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment,"Neural implicit scene representations have recently shown encouraging results in dense visual SLAM. However, existing methods produce low-quality scene reconstruction and low-accuracy localization performance when scaling up to large indoor scenes and long sequences. These limitations are mainly due to their single, global radiance field with finite capacity, which does not adapt to large scenarios. Their end-to-end pose networks are also not robust enough with the growth of cumulative errors in large scenes. To this end, we present PLGSLAM, a neural visual SLAM system which performs high-fidelity surface reconstruction and robust camera tracking in real time. To handle large-scale indoor scenes, PLGSLAM proposes a progressive scene representation method which dynamically allocates new local scene representation trained with frames within a local sliding window. This allows us to scale up to larger indoor scenes and improves robustness (even under pose drifts). In local scene representation, PLGSLAM utilizes tri-planes for local high-frequency features. We also incorporate multi-layer perceptron (MLP) networks for the low-frequency feature, smoothness, and scene completion in unobserved areas. Moreover, we propose local-to-global bundle adjustment method with a global keyframe database to address the increased pose drifts on long sequences. Experimental results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction results and tracking performance across various datasets and scenarios (both in small and large-scale indoor environments). The code will be open-sourced upon paper acceptance.",http://arxiv.org/abs/2312.09866v1,,Tianchen Deng (None) | Guole Shen (None) | Tong Qin (Shanghai Jiao Tong University) | Jianyu Wang (Shanghai Jiao Tong University) | Wentao Zhao (Shanghai Jiao Tong University) | Jingchuan Wang (None) | Danwei Wang (Nanyang Technological University) | Weidong Chen (Shanghai Jiao Tong University),2023-12-15 15:09:30+00:00,,,,,,
Three Pillars improving Vision Foundation Model Distillation for Lidar,"Self-supervised image backbones can be used to address complex 2D tasks (e.g., semantic segmentation, object discovery) very efficiently and with little or no downstream supervision. Ideally, 3D backbones for lidar should be able to inherit these properties after distillation of these powerful 2D features. The most recent methods for image-to-lidar distillation on autonomous driving data show promising results, obtained thanks to distillation methods that keep improving. Yet, we still notice a large performance gap when measuring the quality of distilled and fully supervised features by linear probing. In this work, instead of focusing only on the distillation method, we study the effect of three pillars for distillation: the 3D backbone, the pretrained 2D backbones, and the pretraining dataset. In particular, thanks to our scalable distillation method named ScaLR, we show that scaling the 2D and 3D backbones and pretraining on diverse datasets leads to a substantial improvement of the feature quality. This allows us to significantly reduce the gap between the quality of distilled and fully-supervised 3D features, and to improve the robustness of the pretrained backbones to domain gaps and perturbations.",http://arxiv.org/abs/2310.17504v2,,"Gilles Puy (Valeo.Ai) | Spyros Gidaris (Valeo.Ai) | Alexandre Boulch (Valeo.Ai) | Oriane Sim??oni (Valeo.Ai) | Corentin Sautier (ENPC, Ecole Nationale Des Ponts Et Chausees) | Patrick P??rez (None) | Andrei Bursuc (Valeo.Ai) | Renaud Marlet (INRIA)",2023-10-26 15:54:43+00:00,,,,,,
VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams,"Neural Radiance Fields (NeRFs) excel in photorealistically rendering static scenes. However, rendering dynamic, long-duration radiance fields on ubiquitous devices remains challenging, due to data storage and computational constraints. In this paper, we introduce VideoRF, the first approach to enable real-time streaming and rendering of dynamic radiance fields on mobile platforms. At the core is a serialized 2D feature image stream representing the 4D radiance field all in one. We introduce a tailored training scheme directly applied to this 2D domain to impose the temporal and spatial redundancy of the feature image stream. By leveraging the redundancy, we show that the feature image stream can be efficiently compressed by 2D video codecs, which allows us to exploit video hardware accelerators to achieve real-time decoding. On the other hand, based on the feature image stream, we propose a novel rendering pipeline for VideoRF, which has specialized space mappings to query radiance properties efficiently. Paired with a deferred shading model, VideoRF has the capability of real-time rendering on mobile devices thanks to its efficiency. We have developed a real-time interactive player that enables online streaming and rendering of dynamic scenes, offering a seamless and immersive free-viewpoint experience across a range of devices, from desktops to mobile phones.",http://arxiv.org/abs/2312.01407v1,,Liao Wang (None) | Kaixin Yao (ShanghaiTech University) | Chengcheng Guo (ShanghaiTech University) | Zhirui Zhang (ShanghaiTech University) | Qiang Hu (Shanghai Jiao Tong University) | Jingyi Yu (Shanghai Tech University) | Lan Xu (None) | Minye Wu (KU Leuven),2023-12-03 14:14:35+00:00,,,,,,
CLIP-KD: A Comprehensive Study of Distilling CLIP Models,"CLIP has become a promising language-supervised visual pre-training framework and achieves excellent performance over a wide range of tasks. This paper aims to distill small CLIP models supervised by a large teacher CLIP model. We propose several distillation strategies, including relation, feature, gradient and contrastive paradigm, to examine the impact on CLIP distillation. We show that the simplest feature mimicry with MSE loss performs best. Moreover, interactive contrastive learning and relation-based distillation are also critical in performance improvement. We apply the unified method to distill several student networks trained on 15 million (image, text) pairs. Distillation improves the student CLIP models consistently over zero-shot ImageNet classification and cross-modal retrieval benchmarks. We hope our empirical study will become an important baseline for future CLIP distillation research. The code is available at \url{https://github.com/winycg/CLIP-KD}.",http://arxiv.org/abs/2307.12732v1,,"Chuanguang Yang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Zhulin An (Institute Of Computing Technology, Chinese Academy Of Sciences) | Libo Huang (None) | Junyu Bi (None) | XinQiang Yu (Institute Of Computing Technology, Chinese Academy Of Sciences) | Han Yang (University Of The Chinese Academy Of Sciences) | Boyu Diao (None) | Yongjun Xu (None)",2023-07-24 12:24:07+00:00,,,,,,
TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models,,,,Haomiao Ni (Pennsylvania State University) | Bernhard Egger (Massachusetts Institute Of Technology) | Suhas Lohit (Mitsubishi Electric Research Labs) | Anoop Cherian (None) | Ye Wang (Mitsubishi Electric Research Labs) | Toshiaki Koike-Akino (Mitsubishi Electric Research Labs. (MERL)) | Sharon X. Huang (Pennsylvania State University) | Tim Marks (None),,,,,,,
Sequential Modeling Enables Scalable Learning for Large Vision Models,"We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, we define a common format, ""visual sentences"", in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.",http://arxiv.org/abs/2312.00785v1,,Yutong Bai (Johns Hopkins University) | Xinyang Geng (University Of California Berkeley) | Karttikeya Mangalam (University Of California Berkeley) | Amir Bar (TAU / UC Berkeley) | Alan L. Yuille (Johns Hopkins University) | Trevor Darrell (Electrical Engineering & Computer Science Department) | Jitendra Malik (University Of California At Berkeley) | Alexei A. Efros (UC Berkeley),2023-12-01 18:59:57+00:00,,,,,,
Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?,"Stable Diffusion has established itself as a foundation model in generative AI artistic applications, receiving widespread research and application. Some recent fine-tuning methods have made it feasible for individuals to implant personalized concepts onto the basic Stable Diffusion model with minimal computational costs on small datasets. However, these innovations have also given rise to issues like facial privacy forgery and artistic copyright infringement. In recent studies, researchers have explored the addition of imperceptible adversarial perturbations to images to prevent potential unauthorized exploitation and infringements when personal data is used for fine-tuning Stable Diffusion. Although these studies have demonstrated the ability to protect images, it is essential to consider that these methods may not be entirely applicable in real-world scenarios. In this paper, we systematically evaluate the use of perturbations to protect images within a practical threat model. The results suggest that these approaches may not be sufficient to safeguard image privacy and copyright effectively. Furthermore, we introduce a purification method capable of removing protected perturbations while preserving the original image structure to the greatest extent possible. Experiments reveal that Stable Diffusion can effectively learn from purified images over all protective methods.",http://arxiv.org/abs/2312.00084v1,,"Zhengyue Zhao (University Of Chinese Academy Of Sciences) | Jinhao Duan (Drexel University) | Kaidi Xu (Drexel University) | Chenan Wang (Drexel University) | Rui Zhang (None) | Zidong Du (Institute Of Computing Technology, Chinese Academy Of Sciences) | Qi Guo (Institute Of Computing Technology, Chinese Academy Of Sciences) | Xing Hu (, Chinese Academy Of Sciences)",2023-11-30 07:17:43+00:00,,,,,,
DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting,"Precipitation nowcasting is an important spatio-temporal prediction task to predict the radar echoes sequences based on current observations, which can serve both meteorological science and smart city applications. Due to the chaotic evolution nature of the precipitation systems, it is a very challenging problem. Previous studies address the problem either from the perspectives of deterministic modeling or probabilistic modeling. However, their predictions suffer from the blurry, high-value echoes fading away and position inaccurate issues. The root reason of these issues is that the chaotic evolutionary precipitation systems are not appropriately modeled. Inspired by the nature of the systems, we propose to decompose and model them from the perspective of global deterministic motion and local stochastic variations with residual mechanism. A unified and flexible framework that can equip any type of spatio-temporal models is proposed based on residual diffusion, which effectively tackles the shortcomings of previous methods. Extensive experimental results on four publicly available radar datasets demonstrate the effectiveness and superiority of the proposed framework, compared to state-of-the-art techniques. Our code will be made publicly available soon.",http://arxiv.org/abs/2312.06734v1,,"Demin Yu (Harbin Institute Of Technology) | Xutao Li (Harbin Institute Of Technology, Shenzhen) | Yunming Ye (Harbin Institute Of Technology, Shenzhen) | Baoquan Zhang (, Harbin Institute Of Technology (Shenzhen)) | Luo Chuyao (None) | Kuai Dai (Harbin Institute Of Technology) | Wangrui (Meteorological Bureau Of Shenzhen Municipality) | Chenxunlai (Shenzhen Meteorological Bureau)",2023-12-11 11:26:32+00:00,,,,,,
3D-Aware Face Editing via Warping-Guided Latent Direction Learning,,,,Yuhao Cheng (Shanghai Jiao Tong University) | Zhuo Chen (Shanghai Jiao Tong University) | Xingyu Ren (Shanghai Jiao Tong University) | Wenhan Zhu (None) | Zhengqin Xu (Shanghai Jiao Tong University) | Di Xu (Huawei Technologies Ltd.) | Yang Changpeng (Huawei Cloud) | Yichao Yan (Shanghai Jiao Tong University),,,,,,,
VidLA: Video-Language Alignment at Scale,,,,"Mamshad Nayeem Rizve (Amazon) | Fan Fei (Amazon) | Jayakrishnan Unnikrishnan (Amazon) | Son Dinh Tran (Amazon) | Benjamin Yao (Amazon) | Belinda Zeng (Amazon) | Mubarak Shah (University Of Central Florida) | Trishul Chilimbi (Department Of Computer Science, University Of Wisconsin - Madison)",,,,,,,
Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation,"We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatio-temporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture. It exploits the representational power of language by summarizing the action context. TransFusion leverages pre-trained image captioning and vision-language models to extract the action context from past video frames. This action context together with the next video frame is processed by the multimodal fusion module to forecast the next object interaction. Our model enables more efficient end-to-end learning. The large pre-trained language models add common sense and a generalisation capability. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model. They also highlight the benefits of using language-based context summaries in a task where vision seems to suffice. Our method outperforms state-of-the-art approaches by 40.4% in relative terms in overall mAP on the Ego4D test set. We validate the effectiveness of TransFusion via experiments on EPIC-KITCHENS-100. Video and code are available at https://eth-ait.github.io/transfusion-proj/.",http://arxiv.org/abs/2301.09209v4,,"Razvan Pasca (None) | Alexey Gavryushin (ETHZ - ETH Zurich) | Muhammad Hamza (Department Of Informatics, University Of Zurich) | Yen-Ling Kuo (University Of Virginia, Charlottesville) | Kaichun Mo (NVIDIA Research) | Luc Van Gool (ETH Zurich) | Otmar Hilliges (None) | Xi Wang (None)",2023-01-22 21:30:12+00:00,,,,,,
Learn from View Correlation: An Anchor Enhancement Strategy for Multi-view Clustering,,,,Suyuan Liu (National University Of Defense Technology) | KE LIANG (National University Of Defense Technology) | Zhibin Dong (National University Of Defense Technology) | Siwei Wang (National University Of Defense Technology) | Xihong Yang (National University Of Defense Technology) | Sihang Zhou (National University Of Defense Technology) | En Zhu (National University Of Defense Technology) | Xinwang Liu (National University Of Defense Technology),,,,,,,
Precise Image Editing via Recognition and Generation Tasks,"Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks.",http://arxiv.org/abs/2311.10089v1,,Shelly Sheynin (META) | Adam Polyak (Meta) | Uriel Singer (Meta) | Yuval Kirstain (Tel Aviv University) | Amit Zohar (Meta) | Oron Ashual (Meta) | Devi Parikh (Meta / Georgia Tech) | Yaniv Taigman (Facebook),2023-11-16 18:55:58+00:00,,,,,,
DreamComposer: Controllable 3D Object Generation via Multi-View Conditions,"Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications.",http://arxiv.org/abs/2312.03611v1,,Yunhan Yang (The University Of Hong Kong) | Yukun Huang (University Of Science And Technology Of China) | Xiaoyang Wu (The University Of Hong Kong) | Yuan-Chen Guo (Tsinghua University) | Song-Hai Zhang (Tsinghua University) | Hengshuang Zhao (The University Of Hong Kong) | Tong He (Shanghai AI Lab) | Xihui Liu (The University Of Hong Kong),2023-12-06 16:55:53+00:00,,,,,,
CoSeR: Bridging Image and Language for Cognitive Super-Resolution,"Existing super-resolution (SR) models primarily focus on restoring local texture details, often neglecting the global semantic information within the scene. This oversight can lead to the omission of crucial semantic details or the introduction of inaccurate textures during the recovery process. In our work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering SR models with the capacity to comprehend low-resolution images. We achieve this by marrying image appearance and language understanding to generate a cognitive embedding, which not only activates prior information from large text-to-image diffusion models but also facilitates the generation of high-quality reference images to optimize the SR process. To further improve image fidelity, we propose a novel condition injection scheme called ""All-in-Attention"", consolidating all conditional information into a single module. Consequently, our method successfully restores semantically correct and photorealistic details, demonstrating state-of-the-art performance across multiple benchmarks. Code: https://github.com/VINHYU/CoSeR",http://arxiv.org/abs/2311.16512v4,,"Haoze Sun (Tsinghua University) | Wenbo Li (Huawei Technologies Ltd.) | Jianzhuang Liu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Haoyu Chen (Hong Kong University Of Science And Technology (Guangzhou)) | Renjing Pei (Huawei Technologies Ltd.) | Xueyi Zou (Huawei Technologies Ltd.) | Youliang Yan (Huawei Technologies Ltd.) | Yujiu Yang (Tsinghua University)",2023-11-27 16:33:29+00:00,,,,,,
De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts,,,,Yuzheng Wang (Fudan University) | Dingkang Yang (Fudan University) | Zhaoyu Chen (Fudan University) | Yang Liu (Fudan University) | Siao Liu (Fudan University) | Wenqiang Zhang (None) | Lihua Zhang (Fudan University) | Lizhe Qi (Fudan University),,,,,,,
MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric,"Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.",http://arxiv.org/abs/2403.07839v1,,"Haokun Lin (City University Of Hong Kong) | Haoli Bai (Huawei Technologies Ltd.) | Zhili Liu (Hong Kong University Of Science And Technology) | Lu Hou (Huawei Technologies Ltd.) | Muyi Sun (Institute Of Automation, Chinese Academy Of Sciences) | Linqi Song (City University Of Hong Kong) | Ying Wei (City University Of Hong Kong) | Zhenan Sun (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences)",2024-03-12 17:24:26+00:00,,,,,,
HIT: Estimating Internal Human Implicit Tissues from the Body Surface,,,,"Marilyn Keller (Max Planck Institute For Inteligent Systems) | Vaibhav ARORA (INRIA) | Abdelmouttaleb Dakri (None) | Shivam Chandhok (INRIA) | J??rgen Machann (Institute For Diabetes Research And Metabolic Diseases, Helmholtz Center Munich At The University Of Tuebingen) | Andreas Fritsche (Eberhard-Karls-Universit??t T??bingen) | Michael J. Black (University Of T??bingen) | Sergi Pujades (INRIA)",,,,,,,
Self-supervised Representation Learning from Arbitrary Scenarios,,,,"Zhaowen Li (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yousong Zhu (Institute Of Automation, Chinese Academy Of Sciences) | Zhiyang Chen (Institute Of Automation, Chinese Academy Of Science) | Zongxin Gao (Beijing Institute Of Graphic Communication) | Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) | Chaoyang Zhao (, Institute Of Automation, Chinese Academy Of Science) | Ming Tang (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Jinqiao Wang (Institute Of Automation, Chinese Academy Of Sciences)",,,,,,,
AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings,,,,"Jamie Watson (Niantic) | Filippo Aleotti (Niantic,) | Mohamed Sayed (University College London, University Of London) | Zawar Qureshi (None) | Oisin Mac Aodha (University Of Edinburgh) | Gabriel J. Brostow (Department Of Computer Science, University College London) | Michael Firman (Niantic,) | Sara Vicente (Niantic)",,,,,,,
Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes,,,,Ziqian Bai (Simon Fraser University & Google) | Feitong Tan (Google) | Sean Fanello (Google) | Rohit Pandey (Google) | Mingsong Dou (Google) | Shichen Liu (Google) | Ping Tan (Hong Kong University Of Science And Technology) | Yinda Zhang (Google),,,,,,,
CrossMAE: Cross Modality Masked Autoencoders For Region-Aware Audio-Visual Pretraining,,,,"Yuxin Guo (Institute Of Automation, Chinese Academy Of Sciences) | Siyang Sun (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Shuailei Ma (None) | Kecheng Zheng (Ant Group) | Xiaoyi Bao (CASIA) | Shijie Ma (Institute Of Automation, Chinese Academy Of Sciences) | Wei Zou (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Yun Zheng (SUN YAT-SEN UNIVERSITY)",,,,,,,
Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach,,,,Wei Dong (Xi'an University Of Architecture And Technology) | Xing Zhang (Xi'an University Of Architecture And Technology) | Bihui Chen (Xi'an University Of Architecture And Technology) | Dawei Yan (Xi'an University Of Architecture And Technology) | Zhijun Lin (Northwest Polytechnical University Xi'an) | Qingsen Yan (Northwest Polytechnical University Xi'an) | Peng Wang (University Of Wollonong) | Yang Yang (University Of Electronic Science And Technology Of China),,,,,,,
CapsFusion: Rethinking Image-Text Data at Scale,"Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.",http://arxiv.org/abs/2310.20550v2,,Qiying Yu (Tsinghua University) | Quan Sun (BAAI) | Xiaosong Zhang (Beijing Academy Of Artificial Intelligence) | Yufeng Cui (Beihang University) | Fan Zhang (Beijing Academy Of Artificial Intelligence) | Yue Cao (Beijing Academy Of Artificial Intelligence) | Xinlong Wang (Beijing Academy Of Artificial Intelligence) | Jingjing Liu (Tsinghua University),2023-10-31 15:31:39+00:00,,,,,,
CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement,"Recently, numerous approaches have achieved notable success in compressed video quality enhancement (VQE). However, these methods usually ignore the utilization of valuable coding priors inherently embedded in compressed videos, such as motion vectors and residual frames, which carry abundant temporal and spatial information. To remedy this problem, we propose the Coding Priors-Guided Aggregation (CPGA) network to utilize temporal and spatial information from coding priors. The CPGA mainly consists of an inter-frame temporal aggregation (ITA) module and a multi-scale non-local aggregation (MNA) module. Specifically, the ITA module aggregates temporal information from consecutive frames and coding priors, while the MNA module globally captures spatial information guided by residual frames. In addition, to facilitate research in VQE task, we newly construct the Video Coding Priors (VCP) dataset, comprising 300 videos with various coding priors extracted from corresponding bitstreams. It remedies the shortage of previous datasets on the lack of coding information. Experimental results demonstrate the superiority of our method compared to existing state-of-the-art methods. The code and dataset will be released at https://github.com/CPGA/CPGA.git.",http://arxiv.org/abs/2403.10362v1,,Qiang Zhu (University Of Electronic Science And Technology Of China) | Jinhua Hao (Kuaishou Tech) | Yukang Ding (Kuaishou Tech) | Yu Liu (University Of Electronic Science And Technology Of China) | Qiao Mo (University Of Electronic Science And Technology Of China) | Ming Sun (Kuaishou Tech) | Chao Zhou (Kuaishou) | Shuyuan Zhu (University Of Electronic Science And Technology Of China),2024-03-15 14:53:31+00:00,,,,,,
Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts,,,,"Fei Ni (Tianjin University) | Jianye Hao (Tianjin University) | Shiguang Wu (Huawei Technologies Ltd.) | Longxin Kou (None) | Jiashun Liu (Tianjin University) | YAN ZHENG (Tianjin Unibersity, China) | Bin Wang (Huawei Noah's Ark Lab) | Yuzheng Zhuang (Huawei Technologies Ltd.)",,,,,,,
Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling,"Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",http://arxiv.org/abs/2403.10071v1,,"Baoquan Zhang (, Harbin Institute Of Technology (Shenzhen)) | Huaibin Wang (Harbin Institute Of Technology???Shenzhen) | Luo Chuyao (None) | Xutao Li (Harbin Institute Of Technology, Shenzhen) | Guotao Liang (Harbin Institute Of Technology(Shenzhen)) | Yunming Ye (Harbin Institute Of Technology, Shenzhen) | Joeq (CEO) | Yao He (None)",2024-03-15 07:24:13+00:00,,,,,,
Relightful Harmonization: Lighting-aware Portrait Background Replacement,"Portrait harmonization aims to composite a subject into a new background, adjusting its lighting and color to ensure harmony with the background scene. Existing harmonization techniques often only focus on adjusting the global color and brightness of the foreground and ignore crucial illumination cues from the background such as apparent lighting direction, leading to unrealistic compositions. We introduce Relightful Harmonization, a lighting-aware diffusion model designed to seamlessly harmonize sophisticated lighting effect for the foreground portrait using any background image. Our approach unfolds in three stages. First, we introduce a lighting representation module that allows our diffusion model to encode lighting information from target image background. Second, we introduce an alignment network that aligns lighting features learned from image background with lighting features learned from panorama environment maps, which is a complete representation for scene illumination. Last, to further boost the photorealism of the proposed method, we introduce a novel data simulation pipeline that generates synthetic training pairs from a diverse range of natural images, which are used to refine the model. Our method outperforms existing benchmarks in visual fidelity and lighting coherence, showing superior generalization in real-world testing scenarios, highlighting its versatility and practicality.",http://arxiv.org/abs/2312.06886v1,,Mengwei Ren (None) | Wei Xiong (Adobe Systems) | Jae Shin Yoon (Adobe Systems) | Zhixin Shu (Adobe Systems) | Jianming Zhang (Adobe Systems) | HyunJoon Jung (Adobe Systems) | Guido Gerig (New York University) | He Zhang (Adobe Systems),2023-12-11 23:20:31+00:00,,,,,,
SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field,,,,Lizhe Liu (Xiaomi) | Bohua Wang (Xi'an Jiao Tong University) | Hongwei Xie (Xiaom EV) | Daqi Liu (Xiaomi) | Li Liu (None) | Kuiyuan Yang (DeepMotion) | Bing Wang (Alibaba Group) | Zhiqiang Tian (Xi'an Jiao Tong University),,,,,,,
Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner,"A diffusion model, which is formulated to produce an image using thousands of denoising steps, usually suffers from a slow inference speed. Existing acceleration algorithms simplify the sampling by skipping most steps yet exhibit considerable performance degradation. By viewing the generation of diffusion models as a discretized integrating process, we argue that the quality drop is partly caused by applying an inaccurate integral direction to a timestep interval. To rectify this issue, we propose a timestep aligner that helps find a more accurate integral direction for a particular interval at the minimum cost. Specifically, at each denoising step, we replace the original parameterization by conditioning the network on a new timestep, which is obtained by aligning the sampling distribution to the real distribution. Extensive experiments show that our plug-in design can be trained efficiently and boost the inference performance of various state-of-the-art acceleration methods, especially when there are few denoising steps. For example, when using 10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate set of timesteps. Code will be made publicly available.",http://arxiv.org/abs/2310.09469v1,,Mengfei Xia (Tsinghua University) | Yujun Shen (The Chinese University Of Hong Kong) | Changsong Lei (Tsinghua University) | Yu Zhou (Tsinghua University) | Deli Zhao (Alibaba Group) | Ran Yi (Shanghai Jiao Tong University) | Wenping Wang (Texas A&M University - College Station) | Yong-Jin Liu (Tsinghua University),2023-10-14 02:19:07+00:00,,,,,,
HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative,"Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",http://arxiv.org/abs/2403.02640v2,,"CONG MA (Senseauto Research) | Qiao Lei (SenseAuto Research) | Chengkai Zhu (SenseAuto Research) | Kai Liu (SenseAuto Research) | Zelong Kong (SenseAuto Research) | Liqing (SenseAuto) | Xueqi Zhou (Beijing Sensetime Technology Development Co., Ltd.) | Yuheng KAN (Zhejiang University) | Wei Wu (Tsinghua University)",2024-03-05 04:08:19+00:00,,,,,,
LTA-PCS: Learnable Task-Agnostic Point Cloud Sampling,,,,"Jiaheng Liu (Beihang University) | Jianhao Li (Beihang University) | Kaisiyuan Wang (University Of Sydney) | Hongcheng Guo (Beihang University) | Jian Yang (Alibaba Group) | Junran Peng (Institute Of Automation, Chinese Academy Of Science) | Ke Xu (Beijing University Of Aeronautics And Astronautics) | Xianglong Liu (BUAA) | Jinyang Guo (Beijing University Of Aeronautics And Astronautics)",,,,,,,
FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures,,,,Lisa Mais (Max Delbr??ck Center For Molecular Medicine) | Peter Hirsch (Max Delbr??ck Center For Molecular Medicine) | Claire Managan (HHMI Janelia Research Campus) | Ramya Kandarpa (Environmental Resources Management (ERM)) | Josef Rumberger (Max Delbr??ck Center For Molecular Medicine) | Annika Reinke (German Cancer Research Center) | Lena Maier-Hein (German Cancer Research Center (DKFZ)) | Gudrun Ihrke (HHMI Janelia Research Campus) | Dagmar Kainmueller (Universit??t Potsdam),,,,,,,
Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models,"Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.",http://arxiv.org/abs/2402.19014v1,,Xin Li (Tencent Youtu Lab) | Yunfei Wu (Tencent YouTu Lab) | Xinghua Jiang (None) | ZhiHao Guo (Tencent YOUTU Lab) | Mingming Gong (Tencent YouTu Lab) | Haoyu Cao (Tencent Youtu Lab) | Yinsong Liu (Tencent Youtu Lab) | Deqiang Jiang (Tencent YouTu Lab) | Xing Sun (Tencent YouTu Lab),2024-02-29 10:17:27+00:00,,,,,,
3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling,"Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow data augmentation method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous supervised and unsupervised methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.",http://arxiv.org/abs/2402.18146v2,,Chaokang Jiang (None) | Guangming Wang (University Of Cambridge) | Jiuming Liu (Shanghai Jiao Tong University) | Hesheng Wang (Shanghai Jiao Tong University) | Zhuang Ma (PhiGent) | Zhenqiang Liu (None) | LIANG (None) | Yi Shan (PhiGent Robotics) | Dalong Du (PhiGent Robotics),2024-02-28 08:12:31+00:00,,,,,,
Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld,"While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world (but inapplicable to the visual world). Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds enables EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark highlight EMMA's superior performance to SOTA VLM-based agents across diverse tasks, e.g., 20%-70% improvement in the success rate.",http://arxiv.org/abs/2311.16714v1,,"Yijun Yang (University Of Technology Sydney) | Tianyi Zhou (University Of Maryland, College Park) | Kanxue Li (Yunnan University) | Dapeng Tao (Yunnan University) | Lusong Li (JDT) | Li Shen (JD Explore Academy) | Xiaodong He (JD AI Research) | Jing Jiang (University Of Technology Sydney) | Yuhui Shi (Southern University Of Science And Technology)",2023-11-28 11:53:56+00:00,,,,,,
SceneTextGen: Layout-Agnostic Scene Text Image Synthesis with Integrated Character-Level Diffusion and Contextual Consistency,,,,"Qilong Zhangli (Rutgers University) | Jindong Jiang (Rutgers University) | Di Liu (Rutgers University, New Brunswick) | Licheng Yu (None) | Xiaoliang Dai (Facebook) | Ankit Ramchandani (Meta Platforms,) | Guan Pang (Facebook) | Dimitris N. Metaxas (Rutgers) | Praveen Krishnan (Meta AI)",,,,,,,
"Towards Co-Evaluation of Cameras, HDR, and Algorithms for Industrial-Grade 6DoF Pose Estimation",,,,Agastya Kalra (Google) | Guy Stoppi (Intrinsic) | Dmitrii Marin (Intrinsic) | Vage Taamazyan (Intrinsic) | Aarrushi Shandilya (Intrinsic AI) | Rishav Agarwal (Intrinsic) | Anton Boykov (University Of Waterloo) | Aaron Chong (Google) | Michael Stark (Intrinsic),,,,,,,
Alpha-CLIP: A CLIP Model Focusing on Wherever You Want,"Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.",http://arxiv.org/abs/2312.03818v2,,Zeyi Sun (Shanghai Jiao Tong University) | Ye Fang (None) | Tong Wu (None) | Pan Zhang (Shanghai Artificial Intelligence Laboratory) | Yuhang Zang (Nanyang Technological University) | Shu Kong (None) | Yuanjun Xiong (Mthreads) | Dahua Lin (The Chinese University Of Hong Kong) | Jiaqi Wang (Shanghai AI Laboratory),2023-12-06 18:59:30+00:00,,,,,,
HRVDA: High-Resolution Visual Document Assistant,,,,Chaohu Liu (University Of Science And Technology Of China) | Kun Yin (Tencent YouTu Lab) | Haoyu Cao (Tencent Youtu Lab) | Xinghua Jiang (None) | Xin Li (Tencent Youtu Lab) | Yinsong Liu (Tencent Youtu Lab) | Deqiang Jiang (Tencent YouTu Lab) | Xing Sun (Tencent YouTu Lab) | Linli Xu (University Of Science And Technology Of China),,,,,,,
High-Fidelity Hair Modeling from a Monocular Video,,,,Keyu Wu (Zhejiang University) | LINGCHEN YANG (ETHZ - ETH Zurich) | Zhiyi Kuang (Zhejiang University) | Yao Feng (None) | Xutao Han (Zhejiang University) | Yuefan Shen (Zhejiang University) | Hongbo Fu (City University Of Hong Kong) | Kun Zhou (Zhejiang University) | Youyi Zheng (Zhejiang University),,,,,,,
HOI-M3 : Capture Multiple Humans and Objects Interaction within Contextual Environment ,,,,Juze Zhang (ShanghaiTech University) | Jingyan Zhang (ShanghaiTech University) | Zining Song (ShanghaiTech University) | Zhanhe Shi (ShanghaiTech University) | Chengfeng Zhao (ShanghaiTech University) | Ye Shi (ShanghaiTech University) | Jingyi Yu (Shanghai Tech University) | Lan Xu (None) | Jingya Wang (ShanghaiTech University),,,,,,,
Dr. Bokeh: DiffeRentiable Occlusion-aware Bokeh Rendering,"Bokeh is widely used in photography to draw attention to the subject while effectively isolating distractions in the background. Computational methods simulate bokeh effects without relying on a physical camera lens. However, in the realm of digital bokeh synthesis, the two main challenges for bokeh synthesis are color bleeding and partial occlusion at object boundaries. Our primary goal is to overcome these two major challenges using physics principles that define bokeh formation. To achieve this, we propose a novel and accurate filtering-based bokeh rendering equation and a physically-based occlusion-aware bokeh renderer, dubbed Dr.Bokeh, which addresses the aforementioned challenges during the rendering stage without the need of post-processing or data-driven approaches. Our rendering algorithm first preprocesses the input RGBD to obtain a layered scene representation. Dr.Bokeh then takes the layered representation and user-defined lens parameters to render photo-realistic lens blur. By softening non-differentiable operations, we make Dr.Bokeh differentiable such that it can be plugged into a machine-learning framework. We perform quantitative and qualitative evaluations on synthetic and real-world images to validate the effectiveness of the rendering quality and the differentiability of our method. We show Dr.Bokeh not only outperforms state-of-the-art bokeh rendering algorithms in terms of photo-realism but also improves the depth quality from depth-from-defocus.",http://arxiv.org/abs/2308.08843v1,,Yichen Sheng (Purdue University) | Zixun Yu (Purdue University) | Lu Ling (Purdue University) | Zhiwen Cao (Adobe Systems) | Xuaner Zhang (Adobe) | Xin Lu (Adobe) | Ke Xian (Nanyang Technological University) | Haiting Lin (Adobe Systems) | Bedrich Benes (Purdue University),2023-08-17 08:02:50+00:00,,,,,,
L-MAGIC: Language Model Assisted Generation of Images with Consistency,,,,Zhipeng Cai (Intel Labs) | Matthias Mueller (None) | Reiner Birkl (Intel Corporation) | Diana Wofk (Intel) | Shao-Yen Tseng (Intel) | JunDa Cheng (Huazhong University Of Science And Technology) | Gabriela Ben Melech Stan (Intel) | Vasudev Lal (None) | Michael Paulitsch (Intel),,,,,,,
AVID: Any-Length Video Inpainting with Diffusion Model,"Recent advances in diffusion models have successfully enabled text-guided image inpainting. While it seems straightforward to extend such editing capability into video domain, there has been fewer works regarding text-guided video inpainting. Given a video, a masked region at its initial frame, and an editing prompt, it requires a model to do infilling at each frame following the editing guidance while keeping the out-of-mask region intact. There are three main challenges in text-guided video inpainting: ($i$) temporal consistency of the edited video, ($ii$) supporting different inpainting types at different structural fidelity level, and ($iii$) dealing with variable video length. To address these challenges, we introduce Any-Length Video Inpainting with Diffusion Model, dubbed as AVID. At its core, our model is equipped with effective motion modules and adjustable structure guidance, for fixed-length video inpainting. Building on top of that, we propose a novel Temporal MultiDiffusion sampling pipeline with an middle-frame attention guidance mechanism, facilitating the generation of videos with any desired duration. Our comprehensive experiments show our model can robustly deal with various inpainting types at different video duration range, with high quality. More visualization results is made publicly available at https://zhang-zx.github.io/AVID/ .",http://arxiv.org/abs/2312.03816v1,,Zhixing Zhang (Rutgers University) | Bichen Wu (Facebook) | Xiaoyan Wang (Massachusetts Institute Of Technology) | Yaqiao Luo (Facebook) | Luxin Zhang (Meta) | Yinan Zhao (Facebook) | Peter Vajda (Facebook) | Dimitris N. Metaxas (Rutgers) | Licheng Yu (None),2023-12-06 18:56:14+00:00,,,,,,
Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model,,,,Runmin Dong (Tsinghua University) | Shuai Yuan (The University Of Hong Kong) | Bin Luo (Tsinghua University) | Mengxuan Chen (Tsinghua University) | Jinxiao Zhang (Tsinghua University) | Lixian Zhang (National Supercomputing Center In Shenzhen) | Weijia Li (Sun Yat-Sen University) | Juepeng Zheng (Sun Yat-Sen University) | Haohuan Fu (Tsinghua University),,,,,,,
OmniMotionGPT: Animal Motion Generation with Limited Data,"Our paper aims to generate diverse and realistic animal motion sequences from textual descriptions, without a large-scale animal text-motion dataset. While the task of text-driven human motion synthesis is already extensively studied and benchmarked, it remains challenging to transfer this success to other skeleton structures with limited data. In this work, we design a model architecture that imitates Generative Pretraining Transformer (GPT), utilizing prior knowledge learned from human data to the animal domain. We jointly train motion autoencoders for both animal and human motions and at the same time optimize through the similarity scores among human motion encoding, animal motion encoding, and text CLIP embedding. Presenting the first solution to this problem, we are able to generate animal motions with high diversity and fidelity, quantitatively and qualitatively outperforming the results of training human motion generation baselines on animal data. Additionally, we introduce AnimalML3D, the first text-animal motion dataset with 1240 animation sequences spanning 36 different animal identities. We hope this dataset would mediate the data scarcity problem in text-driven animal motion generation, providing a new playground for the research community.",http://arxiv.org/abs/2311.18303v1,,Zhangsihao Yang (None) | Mingyuan Zhou (Innopeak Technology) | Mengyi Shan (University Of Washington) | Bingbing Wen (University Of Washington) | Ziwei Xuan (Innopeak Technology) | Mitch Hill (None) | Junjie Bai (CuraCloud Corporation) | Guo-Jun Qi (University Of Central Florida) | Yalin Wang (Arizona State University),2023-11-30 07:14:00+00:00,,,,,,
Privacy-Preserving Face Recognition Using Trainable Feature Subtraction,,,,Yuxi Mi (Fudan University) | Zhizhou Zhong (Fudan University) | Yuge Huang (Tencent Youtu Lab) | Jiazhen Ji (Tencent Youtu Lab) | Jianqing Xu (HIT) | Jun Wang (None) | ShaoMing Wang (WeChat Pay Lab33) | Shouhong Ding (Tencent Youtu Lab) | Shuigeng Zhou (Fudan University),,,,,,,
ProMotion: Prototypes As Motion Learners,,,,Yawen Lu (Purdue University; Rochester Institute Of Tech) | Dongfang Liu (Rochester Institute Of Technology) | Qifan Wang (Meta AI) | Cheng Han (Rochester Institute Of Technology) | Yiming Cui (University Of Florida) | Zhiwen Cao (Purdue University) | Xueling Zhang (Rochester Institute Of Technology) | Yingjie Victor Chen (Purdue University) | Heng Fan (University Of North Texas),,,,,,,
"ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation","In this paper, we present ShapeMatcher, a unified self-supervised learning framework for joint shape canonicalization, segmentation, retrieval and deformation. Given a partially-observed object in an arbitrary pose, we first canonicalize the object by extracting point-wise affine-invariant features, disentangling inherent structure of the object with its pose and size. These learned features are then leveraged to predict semantically consistent part segmentation and corresponding part centers. Next, our lightweight retrieval module aggregates the features within each part as its retrieval token and compare all the tokens with source shapes from a pre-established database to identify the most geometrically similar shape. Finally, we deform the retrieved shape in the deformation module to tightly fit the input object by harnessing part center guided neural cage deformation. The key insight of ShapeMaker is the simultaneous training of the four highly-associated processes: canonicalization, segmentation, retrieval, and deformation, leveraging cross-task consistency losses for mutual supervision. Extensive experiments on synthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD demonstrate that ShapeMaker surpasses competitors by a large margin.",http://arxiv.org/abs/2311.11106v2,,"Yan Di (Technische Universit??t M??nchen) | Chenyangguang Zhang (Tsinghua University) | Chaowei Wang (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) | Ruida Zhang (Department Of Automation, Tsinghua University) | Guangyao Zhai (Technical University Of Munich) | Yanyan Li (Technical University Munich) | Bowen Fu (Technische Universit??t M??nchen) | Xiangyang Ji (Tsinghua University) | Shan Gao (Northwest Polytechnical University Xi'an)",2023-11-18 15:44:57+00:00,,,,,,
"OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition",,,,Jianqiang Wan (Alibaba Group) | Sibo Song (Alibaba Group) | Wenwen Yu (Huazhong University Of Science And Technology) | Yuliang Liu (Huazhong University Of Science And Technology) | Wenqing Cheng (Huazhong University Of Science And Technology) | Fei Huang (Alibaba Group) | Xiang Bai (Huazhong University Of Science And Technology) | Cong Yao (Alibaba DAMO Academy) | Zhibo Yang (Alibaba Group),,,,,,,
ACT: Adversarial Consistency Models,"Though diffusion models excel in image generation, their step-by-step denoising leads to slow generation speeds. Consistency training addresses this issue with single-step sampling but often produces lower-quality generations and requires high training costs. In this paper, we show that optimizing consistency training loss minimizes the Wasserstein distance between target and generated distributions. As timestep increases, the upper bound accumulates previous consistency training losses. Therefore, larger batch sizes are needed to reduce both current and accumulated losses. We propose Adversarial Consistency Training (ACT), which directly minimizes the Jensen-Shannon (JS) divergence between distributions at each timestep using a discriminator. Theoretically, ACT enhances generation quality, and convergence. By incorporating a discriminator into the consistency training framework, our method achieves improved FID scores on CIFAR10, ImageNet 64$\times$64 and LSUN Cat 256$\times$256 datasets, retains zero-shot image inpainting capabilities, and uses less than $1/6$ of the original batch size and fewer than $1/2$ of the model parameters and training steps compared to the baseline method, this leads to a substantial reduction in resource consumption.",http://arxiv.org/abs/2311.14097v2,,Fei Kong (University Of Electronic Science And Technology Of China) | Jinhao Duan (Drexel University) | Lichao Sun (Lehigh University) | Hao Cheng (Hong Kong University Of Science And Technology(Guangzhou)) | Renjing Xu (Hong Kong University Of Science And Technology (Guangzhou)) | Heng Tao Shen (University Of Electronic Science And Technology Of China) | Xiaofeng Zhu (University Of Electronic Science And Technology Of China) | Xiaoshuang Shi (University Of Electronic Science And Technology Of China) | Kaidi Xu (Drexel University),2023-11-23 16:49:06+00:00,,,,,,
Fairy: Fast Parallellized Instruction-Guided Video-to-Video Synthesis,"In this paper, we introduce Fairy, a minimalist yet robust adaptation of image-editing diffusion models, enhancing them for video editing applications. Our approach centers on the concept of anchor-based cross-frame attention, a mechanism that implicitly propagates diffusion features across frames, ensuring superior temporal coherence and high-fidelity synthesis. Fairy not only addresses limitations of previous models, including memory and processing speed. It also improves temporal consistency through a unique data augmentation strategy. This strategy renders the model equivariant to affine transformations in both source and target images. Remarkably efficient, Fairy generates 120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds, outpacing prior works by at least 44x. A comprehensive user study, involving 1000 generated samples, confirms that our approach delivers superior quality, decisively outperforming established methods.",http://arxiv.org/abs/2312.13834v1,,"Bichen Wu (Facebook) | Ching-Yao Chuang (Meta) | Xiaoyan Wang (Massachusetts Institute Of Technology) | Yichen Jia (Facebook) | Kapil Krishnakumar (Meta,) | Tong Xiao (None) | Feng Liang (The University Of Texas At Austin) | Licheng Yu (None) | Peter Vajda (Facebook)",2023-12-20 01:49:47+00:00,,,,,,
"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model","The ability of large language models (LLMs) to process visual inputs has given rise to general-purpose vision systems, unifying various vision-language (VL) tasks by instruction tuning. However, due to the enormous diversity in input-output formats in the vision domain, existing general-purpose models fail to successfully integrate segmentation and multi-image inputs with coarse-level tasks into a single framework. In this work, we introduce VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework. VistaLLM utilizes an instruction-guided image tokenizer that filters global embeddings using task descriptions to extract compressed and refined features from numerous images. Moreover, VistaLLM employs a gradient-aware adaptive sampling technique to represent binary segmentation masks as sequences, significantly improving over previously used uniform sampling. To bolster the desired capability of VistaLLM, we curate CoinIt, a comprehensive coarse-to-fine instruction tuning dataset with 6.8M samples. We also address the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images. Extensive experiments on a wide range of V- and VL tasks demonstrate the effectiveness of VistaLLM by achieving consistent state-of-the-art performance over strong baselines across all downstream tasks. Our project page can be found at https://shramanpramanick.github.io/VistaLLM/.",http://arxiv.org/abs/2312.12423v1,,Shraman Pramanick (None) | Guangxing Han (Columbia University) | Rui Hou (Meta ) | Sayan Nag (University Of Toronto) | Ser-Nam Lim (Meta AI) | Nicolas Ballas (Facebook) | Qifan Wang (Meta AI) | Rama Chellappa (Johns Hopkins University) | Amjad Almahairi (Facebook),2023-12-19 18:53:01+00:00,,,,,,
GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image,,,,Chong Bao (Zhejiang University) | Yinda Zhang (Google) | Yuan Li (Zhejiang University) | Xiyu Zhang (Zhejiang University) | Bangbang Yang (ByteDance Inc) | Hujun Bao (Zhejiang University) | Marc Pollefeys (ETH Zurich / Microsoft) | Guofeng Zhang (Zhejiang University) | Zhaopeng Cui (None),,,,,,,
mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration,"Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.",http://arxiv.org/abs/2311.04257v2,,"Qinghao Ye (Alibaba Group) | Haiyang Xu (Alibaba Group) | Jiabo Ye (East China Normal University) | Ming Yan (Alibaba Group) | Anwen Hu (Alibaba Group) | Haowei Liu (Institute Of Automation, Chinese Academy Of Sciences) | Qi Qian (Alibaba Group) | Ji Zhang (Alibaba Group) | Fei Huang (Alibaba Group)",2023-11-07 14:21:29+00:00,,,,,,
MV-Adapter: Exploring Parameter Efficient Learning for Video Text Retrieval,,,,Bowen Zhang (Bytedance) | Xiaojie Jin (ByteDance/TikTok) | Weibo Gong (ByteDance) | Kai Xu (University Of Chinese Academy Of Sciences) | Xueqing Deng (ByteDance Research) | Peng Wang (Bytedance US AILab) | Zhao Zhang (Hefei University Of Technology) | Xiaohui Shen (ByteDance) | Jiashi Feng (ByteDance),,,,,,,
Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild,"We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each enriched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capacity to manipulate restoration through textual prompts.",http://arxiv.org/abs/2401.13627v1,,"Fanghua Yu (Shenzhen Institute Of Advanced Technology, Chinese Academy Of Sciences) | Jinjin Gu (University Of Sydney) | Zheyuan Li (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Jinfan Hu (Shenzhen Institutes Of Advanced Technology, Chinese Academy Of Sciences) | Xiangtao Kong (Hong Kong Polytechnic University) | Xintao Wang (Tencent) | Jingwen He (Shanghai Ai Lab) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Chao Dong (SIAT)",2024-01-24 17:58:07+00:00,,,,,,
Event-assisted Low-Light Video Object Segmentation,,,,Li Hebei (University Of Science And Technology Of China) | Jin Wang (University Of Science And Technology Of China) | Jiahui Yuan (University Of Science And Technology Of China) | Yue Li (None) | Wenming Weng (None) | Yansong Peng (None) | Yueyi Zhang (University Of Science And Technology Of China) | Zhiwei Xiong (None) | Xiaoyan Sun (University Of Science And Technology Of China),,,,,,,
TextCraft: Your Text Encoder Can be Image Quality Controller,,,,Yanyu Li (Northeastern University) | Xian Liu (The Chinese University Of Hong Kong) | Anil Kag (Snap) | Ju Hu (Snap) | Yerlan Idelbayev (Snap) | Dhritiman Sagar (Snap) | Yanzhi Wang (Northeastern University) | Sergey Tulyakov (Snap) | Jian Ren (Snap),,,,,,,
L2B: Learning to Bootstrap Robust Models for Combating Label Noise,"Deep neural networks are powerful tools for representation learning, but can easily overfit to noisy labels which are prevalent in many real-world scenarios. Generally, noisy supervision could stem from variation among labelers, label corruption by adversaries, etc. To combat such label noises, one popular line of approach is to apply customized weights to the training instances, so that the corrupted examples contribute less to the model learning. However, such learning mechanisms potentially erase important information about the data distribution and therefore yield suboptimal results. To leverage useful information from the corrupted instances, an alternative is the bootstrapping loss, which reconstructs new training targets on-the-fly by incorporating the network's own predictions (i.e., pseudo-labels).   In this paper, we propose a more generic learnable loss objective which enables a joint reweighting of instances and labels at once. Specifically, our method dynamically adjusts the per-sample importance weight between the real observed labels and pseudo-labels, where the weights are efficiently determined in a meta process. Compared to the previous instance reweighting methods, our approach concurrently conducts implicit relabeling, and thereby yield substantial improvements with almost no extra cost. Extensive experimental results demonstrated the strengths of our approach over existing methods on multiple natural and medical image benchmark datasets, including CIFAR-10, CIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at https://github.com/yuyinzhou/L2B.",http://arxiv.org/abs/2202.04291v1,,"Yuyin Zhou (UC Santa Cruz) | Xianhang Li (University Of California, Santa Cruz) | Fengze Liu (ByteDance) | Qingyue Wei (Stanford University) | Xuxi Chen (University Of Texas At Austin) | Lequan Yu (The University Of Hong Kong) | Cihang Xie (University Of California, Santa Cruz) | Matthew P. Lungren (Microsoft) | Lei Xing (Stanford University)",2022-02-09 05:57:08+00:00,,,,,,
SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis,"Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although they can address this issue, often produce mismatched lip movements, inadequate facial expressions, and unstable head poses. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic and artificial outcomes. To address the critical issue of synchronization, identified as the ""devil"" in creating realistic talking heads, we introduce SyncTalk. This NeRF-based method effectively maintains subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk employs a Face-Sync Controller to align lip movements with speech and innovatively uses a 3D facial blendshape model to capture accurate facial expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more natural head movements. The Portrait-Sync Generator restores hair details and blends the generated head with the torso for a seamless visual experience. Extensive experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk",http://arxiv.org/abs/2311.17590v1,,"Ziqiao Peng (Renmin University Of China) | Wentao Hu (Beijing University Of Posts And Telecommunications) | Yue Shi (Psyche AI) | Xiangyu Zhu (None) | Xiaomei Zhang (None) | Hao Zhao (Tsinghua University) | Jun He (Renmin University Of China) | Hongyan Liu (Tsinghua University) | Zhaoxin Fan (Renmin University Of China, Tsinghua University)",2023-11-29 12:35:34+00:00,,,,,,
Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks,"To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot semantic segmentation methods first pre-train the models on `seen' classes, and then evaluate their generalization performance on `unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead, but also incurs a significant domain gap on `unseen' classes. To tackle these issues, we propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and a further training-based variant, TFS3D-T. Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings, and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to train a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.",http://arxiv.org/abs/2308.12961v1,,"Xiangyang Zhu (City University Of Hong Kong) | Renrui Zhang (MMLab Of CUHK & Shanghai AI Laboratory) | Bowei He (City University Of Hong Kong) | Ziyu Guo (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Jiaming Liu (Peking University) | Han Xiao (The Chinese University Of Hong Kong & Shanghai AI Laboratory) | Chaoyou Fu (Institute Of Automation, Chinese Academy Of Science, Chinese Academy Of Sciences) | Hao Dong (None) | Peng Gao (The Chinese University Of Hong Kong)",2023-08-24 17:58:03+00:00,,,,,,
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models,"Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10000x smaller than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io",http://arxiv.org/abs/2307.06949v1,,Nataniel Ruiz (Boston University) | Yuanzhen Li (Massachusetts Institute Of Technology) | Varun Jampani (Google Research) | Wei Wei (Google) | Tingbo Hou (Google Research) | Yael Pritch (Google Research) | Neal Wadhwa (Google) | Michael Rubinstein (Google) | Kfir Aberman (Google),2023-07-13 17:59:47+00:00,,,,,,
Logarithmic Lenses: Exploring Log RGB Data for Image Classification,,,,Bruce Maxwell (Northeastern University) | Sumegha Singhania (Northeastern University) | Avnish Patel (Northeastern University) | Rahul Kumar (Northeastern University) | Heather Fryling (Northeastern University) | Sihan Li (None) | Haonan Sun (Northeastern University) | Ping He (Northeastern University) | Zewen Li (Northeastern University),,,,,,,
Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks,"We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.",http://arxiv.org/abs/2311.06242v1,,Bin Xiao (Microsoft) | Haiping Wu (Microsoft) | Weijian Xu (Microsoft) | Xiyang Dai (Microsoft) | Houdong Hu (Microsoft) | Yumao Lu (Microsoft) | Michael Zeng (Microsoft) | Ce Liu (Microsoft) | Lu Yuan (Microsoft),2023-11-10 18:59:08+00:00,,,,,,
Semantics-aware Motion Retargeting with Vision-Language Models,"Capturing and preserving motion semantics is essential to motion retargeting between animation characters. However, most of the previous works neglect the semantic information or rely on human-designed joint-level representations. Here, we present a novel Semantics-aware Motion reTargeting (SMT) method with the advantage of vision-language models to extract and maintain meaningful motion semantics. We utilize a differentiable module to render 3D motions. Then the high-level motion semantics are incorporated into the motion retargeting process by feeding the vision-language model with the rendered images and aligning the extracted semantic embeddings. To ensure the preservation of fine-grained motion details and high-level semantics, we adopt a two-stage pipeline consisting of skeleton-aware pre-training and fine-tuning with semantics and geometry constraints. Experimental results show the effectiveness of the proposed method in producing high-quality motion retargeting results while accurately preserving motion semantics. Project page can be found at https://sites.google.com/view/smtnet.",http://arxiv.org/abs/2312.01964v2,,Haodong Zhang (None) | ZhiKe Chen (None) | Haocheng Xu (Zhejiang University) | Lei Hao (Huawei Noah's Ark Lab) | Xiaofei Wu (Huawei Technologies Ltd.) | Songcen Xu (Huawei Noah's Ark Lab) | Zhensong Zhang (Huawei Noah's Ark Lab) | Yue Wang (Zhejiang University) | Rong Xiong (Zhejiang University),2023-12-04 15:23:49+00:00,,,,,,
HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data,"Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations. Based on that, we execute counterfactual visual instruction expansion to balance data distribution, thereby enhancing MLLMs' resistance to hallucinations. Comprehensive experiments on hallucination evaluation benchmarks show that our method successfully mitigates 44.6% hallucinations relatively and maintains competitive performance compared to LLaVA.The source code will be released at \url{https://github.com/Yuqifan1117/HalluciDoctor}.",http://arxiv.org/abs/2311.13614v1,,"Qifan Yu (None) | Juncheng Li (Zhejiang University) | Longhui Wei (Huawei Cloud Technologies Ltd.) | Liang Pang (Institute Of Computing Technology, Chinese Academy Of Sciences) | Wentao Ye (Zhejiang University) | Bosheng Qin (Zhejiang University) | Siliang Tang (Zhejiang University) | Qi Tian (Huawei Technologies Ltd.) | Yueting Zhuang (Zhejiang University)",2023-11-22 04:52:58+00:00,,,,,,
SODA: Bottleneck Diffusion Models for Representation Learning,"We introduce SODA, a self-supervised diffusion model, designed for representation learning. The model incorporates an image encoder, which distills a source view into a compact representation, that, in turn, guides the generation of related novel views. We show that by imposing a tight bottleneck between the encoder and a denoising decoder, and leveraging novel view synthesis as a self-supervised objective, we can turn diffusion models into strong representation learners, capable of capturing visual semantics in an unsupervised manner. To the best of our knowledge, SODA is the first diffusion model to succeed at ImageNet linear-probe classification, and, at the same time, it accomplishes reconstruction, editing and synthesis tasks across a wide range of datasets. Further investigation reveals the disentangled nature of its emergent latent space, that serves as an effective interface to control and manipulate the model's produced images. All in all, we aim to shed light on the exciting and promising potential of diffusion models, not only for image generation, but also for learning rich and robust representations.",http://arxiv.org/abs/2311.17901v1,,Drew Hudson (Google DeepMind) | Daniel Zoran (DeepMind) | Mateusz Malinowski (DeepMind) | Andrew Lampinen (Google DeepMind) | Andrew Jaegle (Google DeepMind) | James McClelland (Stanford University And Google DeepMind) | Loic Matthey (DeepMind) | Felix Hill (Google) | Alexander Lerchner (Google DeepMind),2023-11-29 18:53:34+00:00,,,,,,
Carve3D: Improving Multiview Reconstruction Consistency for Diffusion Models with RL Finetuning,"Recent advancements in the text-to-3D task leverage finetuned text-to-image diffusion models to generate multi-view images, followed by NeRF reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still suffer from multi-view inconsistency and the resulting NeRF artifacts. Although training longer with SFT improves consistency, it also causes distribution shift, which reduces diversity and realistic details. We argue that the SFT of multi-view diffusion models resembles the instruction finetuning stage of the LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods. Essentially, RLFT methods optimize models beyond their SFT data distribution by using their own outputs, effectively mitigating distribution shift. To this end, we introduce Carve3D, a RLFT method coupled with the Multi-view Reconstruction Consistency (MRC) metric, to improve the consistency of multi-view diffusion models. To compute MRC on a set of multi-view images, we compare them with their corresponding renderings of the reconstructed NeRF at the same viewpoints. We validate the robustness of MRC with extensive experiments conducted under controlled inconsistency levels. We enhance the base RLFT algorithm to stabilize the training process, reduce distribution shift, and identify scaling laws. Through qualitative and quantitative experiments, along with a user study, we demonstrate Carve3D's improved multi-view consistency, the resulting superior NeRF reconstruction quality, and minimal distribution shift compared to longer SFT. Project webpage: https://desaixie.github.io/carve-3d.",http://arxiv.org/abs/2312.13980v1,,Desai Xie (Stony Brook University) | Jiahao Li (Toyota Technological Institute At Chicago) | Hao Tan (Adobe Systems) | Xin Sun (Adobe Systems) | Zhixin Shu (Adobe Systems) | Yi Zhou (Adobe Systems) | Sai Bi (Adobe Systems) | Soeren Pirk (Adobe) | ARIE KAUFMAN (Stony Brook University),2023-12-21 16:10:33+00:00,,,,,,
HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting,,,,Hongyu Zhou (Zhejiang University) | Jiahao Shao (Zhejiang University) | Lu Xu (Zhejiang University) | Dongfeng Bai (Huawei Technologies Ltd.) | Weichao Qiu (Huawei Technologies Ltd.) | Bingbing Liu (Huawei Technologies Ltd.) | Yue Wang (Zhejiang University) | Andreas Geiger (University Of T??bingen) | Yiyi Liao (Zhejiang University),,,,,,,
VRP-SAM: SAM with Visual Reference Prompt,"In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.",http://arxiv.org/abs/2402.17726v1,,Yanpeng Sun (Nanjing University Of Science And Technology) | Jiahui Chen (Beihang University) | Shan Zhang (Australian National University) | Xinyu Zhang (None) | Qiang Chen (Baidu) | Gang Zhang (Baidu) | Errui Ding (Baidu) | Jingdong Wang (Baidu) | Zechao Li (Nanjing University Of Science And Techonolgy),2024-02-27 17:58:09+00:00,,,,,,
Paint3D: Paint Anything 3D with Lighting-less Texture Diffusion Models,"This paper presents Paint3D, a novel coarse-to-fine generative framework that is capable of producing high-resolution, lighting-less, and diverse 2K UV texture maps for untextured 3D meshes conditioned on text or image inputs. The key challenge addressed is generating high-quality textures without embedded illumination information, which allows the textures to be re-lighted or re-edited within modern graphics pipelines. To achieve this, our method first leverages a pre-trained depth-aware 2D diffusion model to generate view-conditional images and perform multi-view texture fusion, producing an initial coarse texture map. However, as 2D models cannot fully represent 3D shapes and disable lighting effects, the coarse texture map exhibits incomplete areas and illumination artifacts. To resolve this, we train separate UV Inpainting and UVHD diffusion models specialized for the shape-aware refinement of incomplete areas and the removal of illumination artifacts. Through this coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that maintain semantic consistency while being lighting-less, significantly advancing the state-of-the-art in texturing 3D objects.",http://arxiv.org/abs/2312.13913v2,,"Xianfang Zeng (Tencent PCG) | Xin Chen (University Of Chinese Academy Of Sciences, ShanghaiTech University) | Zhongqi Qi (Tencent PCG) | Wen Liu (Tencent PCG) | Zibo Zhao (None) | Zhibin Wang (Tencent LightAI Lab) | Bin Fu (Tencent) | Yong Liu (Zhejiang University) | Gang Yu (Tencent)",2023-12-21 15:01:47+00:00,,,,,,
CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images,"We assemble a dataset of Creative-Commons-licensed (CC) images, which we use to train a set of open diffusion models that are qualitatively competitive with Stable Diffusion 2 (SD2). This task presents two challenges: (1) high-resolution CC images lack the captions necessary to train text-to-image generative models; (2) CC images are relatively scarce. In turn, to address these challenges, we use an intuitive transfer learning technique to produce a set of high-quality synthetic captions paired with curated CC images. We then develop a data- and compute-efficient training recipe that requires as little as 3% of the LAION-2B data needed to train existing SD2 models, but obtains comparable quality. These results indicate that we have a sufficient number of CC images (~70 million) for training high-quality models. Our training recipe also implements a variety of optimizations that achieve ~3X training speed-ups, enabling rapid model iteration. We leverage this recipe to train several high-quality text-to-image models, which we dub the CommonCanvas family. Our largest model achieves comparable performance to SD2 on a human evaluation, despite being trained on our CC dataset that is significantly smaller than LAION and using synthetic captions for training. We release our models, data, and code at https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md",http://arxiv.org/abs/2310.16825v1,,"Aaron Gokaslan (Cornell University) | A. Feder Cooper (Cornell University) | Jasmine Collins (University Of California Berkeley) | Landan Seguin (Databricks) | Austin Jacobson (Databricks) | Mihir Patel (Databricks MosaicML) | Jonathan Frankle (School Of Engineering And Applied Sciences, Harvard University) | Cory Stephenson (Databricks) | Volodymyr Kuleshov (Cornell University)",2023-10-25 17:56:07+00:00,,,,,,
DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance,,,,"Zixuan Wang (None) | Jia Jia (Department Of Computer Science And Technology, Tsinghua University) | Shikun Sun (Tsinghua University) | Haozhe Wu (Tsinghua University) | Rong Han (Tsinghua University) | Zhenyu Li (Tsinghua University) | Di Tang (ByteDance) | Jiaqing Zhou (Bytedance) | Jiebo Luo (University Of Rochester)",,,,,,,
"Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation","In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generalizability, we introduce a robust fine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing any real urban-scene datasets.Code is available at https://github.com/w1oves/Rein.git.",http://arxiv.org/abs/2312.04265v4,,ZHIXIANG WEI (University Of Science And Technology Of China) | Lin Chen (University Of Science And Technology Of China) | Xiaoxiao Ma (University Of Science And Technology Of China) | Huaian Chen (University Of Science And Technology Of China) | Tianle Liu (University Of Science And Technology Of China) | Pengyang Ling (University Of Science And Technology Of China) | Jinjin Zheng (University Of Science And Technology Of China) | Ben Wang (University Of Science And Technology Of China) | Yi Jin (University Of Science And Technology Of China),2023-12-07 12:43:00+00:00,,,,,,
DaReNeRF: Direction-aware Representation for Dynamic Scenes,"Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance.",http://arxiv.org/abs/2403.02265v1,,Ange Lou (Vanderbilt University) | Benjamin Planche (United Imaging Intelligence) | Zhongpai Gao (United Imaging Intelligence) | Yamin Li (Vanderbilt University) | Tianyu Luan (State University Of New York At Buffalo) | Hao Ding (Johns Hopkins University) | Terrence Chen (United Imaging Intelligence) | Jack Noble (Vanderbilt University) | Ziyan Wu (United Imaging Intelligence),2024-03-04 17:54:33+00:00,,,,,,
Learning from Synthetic Human Group Activities,"The study of complex human interactions and group activities has become a focal point in human-centric computer vision. However, progress in related tasks is often hindered by the challenges of obtaining large-scale labeled datasets from real-world scenarios. To address the limitation, we introduce M3Act, a synthetic data generator for multi-view multi-group multi-person human atomic actions and group activities. Powered by the Unity engine, M3Act features multiple semantic groups, highly diverse and photorealistic images, and a comprehensive set of annotations, which facilitates the learning of human-centered tasks across single-person, multi-person, and multi-group conditions. We demonstrate the advantages of M3Act across three core experiments using various input modalities. First, adding our synthetic data significantly improves the performance of MOTRv2 on DanceTrack, leading to a hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking results on par with MOTRv2*, which is trained with 62.5% more real-world data. Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on group activity and atomic action accuracy respectively. Moreover, M3Act opens new research for controllable 3D group activity generation. We define multiple metrics and propose a competitive baseline for the novel task.",http://arxiv.org/abs/2306.16772v4,,Chang (None) | Danrui Li (Rutgers University) | Deep Patel (NEC Laboratories America) | Parth Goel (Oracle) | Seonghyeon Moon (Roblox) | Samuel Sohn (Rutgers University) | Honglu Zhou (Rutgers University) | Sejong Yoon (The College Of New Jersey) | Vladimir Pavlovic (Rutgers University) | Mubbasir Kapadia (Rutgers University ),2023-06-29 08:13:57+00:00,,,,,,
RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception,"The value of roadside perception, which could extend the boundaries of autonomous driving and traffic management, has gradually become more prominent and acknowledged in recent years. However, existing roadside perception approaches only focus on the single-infrastructure sensor system, which cannot realize a comprehensive understanding of a traffic area because of the limited sensing range and blind spots. Orienting high-quality roadside perception, we need Roadside Cooperative Perception (RCooper) to achieve practical area-coverage roadside perception for restricted traffic areas. Rcooper has its own domain-specific challenges, but further exploration is hindered due to the lack of datasets. We hence release the first real-world, large-scale RCooper dataset to bloom the research on practical roadside cooperative perception, including detection and tracking. The manually annotated dataset comprises 50k images and 30k point clouds, including two representative traffic scenes (i.e., intersection and corridor). The constructed benchmarks prove the effectiveness of roadside cooperation perception and demonstrate the direction of further research. Codes and dataset can be accessed at: https://github.com/AIR-THU/DAIR-RCooper.",http://arxiv.org/abs/2403.10145v1,,"Ruiyang Hao (Institute For AI Industry Research, Tsinghua University) | Siqi Fan (Institute For AI Industry Research, Tsinghua University) | Yingru Dai (Tsinghua University) | Zhenlin Zhang (China Automotive Innovation Corporation) | Chenxi Li (CAIC) | YuntianWang (China Automotive Innovation Corporation) | Haibao Yu (University Of Hong Kong) | Wenxian Yang (Tsinghua University) | Jirui Yuan (Tsinghua University) | Zaiqing Nie (Tsinghua University)",2024-03-15 09:44:02+00:00,,,,,,
Region-Based Representations Revisited,,,,Michal Shlapentokh-Rothman (University Of Illinois At Urbana Champaign) | Ansel Blume (University Of Illinois Urbana Champaign) | Yao Xiao (University Of Illinois At Urbana-Champaign) | Yuqun Wu (Department Of Computer Science) | Sethuraman T V (Department Of Computer Science) | Heyi Tao (University Of Illinois At Urbana-Champaign) | Jae Yong Lee (University Of Illinois At Urbana-Champaign) | Wilfredo Torres-Calderon (Reconstruct) | Yu-Xiong Wang (None) | Derek Hoiem (University Of Illinois At Urbana-Champaign),,,,,,,
RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback,"Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V.",http://arxiv.org/abs/2312.00849v2,,Tianyu Yu (Tsinghua University) | Yuan Yao (Tsinghua University) | Haoye Zhang (Tsinghua University) | Taiwen He (Tsinghua University) | Yifeng Han (Zhejiang University) | Ganqu Cui (Tsinghua University) | Jinyi Hu (Tsinghua University) | Zhiyuan Liu (Tsinghua University) | Hai-Tao Zheng (Tsinghua University) | Maosong Sun (Tsinghua University),2023-12-01 11:36:08+00:00,,,,,,
Improving Unsupervised Hierarchical Representation with Reinforcement Learning,,,,Ruyi An (Nanyang Technological University) | Yewen Li (Nanyang Technological University) | Xu He (Huawei Technologies Ltd.) | Pengjie Gu (Nanyang Technological University) | Mengchen Zhao (South China University Of Technology) | Dong Li (Huawei Technologies Ltd.) | Jianye Hao (Tianjin University) | Bo An (Nanyang Technological University) | Chaojie Wang (Skywork AI) | Mingyuan Zhou (The University Of Texas At Austin),,,,,,,
CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation,,,,Townim Chowdhury (None) | Kewen Liao (Australian Catholic University) | Vu Minh Hieu Phan (University Of Adelaide) | Minh-Son To (Flinders University Of South Australia) | Yutong Xie (University Of Adelaide) | Kevin Hung (Royal Adelaide Hospital) | David Ross (University Of South Australia) | Anton Van Den Hengel (University Of Adelaide) | Johan Verjans (University Of Adelaide) | Zhibin Liao (University Of Adelaide),,,,,,,
ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image,"Recent progress in human shape learning, shows that neural implicit models are effective in generating 3D human surfaces from limited number of views, and even from a single RGB image. However, existing monocular approaches still struggle to recover fine geometric details such as face, hands or cloth wrinkles. They are also easily prone to depth ambiguities that result in distorted geometries along the camera optical axis. In this paper, we explore the benefits of incorporating depth observations in the reconstruction process by introducing ANIM, a novel method that reconstructs arbitrary 3D human shapes from single-view RGB-D images with an unprecedented level of accuracy. Our model learns geometric details from both multi-resolution pixel-aligned and voxel-aligned features to leverage depth information and enable spatial relationships, mitigating depth ambiguities. We further enhance the quality of the reconstructed shape by introducing a depth-supervision strategy, which improves the accuracy of the signed distance field estimation of points that lie on the reconstructed surface. Experiments demonstrate that ANIM outperforms state-of-the-art works that use RGB, surface normals, point cloud or RGB-D data as input. In addition, we introduce ANIM-Real, a new multi-modal dataset comprising high-quality scans paired with consumer-grade RGB-D camera, and our protocol to fine-tune ANIM, enabling high-quality reconstruction from real-world human capture.",http://arxiv.org/abs/2403.10357v1,,Marco Pesavento (University Of Surrey) | Yuanlu Xu (Meta Reality Labs Research) | Nikolaos Sarafianos (Meta Reality Labs) | Robert Maier (Meta) | Ziyan Wang (Carnegie Mellon University) | Chun-Han Yao (University Of California At Merced) | Marco Volino (University Of Surrey) | Edmond Boyer (INRIA Grenoble Rh??ne-Alpes) | Adrian Hilton (University Of Surrey) | Tony Tung (Meta),2024-03-15 14:45:38+00:00,,,,,,
Learning Equi-angular Representations for Online Continual Learning,,,,Minhyuk Seo (Yonsei University) | Hyunseo Koh (Gwangju Institute Of Science And Technology) | Wonje Jeung (Yonsei University) | Min Jae Lee (Yonsei University) | San Kim (Yonsei University) | Hankook Lee (Sungkyunkwan University) | Sungjun Cho (LG AI Research) | Sungik Choi (LG AI Research) | Hyunwoo Kim (Zhejiang Lab) | Jonghyun Choi (Seoul National University),,,,,,,
Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework,"Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease's textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a large language model and medical experts. Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively. Our code is released at \href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.",http://arxiv.org/abs/2403.07636v1,,Vu Minh Hieu Phan (University Of Adelaide) | Yutong Xie (University Of Adelaide) | Yuankai Qi (The University Of Adelaide) | Lingqiao Liu (None) | Liyang Liu (University Of Adelaide) | Bowen Zhang (The University Of Adelaide) | Zhibin Liao (University Of Adelaide) | Qi Wu (University Of Adelaide) | Minh-Son To (Flinders University Of South Australia) | Johan Verjans (University Of Adelaide),2024-03-12 13:18:22+00:00,,,,,,
Delving Deep into Diffusion Transformers for Image and Video Generation,"In this study, we explore Transformer-based diffusion models for image and video generation. Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models. We introduce GenTron, a family of Generative models employing Transformer-based diffusion, to address this gap. Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism. We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality. Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation. We believe this work will provide meaningful insights and serve as a valuable reference for future research.",http://arxiv.org/abs/2312.04557v1,,"Shoufa Chen (The University Of Hong Kong) | Mengmeng Xu (Meta AI) | Jiawei Ren (Nanyang Technological University) | Yuren Cong (Institute Of Information Processing, Leibniz University Hanover) | Sen He (Meta AI) | Yanping Xie (Meta) | Animesh Sinha (Meta AI) | Ping Luo (The University Of Hong Kong) | Tao Xiang (University Of Surrey) | Juan-Manuel P??rez-R??a (Meta AI)",2023-12-07 18:59:30+00:00,,,,,,
A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames,"Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema).",http://arxiv.org/abs/2312.07395v1,,Pinelopi Papalampidi (Google) | Skanda Koppula (Google Deepmind) | Shreya Pathak (Google) | Justin Chiu (Google) | Joseph Heyward (Google) | Viorica Patraucean (DeepMind) | Jiajun Shen (DeepMind) | Antoine Miech (DeepMind) | Andrew Zisserman (University Of Oxford) | Aida Nematzadeh (Google Deepmind),2023-12-12 16:10:19+00:00,,,,,,
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation,"Diffusion generative modeling has become a promising approach for learning robotic manipulation tasks from stochastic human demonstrations. In this paper, we present Diffusion-EDFs, a novel SE(3)-equivariant diffusion-based approach for visual robotic manipulation tasks. We show that our proposed method achieves remarkable data efficiency, requiring only 5 to 10 human demonstrations for effective end-to-end training in less than an hour. Furthermore, our benchmark experiments demonstrate that our approach has superior generalizability and robustness compared to state-of-the-art methods. Lastly, we validate our methods with real hardware experiments. Project Website: https://sites.google.com/view/diffusion-edfs/home",http://arxiv.org/abs/2309.02685v3,,"Hyunwoo Ryu (Yonsei University) | Jiwoo Kim (Yonsei University) | Hyunseok An (Yonsei University) | Junwoo Chang (Yonsei University) | Joohwan Seo (University Of California, Berkeley) | Taehan Kim (Samsung) | Yubin Kim (Massachusetts Institute Of Technology) | Chaewon Hwang (Ewha Women's University) | Jongeun Choi (Yonsei University) | Roberto Horowitz (University Of California, Berkeley)",2023-09-06 03:42:20+00:00,,,,,,
Controlling Encoder of Deep Video Compression for Machine,,,,Xingtong Ge (Beijing Institute Of Technology) | Jixiang Luo (None) | XINJIE ZHANG (The Hong Kong University Of Science And Technology) | Tongda Xu (Tsinghua University) | Guo Lu (Shanghai Jiao Tong University) | Dailan He (The Chinese University Of Hong Kong) | Jing Geng (Beijing Institute Of Technology) | Yan Wang (Tsinghua University) | Jun Zhang (The Hong Kong University Of Science And Technology) | Hongwei Qin (SenseTime Co.),,,,,,,
Practical Measurements of Translucent Materials with Inter-Pixel Translucency Prior,,,,"Zhenyu Chen (Nanjing University) | Jie Guo (Nanjing University) | Shuichang Lai (Nanjing University) | Ruoyu Fu (Nanjing University) | Mengxun Kong (None) | Chen Wang (Nanjing University) | Hongyu Sun (Guangdong Oppo Mobile Telecommunications Corp., Ltd) | Zhebin Zhang (OPPO) | Chen Li (Innopeak Technology) | Yanwen Guo (Nanjing University)",,,,,,,
InstructVideo: Instructing Video Diffusion Models with Human Feedback,"Diffusion models have emerged as the de facto paradigm for video generation. However, their reliance on web-scale data of varied quality often yields results that are visually unappealing and misaligned with the textual prompts. To tackle this problem, we propose InstructVideo to instruct text-to-video diffusion models with human feedback by reward fine-tuning. InstructVideo has two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by generating through the full DDIM sampling chain, we recast reward fine-tuning as editing. By leveraging the diffusion process to corrupt a sampled video, InstructVideo requires only partial inference of the DDIM sampling chain, reducing fine-tuning cost while improving fine-tuning efficiency. 2) To mitigate the absence of a dedicated video reward model for human preferences, we repurpose established image reward models, e.g., HPSv2. To this end, we propose Segmental Video Reward, a mechanism to provide reward signals based on segmental sparse sampling, and Temporally Attenuated Reward, a method that mitigates temporal modeling degradation during fine-tuning. Extensive experiments, both qualitative and quantitative, validate the practicality and efficacy of using image reward models in InstructVideo, significantly enhancing the visual quality of generated videos without compromising generalization capabilities. Code and models will be made publicly available.",http://arxiv.org/abs/2312.12490v1,,Hangjie Yuan (Nanyang Technological University) | Shiwei Zhang (Alibaba Group) | Xiang Wang (Huazhong University Of Science And Technology) | Yujie Wei (Fudan University) | Tao Feng (Tsinghua University) | Yining Pan (Singapore University Of Technology And Design) | Yingya Zhang (Alibaba Group) | Ziwei Liu (Nanyang Technological University) | Samuel Albanie (University Of Cambridge) | Dong Ni (Zhejiang University),2023-12-19 17:55:16+00:00,,,,,,
LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning,,,,"Siyuan Cheng (Purdue University) | Guanhong Tao (Purdue University) | Yingqi Liu (Microsoft) | Guangyu Shen (Purdue University) | Shengwei An (Purdue University) | Shiwei Feng (Purdue University, West Lafayette) | Xiangzhe Xu (Purdue University) | Kaiyuan Zhang (Computer Science, Purdue University) | Shiqing Ma (University Of Massachusetts At Amherst) | Xiangyu Zhang (, Purdue University)",,,,,,,
IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation,"Generative object compositing emerges as a promising new avenue for compositional image editing. However, the requirement of object identity preservation poses a significant challenge, limiting practical usage of most existing methods. In response, this paper introduces IMPRINT, a novel diffusion-based generative model trained with a two-stage learning framework that decouples learning of identity preservation from that of compositing. The first stage is targeted for context-agnostic, identity-preserving pretraining of the object encoder, enabling the encoder to learn an embedding that is both view-invariant and conducive to enhanced detail preservation. The subsequent stage leverages this representation to learn seamless harmonization of the object composited to the background. In addition, IMPRINT incorporates a shape-guidance mechanism offering user-directed control over the compositing process. Extensive experiments demonstrate that IMPRINT significantly outperforms existing methods and various baselines on identity preservation and composition quality.",http://arxiv.org/abs/2403.10701v1,,Yizhi Song (Purdue University) | Zhifei Zhang (Adobe Research) | Zhe Lin (Adobe Research) | Scott Cohen (Adobe Systems) | Brian Price (Adobe Research) | Jianming Zhang (Adobe Systems) | Soo Ye Kim (Adobe Systems) | He Zhang (Adobe Systems) | Wei Xiong (Adobe Systems) | Daniel Aliaga (Purdue University),2024-03-15 21:37:04+00:00,,,,,,
Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data,,,,Xinting Liao (Zhejiang Univerisity) | Weiming Liu (Zhejiang University) | Chaochao Chen (Zhejiang University) | Pengyang Zhou (Zhejiang University) | Fengyuan Yu (Zhejiang University) | Huabin Zhu (Zhejiang University) | Binhui Yao (University Of Canberra) | Tao Wang (Midea Group) | Xiaolin Zheng (Zhejiang University) | Yanchao Tan (Fuzhou University),,,,,,,
Diffusion Model Alignment Using Direct Preference Optimization,"Large language models (LLMs) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback (RLHF) methods to make them better aligned with users' preferences. In contrast to LLMs, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-DPO, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-DPO is adapted from the recently developed Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate DPO to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our fine-tuned base model significantly outperforms both base SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses AI feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.",http://arxiv.org/abs/2311.12908v1,,Bram Wallace (SalesForce.Com) | Meihua Dang (Stanford University) | Rafael Rafailov (Stanford University) | Linqi Zhou (Stanford University) | Aaron Lou (Stanford University) | Senthil Purushwalkam (None) | Stefano Ermon (Stanford University) | Caiming Xiong (Salesforce Research) | Shafiq Joty (SalesForce.Com) | Nikhil Naik (MIT),2023-11-21 15:24:05+00:00,,,,,,
Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields,"3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework leads to warp-level divergence. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/",http://arxiv.org/abs/2312.03203v1,,"Shijie Zhou (University Of California, Los Angeles) | Haoran Chang (University Of California, Los Angeles) | Sicheng Jiang (University Of California, Los Angeles) | Zhiwen Fan (University Of Texas, Austin) | Zehao Zhu (University Of Texas At Austin) | Dejia Xu (University Of Texas At Austin) | Pradyumna Chari (University Of California, Los Angeles) | Suya You (University Of Southern California) | Zhangyang Wang (University Of Texas At Austin) | Achuta Kadambi (UCLA)",2023-12-06 00:46:30+00:00,,,,,,
SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology,"Introducing interpretability and reasoning into Multiple Instance Learning (MIL) methods for Whole Slide Image (WSI) analysis is challenging, given the complexity of gigapixel slides. Traditionally, MIL interpretability is limited to identifying salient regions deemed pertinent for downstream tasks, offering little insight to the end-user (pathologist) regarding the rationale behind these selections. To address this, we propose Self-Interpretable MIL (SI-MIL), a method intrinsically designed for interpretability from the very outset. SI-MIL employs a deep MIL framework to guide an interpretable branch grounded on handcrafted pathological features, facilitating linear predictions. Beyond identifying salient regions, SI-MIL uniquely provides feature-level interpretations rooted in pathological insights for WSIs. Notably, SI-MIL, with its linear prediction constraints, challenges the prevalent myth of an inevitable trade-off between model interpretability and performance, demonstrating competitive results compared to state-of-the-art methods on WSI-level prediction tasks across three cancer types. In addition, we thoroughly benchmark the local- and global-interpretability of SI-MIL in terms of statistical analysis, a domain expert study, and desiderata of interpretability, namely, user-friendliness and faithfulness.",http://arxiv.org/abs/2312.15010v1,,"Saarthak Kapse (State University Of New York At Stony Brook) | Pushpak Pati (International Business Machines) | Srijan Das (University Of North Carolina At Charlotte) | Jingwei Zhang (None) | Chao Chen (State University Of New York, Stony Brook) | Maria Vakalopoulou (CentraleSupelec) | Joel Saltz (State University Of New York At Stony Brook) | Dimitris Samaras (Stony Brook University) | Rajarsi Gupta (Academic Medical Center At State University Of New York At Stony Brook) | Prateek Prasanna (State University Of New York, Stony Brook)",2023-12-22 18:59:05+00:00,,,,,,
Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model,,,,Xu He (Tsinghua University) | Qiaochu Huang (Tsinghua University) | Zhensong Zhang (Huawei Noah's Ark Lab) | Zhiwei Lin (Tsinghua Shenzhen International Graduate School) | Zhiyong Wu (Tsinghua University) | Sicheng Yang (None) | Minglei Li (Huawei Cloud Computing Technologies Ltd.) | Zhiyi Chen (Huawei Technologies Ltd.) | Songcen Xu (Huawei Noah's Ark Lab) | Xiaofei Wu (Huawei Technologies Ltd.),,,,,,,
Backpropagation-free Network for 3D Test-Time Adaptation,,,,YANSHUO WANG (CSIRO) | Ali Cheraghian (CSIRO) | Zeeshan Hayder (CSIRO) | JIE HONG (Australian National University) | Sameera Ramasinghe (Amazon) | Shafin Rahman (North South University) | David Ahmedt-Aristizabal (CSIRO) | Xuesong Li (Australian National University) | Lars Petersson (CSIRO) | Mehrtash Harandi (Monash University),,,,,,,
LTM: Lightweight Textured Mesh Reconstruction of Unbounded Scenes Using Neural Fields,,,,"Jaehoon Choi (University Of Maryland, College Park) | Rajvi Shah (Facebook) | Qinbo Li (Facebook) | Yipeng Wang (Meta Reality Labs) | Ayush Saraf (Meta Platforms,) | Changil Kim (Facebook) | Jia-Bin Huang (University Of Maryland, College Park) | Dinesh Manocha (University Of Maryland, College Park) | Suhib Alsisan (Meta) | Johannes Kopf (Facebook)",,,,,,,
Generative Multimodal Models are In-Context Learners,"The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.",http://arxiv.org/abs/2312.13286v1,,Quan Sun (BAAI) | Yufeng Cui (Beihang University) | Xiaosong Zhang (Beijing Academy Of Artificial Intelligence) | Fan Zhang (Beijing Academy Of Artificial Intelligence) | Qiying Yu (Tsinghua University) | Yueze Wang (Beijing Academy Of Artificial Intelligence) | Yongming Rao (Tsinghua University) | Jingjing Liu (Tsinghua University) | Tiejun Huang (Peking University) | Xinlong Wang (Beijing Academy Of Artificial Intelligence),2023-12-20 18:59:58+00:00,,,,,,
Hallucination Augmented Contrastive Learning for Multimodal Large Language Model,"Multi-modal large language models (MLLMs) have been shown to efficiently integrate natural language with visual information to handle multi-modal tasks. However, MLLMs still face a fundamental limitation of hallucinations, where they tend to generate erroneous or fabricated information. In this paper, we address hallucinations in MLLMs from a novel perspective of representation learning. We first analyzed the representation distribution of textual and visual tokens in MLLM, revealing two important findings: 1) there is a significant gap between textual and visual representations, indicating unsatisfactory cross-modal representation alignment; 2) representations of texts that contain and do not contain hallucinations are entangled, making it challenging to distinguish them. These two observations inspire us with a simple yet effective method to mitigate hallucinations. Specifically, we introduce contrastive learning into MLLMs and use text with hallucination as hard negative examples, naturally bringing representations of non-hallucinative text and visual samples closer while pushing way representations of non-hallucinating and hallucinative text. We evaluate our method quantitatively and qualitatively, showing its effectiveness in reducing hallucination occurrences and improving performance across multiple benchmarks. On the MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the baseline MiniGPT-4/LLaVA. Our code is available on https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.",http://arxiv.org/abs/2312.06968v4,,Chaoya Jiang (Peking University) | Haiyang Xu (Alibaba Group) | Mengfan Dong (Peking University) | Jiaxing Chen (Peking University) | Wei Ye (Peking University) | Ming Yan (Alibaba Group) | Qinghao Ye (Alibaba Group) | Ji Zhang (Alibaba Group) | Fei Huang (Alibaba Group) | Shikun Zhang (Peking University),2023-12-12 04:05:15+00:00,,,,,,
Visual Objectification in Films: Towards a New AI Task for Video Interpretation,"In film gender studies, the concept of 'male gaze' refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.",http://arxiv.org/abs/2401.13296v1,,Julie Tores (Universit?? C??te d'Azur) | Lucile Sassatelli (Universite Cote d'Azur) | Hui-Yin Wu (Inria At Universit?? C??te d'Azur) | Clement Bergman (INRIA) | L??a Andolfi (CELSA-Sorbonne) | Victor Ecrement (Sorbonne Universit??) | Frederic Precioso (Universite Cote d'Azur) | Thierry Devars (Universit?? Paris-Sorbonne (Paris IV)) | Magali GUARESI (CNRS) | Virginie Julliard (Universit?? Paris-Sorbonne (Paris IV)) | Sarah L??cossais (Sorbonne Paris Nord),2024-01-24 08:35:29+00:00,,,,,,
MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception,"Perception plays a crucial role in various robot applications. However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations. To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses. MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely unexplored lidar modality. Finally, we propose, for the first time to the best of our knowledge, continuous-time ground truth based on optimization-based registration of lidar-inertial data on large survey-grade prior maps, which are also publicly released, each several times the size of existing ones. We conduct a rigorous evaluation of numerous state-of-the-art algorithms on MCD, report their performance, and highlight the challenges awaiting solutions from the research community.",http://arxiv.org/abs/2403.11496v1,,"Thien-Minh Nguyen (Nanyang Technological University) | Shenghai Yuan (National Technological University) | Thien Nguyen (Nanyang Technological University) | Pengyu Yin (Nanyang Technological University) | Haozhi Cao (Nanyang Technological University) | Lihua Xie (Nanyang Technological University) | Maciej Wozniak (KTH Royal Institute Of Technology) | Patric Jensfelt (KTH Royal Institute Of Technology, Stockholm, Sweden) | Marko Thiel (Hamburg University Of Technology) | Justin Ziegenbein (Hamburg University Of Technology) | Noel Blunder (Institute For Technical Logistics - Hamburg University Of Technology)",2024-03-18 06:00:38+00:00,,,,,,
Improving Bird??s Eye View Semantic Segmentation by Task Decomposition,,,,"Tianhao Zhao (Wuhan University) | Yongcan Chen (Wuhan University) | Yu Wu (Wuhan University) | Tianyang Liu (Wuhan University) | Bo Du (Wuhan University) | Peilun Xiao (Didi Research) | Shi Qiu (None) | Hongda Yang (Beijing DiDi Infinity Technology And Development Co., Ltd.) | Guozhen Li (Didi Global) | Yi Yang (Didi Global) | Yutian Lin (Wuhan University)",,,,,,,
4SAVED - Four Seasons Autonomous Vehicle Environment Dataset,,,,Daniel Kent (Michigan State University) | Mohammed Alyaqoub (Michigan State University) | Xiaohu Lu (Michigan State University) | Sayed Khatounabadi (Michigan State University) | Kookjin Sung (Michigan State University) | Cole Scheller (Michigan State University) | Alexander Dalat (University Of Michigan - Ann Arbor) | Xinwei Guo (Michigan State University) | Asma Bin Thabit (Michigan State University) | Roberto Muntaner Whitley (Michigan State University) | Hayder Radha (Michigan State University),,,,,,,
AiOS: All-in-One-Stage 3D Wholebody Mesh Recovery,,,,Qingping SUN (City University Of Hong Kong) | Yanjun Wang (Shanghai Jiao Tong University) | Ailing Zeng (IDEA) | Wanqi Yin (SenseTime Research ) | Chen Wei (SenseTime International PTE. LTD.) | Wenjia Wang (University Of Hong Kong) | Haiy Mei (None) | Chi LEUNG (City University Of Hong Kong) | Ziwei Liu (Nanyang Technological University) | Lei Yang (The Chinese University Of Hong Kong) | Zhongang Cai (Nanyang Technological University),,,,,,,
PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks,,,,"Marina Neseem (Brown University) | Conor McCullough (Google) | Randy Hsin (Google) | Chas Leichner (Google) | Shan Li (Google) | In Suk Chong (Google) | Andrew Howard (Google) | Lukasz Lew (Research, Google) | Sherief Reda (Brown University) | Ville-Mikko Rautio (Google) | Daniele Moro (Google Research)",,,,,,,
TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion,"We present TextureDreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3D shapes across arbitrary categories. Texture creation is a pivotal challenge in vision and graphics. Industrial companies hire experienced artists to manually craft textures for 3D assets. Classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. In contrast, TextureDreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. Our core idea, personalized geometry-aware score distillation (PGSD), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with ControlNet. Our integration and several essential modifications substantially improve the texture quality. Experiments on real images spanning different categories show that TextureDreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.",http://arxiv.org/abs/2401.09416v1,,"Yu-Ying Yeh (University Of California, San Diego) | Jia-Bin Huang (University Of Maryland, College Park) | Changil Kim (Facebook) | Lei Xiao (None) | Thu Nguyen-Phuoc (Reality Labs Research, Meta) | Numair Khan (None) | Cheng Zhang (Facebook) | Manmohan Chandraker (UC San Diego) | Carl Marshall (Reality Labs Research) | Zhao Dong (Meta RL Research) | Zhengqin Li (Facebook)",2024-01-17 18:55:49+00:00,,,,,,
DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception,,,,"Yibo Wang (Tsinghua University) | Ruiyuan Gao (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Kai Chen (The Hong Kong University Of Science And Technology) | Kaiqiang Zhou (Huawei Technologies Ltd.) | Yingjie CAI (The Chinese University Of Hong Kong) | Lanqing Hong (Huawei Technologies Ltd.) | Zhenguo Li (Huawei) | Lihui Jiang (Huawei Technologies Ltd.) | Dit-Yan Yeung (Hong Kong University Of Science And Technology) | Qiang Xu (The Chinese University Of Hong Kong) | Kai Zhang (Shenzhen International Graduate School, Tsinghua University)",,,,,,,
EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion,"Generating multiview images from a single view facilitates the rapid generation of a 3D mesh conditioned on a single image. Recent methods that introduce 3D global representation into diffusion models have shown the potential to generate consistent multiviews, but they have reduced generation speed and face challenges in maintaining generalizability and quality. To address this issue, we propose EpiDiff, a localized interactive multiview diffusion model. At the core of the proposed approach is to insert a lightweight epipolar attention block into the frozen diffusion model, leveraging epipolar constraints to enable cross-view interaction among feature maps of neighboring views. The newly initialized 3D modeling module preserves the original feature distribution of the diffusion model, exhibiting compatibility with a variety of base diffusion models. Experiments show that EpiDiff generates 16 multiview images in just 12 seconds, and it surpasses previous methods in quality evaluation metrics, including PSNR, SSIM and LPIPS. Additionally, EpiDiff can generate a more diverse distribution of views, improving the reconstruction quality from generated multiviews. Please see our project page at https://huanngzh.github.io/EpiDiff/.",http://arxiv.org/abs/2312.06725v2,,Zehuan Huang (Beijing University Of Aeronautics And Astronautics) | Hao Wen (Beijing University Of Aeronautics And Astronautics) | Junting Dong (None) | Yaohui Wang (Shanghai AI Laboratory) | Yangguang Li (Shanghai AI Laboratory) | Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) | Yan-Pei Cao (Tencent ARC Lab) | Ding Liang (Tsinghua University) | Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) | Bo Dai (Shanghai AI Laboratory) | Lu Sheng (Beihang University),2023-12-11 05:20:52+00:00,,,,,,
ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding,"Recent advancements in multimodal pre-training methods have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing multimodal pre-training frameworks to gather multimodal data for 3D applications lack scalability and comprehensiveness, potentially constraining the full potential of multimodal learning. The main bottleneck lies in the language modality's scalability and comprehensiveness. To address this, we introduce ULIP-2, a tri-modal pre-training framework that leverages state-of-the-art large multimodal models to automatically generate holistic language counterparts for 3D objects. It does not require any 3D annotations, and is therefore scalable to large datasets. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. ULIP-2 achieves significant improvements on downstream zero-shot classification on ModelNet40 (74.0% in top-1 accuracy); on the real-world ScanObjectNN benchmark, it obtains 91.5% in overall accuracy with only 1.4 million parameters, signifying a breakthrough in scalable multimodal 3D representation learning without human 3D annotations. The code, along with the generated tri-modal datasets, can be found at https://github.com/salesforce/ULIP.",http://arxiv.org/abs/2305.08275v2,,Le Xue (None) | Ning Yu (Salesforce Research) | Shu Zhang (SalesForce.Com) | Artemis Panagopoulou (University Of Pennsylvania) | Junnan Li (None) | Roberto Mart??n-Mart??n (University Of Texas At Austin) | Jiajun Wu (Stanford University) | Caiming Xiong (Salesforce Research) | Ran Xu (SalesForce.Com) | Juan Carlos Niebles (Salesforce Research) | Silvio Savarese (Salesforce),2023-05-14 23:14:09+00:00,,,,,,
Zero-Shot Structure-Preserving Diffusion Model for High Dynamic Range Tone Mapping,,,,Ruoxi Zhu (Fudan University) | Shusong Xu (Alibaba Group) | Peiye Liu (Alibaba Group) | Sicheng Li (Alibaba Group) | Yanheng Lu (Alibaba Group) | Dimin Niu (Alibaba Group) | Zihao Liu (Alibaba Group) | Zihao Meng (Alibaba Group) | Li Zhiyong (Alibaba Group) | Xinhua Chen (Fudan University) | Yibo Fan (Fudan University),,,,,,,
SynSP: Synergy of Smoothness and Precision in Pose Sequences Refinement,,,,Tao Wang (Beijing University Of Posts And Telecommunications) | Lei Jin (Beijing University Of Posts And Telecommunications) | Zheng Wang (Wuhan University) | Jianshu Li (Ant Group) | Liang Li (None) | Fang Zhao (Tencent AI Lab) | Yu Cheng (National University Of Singapore) | Li Yuan (Peking University) | Li ZHOU (Wuhan University) | Junliang Xing (Tsinghua University) | Jian Zhao (None),,,,,,,
Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation,"Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label, they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition, the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal, we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult, we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically, to enhance the coordination of transition gestures w.r.t different emotional ones, we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last, we present a keyframe sampler to supply effective initial posture cues in long sequences, enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition/.",http://arxiv.org/abs/2311.17532v2,,Xingqun Qi (The Hong Kong University Of Science And Technology) | Jiahao Pan (Hong Kong University Of Science And Technology) | Peng Li (Tsinghua University) | Ruibin Yuan (Hong Kong University Of Science And Technology) | Xiaowei Chi (Hong Kong University Of Science And Technology) | Mengfei Li (Hong Kong University Of Science And Technology) | Wenhan Luo (SUN YAT-SEN UNIVERSITY) | Wei Xue (Hong Kong University Of Science And Technology) | Shanghang Zhang (Peking University) | Qifeng Liu (The Hong Kong University Of Science And Technology) | Yike Guo (Imperial College London),2023-11-29 11:10:40+00:00,,,,,,
Listening and Imagining: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio,"In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at https://github.com/modelscope/facechain.",http://arxiv.org/abs/2403.01901v1,,"Chao Xu (Zhejiang University) | Yang Liu (Alibaba Group) | Jiazheng Xing (Zhejiang University) | Weida Wang (Xingji Meizu Group) | Mingze Sun (None) | Jun Dan (Zhejiang University) | Tianxin Huang (Tencent Youtu Lab) | Siyuan Li (Westlake University, Zhejiang University) | Zhi-Qi Cheng (Carnegie Mellon University) | Ying Tai (Nanjing University) | Baigui Sun (Alibaba Group)",2024-03-04 09:59:48+00:00,,,,,,
MAPLM: A Real-World Large-Scale Vision-Language Dataset for Map and Traffic Scene Understanding,,,,Xu Cao (University Of Illinois Urbana-Champaign) | Tong Zhou (Tencent AI Lab) | Yunsheng Ma (Purdue University) | Wenqian Ye (University Of Virginia) | Can Cui (Purdue University) | Kun Tang (Tencent) | Zhipeng Cao (Tencent) | Kaizhao Liang (University Of Texas At Austin) | Ziran Wang (Purdue University) | James Rehg (None) | Chao Zheng (Tencent),,,,,,,
Instruct-Imagen: Image Generation with Multi-modal Instruction,"This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.   We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various image generation datasets reveals that instruct-imagen matches or surpasses prior task-specific models in-domain and demonstrates promising generalization to unseen and more complex tasks.",http://arxiv.org/abs/2401.01952v1,,Hexiang Hu (Google Deepmind) | Kelvin C.K. Chan (Google) | Yu-Chuan Su (Google) | Wenhu Chen (University Of Waterloo) | Yandong Li (Google Research) | Kihyuk Sohn (Google) | Yang Zhao (Google) | Xue Ben (Google) | William Cohen (Google DeepMind) | Ming-Wei Chang (Google) | Xuhui Jia (Google),2024-01-03 19:31:58+00:00,,,,,,
WonderJourney: Going from Anywhere to Everywhere,"We introduce WonderJourney, a modularized framework for perpetual 3D scene generation. Unlike prior work on view generation that focuses on a single type of scenes, we start at any user-provided location (by a text description or an image) and generate a journey through a long sequence of diverse yet coherently connected 3D scenes. We leverage an LLM to generate textual descriptions of the scenes in this journey, a text-driven point cloud generation pipeline to make a compelling and coherent sequence of 3D scenes, and a large VLM to verify the generated scenes. We show compelling, diverse visual results across various scene types and styles, forming imaginary ""wonderjourneys"". Project website: https://kovenyu.com/WonderJourney/",http://arxiv.org/abs/2312.03884v1,,"Hong-Xing Yu (Computer Science Department, Stanford University) | Haoyi Duan (Stanford University) | Junhwa Hur (Google) | Kyle Sargent (Computer Science Department, Stanford University) | Michael Rubinstein (Google) | William Freeman (MIT And Google) | Forrester Cole (Google) | Deqing Sun (Google) | Noah Snavely (Google / Cornell) | Jiajun Wu (Stanford University) | Charles Herrmann (Google)",2023-12-06 20:22:32+00:00,,,,,,
SSR-Encoder: Selective Subject Representation Encoder for Subject-Driven Generation,"Recent advancements in subject-driven image generation have led to zero-shot generation, yet precise selection and focus on crucial subject representations remain challenging. Addressing this, we introduce the SSR-Encoder, a novel architecture designed for selectively capturing any subject from single or multiple reference images. It responds to various query modalities including text and masks, without necessitating test-time fine-tuning. The SSR-Encoder combines a Token-to-Patch Aligner that aligns query inputs with image patches and a Detail-Preserving Subject Encoder for extracting and preserving fine features of the subjects, thereby generating subject embeddings. These embeddings, used in conjunction with original text embeddings, condition the generation process. Characterized by its model generalizability and efficiency, the SSR-Encoder adapts to a range of custom models and control modules. Enhanced by the Embedding Consistency Regularization Loss for improved training, our extensive experiments demonstrate its effectiveness in versatile and high-quality image generation, indicating its broad applicability. Project page: https://ssr-encoder.github.io",http://arxiv.org/abs/2312.16272v2,,"Yuxuan Zhang (Shanghai Jiao Tong University) | Yiren Song (Shanghai Jiao Tong University) | Jiaming Liu (Xiaohongshu) | Rui Wang (Beijing University Of Posts And Telecommunications) | Jinpeng Yu (None) | Hao Tang (ETH Zurich And CMU) | Huaxia Li (Department Of Computer Science And Engineering, The Chinese University Of Hong Kong) | Xu Tang (Shanghaitech University) | Yao Hu (Zhejiang University, Tsinghua University) | Han Pan (Shanghai Jiao Tong University) | Zhongliang Jing (Shanghai Jiao Tong University)",2023-12-26 14:39:11+00:00,,,,,,
Improving Graph Contrastive Learning via Adaptive Positive Sampling,,,,Jiaming Zhuo (Hebei University Of Technology) | Feiyang Qin (None) | Can Cui (Hebei University Of Technology) | Kun Fu (Hebei University Of Technology) | Bingxin Niu (Hebei University Of Techonology) | Mengzhu Wang (Hebei University Of Technology) | Yuanfang Guo (Beihang University) | Chuan Wang (Institute Of Information Engineering) | Zhen Wang (None) | Xiaochun Cao (SUN YAT-SEN UNIVERSITY) | Liang Yang (Hebei University Of Technology),,,,,,,
CogAgent: Visual Language Model as GUI Agent,"People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM .",http://arxiv.org/abs/2312.08914v2,,"Wenyi Hong (Tsinghua University) | Weihan Wang (Tsinghua University) | Qingsong Lv (Tsinghua University) | Jiazheng Xu (, Tsinghua University) | Wenmeng Yu (None) | Junhui Ji (Zhipu.AI) | Yan Wang (Zhipu AI) | Zihan Wang (Tsinghua University) | Yuxiao Dong (Tsinghua University) | Ming Ding (ZHIPU AI) | Jie Tang (Tsinghua University)",2023-12-14 13:20:57+00:00,,,,,,
"EVS-assisted joint Deblurring, Rolling-Shutter Correction and Video Frame Interpolation through Sensor Inverse Modeling",,,,Rui Jiang (OMNIVISION) | Fangwen Tu (OMNIVISION) | Yixuan Long (OMNIVISION) | Aabhaas Vaish (OMNIVISION) | Bowen Zhou (OMNIVISION) | Qinyi Wang (OmniVision) | Wei Zhang (OMNIVISION) | Yuntan Fang (OMNIVISION) | Luis Eduardo Garc??a Capel (OMNIVISION) | Bo Mu (OMNIVISION) | Tiejun Dai (OMNIVISION) | Andreas Suess (OMNIVISION),,,,,,,
FairVLMed Dataset: Harnessing Fairness in Vision-and-Language Learning via FairCLIP,,,,Yan Luo (Harvard Ophthalmology AI Lab) | MIN SHI (Harvard University) | Muhammad Osama Khan (New York University) | Muhammad Muneeb Afzal (New York University) | Hao Huang (New York University) | Shuaihang Yuan (New York University) | Yu Tian (None) | Luo Song (Mass Eye And Ear) | Ava Kouhana (Harvard Ophthalmology AI Lab) | Tobias Elze (Harvard University) | Yi Fang (New York University) | Mengyu Wang (Harvard University),,,,,,,
CycleINR: Cycle Implicit Neural Representation for Arbitrary-Scale Volumetric Super-Resolution of Medical Data,,,,Wei Fang (Alibaba Group) | Yuxing Tang (Alibaba Group) | Heng Guo (Alibaba Group) | Mingze Yuan (Peking University) | Tony C. W. MOK (Alibaba DAMO Academy) | Ke Yan (Alibaba DAMO Academy) | Jiawen Yao (Alibaba Group) | Xin Chen (Guangzhou First People's Hospital) | Zaiyi Liu (Guangdong General Hospital) | Le Lu (Alibaba Group) | Ling Zhang (Alibaba Group) | Minfeng Xu (Alibaba Group),,,,,,,
HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models,"We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.",http://arxiv.org/abs/2310.14566v4,,"Tianrui Guan (University Of Maryland, College Park) | Fuxiao Liu (University Of Maryland) | Xiyang Wu (University Of Maryland, College Park) | Ruiqi Xian (University Of Maryland, College Park) | Zongxia Li (University Of Maryland, College Park) | Xiaoyu Liu (University Of Maryland, College Park) | Xijun Wang (University Of Maryland, College Park) | Lichang Chen (Department Of Computer Science, University Of Maryland, College Park) | Furong Huang (Department Of Computer Science, University Of Maryland) | Yaser Yacoob (University Of Maryland, College Park) | Dinesh Manocha (University Of Maryland, College Park) | Tianyi Zhou (University Of Maryland, College Park)",2023-10-23 04:49:09+00:00,,,,,,
HIVE: Harnessing Human Feedback for Instructional Visual Editing,"Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.",http://arxiv.org/abs/2303.09618v1,,Shu Zhang (SalesForce.Com) | Xinyi Yang (Salesforce Research) | Yihao Feng (Salesforce Research) | Can Qin (Northeastern University) | Chia-Chih Chen (Salesforce) | Ning Yu (Salesforce Research) | Zeyuan Chen (SalesForce.Com) | Huan Wang (SalesForce.Com) | Silvio Savarese (Salesforce) | Stefano Ermon (Stanford University) | Caiming Xiong (Salesforce Research) | Ran Xu (SalesForce.Com),2023-03-16 19:47:41+00:00,,,,,,
Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions,"Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a new instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks can be viewed as special cases by designing different instructions. We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline method to facilitate research in this new setting. Experimental results show that the proposed multi-purpose ReID model, trained on our OmniReID benchmark without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17, CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+ real2 for our newly defined language-instructed ReID, +4.3% on LLCM for visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The datasets, the model, and code will be available at https://github.com/hwz-zju/Instruct-ReID.",http://arxiv.org/abs/2306.07520v4,,"Weizhen He (None) | Yiheng Deng (Zhejiang University) | SHIXIANG TANG (The Chinese University Of Hong Kong) | Qihao CHEN (Liaoning Technical University) | Qingsong Xie (OPPO) | Yizhou Wang (None) | Lei Bai (Shanghai AI Laboratory) | Feng Zhu (SenseTime Group LTD) | Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) | Wanli Ouyang (University Of Sydney) | Donglian Qi (Zhejiang University) | Yunfeng Yan (Zhejiang University)",2023-06-13 03:25:33+00:00,,,,,,
Distilling Vision-Language Models on Millions of Videos,"The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%.",http://arxiv.org/abs/2401.06129v1,,Yue Zhao (UT Austin) | Long Zhao (Google Research) | Xingyi Zhou (Google) | Jialin Wu (Google) | Chun-Te Chu (Google) | Hui Miao (Google) | Florian Schroff (Google) | Hartwig Adam (Google Research) | Ting Liu (Google Research) | Boqing Gong (Google) | Philipp Kr??henb??hl (University Of Texas At Austin) | Liangzhe Yuan (Google),2024-01-11 18:59:53+00:00,,,,,,
BioCLIP: A Vision Foundation Model for the Tree of Life,"Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TreeOfLife-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BioCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TreeOfLife-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability. Our code, models and data will be made available at https://github.com/Imageomics/bioclip.",http://arxiv.org/abs/2311.18803v2,,"Samuel Stevens (Ohio State University, Columbus) | Jiaman Wu (Ohio State University, Columbus) | Matthew Thompson (Ohio State University, Columbus) | Elizabeth Campolongo (The Ohio State University) | Chan Hee Song (The Ohio State University) | David Carlyn (Ohio State University) | Li Dong (Microsoft Research) | Wasila Dahdul (University Of California, Irvine) | Charles Stewart (Rensselaer Polytechnic Institute) | Tanya Berger-Wolf (None) | Wei-Lun Chao (Ohio State University) | Yu Su (Ohio State University)",2023-11-30 18:49:43+00:00,,,,,,
Visual In-Context Prompting,,,,"Feng Li (The Hong Kong University Of Science And Technology) | Qing Jiang (South China University Of Technology) | Hao Zhang (The Hong Kong University Of Science And Technology) | Shilong Liu (Tsinghua University) | Huaizhe Xu (Hong Kong University Of Science And Technology) | Xueyan Zou (None) | Tianhe Ren (The International Digital Economy Academy) | Hongyang Li (South China University Of Technology) | Lei Zhang (International Digital Economy Academy (IDEA)) | Chunyuan Li (Microsoft Research, Redmond) | Jianwei Yang (Microsoft Research) | Jianfeng Gao (Microsoft Research)",,,,,,,
MPOD123: One Image to 3D Content Generation Using Mask-enhanced Progressive Outline-to-Detail Optimization,,,,"Jimin Xu (Zhejiang University) | Tianbao Wang (Zhejiang University) | Tao Jin (Zhejiang University) | Shengyu Zhang (Zhejiang University) | Dongjie Fu (Zhejiang University) | Zhe Wang (Alibaba) | Jiangjing Lyu (None) | Chengfei Lv (Zhejiang University) | Chaoyue Niu (Shanghai Jiao Tong University) | Zhou Yu (Hangzhou Dianzi University) | Zhou Zhao (Zhejiang University, Tsinghua University) | Fei Wu (Zhejiang University)",,,,,,,
MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling,"Robust segmentation is critical for deriving quantitative measures from large-scale, multi-center, and longitudinal medical scans. Manually annotating medical scans, however, is expensive and labor-intensive and may not always be available in every domain. Unsupervised domain adaptation (UDA) is a well-studied technique that alleviates this label-scarcity problem by leveraging available labels from another domain. In this study, we introduce Masked Autoencoding and Pseudo-Labeling Segmentation (MAPSeg), a $\textbf{unified}$ UDA framework with great versatility and superior performance for heterogeneous and volumetric medical image segmentation. To the best of our knowledge, this is the first study that systematically reviews and develops a framework to tackle four different domain shifts in medical image segmentation. More importantly, MAPSeg is the first framework that can be applied to $\textbf{centralized}$, $\textbf{federated}$, and $\textbf{test-time}$ UDA while maintaining comparable performance. We compare MAPSeg with previous state-of-the-art methods on a private infant brain MRI dataset and a public cardiac CT-MRI dataset, and MAPSeg outperforms others by a large margin (10.5 Dice improvement on the private MRI dataset and 5.7 on the public CT-MRI dataset). MAPSeg poses great practical value and can be applied to real-world problems. Our code and pretrained model will be available later.",http://arxiv.org/abs/2303.09373v2,,"Xuzhe Zhang (Columbia University) | Yuhao Wu (Duke University) | Elsa Angelini (T??l??com ParisTech) | Ang Li (University Of Maryland, College Park) | Jia Guo (Columbia University) | Jerod Rasmussen (University Of California, Irvine) | Thomas O'Connor (University Of Rochester) | Pathik Wadhwa (University Of California, Irvine) | Andrea Jackowski (None) | Hai Li (Duke University) | Jonathan Posner (Duke University) | Andrew Laine (Columbia University) | Yun Wang (Emory University)",2023-03-16 15:01:50+00:00,,,,,,
WildlifeMapper: Aerial Image Analysis for Multi-Species Detection and Identification,,,,"Satish Kumar (None) | Bowen Zhang (University Of California, Santa Barbara) | Chandrakanth Gudavalli (University Of California, Santa Barbara) | Connor Levenson (University Of California, Santa Barbara) | Lacey Hughey (Smithsonian National Zoo And Conservation Biology Institute) | Jared Stabach (Smithsonian Conservation Biology Institute) | Irene Amoke (Kenya Wildlife Trust) | Gordon Ojwang (University Of Groningen) | Joseph Mukeka (Wildlife Reserach And Training Institute) | Howard Frederick (Tanzania Wildlife Research Institute) | Stephen Mwiu (Wildlife Research And Training Institute) | Joseph Ochieng Ogutu (Universit??t Hohenheim) | B S Manjunath (UC Santa Barbara)",,,,,,,
DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving,,,,Chen Min (Peking University) | Dawei Zhao (Defense Innovation Institute) | Liang Xiao (Defense Innovation Institute) | Jian Zhao (None) | Xinli Xu (Hong Kong University Of Science And Technology) | Zheng Zhu (Tsinghua University) | Lei Jin (Beijing University Of Posts And Telecommunications) | Jianshu Li (Ant Group) | Yulan Guo (SUN YAT-SEN UNIVERSITY) | Junliang Xing (Tsinghua University) | Liping Jing (Beijing Jiao Tong University) | Yiming Nie (National University Of Defense Technology) | Bin Dai (National University Of Defense Technology),,,,,,,
MovieChat: From Dense Token to Sparse Memory for Long Video Understanding,"Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing systems can only handle videos with very few frames. For long videos, the computation complexity, memory cost, and long-term temporal connection impose additional challenges. Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose the MovieChat to overcome these challenges. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video and 14K manual annotations for validation of the effectiveness of our method.",http://arxiv.org/abs/2307.16449v4,,"Enxin Song (None) | Wenhao Chai (University Of Washington) | Guanhong Wang (Zhejiang University) | Haoyang Zhou (Zhejiang University) | Feiyang Wu (Zhejiang University) | Yucheng Zhang (Zhejiang University) | Tian Ye (Hong Kong University Of Science And Technology, Guangzhou Campus) | Haozhe Chi (Zhejiang University) | Xun Guo (Microsoft Research Asia) | Yanting Zhang (Donghua University, Shanghai) | Yan Lu (Microsoft Research Asia) | Jenq-Neng Hwang (None) | Gaoang Wang (Zhejiang University)",2023-07-31 07:15:45+00:00,,,,,,
Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use,"From content moderation to wildlife conservation, the number of applications that require models to recognize nuanced or subjective visual concepts is growing. Traditionally, developing classifiers for such concepts requires substantial manual effort measured in hours, days, or even months to identify and annotate data needed for training. Even with recently proposed Agile Modeling techniques, which enable rapid bootstrapping of image classifiers, users are still required to spend 30 minutes or more of monotonous, repetitive data labeling just to train a single classifier. Drawing on Fiske's Cognitive Miser theory, we propose a new framework that alleviates manual effort by replacing human labeling with natural language interactions, reducing the total effort required to define a concept by an order of magnitude: from labeling 2,000 images to only 100 plus some natural language interactions. Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points. Most importantly, our framework eliminates the need for crowd-sourced annotations. Moreover, our framework ultimately produces lightweight classification models that are deployable in cost-sensitive scenarios. Across 15 subjective concepts and across 2 public image classification datasets, our trained models outperform traditional Agile Modeling as well as state-of-the-art zero-shot classification models like ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.",http://arxiv.org/abs/2403.02626v1,,"Imad Eddine Toubal (University Of Missouri) | Aditya Avinash (Google) | Neil Alldrin (Google) | Jan Dlabal (Research, Google) | Wenlei Zhou (Google) | Enming Luo (Google) | Otilia Stretcu (Google Research) | Hao Xiong (Google) | Chun-Ta Lu (Google Research) | Howard Zhou (Google Research) | Ranjay Krishna (University Of Washington) | Ariel Fuxman (Google) | Tom Duerig (Google)",2024-03-05 03:34:11+00:00,,,,,,
ControlRoom3D: Room Generation using Semantic Controls,"Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.",http://arxiv.org/abs/2312.05208v1,,Jonas Schult (Rheinisch Westf??lische Technische Hochschule Aachen) | Sam Tsai (Meta) | Lukas Hoellein (None) | Bichen Wu (Facebook) | Jialiang Wang (Facebook) | Chih-Yao Ma (Facebook) | Kunpeng Li (Meta) | Xiaofang Wang (Meta) | Felix Wimbauer (Technical University Of Munich) | Zijian He (None) | Peizhao Zhang (Facebook) | Bastian Leibe (RWTH Aachen University) | Peter Vajda (Facebook) | Ji Hou (Facebook),2023-12-08 17:55:44+00:00,,,,,,
MoST: Multi-modality Scene Tokenization for Motion Prediction,,,,Norman Mu (UC Berkeley) | Jingwei Ji (Waymo LLC) | Zhenpei Yang (Waymo LLC) | Nathan Harada (Waymo LLC) | Haotian Tang (Massachusetts Institute Of Technology) | Kan Chen (Waymo) | Charles R. Qi (Waymo) | Runzhou Ge (Waymo) | Kratarth Goel (Waymo) | Zoey Yang (Waymo) | Scott Ettinger (Waymo LLC) | Rami Al-Rfou (Waymo) | Dragomir Anguelov (Waymo) | Yin Zhou (Waymo),,,,,,,
PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation,"Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research. The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus). Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge. In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy.   In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney.",http://arxiv.org/abs/2402.19286v1,,Ruining Deng (Vanderbilt University) | Quan Liu (Vanderbilt University) | Can Cui (Vanderbilt University) | Tianyuan Yao (Vanderbilt University) | Jialin Yue (Vanderbilt University) | Juming Xiong (Vanderbilt University) | Lining Yu (Vanderbilt University) | Yifei Wu (Vanderbilt University) | Mengmeng Yin (Vanderbilt University) | Yu Wang (Vanderbilt University Medical Center) | Shilin Zhao (Vanderbilt University) | Yucheng Tang (NVIDIA) | Haichun Yang (Vanderbilt Unversity Medical School) | Yuankai Huo (Vanderbilt University),2024-02-29 15:51:14+00:00,,,,,,
Cache Me if You Can: Accelerating Diffusion Models through Block Caching,"Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).",http://arxiv.org/abs/2312.03209v2,,Felix Wimbauer (Technical University Of Munich) | Bichen Wu (Facebook) | Edgar Schoenfeld (None) | Xiaoliang Dai (Facebook) | Ji Hou (Facebook) | Zijian He (None) | Artsiom Sanakoyeu (RL) | Peizhao Zhang (Facebook) | Sam Tsai (Meta) | Jonas Kohler (Facebook) | Christian Rupprecht (University Of Oxford) | Daniel Cremers (Technical University Munich) | Peter Vajda (Facebook) | Jialiang Wang (Facebook),2023-12-06 00:51:38+00:00,,,,,,
HOLODECK: Language Guided Generation of 3D Embodied AI Environments,"3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as ""apartment for a researcher with a cat"" and ""office of a professor who is a fan of Star Wars"". Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents.",http://arxiv.org/abs/2312.09067v1,,"Yue Yang (University Of Pennsylvania) | Fan-Yun Sun (None) | Luca Weihs (Allen Institute For Artificial Intelligence) | Eli VanderBilt (University Of Idaho) | Alvaro Herrasti (Allen Institute For Artificial Intelligence) | Winson Han (Allen Institute For Artificial Intelligence) | Jiajun Wu (Stanford University) | Nick Haber (Stanford University) | Ranjay Krishna (University Of Washington) | Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) | Chris Callison-Burch (University Of Pennsylvania) | Mark Yatskar (Department Of Computer And Information Science, School Of Engineering And Applied Science) | Aniruddha Kembhavi (Allen Institute For Artificial Intelligence) | Christopher Clark (None)",2023-12-14 16:04:14+00:00,,,,,,
Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology,"Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how self-supervised deep learning approaches scale when training larger models on larger microscopy datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised baselines. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 93-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised baseline at inferring known biological relationships curated from public databases. Relevant code and select models released with this work can be found at: https://github.com/recursionpharma/maes_microscopy.",http://arxiv.org/abs/2309.16064v2,,Oren Kraus (Recursion) | Kian Kenyon-Dean (Recursion Pharma) | Saber Saberian (Recursion Pharma) | Maryam Fallah (Recursion Pharmaceuticals) | Peter McLean (Recursion) | Jess Leung (Recursion) | Vasudev Sharma (Recursion) | Ayla Khan (University Of Utah) | Jia Balakrishnan (Recursion Pharmaceuticals) | Safiye Celik (Recursion) | Dominique Beaini (Valence Labs) | Maciej Sypetkowski (Valence Labs) | Chi Cheng (Boston University) | Kristen Morse (Recursion) | Maureen Makes (University Of Utah) | Ben Mabey (None) | Berton Earnshaw (University Of Utah),2023-09-27 23:11:35+00:00,,,,,,
Rich Human Feedback for Text-to-Image Generation,"Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants).",http://arxiv.org/abs/2312.10240v1,,"Youwei Liang (University Of California, San Diego) | Junfeng He (Google) | Gang Li (Google) | Peizhao Li (GE HealthCare) | Arseniy Klimovskiy (Google) | Nicholas Carolan (Google) | Jiao Sun (University Of Southern California) | Jordi Pont-Tuset (Google Research) | Sarah Young (Google) | Feng Yang (Google Research) | Junjie Ke (None) | Krishnamurthy Dvijotham (Google DeepMind) | Katherine Collins (University Of Cambridge) | Yiwen Luo (Research, Google) | Yang Li (Google) | Kai Kohlhoff (Google Research) | Deepak Ramachandran (Google) | Vidhya Navalpakkam (Research, Google)",2023-12-15 22:18:38+00:00,,,,,,
"Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly",,,,Hang Du (Beijing University Of Posts And Telecommunications) | Sicheng Zhang (Beijing University Of Posts And Telecommunications) | Binzhu Xie (Beijing University Of Posts And Telecommunications) | Guoshun Nan (Beijing University Of Posts And Telecommunications) | Jiayang Zhang (Beijing University Of Posts And Telecommunications) | Junrui Xu (Beijing University Of Posts And Telecommunications) | Hangyu Liu (Beijing University Of Posts And Telecommunications) | Sicong Leng (Nanyang Technological University) | Jiangming Liu (Yunnan University) | Hehe Fan (None) | Dajiu Huang (South China University) | Jing Feng (Beijing University Of Posts And Telecommunications) | Linli Chen (Sichuan University) | Can Zhang (Beijing University Of Posts And Telecommunications) | Xuhuan Li (Beijing University Of Posts And Telecommunications) | Hao Zhang (None) | Jianhang Chen (Beijing University Of Posts And Telecommunications) | Qimei Cui (Beijing University Of Posts And Telecommunications) | Xiaofeng Tao (Beijing University Of Posts And Telecommunications),,,,,,,
DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision,"We have witnessed significant progress in deep learning-based 3D vision, ranging from neural radiance field (NeRF) based 3D representation learning to applications in novel view synthesis (NVS). However, existing scene-level datasets for deep learning-based 3D vision, limited to either synthetic environments or a narrow selection of real-world scenes, are quite insufficient. This insufficiency not only hinders a comprehensive benchmark of existing methods but also caps what could be explored in deep learning-based 3D analysis. To address this critical gap, we present DL3DV-10K, a large-scale scene dataset, featuring 51.2 million frames from 10,510 videos captured from 65 types of point-of-interest (POI) locations, covering both bounded and unbounded scenes, with different levels of reflection, transparency, and lighting. We conducted a comprehensive benchmark of recent NVS methods on DL3DV-10K, which revealed valuable insights for future research in NVS. In addition, we have obtained encouraging results in a pilot study to learn generalizable NeRF from DL3DV-10K, which manifests the necessity of a large-scale scene-level dataset to forge a path toward a foundation model for learning 3D representation. Our DL3DV-10K dataset, benchmark results, and models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.",http://arxiv.org/abs/2312.16256v2,,Lu Ling (Purdue University) | Yichen Sheng (Purdue University) | Zhi Tu (Purdue University) | Wentian Zhao (Adobe Systems) | Cheng Xin (Rutgers University) | Kun Wan (Adobe) | Lantao Yu (Adobe) | Qianyu Guo (None) | Zixun Yu (Purdue University) | Yawen Lu (Purdue University; Rochester Institute Of Tech) | Xuanmao Li (Huazhong University Of Science And Technology) | Xingpeng Sun (Purdue University) | Rohan Ashok (Purdue University) | Aniruddha Mukherjee (Purdue University) | Hao Kang (Wormpex AI Research) | Xiangrui Kong (Purdue University) | Gang Hua (Wormpex AI Research) | Tianyi Zhang (Purdue University) | Bedrich Benes (Purdue University) | Aniket Bera (Purdue University),2023-12-26 01:12:12+00:00,,,,,,
LiDAR-Net: A Real-scanned 3D Point Cloud Dataset for Indoor Scenes,,,,Yanwen Guo (Nanjing University) | Yuanqi Li (Nanjing University) | Dayong Ren (Nanjing University) | Xiaohong Zhang (None) | Jiawei Li (Nanjing University) | Liang Pu (None) | Changfeng Ma (Nanjing University) | Xiaoyu Zhan (None) | Jie Guo (Nanjing University) | Mingqiang Wei (Nanjing University Of Aeronautics And Astronautics) | Yan Zhang (None) | Piaopiao Yu (Nanjing University) | Shuangyu Yang (Nanjing University) | Donghao Ji (Nanjing University) | Huisheng Ye (Nanjing University) | Hao Sun (Nanjing University) | Yansong Liu (Nanjing University) | Yinuo Chen (Nanjing University) | Jiaqi Zhu (Nanjing University) | Hongyu Liu (Nanjing University),,,,,,,
SubT-MRS Datasets: Pushing SLAM Towards All-weather Environments,"Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We develop accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.",http://arxiv.org/abs/2307.07607v4,,"Shibo Zhao (Carnegie Mellon University) | Yuanjun Gao (None) | Tianhao Wu (University Of Virginia, Charlottesville) | Damanpreet Singh (CMU, Carnegie Mellon University) | Rushan Jiang (Oracle) | Haoxiang Sun (Carnegie Mellon University) | Mansi Sarawata (CMU, Carnegie Mellon University) | Warren Whittaker (Carnegie Mellon University) | Ian Higgins (Carnegie Mellon University) | Shaoshu Su (State University Of New York At Buffalo) | Yi Du (State University Of New York At Buffalo) | Can Xu (None) | John Keller (Carnegie Mellon University) | Jay Karhade (Carnegie Mellon University) | Lucas Nogueira (Carnegie Mellon University) | Sourojit Saha (CMU, Carnegie Mellon University) | Yuheng Qiu (CMU, Carnegie Mellon University) | Ji Zhang (Carnegie Mellon University) | Wenshan Wang (School Of Computer Science, Carnegie Mellon University) | Chen Wang (University At Buffalo) | Sebastian Scherer (None)",2023-07-14 20:05:14+00:00,,,,,,
MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,"We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",http://arxiv.org/abs/2311.16502v3,,"Xiang Yue (Ohio State University) | Yuansheng Ni (University Of Waterloo) | Kai Zhang (Ohio State University, Columbus) | Tianyu Zheng (Beijing University Of Posts And Telecommunications) | Ruoqi Liu (Ohio State University) | Ge Zhang (University Of Waterloo) | Samuel Stevens (Ohio State University, Columbus) | Dongfu Jiang (University Of Waterloo) | Weiming Ren (University Of Waterloo) | Yuxuan Sun (Westlake University) | Cong Wei (University Of Waterloo) | Botao Yu (The Ohio State University) | Ruibin Yuan (Hong Kong University Of Science And Technology) | Renliang Sun (International Digital Economy Academy) | Ming Yin (Princeton University) | Boyuan Zheng (Ohio State University, Columbus) | Zhenzhu Yang (China University Of Geoscience Beijing) | Yibo Liu (University Of Victoria) | Wenhao Huang (BAAI) | Huan Sun (Ohio State University, Columbus) | Yu Su (Ohio State University) | Wenhu Chen (University Of Waterloo)",2023-11-27 17:33:21+00:00,,,,,,
URHand: Universal Relightable Hands,"Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistically rendered under novel illuminations. To simplify the personalization process while retaining photorealism, we build a powerful universal relightable prior based on neural relighting from multi-view images of hands captured in a light stage with hundreds of identities. The key challenge is scaling the cross-identity training while maintaining personalized fidelity and sharp details without compromising generalization under natural illuminations. To this end, we propose a spatially varying linear lighting model as the neural renderer that takes physics-inspired shading as input feature. By removing non-linear activations and bias, our specifically designed lighting model explicitly keeps the linearity of light transport. This enables single-stage training from light-stage data while generalizing to real-time rendering under arbitrary continuous illuminations across diverse identities. In addition, we introduce the joint learning of a physically based model and our neural relighting model, which further improves fidelity and generalization. Extensive experiments show that our approach achieves superior performance over existing methods in terms of both quality and generalizability. We also demonstrate quick personalization of URHand from a short phone scan of an unseen identity.",http://arxiv.org/abs/2401.05334v1,,"Zhaoxi Chen (Nanyang Technological University) | Gyeongsik Moon (None) | Kaiwen Guo (Google) | Chen Cao (Facebook) | Stanislav Pidhorskyi (Meta) | Tomas Simon (Meta) | Rohan Joshi (Facebook) | Yuan Dong (Facebook) | Yichen Xu (Meta Platforms Inc) | Bernardo Pires (Meta Platforms) | He Wen (Meta Platformts,) | Lucas Evans (Meta) | Bo Peng (Meta Platforms) | Julia Buffalini (Meta) | Autumn Trimble (Meta) | Kevyn McPhail (Meta) | Melissa Schoeller (Meta Platforms Inc) | Shoou-I Yu (Reality Labs Research, Meta) | Javier Romero (None) | Michael Zollhoefer (Meta) | Yaser Sheikh (Meta) | Ziwei Liu (Nanyang Technological University) | Shunsuke Saito (Reality Labs Research)",2024-01-10 18:59:51+00:00,,,,,,
OpenEQA: Embodied Question Answering in the Era of Foundation Models,,,,Arjun Majumdar (Georgia Institute Of Technology) | Anurag Ajay (Massachusetts Institute Of Technology) | Xiaohan Zhang (State University Of New York At Binghamton) | Sriram Yenamandra (Georgia Institute Of Technology) | Mikael Henaff (Facebook) | Alexander Sax (University Of California Berkeley) | Sneha Silwal (AI At Meta) | Paul McVay (Meta) | Oleksandr Maksymets (Facebook) | Sergio Arnaud (None) | Pranav Putta (Georgia Institute Of Technology) | Karmesh Yadav (Meta AI) | Qiyang Li (University Of California Berkeley) | Benjamin Newman (Meta Platforms) | Mohit Sharma (Carnegie Mellon University) | Vincent-Pierre Berges (Meta) | Shiqi Zhang (State University Of New York At Binghamton) | Pulkit Agrawal (Massachusetts Institute Of Technology) | Dhruv Batra (FAIR (Meta) And Georgia Tech) | Yonatan Bisk (Carnegie Mellon University) | Mrinal Kalakrishnan (Meta) | Franziska Meier (Facebook) | Chris Paxton (Meta) | Aravind Rajeswaran (Facebook AI Research),,,,,,,
Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives,"We present Ego-Exo4D, a diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). More than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. The multimodal nature of the dataset is unprecedented: the video is accompanied by multichannel audio, eye gaze, 3D point clouds, camera poses, IMU, and multiple paired language descriptions -- including a novel ""expert commentary"" done by coaches and teachers and tailored to the skilled-activity domain. To push the frontier of first-person video understanding of skilled human activity, we also present a suite of benchmark tasks and their annotations, including fine-grained activity understanding, proficiency estimation, cross-view translation, and 3D hand/body pose. All resources will be open sourced to fuel new research in the community.",http://arxiv.org/abs/2311.18259v2,,"Kristen Grauman (University Of Texas At Austin) | Andrew Westbury (Facebook AI Research) | Lorenzo Torresani (Facebook) | Kris Kitani (Carnegie Mellon University) | Jitendra Malik (University Of California At Berkeley) | Triantafyllos Afouras (University Of Oxford) | Kumar Ashutosh (None) | Vijay Baiyya (University Of Louisiana At Lafayette) | Siddhant Bansal (University Of Bristol, UK) | Bikram Boote (University Of Illinois, Urbana Champaign) | Eugene Byrne (Meta) | Zachary Chavis (University Of Minnesota) | Joya Chen (National University Of Singapore) | Feng Cheng (University Of North Carolina At Chapel Hill) | Fu-Jen Chu (Facebook) | Sean Crane (School Of Computer Science, Carnegie Mellon University) | Avijit Dasgupta (IIIT Hyderabad) | Jing Dong (Meta) | Maria Escobar (Universidad De Los Andes) | Cristhian David Forigua Diaz (Universidad De Los Andes) | Abrham Gebreselasie (Carnegie Mellon University) | Sanjay Haresh (Qualcomm Inc, QualComm) | Jing Huang (Facebook) | Md Mohaiminul Islam (UNC Chapel Hill) | Suyog Jain (Meta) | Rawal Khirodkar (Meta) | Devansh Kukreja (Carnegie Mellon University) | Kevin Liang (FAIR At Meta) | Jia-Wei Liu (National University Of Singapore) | Sagnik Majumder (UT Austin & Meta AI) | Yongsen Mao (Simon Fraser University) | Miguel Martin (Meta Platforms,) | Effrosyni Mavroudi (None) | Tushar Nagarajan (Meta) | Francesco Ragusa (None) | Santhosh Kumar Ramakrishnan (University Of Texas, Austin) | Luigi Seminara (University Of Catania) | Arjun Somayazulu (University Of Texas At Austin) | Yale Song (Meta) | Shan Su (University Of Pennsylvania) | Zihui Xue (None) | Edward Zhang (University Of Pennsylvania) | Jinxu Zhang (University Of Pennsylvania) | Angela Castillo (Universidad De Los Andes) | Changan Chen (University Of Texas At Austin) | Fu Xinzhu (National University Of Singapore) | Ryosuke Furuta (The University Of Tokyo) | Cristina Gonz??lez (Universidad De Los Andes) | Gupta (None) | Jiabo Hu (Facebook) | Yifei Huang (The University Of Tokyo) | Yiming Huang (University Of Pennsylvania) | Weslie Khoo (Indiana University) | Anush Kumar (Torc Robotics) | Robert Kuo (Facebook) | Sach Lakhavani (None) | Miao Liu (META AI) | Mi Luo (The University Of Texas At Austin) | Zhengyi Luo (Carnegie Mellon University) | Brighid Meredith (Meta) | Austin Miller (Meta) | Oluwatumininu Oguntola (University Of North Carolina At Chapel Hill) | Xiaqing Pan (Meta) | Penny Peng (Meta) | Shraman Pramanick (None) | Merey Ramazanova (KAUST) | Fiona Ryan (Georgia Institute Of Technology) | Wei Shan (University Of North Carolina At Chapel Hill) | Kiran Somasundaram (None) | Chenan Song (National University Of Singaore, National University Of Singapore) | Audrey Southerland (Georgia Institute Of Technology) | Masatoshi Tateno (AIST, National Institute Of Advanced Industrial Science And Technology) | Huiyu Wang (Facebook) | Yuchen Wang (Indiana University) | Takuma Yagi (None) | Mingfei Yan (None) | Xitong Yang (Meta) | Zecheng Yu (University Of Tokyo) | Shengxin Zha (Meta GenAI) | Chen Zhao (King Abdullah University Of Science And Technology (KAUST)) | Ziwei Zhao (Indiana University) | Zhifan Zhu (University Of Bristol) | Jeff Zhuo (University Of North Carolina At Chapel Hill) | Pablo ARBELAEZ (Universidad De Los Andes) | Gedas Bertasius (UNC Chapel Hill) | Dima Damen (None) | Jakob Engel (Research, Meta Reality Labs) | Giovanni Maria Farinella (University Of Catania, Italy) | Antonino Furnari (University Of Catania) | Bernard Ghanem (KAUST) | Judy Hoffman (Georgia Institute Of Technology) | C.V. Jawahar (IIIT-Hyderabad) | Richard Newcombe (Meta, Reality Labs Research) | Hyun Soo Park (The University Of Minnesota) | James Rehg (None) | Yoichi Sato (University Of Tokyo) | Manolis Savva (Simon Fraser University) | Jianbo Shi (None) | Mike Zheng Shou (National University Of Singapore) | Michael Wray (University Of Bristol)",2023-11-30 05:21:07+00:00,,,,,,
